{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e28b4f9399e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install torch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import math\n",
    "\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# check the system we're on, gpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes on HyperParameters\n",
    "\"\"\"\n",
    "class HyperParameters():\n",
    "    def __init__(self):\n",
    "        self.data = '../Datasets/sketchrnn_chair.npz'\n",
    "        self.encoder_hidden_size = 64\n",
    "        self.decoder_hidden_size = 128\n",
    "        # latent vector\n",
    "        self.Nz = 32\n",
    "        \n",
    "        #return\n",
    "        self.M = 20\n",
    "        self.dropout = 0.9\n",
    "        self.batch_size = 100\n",
    "        self.eta_min = 0.01\n",
    "        self.R = 0.99995\n",
    "        self.KL_min = 0.2\n",
    "        self.wKL = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.lr_decay = 0.9999\n",
    "        self.min_lr = 0.00001\n",
    "        self.grad_clip = 1.\n",
    "        self.temperature = 0.4\n",
    "        self.max_seq_length = 200\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        \n",
    "    def load_npz(self):\n",
    "        data = np.load(file=self.file, encoding='latin1', allow_pickle=True)\n",
    "        return data\n",
    "\n",
    "dataLoader = DataLoader(hp.data)\n",
    "data = dataLoader.load_npz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes on Data Handler\n",
    "\"\"\"\n",
    "class DataHandler:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def padding(self, strokedata, max_len):\n",
    "        s = torch.from_numpy(strokedata)\n",
    "        s_len = s.shape[0]\n",
    "        result = torch.zeros((max_len, 5))\n",
    "        result[0:s_len, 0:2] = s[:, 0:2]\n",
    "        result[0:s_len, 3] = s[:, 2]\n",
    "        result[0:s_len, 2] = 1 - result[0:s_len, 3]    # 1 to 0, 0 to 1\n",
    "        if s_len < max_len:\n",
    "            result[(s_len - 1):, 4] = 1\n",
    "        return result\n",
    "        \n",
    "    def handle(self):\n",
    "        max_len = 0    # max length of a sketch - max number of stroke vectors in a sketch\n",
    "        for datatype in self.data:\n",
    "            for strokedata in self.data[datatype]:\n",
    "                if strokedata.shape[0] > max_len:\n",
    "                    max_len = strokedata.shape[0]\n",
    "        # lengths of data - the number of sketchs in data\n",
    "        train_len = self.data['train'].shape[0]\n",
    "        test_len = self.data['test'].shape[0]\n",
    "        valid_len = self.data['valid'].shape[0]\n",
    "        \n",
    "        data_train = torch.zeros((train_len, max_len, 5), dtype=float)\n",
    "        data_test = torch.zeros((test_len, max_len, 5), dtype=float)\n",
    "        data_valid = torch.zeros((valid_len, max_len, 5), dtype=float)\n",
    "        \n",
    "        i = j = k = 0\n",
    "        for datatype in self.data:\n",
    "            for strokedata in self.data[datatype]:\n",
    "                if datatype == 'train':\n",
    "                    data_train[i] = self.padding(strokedata, max_len)\n",
    "                    i += 1\n",
    "                if datatype == 'test':\n",
    "                    data_test[j] = self.padding(strokedata, max_len)\n",
    "                    j += 1\n",
    "                if datatype == 'valid':\n",
    "                    data_valid[k] = self.padding(strokedata, max_len)\n",
    "                    k += 1 \n",
    "\n",
    "        return data_train, data_test, data_valid, max_len\n",
    "    \n",
    "    def make_batch(self, data, batch_size):\n",
    "        stroke_lengths = []\n",
    "        data_len = data.shape[0] * data.shape[1]\n",
    "        reshaped_data = data.reshape(data_len, 5)\n",
    "        batch_num = math.ceil(data_len / batch_size)\n",
    "        batches = torch.zeros((batch_num, batch_size, 5), dtype=float)\n",
    "        i = j = 0\n",
    "        for k in range(0, batch_num - 1):\n",
    "            seq_len = len(reshaped_data[:,0])\n",
    "            batches[i] = reshaped_data[j:j+batch_size]\n",
    "            j += batch_size\n",
    "            stroke_lengths.append(seq_len)\n",
    "            i += 1\n",
    "        batches[i] = reshaped_data[j:]\n",
    "        if use_cuda:\n",
    "            batches = batches.cuda()\n",
    "        return batches, stroke_lengths\n",
    "    \n",
    "        \n",
    "datahandler = DataHandler(data)\n",
    "data_train, data_test, data_valid, max_length = datahandler.handle()\n",
    "Nmax = max_length\n",
    "# batches_test, stroke_lengths = datahandler.make_batch(data_test, hp.batch_size)\n",
    "# print(batches_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # bidirectional lstm:\n",
    "        self.lstm = nn.LSTM(5, hp.encoder_hidden_size, bidirectional=True)\n",
    "        # create mu and sigma from lstm's last output:\n",
    "        self.fc_mu = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "        self.fc_sigma = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "        # active dropout:\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            # then must init with zeros\n",
    "            if use_cuda:\n",
    "                hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "                cell = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "            else:\n",
    "                hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "                cell = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "            hidden_cell = (hidden, cell)\n",
    "        _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
    "        # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "        hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
    "        hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
    "        # mu and sigma:\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "        sigma_hat = self.fc_sigma(hidden_cat)\n",
    "        sigma = torch.exp(sigma_hat/2.)\n",
    "        # N ~ N(0,1)\n",
    "        z_size = mu.size()\n",
    "                                   \n",
    "        if use_cuda:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda()\n",
    "        else:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size))\n",
    "        z = mu + sigma*N\n",
    "        # mu and sigma_hat are needed for LKL loss\n",
    "        return z, mu, sigma_hat\n",
    "    \n",
    "# encoder = Encoder()\n",
    "# encoder.train()\n",
    "# z, _, _ = encoder(batches_test, hp.batch_size)\n",
    "# print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.rand((3,1,5))\n",
    "# b = torch.tensor([[1,2], \n",
    "#                   [3,4]])\n",
    "# c = torch.tensor([[5,6],\n",
    "#                   [7,8]])\n",
    "# d = torch.stack([b.unsqueeze(0),c.unsqueeze(0)], 0)\n",
    "# e = (b.unsqueeze(0).contiguous(),c.unsqueeze(0).contiguous())\n",
    "# print(d.is_contiguous())\n",
    "# print(d)\n",
    "# print(e)\n",
    "# # print(a.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes on Decoder\n",
    "\"\"\"\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hp.Nz + 5, hp.decoder_hidden_size)\n",
    "        self.fc_hc = nn.Linear(hp.Nz, 2*hp.decoder_hidden_size)\n",
    "        self.fc_output = nn.Linear(hp.decoder_hidden_size, 6 * hp.M + 3)\n",
    "        \n",
    "    def forward(self, inputs, z, hidden_cell=None):\n",
    "        if hidden_cell is None:       \n",
    "            hidden, cell = torch.split(F.tanh(self.fc_hc(z)), hp.decoder_hidden_size, 1)\n",
    "            hidden_cell = (hidden.unsqueeze(0), cell.unsqueeze(0))\n",
    "        outputs, (hidden, cell) = self.lstm(inputs, hidden_cell)\n",
    "        if self.training:\n",
    "            y = self.fc_output(outputs)\n",
    "        else:\n",
    "            y = self.fc_output(hidden)\n",
    "        q = y[:,-2:].softmax(-1)\n",
    "        rest = y[:,:-2]\n",
    "        params = rest.reshape(-1, hp.M, 6)\n",
    "        pi = params[:,:,0].softmax(-1)\n",
    "        mu_x = params[:,:,1]\n",
    "        mu_y = params[:,:,2]\n",
    "        sigma_x = params[:,:,3].exp()\n",
    "        sigma_y = params[:,:,4].exp()\n",
    "        rho = params[:,:,5].tanh()\n",
    "        return pi, mu_x, mu_y, sigma_x, sigma_y, rho, q, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notes on Model\n",
    "\"\"\"\n",
    "class Model():\n",
    "    def __init__(self):\n",
    "        if use_cuda:\n",
    "            self.encoder = Encoder().cuda()\n",
    "            self.decoder = Decoder().cuda()\n",
    "        else:\n",
    "            self.encoder = Encoder()\n",
    "            self.decoder = Decoder()\n",
    "            self.encoder_optimizer = optim.Adam(self.encoder.parameters(), hp.lr)\n",
    "            self.decoder_optimizer = optim.Adam(self.decoder.parameters(), hp.lr)\n",
    "            self.eta_step = hp.eta_min\n",
    "           \n",
    "    def make_target(self, batch, lengths):\n",
    "        if use_cuda:\n",
    "            eos = torch.stack([torch.Tensor([0,0,0,0,1])]*batch.size()[1]).cuda().unsqueeze(0)\n",
    "        else:\n",
    "            eos = torch.stack([torch.Tensor([0,0,0,0,1])]*batch.size()[1]).unsqueeze(0)\n",
    "        batch = torch.cat([batch, eos], 0)\n",
    "        mask = torch.zeros(Nmax+1, batch.size()[1])\n",
    "        \n",
    "        for indice,length in enumerate(lengths):\n",
    "            mask[:length,indice] = 1\n",
    "        if use_cuda:\n",
    "            mask = mask.cuda()\n",
    "            \n",
    "        dx = torch.stack([batch.data[:,:,0]]*hp.M,2) # reduce the dimensions\n",
    "        dy = torch.stack([batch.data[:,:,1]]*hp.M,2)\n",
    "        p1 = batch.data[:,:,2]\n",
    "        p2 = batch.data[:,:,3]\n",
    "        p3 = batch.data[:,:,4]\n",
    "        p = torch.stack([p1,p2,p3],2)\n",
    "        \n",
    "        return mask,dx,dy,p\n",
    "    \n",
    "    def train(self, epoch):\n",
    "        # set the dropoff parameters for lstm\n",
    "        self.encode().train\n",
    "        self.decode().train\n",
    "    \n",
    "    def bivariate_normal_pdf(self, dx, dy):\n",
    "        z_x = ((dx-self.mu_x)/self.sigma_x)**2\n",
    "        z_y = ((dy-self.mu_y)/self.sigma_y)**2\n",
    "        z_xy = (dx-self.mu_x)*(dy-self.mu_y)/(self.sigma_x*self.sigma_y)\n",
    "        z = z_x + z_y -2*self.rho_xy*z_xy\n",
    "        exp = torch.exp(-z/(2*(1-self.rho_xy**2)))\n",
    "        norm = 2*np.pi*self.sigma_x*self.sigma_y*torch.sqrt(1-self.rho_xy**2)\n",
    "        return exp/norm\n",
    "    \n",
    "    def reconstruction_loss(self, mask, dx, dy, p, epoch):\n",
    "        pdf = self.bivariate_normal_pdf(dx, dy)\n",
    "        LS = -torch.sum(mask*torch.log(1e-5+torch.sum(self.pi * pdf, 2)))\\\n",
    "        /float(Nmax*hp.batch_size)\n",
    "        LP = -torch.sum(p*torch.log(self.q))/float(Nmax*hp.batch_size)\n",
    "        return LS+LP\n",
    "\n",
    "    def kullback_leibler_loss(self):\n",
    "        LKL = -0.5*torch.sum(1+self.sigma-self.mu**2-torch.exp(self.sigma))\\\n",
    "        /float(hp.Nz*hp.batch_size)\n",
    "        if use_cuda:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min]).cuda()).detach()\n",
    "        else:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min])).detach()\n",
    "        return hp.wKL*self.eta_step * torch.max(LKL,KL_min)\n",
    "    \n",
    "    def save(self, epoch):\n",
    "        model_number = np.random.rand()\n",
    "        torch.save(self.encoder.state_dict(), './outputs/encoderRNN_model_number_%3f_epoch_%d.pth' % (model_number,epoch))\n",
    "        torch.save(self.decoder.state_dict(), './outputs/decoderRNN_model_number_%3f_epoch_%d.pth' % (model_number,epoch))\n",
    "\n",
    "    def load(self, encoder_name, decoder_name):\n",
    "        saved_encoder = torch.load(encoder_name)\n",
    "        saved_decoder = torch.load(decoder_name)\n",
    "        self.encoder.load_state_dict(saved_encoder)\n",
    "        self.decoder.load_state_dict(saved_decoder)\n",
    "    \n",
    "    def conditional_generation(self, epoch):\n",
    "        batch,lengths = make_batch(1)\n",
    "        # should remove dropouts:\n",
    "        self.encoder.train(False)\n",
    "        self.decoder.train(False)\n",
    "        # encode:\n",
    "        z, _, _ = self.encoder(batch, 1)\n",
    "        if use_cuda:\n",
    "            sos = Variable(torch.Tensor([0,0,1,0,0]).view(1,1,-1).cuda())\n",
    "        else:\n",
    "            sos = Variable(torch.Tensor([0,0,1,0,0]).view(1,1,-1))\n",
    "        s = sos\n",
    "        seq_x = []\n",
    "        seq_y = []\n",
    "        seq_z = []\n",
    "        hidden_cell = None\n",
    "        for i in range(Nmax):\n",
    "            input = torch.cat([s,z.unsqueeze(0)],2)\n",
    "            # decode:\n",
    "            self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
    "                self.rho_xy, self.q, hidden, cell = \\\n",
    "                    self.decoder(input, z, hidden_cell)\n",
    "            hidden_cell = (hidden, cell)\n",
    "            # sample from parameters:\n",
    "            s, dx, dy, pen_down, eos = self.sample_next_state()\n",
    "            #------\n",
    "            seq_x.append(dx)\n",
    "            seq_y.append(dy)\n",
    "            seq_z.append(pen_down)\n",
    "            if eos:\n",
    "                print(i)\n",
    "                break\n",
    "        # visualize result:\n",
    "        x_sample = np.cumsum(seq_x, 0)\n",
    "        y_sample = np.cumsum(seq_y, 0)\n",
    "        z_sample = np.array(seq_z)\n",
    "        sequence = np.stack([x_sample,y_sample,z_sample]).T\n",
    "        draw_sketch(sequence, epoch)\n",
    "    \n",
    "    #Used for conditional generation...\n",
    "    def tweak_temperature(self, pi_pdf):\n",
    "        # sigma x = exp(sigma_hat), sigma_hat = log(Sigma x)\n",
    "        pi_pdf = np.log(pi_pdf)/hp.temperature\n",
    "        pi_pdf -= pi_pdf.max()\n",
    "        pi_pdf = np.exp(pi_pdf)\n",
    "        pi_pdf /= pi_pdf.sum()\n",
    "        return pi_pdf\n",
    "\n",
    "    def sample_sketch_state(self):\n",
    "        # Get GMM index for mixture distribution\n",
    "        pi = self.pi.data[0,0,:].numpy()\n",
    "        pi = tweak_temperature(pi)\n",
    "        pi_idx = np.random.choice(hp.M, p=pi)\n",
    "\n",
    "        # Get the pen state, or sketch state\n",
    "        pi = self.q.data[0,0,:].numpy()\n",
    "        q = tweak_temperature(q)\n",
    "        q_idx = np.random.choice(3, p=q)\n",
    "\n",
    "        # Get mixture parameters\n",
    "        mu_x = self.mu_x.data[0,0,pi_idx]\n",
    "        mu_y = self.mu_y.data[0,0,pi_idx]\n",
    "        sigma_x = self.sigma_x.data[0,0,pi_idx]\n",
    "        sigma_y = self.sigma_y.data[0,0,pi_idx]\n",
    "        rho_xy = self.rho_xy.data[0,0,pi_idx]\n",
    "        x,y = sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy,greedy=False)\n",
    "        next_state = torch.zeros(5)\n",
    "        next_state[0] = x\n",
    "        next_state[1] = y\n",
    "        next_state[q_idx+2] = 1\n",
    "        return Variable(next_state).view(1,1,-1),x,y,q_idx==1,q_idx==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the parameters to form the normal distribution and sample from it\n",
    "def sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "    if greedy:\n",
    "        return mu_x, mu_y\n",
    "    mean = [mu_x,mu_y]\n",
    "    # https://mathworld.wolfram.com/BivariateNormalDistribution.html\n",
    "    # to read more about bivariate normal and it's parameters\n",
    "    sigma_x *= np.sqrt(hp.temperature)\n",
    "    sigma_y *= np.sqrt(hp.temperature)\n",
    "    covariance = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],[rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
    "    x = np.random.multivariate_normal(mean, covariance, 1)\n",
    "    return x[0][0], x[0][1]\n",
    "\n",
    "# Will sketch the image out using different colours for the strokes\n",
    "def draw_sketch(sequence, epoch, name='_output_'):\n",
    "    strokes = np.split(sequence, np.where(sequence[:,2]>0)[0]+1)\n",
    "    fig = plt.figure()\n",
    "    axis_1 = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    for stroke in strokes:\n",
    "        plt.plot(stoke[:,0],-stroke[:,1], color=numpy.random.rand(3,))\n",
    "    \n",
    "    canvas = plt.get_current_fig_manager().canvas\n",
    "    canvas.draw()\n",
    "    \n",
    "    pil_sketch = PIL.Image.frombytes('RGB', canvas.get_width_height(), canvas.tostring_rgb())\n",
    "    name = 'sketch_after_'+str(epoch)+'epochs'+name+'.jpg'\n",
    "    pil_image.save(name,\"JPEG\")\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNote on conditional generation:\\n\\nThis for after training, the paper says we can sample sketches for the model.\\nDuring the sampling process we generate parameters for both GMM and categorical\\ndistributions at each time step, and sample an outcome S'i for that time step.\\n\\nGMM ->\\nCategorical distributions -> calculated using outputs as logit values\\n\\nWe continue to sample until p3, so the end of sketch, is 1 or the number of iterations\\nhas reached Nmax (length of longest sketch in training dataset). \\nSampling the output is not deterministic, it's a random sequence,\\nconditioned on the input latent vector z. We can control the level of randomness we'd like\\nour samples to have during the sampling process by using a temperature parameter.\\n\\n- state_q_hat tends to state_q_hat / temperature\\n- prime_k tends to prime_k / temperature\\n- sigma_squared_x tends to sigma_squared_temperature_x\\n- sigma_squared_y tends to sigma_squared_temperature_y\\n\\n- the softmax parameters can be scaled, of the categorical distribution and also sigma\\nparameters of the bivariate normal distribution by a temperature parameter temperature.\\nTo control the level of randomness in our samples.\\n- temperature is usually between 0 and 1, so limit T -> 0, model becomes deterministic and\\nsamples will consist of the most likely point in the probability density function.\\n\\n- Unconditinal generation uses this temperature methodology to control randomness of the\\ndistribution\\n\\nUnconditional generation -> train model to generate sketches unconditionally where train\\ndecoder RNN module\\n-> Initial hidden states and cell states of decoder RNN are initialized to zero, inputs xi\\nof decoder RNN at each step is only Si-1 or Si-1', don't need to concatenate latent vectorZ\\n\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note on conditional generation:\n",
    "\n",
    "This for after training, the paper says we can sample sketches for the model.\n",
    "During the sampling process we generate parameters for both GMM and categorical\n",
    "distributions at each time step, and sample an outcome S'i for that time step.\n",
    "\n",
    "GMM ->\n",
    "Categorical distributions -> calculated using outputs as logit values\n",
    "\n",
    "We continue to sample until p3, so the end of sketch, is 1 or the number of iterations\n",
    "has reached Nmax (length of longest sketch in training dataset). \n",
    "Sampling the output is not deterministic, it's a random sequence,\n",
    "conditioned on the input latent vector z. We can control the level of randomness we'd like\n",
    "our samples to have during the sampling process by using a temperature parameter.\n",
    "\n",
    "- state_q_hat tends to state_q_hat / temperature\n",
    "- prime_k tends to prime_k / temperature\n",
    "- sigma_squared_x tends to sigma_squared_temperature_x\n",
    "- sigma_squared_y tends to sigma_squared_temperature_y\n",
    "\n",
    "- the softmax parameters can be scaled, of the categorical distribution and also sigma\n",
    "parameters of the bivariate normal distribution by a temperature parameter temperature.\n",
    "To control the level of randomness in our samples.\n",
    "- temperature is usually between 0 and 1, so limit T -> 0, model becomes deterministic and\n",
    "samples will consist of the most likely point in the probability density function.\n",
    "\n",
    "- Unconditinal generation uses this temperature methodology to control randomness of the\n",
    "distribution\n",
    "\n",
    "Unconditional generation -> train model to generate sketches unconditionally where train\n",
    "decoder RNN module\n",
    "-> Initial hidden states and cell states of decoder RNN are initialized to zero, inputs xi\n",
    "of decoder RNN at each step is only Si-1 or Si-1', don't need to concatenate latent vectorZ\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Note on training iteration:\n",
    "Epoch - indicates the number of passes entire training dataset\n",
    "the model completes. Dataset grouped into batches, if the batch size is the whole training dataset the no. of epochs is the no. of iterations.\n",
    "datasize d, epochs e, number of iterations i, batchsize b, d*e = i*b\n",
    "- data*epochs = number of iterations * batchsize\n",
    "\n",
    "def sample_sketch_state(self):\n",
    "    # Get GMM index for mixture distribution\n",
    "    pi = self.pi.data[0,0,:].numpy()\n",
    "    pi = tweak_temperature(pi)\n",
    "    pi_idx = np.random.choice(hp.M, p=pi)\n",
    "    \n",
    "    # Get the pen state, or sketch state\n",
    "    pi = self.q.data[0,0,:].numpy()\n",
    "    q = tweak_temperature(q)\n",
    "    q_idx = np.random.choice(3, p=q)\n",
    "    \n",
    "    # Get mixture parameters\n",
    "    mu_x = self.mu_x.data[0,0,pi_idx]\n",
    "    mu_y = self.mu_y.data[0,0,pi_idx]\n",
    "    sigma_x = self.sigma_x.data[0,0,pi_idx]\n",
    "    sigma_y = self.sigma_y.data[0,0,pi_idx]\n",
    "    rho_xy = self.rho_xy.data[0,0,pi_idx]\n",
    "    x,y = sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy,greedy=False)\n",
    "    next_state = torch.zeros(5)\n",
    "    next_state[0] = x\n",
    "    next_state[1] = y\n",
    "    next_state[q_idx+2] = 1\n",
    "    return Variable(next_state).view(1,1,-1),x,y,q_idx==1,q_idx==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        if use_cuda:\n",
    "            self.encoder = EncoderRNN().cuda()\n",
    "            self.decoder = DecoderRNN().cuda()\n",
    "        else:\n",
    "            self.encoder = EncoderRNN()\n",
    "            self.decoder = DecoderRNN()\n",
    "        #self.encoder_optimizer = optim.Adam(self.encoder.parameters(), hp.lr)\n",
    "        #self.decoder_optimizer = optim.Adam(self.decoder.parameters(), hp.lr)\n",
    "        #self.eta_step = hp.eta_min\n",
    "    \n",
    "# take the parameters to form the normal distribution and sample from it\n",
    "    def sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "        if greedy:\n",
    "            return mu_x, mu_y\n",
    "        mean = [mu_x,mu_y]\n",
    "        # https://mathworld.wolfram.com/BivariateNormalDistribution.html\n",
    "    # to read more about bivariate normal and it's parameters\n",
    "        sigma_x *= np.sqrt(hp.temperature)\n",
    "        sigma_y *= np.sqrt(hp.temperature)\n",
    "        covariance = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],[rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
    "        x = np.random.multivariate_normal(mean, covariance, 1)\n",
    "        return x[0][0], x[0][1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will sketch the image out using different colours for the strokes\n",
    "def draw_sketch(sequence, epoch, name='_output_'):\n",
    "    strokes = np.split(sequence, np.where(sequence[:,2]>0)[0]+1)\n",
    "    fig = plt.figure()\n",
    "    axis_1 = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    for stroke in strokes:\n",
    "        plt.plot(stoke[:,0],-stroke[:,1], color=numpy.random.rand(3,))\n",
    "    \n",
    "    canvas = plt.get_current_fig_manager().canvas\n",
    "    canvas.draw()\n",
    "    \n",
    "    pil_sketch = PIL.Image.frombytes('RGB', canvas.get_width_height(), canvas.tostring_rgb())\n",
    "    name = 'sketch_after_'+str(epoch)+'epochs'+name+'.jpg'\n",
    "    pil_image.save(name,\"JPEG\")\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def reconstruction_loss(self, mask, dx, dy, p, epoch):\n",
    "        # compute reconstruction loss (difference between the origin image and the reconstructed image):\n",
    "        pdf = self.bivariate_normal_pdf(dx, dy)\n",
    "        # loss for x and y\n",
    "        LS = -torch.sum(mask*torch.log(1e-5+torch.sum(self.pi * pdf, 2)))\\\n",
    "            /float(Nmax*hp.batch_size)\n",
    "        # loss for pen states\n",
    "        LP = -torch.sum(p*torch.log(self.q))/float(Nmax*hp.batch_size)\n",
    "        return LS+LP\n",
    "\n",
    "    def kullback_leibler_loss(self):\n",
    "        # compute kullbackleibler loss (difference between the distribution of latenet vector and N(0,1)):\n",
    "        LKL = -0.5*torch.sum(1+self.sigma-self.mu**2-torch.exp(self.sigma))\\\n",
    "            /float(hp.Nz*hp.batch_size)\n",
    "        if use_cuda:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min]).cuda()).detach()\n",
    "        else:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min])).detach()\n",
    "        return hp.wKL*self.eta_step * torch.max(LKL,KL_min)\n",
    "    \n",
    "    def train_temp(self, epoch):\n",
    "        # compute losses:\n",
    "        LKL = self.kullback_leibler_loss()\n",
    "        LR = self.reconstruction_loss(mask,dx,dy,p,epoch)\n",
    "        loss = LR + LKL\n",
    "        # gradient step\n",
    "        loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
