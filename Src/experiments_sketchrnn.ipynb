{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fyp-Z_Z2FZpN",
    "outputId": "1858b590-53bb-43c4-d693-c242728106e0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.16.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H5-gnddAFkZb"
   },
   "outputs": [],
   "source": [
    "###################################### hyperparameters\n",
    "class HParams():\n",
    "    def __init__(self):\n",
    "        self.data_location = '../Datasets/cat.npz'\n",
    "        self.enc_hidden_size = 256\n",
    "        self.dec_hidden_size = 512\n",
    "        self.Nz = 128\n",
    "        self.M = 20\n",
    "        self.dropout = 0.9\n",
    "        self.batch_size = 100\n",
    "        self.eta_min = 0.01\n",
    "        self.R = 0.99995\n",
    "        self.KL_min = 0.2\n",
    "        self.wKL = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.lr_decay = 0.9999\n",
    "        self.min_lr = 0.00001\n",
    "        self.grad_clip = 1.\n",
    "        self.temperature = 0.4\n",
    "        self.max_seq_length = 200\n",
    "\n",
    "hp = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N0UaoqWEFoiM"
   },
   "outputs": [],
   "source": [
    "################################# load and prepare data\n",
    "def max_size(data):\n",
    "    \"\"\"larger sequence length in the data set\"\"\"\n",
    "    sizes = [len(seq) for seq in data]\n",
    "    return max(sizes)\n",
    "\n",
    "def purify(strokes):\n",
    "    \"\"\"removes to small or too long sequences + removes large gaps\"\"\"\n",
    "    data = []\n",
    "    for seq in strokes:\n",
    "        if seq.shape[0] <= hp.max_seq_length and seq.shape[0] > 10:\n",
    "            seq = np.minimum(seq, 1000)\n",
    "            seq = np.maximum(seq, -1000)\n",
    "            seq = np.array(seq, dtype=np.float32)\n",
    "            data.append(seq)\n",
    "    return data\n",
    "\n",
    "def calculate_normalizing_scale_factor(strokes):\n",
    "    \"\"\"Calculate the normalizing factor explained in appendix of sketch-rnn.\"\"\"\n",
    "    data = []\n",
    "    for i in range(len(strokes)):\n",
    "        for j in range(len(strokes[i])):\n",
    "            data.append(strokes[i][j, 0])\n",
    "            data.append(strokes[i][j, 1])\n",
    "    data = np.array(data)\n",
    "    return np.std(data)\n",
    "\n",
    "def normalize(strokes):\n",
    "    \"\"\"Normalize entire dataset (delta_x, delta_y) by the scaling factor.\"\"\"\n",
    "    data = []\n",
    "    scale_factor = calculate_normalizing_scale_factor(strokes)\n",
    "    for seq in strokes:\n",
    "        seq[:, 0:2] /= scale_factor\n",
    "        data.append(seq)\n",
    "    return data\n",
    "\n",
    "dataset = np.load(hp.data_location, encoding='latin1')\n",
    "data = dataset['train']\n",
    "data = purify(data)\n",
    "data = normalize(data)\n",
    "Nmax = max_size(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "CXVFBDJyFq47"
   },
   "outputs": [],
   "source": [
    "############################## function to generate a batch:\n",
    "def make_batch(batch_size):\n",
    "    batch_idx = np.random.choice(len(data),batch_size)\n",
    "    batch_sequences = [data[idx] for idx in batch_idx]\n",
    "    strokes = []\n",
    "    lengths = []\n",
    "    indice = 0\n",
    "    for seq in batch_sequences:\n",
    "        len_seq = len(seq[:,0])\n",
    "        new_seq = np.zeros((Nmax,5))\n",
    "        new_seq[:len_seq,:2] = seq[:,:2]\n",
    "        new_seq[:len_seq-1,2] = 1-seq[:-1,2]\n",
    "        new_seq[:len_seq,3] = seq[:,2]\n",
    "        new_seq[(len_seq-1):,4] = 1\n",
    "        new_seq[len_seq-1,2:4] = 0\n",
    "        lengths.append(len(seq[:,0]))\n",
    "        strokes.append(new_seq)\n",
    "        indice += 1\n",
    "\n",
    "    if use_cuda:\n",
    "        batch = Variable(torch.from_numpy(np.stack(strokes,1)).cuda().float())\n",
    "    else:\n",
    "        batch = Variable(torch.from_numpy(np.stack(strokes,1)).float())\n",
    "    return batch, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "__aLIHMzFs3n"
   },
   "outputs": [],
   "source": [
    "################################ adaptive lr\n",
    "def lr_decay(optimizer):\n",
    "    \"\"\"Decay learning rate by a factor of lr_decay\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr']>hp.min_lr:\n",
    "            param_group['lr'] *= hp.lr_decay\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "o03OoUu8Fw3I"
   },
   "outputs": [],
   "source": [
    "################################# encoder and decoder modules\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # bidirectional lstm:\n",
    "        self.lstm = nn.LSTM(5, hp.enc_hidden_size, \\\n",
    "            dropout=hp.dropout, bidirectional=True)\n",
    "        # create mu and sigma from lstm's last output:\n",
    "        self.fc_mu = nn.Linear(2*hp.enc_hidden_size, hp.Nz)\n",
    "        self.fc_sigma = nn.Linear(2*hp.enc_hidden_size, hp.Nz)\n",
    "        # active dropout:\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            # then must init with zeros\n",
    "            if use_cuda:\n",
    "                hidden = torch.zeros(2, batch_size, hp.enc_hidden_size).cuda()\n",
    "                cell = torch.zeros(2, batch_size, hp.enc_hidden_size).cuda()\n",
    "            else:\n",
    "                hidden = torch.zeros(2, batch_size, hp.enc_hidden_size)\n",
    "                cell = torch.zeros(2, batch_size, hp.enc_hidden_size)\n",
    "            hidden_cell = (hidden, cell)\n",
    "        _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
    "        # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "        hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
    "        hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
    "        # mu and sigma:\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "        sigma_hat = self.fc_sigma(hidden_cat)\n",
    "        sigma = torch.exp(sigma_hat/2.)\n",
    "        # N ~ N(0,1)\n",
    "        z_size = mu.size()\n",
    "        if use_cuda:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda()\n",
    "        else:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size))\n",
    "        z = mu + sigma*N\n",
    "        # mu and sigma_hat are needed for LKL loss\n",
    "        return z, mu, sigma_hat\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # to init hidden and cell from z:\n",
    "        self.fc_hc = nn.Linear(hp.Nz, 2*hp.dec_hidden_size)\n",
    "        # unidirectional lstm:\n",
    "        self.lstm = nn.LSTM(hp.Nz+5, hp.dec_hidden_size, dropout=hp.dropout)\n",
    "        # create proba distribution parameters from hiddens:\n",
    "        self.fc_params = nn.Linear(hp.dec_hidden_size,6*hp.M+3)\n",
    "\n",
    "    def forward(self, inputs, z, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            # then we must init from z\n",
    "            hidden,cell = torch.split(F.tanh(self.fc_hc(z)),hp.dec_hidden_size,1)\n",
    "            hidden_cell = (hidden.unsqueeze(0).contiguous(), cell.unsqueeze(0).contiguous())\n",
    "        outputs,(hidden,cell) = self.lstm(inputs, hidden_cell)\n",
    "        # in training we feed the lstm with the whole input in one shot\n",
    "        # and use all outputs contained in 'outputs', while in generate\n",
    "        # mode we just feed with the last generated sample:\n",
    "        if self.training:\n",
    "            y = self.fc_params(outputs.view(-1, hp.dec_hidden_size))\n",
    "        else:\n",
    "            y = self.fc_params(hidden.view(-1, hp.dec_hidden_size))\n",
    "        # separate pen and mixture params:\n",
    "        params = torch.split(y,6,1)\n",
    "        params_mixture = torch.stack(params[:-1]) # trajectory\n",
    "        params_pen = params[-1] # pen up/down\n",
    "        # identify mixture params:\n",
    "        pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy = torch.split(params_mixture,1,2)\n",
    "        # preprocess params::\n",
    "        if self.training:\n",
    "            len_out = Nmax+1\n",
    "        else:\n",
    "            len_out = 1\n",
    "                                   \n",
    "        pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        sigma_x = torch.exp(sigma_x.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        sigma_y = torch.exp(sigma_y.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        rho_xy = torch.tanh(rho_xy.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        mu_x = mu_x.transpose(0,1).squeeze().contiguous().view(len_out,-1,hp.M)\n",
    "        mu_y = mu_y.transpose(0,1).squeeze().contiguous().view(len_out,-1,hp.M)\n",
    "        q = F.softmax(params_pen).view(len_out,-1,3)\n",
    "        return pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy,q,hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Rt63scH4Fxlu"
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        if use_cuda:\n",
    "            self.encoder = EncoderRNN().cuda()\n",
    "            self.decoder = DecoderRNN().cuda()\n",
    "        else:\n",
    "            self.encoder = EncoderRNN()\n",
    "            self.decoder = DecoderRNN()\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), hp.lr)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), hp.lr)\n",
    "        self.eta_step = hp.eta_min\n",
    "        self.model_count = 0\n",
    "\n",
    "    def make_target(self, batch, lengths):\n",
    "        if use_cuda:\n",
    "            eos = torch.stack([torch.Tensor([0,0,0,0,1])]*batch.size()[1]).cuda().unsqueeze(0)\n",
    "        else:\n",
    "            eos = torch.stack([torch.Tensor([0,0,0,0,1])]*batch.size()[1]).unsqueeze(0)\n",
    "        batch = torch.cat([batch, eos], 0)\n",
    "        mask = torch.zeros(Nmax+1, batch.size()[1])\n",
    "        for indice,length in enumerate(lengths):\n",
    "            mask[:length,indice] = 1\n",
    "        if use_cuda:\n",
    "            mask = mask.cuda()\n",
    "        dx = torch.stack([batch.data[:,:,0]]*hp.M,2)\n",
    "        dy = torch.stack([batch.data[:,:,1]]*hp.M,2)\n",
    "        p1 = batch.data[:,:,2]\n",
    "        p2 = batch.data[:,:,3]\n",
    "        p3 = batch.data[:,:,4]\n",
    "        p = torch.stack([p1,p2,p3],2)\n",
    "        return mask,dx,dy,p\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        batch, lengths = make_batch(hp.batch_size)\n",
    "        # encode:\n",
    "        z, self.mu, self.sigma = self.encoder(batch, hp.batch_size)\n",
    "        # create start of sequence:\n",
    "        if use_cuda:\n",
    "            sos = torch.stack([torch.Tensor([0,0,1,0,0])]*hp.batch_size).cuda().unsqueeze(0)\n",
    "        else:\n",
    "            sos = torch.stack([torch.Tensor([0,0,1,0,0])]*hp.batch_size).unsqueeze(0)\n",
    "        # had sos at the begining of the batch:\n",
    "        batch_init = torch.cat([sos, batch],0)\n",
    "        # expend z to be ready to concatenate with inputs:\n",
    "        z_stack = torch.stack([z]*(Nmax+1))\n",
    "        # inputs is concatenation of z and batch_inputs\n",
    "        inputs = torch.cat([batch_init, z_stack],2)\n",
    "        # decode:\n",
    "        self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
    "            self.rho_xy, self.q, _, _ = self.decoder(inputs, z)\n",
    "        # prepare targets:\n",
    "        mask,dx,dy,p = self.make_target(batch, lengths)\n",
    "        # prepare optimizers:\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        # update eta for LKL:\n",
    "        self.eta_step = 1-(1-hp.eta_min)*hp.R\n",
    "        # compute losses:\n",
    "        LKL = self.kullback_leibler_loss()\n",
    "        LR = self.reconstruction_loss(mask,dx,dy,p,epoch)\n",
    "        loss = LR + LKL\n",
    "        # gradient step\n",
    "        loss.backward()\n",
    "        # gradient cliping\n",
    "        nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
    "        nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n",
    "        # optim step\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        # some print and save:\n",
    "        if epoch%1==0:\n",
    "            #print('epoch',epoch,'loss',loss.data[0],'LR',LR.data[0],'LKL',LKL.data[0])\n",
    "            self.encoder_optimizer = lr_decay(self.encoder_optimizer)\n",
    "            self.decoder_optimizer = lr_decay(self.decoder_optimizer)\n",
    "        if epoch%100==0:\n",
    "          # test out the different models and pick the one with the best epoch\n",
    "            self.save(epoch)\n",
    "            # used to test the trained model\n",
    "            self.conditional_generation(epoch)\n",
    "\n",
    "    def bivariate_normal_pdf(self, dx, dy):\n",
    "        z_x = ((dx-self.mu_x)/self.sigma_x)**2\n",
    "        z_y = ((dy-self.mu_y)/self.sigma_y)**2\n",
    "        z_xy = (dx-self.mu_x)*(dy-self.mu_y)/(self.sigma_x*self.sigma_y)\n",
    "        z = z_x + z_y -2*self.rho_xy*z_xy\n",
    "        exp = torch.exp(-z/(2*(1-self.rho_xy**2)))\n",
    "        norm = 2*np.pi*self.sigma_x*self.sigma_y*torch.sqrt(1-self.rho_xy**2)\n",
    "        return exp/norm\n",
    "\n",
    "    def reconstruction_loss(self, mask, dx, dy, p, epoch):\n",
    "        pdf = self.bivariate_normal_pdf(dx, dy)\n",
    "        LS = -torch.sum(mask*torch.log(1e-5+torch.sum(self.pi * pdf, 2)))\\\n",
    "            /float(Nmax*hp.batch_size)\n",
    "        LP = -torch.sum(p*torch.log(self.q))/float(Nmax*hp.batch_size)\n",
    "        return LS+LP\n",
    "\n",
    "    def kullback_leibler_loss(self):\n",
    "        LKL = -0.5*torch.sum(1+self.sigma-self.mu**2-torch.exp(self.sigma))\\\n",
    "            /float(hp.Nz*hp.batch_size)\n",
    "        if use_cuda:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min]).cuda()).detach()\n",
    "        else:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min])).detach()\n",
    "        return hp.wKL*self.eta_step * torch.max(LKL,KL_min)\n",
    "\n",
    "    def save(self, epoch):\n",
    "        enc_model_name = F\"sketchRNN_encoder_{self.model_count}_{epoch}f.pt\"\n",
    "        enc_path = F\"../Models/{enc_model_name}\"\n",
    "        torch.save(self.encoder.state_dict(), enc_path)\n",
    "        dec_model_name = F\"sketchRNN_decoder_{self.model_count}_{epoch}f.pt\"\n",
    "        dec_path = F\"../Models/{dec_model_name}\"\n",
    "        torch.save(self.decoder.state_dict(), dec_path)\n",
    "        self.model_count += 1\n",
    "\n",
    "    def load(self, encoder_name, decoder_name):\n",
    "        saved_encoder = torch.load(encoder_name)\n",
    "        saved_decoder = torch.load(decoder_name)\n",
    "        self.encoder.load_state_dict(saved_encoder)\n",
    "        self.decoder.load_state_dict(saved_decoder)\n",
    "\n",
    "    def conditional_generation(self, epoch):\n",
    "        batch,lengths = make_batch(1)\n",
    "        # should remove dropouts:\n",
    "        self.encoder.train(False)\n",
    "        self.decoder.train(False)\n",
    "        # encode:\n",
    "        z, _, _ = self.encoder(batch, 1)\n",
    "        if use_cuda:\n",
    "            sos = Variable(torch.Tensor([0,0,1,0,0]).view(1,1,-1).cuda())\n",
    "        else:\n",
    "            sos = Variable(torch.Tensor([0,0,1,0,0]).view(1,1,-1))\n",
    "        s = sos\n",
    "        seq_x = []\n",
    "        seq_y = []\n",
    "        seq_z = []\n",
    "        hidden_cell = None\n",
    "        for i in range(Nmax):\n",
    "            input = torch.cat([s,z.unsqueeze(0)],2)\n",
    "            # decode:\n",
    "            self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
    "                self.rho_xy, self.q, hidden, cell = \\\n",
    "                    self.decoder(input, z, hidden_cell)\n",
    "            hidden_cell = (hidden, cell)\n",
    "            # sample from parameters:\n",
    "            s, dx, dy, pen_down, eos = self.sample_next_state()\n",
    "            #------\n",
    "            seq_x.append(dx)\n",
    "            seq_y.append(dy)\n",
    "            seq_z.append(pen_down)\n",
    "            if eos:\n",
    "                print(i)\n",
    "                break\n",
    "        # visualize result:\n",
    "        x_sample = np.cumsum(seq_x, 0)\n",
    "        y_sample = np.cumsum(seq_y, 0)\n",
    "        z_sample = np.array(seq_z)\n",
    "        sequence = np.stack([x_sample,y_sample,z_sample]).T\n",
    "\n",
    "        # output the image produced\n",
    "        make_image(sequence, epoch)\n",
    "\n",
    "    def sample_next_state(self):\n",
    "\n",
    "        def adjust_temp(pi_pdf):\n",
    "            pi_pdf = np.log(pi_pdf)/hp.temperature\n",
    "            pi_pdf -= pi_pdf.max()\n",
    "            pi_pdf = np.exp(pi_pdf)\n",
    "            pi_pdf /= pi_pdf.sum()\n",
    "            return pi_pdf\n",
    "\n",
    "        # get mixture indice:\n",
    "        pi = self.pi.data[0,0,:].cpu().numpy()\n",
    "        pi = adjust_temp(pi)\n",
    "        pi_idx = np.random.choice(hp.M, p=pi)\n",
    "        # get pen state:\n",
    "        q = self.q.data[0,0,:].cpu().numpy()\n",
    "        q = adjust_temp(q)\n",
    "        q_idx = np.random.choice(3, p=q)\n",
    "        # get mixture params:\n",
    "        mu_x = self.mu_x.data[0,0,pi_idx]\n",
    "        mu_y = self.mu_y.data[0,0,pi_idx]\n",
    "        sigma_x = self.sigma_x.data[0,0,pi_idx]\n",
    "        sigma_y = self.sigma_y.data[0,0,pi_idx]\n",
    "        rho_xy = self.rho_xy.data[0,0,pi_idx]\n",
    "        x,y = sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy,greedy=False)\n",
    "        next_state = torch.zeros(5)\n",
    "        next_state[0] = x\n",
    "        next_state[1] = y\n",
    "        next_state[q_idx+2] = 1\n",
    "        if use_cuda:\n",
    "            return Variable(next_state.cuda()).view(1,1,-1),x,y,q_idx==1,q_idx==2\n",
    "        else:\n",
    "            return Variable(next_state).view(1,1,-1),x,y,q_idx==1,q_idx==2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xCsDQGDcF9G6"
   },
   "outputs": [],
   "source": [
    "def sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "    # inputs must be floats\n",
    "    if greedy:\n",
    "      return mu_x,mu_y\n",
    "    mean = [mu_x, mu_y]\n",
    "    sigma_x *= np.sqrt(hp.temperature)\n",
    "    sigma_y *= np.sqrt(hp.temperature)\n",
    "    cov = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],[rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
    "    x = np.random.multivariate_normal(mean, cov, 1)\n",
    "    return x[0][0], x[0][1]\n",
    "\n",
    "def make_image(sequence, epoch, name='_output_'):\n",
    "  # edit this output\n",
    "    \"\"\"plot drawing with separated strokes\"\"\"\n",
    "    strokes = np.split(sequence, np.where(sequence[:,2]>0)[0]+1)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    for s in strokes:\n",
    "        plt.plot(s[:,0],-s[:,1])\n",
    "    #plt.imshow()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Don't want to save it\n",
    "    # canvas = plt.get_current_fig_manager().canvas\n",
    "    # canvas.draw()\n",
    "    # pil_image = PIL.Image.frombytes('RGB', canvas.get_width_height(),canvas.tostring_rgb())\n",
    "    # name = str(epoch)+name+'.jpg'\n",
    "    # pil_image.save(name,\"JPEG\")\n",
    "    # plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 576
    },
    "id": "ch4EV1l1F_Tr",
    "outputId": "7d42dafb-0ffa-4c01-c1c3-21723dbfa9f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:9: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:77: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:83: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    model = Model()\n",
    "    for epoch in range(1000):\n",
    "        model.train(epoch)\n",
    "\n",
    "# Use this to test the models after training\n",
    "    '''\n",
    "    model.load('encoder.pth','decoder.pth')\n",
    "    model.conditional_generation(0)\n",
    "    #'''"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "experiments_sketchrnn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
