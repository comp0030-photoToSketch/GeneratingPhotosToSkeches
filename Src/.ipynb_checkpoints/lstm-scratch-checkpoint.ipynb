{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 13,
>>>>>>> 3c1bfd74e7076060122b8453984ba8354cbf0065
=======
   "execution_count": 1,
>>>>>>> parent of 96df3e5... commenting code for sampling
=======
   "execution_count": 1,
>>>>>>> parent of 96df3e5... commenting code for sampling
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
<<<<<<< HEAD
      "Requirement already satisfied: torch in /home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages (1.7.0)\n",
      "Requirement already satisfied: future in /home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: dataclasses in /home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages (from torch) (0.8)\n",
      "Requirement already satisfied: numpy in /home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages (from torch) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
=======
      "Requirement already satisfied: torch in c:\\users\\usera\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\usera\\anaconda3\\lib\\site-packages (from torch) (3.7.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\usera\\anaconda3\\lib\\site-packages (from torch) (1.18.5)\n",
>>>>>>> parent of 96df3e5... commenting code for sampling
=======
      "Requirement already satisfied: torch in c:\\users\\usera\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\usera\\anaconda3\\lib\\site-packages (from torch) (3.7.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\usera\\anaconda3\\lib\\site-packages (from torch) (1.18.5)\n",
>>>>>>> parent of 96df3e5... commenting code for sampling
      "False\n"
=======
      "Requirement already satisfied: torch in c:\\users\\usera\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\usera\\anaconda3\\lib\\site-packages (from torch) (3.7.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\usera\\anaconda3\\lib\\site-packages (from torch) (1.18.5)\n"
>>>>>>> 3c1bfd74e7076060122b8453984ba8354cbf0065
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# check the system we're on, gpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    def __init__(self):\n",
    "        self.data = '../Datasets/sketchrnn_chair.npz'\n",
    "        self.encoder_hidden_size = 64\n",
    "        self.decoder_hidden_size = 128\n",
    "        # latent vector\n",
    "        self.Nz = 32\n",
    "        \n",
    "        #return\n",
    "        self.M = 20\n",
    "        self.dropout = 0.9\n",
    "        self.batch_size = 100\n",
    "        self.eta_min = 0.01\n",
    "        self.R = 0.99995\n",
    "        self.KL_min = 0.2\n",
    "        self.wKL = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.lr_decay = 0.9999\n",
    "        self.min_lr = 0.00001\n",
    "        self.grad_clip = 1.\n",
    "        self.temperature = 0.4\n",
    "        self.max_seq_length = 200\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        \n",
    "    def load_npz(self):\n",
    "        data = np.load(file=self.file, encoding='latin1', allow_pickle=True)\n",
    "        return data\n",
    "\n",
    "dataLoader = DataLoader(hp.data)\n",
    "data = dataLoader.load_npz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def padding(self, strokedata, max_len):\n",
    "        s = torch.from_numpy(strokedata)\n",
    "        s_len = s.shape[0]\n",
    "        result = torch.zeros((max_len, 5))\n",
    "        result[0:s_len, 0:2] = s[:, 0:2]\n",
    "        result[0:s_len, 3] = s[:, 2]\n",
    "        result[0:s_len, 2] = 1 - result[0:s_len, 3]    # 1 to 0, 0 to 1\n",
    "        if s_len < max_len:\n",
    "            result[(s_len - 1):, 4] = 1\n",
    "        return result\n",
    "        \n",
    "    def handle(self):\n",
    "        max_len = 0    # max length of a sketch - max number of stroke vectors in a sketch\n",
    "        for datatype in self.data:\n",
    "            for strokedata in self.data[datatype]:\n",
    "                if strokedata.shape[0] > max_len:\n",
    "                    max_len = strokedata.shape[0]\n",
    "        # lengths of data - the number of sketchs in data\n",
    "        train_len = self.data['train'].shape[0]\n",
    "        test_len = self.data['test'].shape[0]\n",
    "        valid_len = self.data['valid'].shape[0]\n",
    "        \n",
    "        data_train = torch.zeros((train_len, max_len, 5), dtype=float)\n",
    "        data_test = torch.zeros((test_len, max_len, 5), dtype=float)\n",
    "        data_valid = torch.zeros((valid_len, max_len, 5), dtype=float)\n",
    "        \n",
    "        i = j = k = 0\n",
    "        for datatype in self.data:\n",
    "            for strokedata in self.data[datatype]:\n",
    "                if datatype == 'train':\n",
    "                    data_train[i] = self.padding(strokedata, max_len)\n",
    "                    i += 1\n",
    "                if datatype == 'test':\n",
    "                    data_test[j] = self.padding(strokedata, max_len)\n",
    "                    j += 1\n",
    "                if datatype == 'valid':\n",
    "                    data_valid[k] = self.padding(strokedata, max_len)\n",
    "                    k += 1 \n",
    "\n",
    "        return data_train, data_test, data_valid\n",
    "    \n",
    "        \n",
    "datahandler = DataHandler(data)\n",
    "data_train, data_test, data_valid = datahandler.handle()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoder(\n",
       "  (lstm): LSTM(5, 64, bidirectional=True)\n",
       "  (fc_mu): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (fc_sigma): Linear(in_features=128, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD
<<<<<<< HEAD
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
<<<<<<< HEAD
    "        super(Encoder, self).__init__()\n",
=======
    "        super().__init__()\n",
>>>>>>> 3c1bfd74e7076060122b8453984ba8354cbf0065
    "        # bidirectional lstm:\n",
    "        self.lstm = nn.LSTM(5, hp.encoder_hidden_size, bidirectional=True)\n",
    "        # create mu and sigma from lstm's last output:\n",
    "        self.fc_mu = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "        self.fc_sigma = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "        # active dropout:\n",
<<<<<<< HEAD
    "        self.train()\n",
=======
    "        # self.train()\n",
>>>>>>> 3c1bfd74e7076060122b8453984ba8354cbf0065
    "\n",
    "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            # then must init with zeros\n",
    "            if use_cuda:\n",
    "                hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "                cell = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "            else:\n",
    "                hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "                cell = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "            hidden_cell = (hidden, cell)\n",
    "        _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
<<<<<<< HEAD
    "        \n",
    "        \n",
    "        # Q1. Why is squeeze used here?\n",
    "        \n",
    "        \n",
    "        # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "        hidden_forward, hidden_backward = torch.split(hidden,1,0) # (1, batch_sz, hidden_sz)\n",
    "        # removes the '1' (batch_sz, hidden_sz) -> (batch_sz, hidden_sz * 2)\n",
    "        hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
    "        \n",
    "        \n",
=======
    "        # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "        hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
    "        hidden_cat = torch.cat([hidden_forward, hidden_backward],1)\n",
>>>>>>> 3c1bfd74e7076060122b8453984ba8354cbf0065
    "        # mu and sigma:\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "        sigma_hat = self.fc_sigma(hidden_cat)\n",
    "        sigma = torch.exp(sigma_hat/2.)\n",
    "        # N ~ N(0,1)\n",
    "        z_size = mu.size()\n",
    "                                   \n",
<<<<<<< HEAD
    "        if use_cuda: # use to(mu.device) instead\n",
=======
    "        if use_cuda:\n",
>>>>>>> 3c1bfd74e7076060122b8453984ba8354cbf0065
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda()\n",
    "        else:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size))\n",
    "        z = mu + sigma*N\n",
    "        # mu and sigma_hat are needed for LKL loss\n",
<<<<<<< HEAD
    "        return z, mu, sigma_hat"
=======
    "        return z, mu, sigma_hat\n",
    "    \n",
    "encoder = Encoder()\n",
    "encoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 5, 6],\n",
      "        [3, 4, 7, 8]])\n",
      "torch.Size([3, 1, 5])\n"
     ]
    }
   ],
   "source": [
    "# a = torch.rand((3,1,5))\n",
    "# b = torch.tensor([[1,2], \n",
    "#                   [3,4]])\n",
    "# c = torch.tensor([[5,6],\n",
    "#                   [7,8]])\n",
    "# d = torch.cat([b.squeeze(0),c.squeeze(0)], 1)\n",
    "# print(d)\n",
    "# print(a.squeeze(0).shape)"
>>>>>>> 3c1bfd74e7076060122b8453984ba8354cbf0065
=======
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         # bidirectional lstm:\n",
    "#         self.lstm = nn.LSTM(5, hp.encoder_hidden_size, bidirectional=True)\n",
    "#         # create mu and sigma from lstm's last output:\n",
    "#         self.fc_mu = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "#         self.fc_sigma = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "#         # active dropout:\n",
    "#         self.train()\n",
    "\n",
    "#     def forward(self, inputs, batch_size, hidden_cell=None):\n",
    "#         if hidden_cell is None:\n",
    "#             # then must init with zeros\n",
    "#             if use_cuda:\n",
    "#                 hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "#                 cell = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "#             else:\n",
    "#                 hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "#                 cell = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "#             hidden_cell = (hidden, cell)\n",
    "#         _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
    "#         # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "#         hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
    "#         hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
    "#         # mu and sigma:\n",
    "#         mu = self.fc_mu(hidden_cat)\n",
    "#         sigma_hat = self.fc_sigma(hidden_cat)\n",
    "#         sigma = torch.exp(sigma_hat/2.)\n",
    "#         # N ~ N(0,1)\n",
    "#         z_size = mu.size()\n",
    "                                   \n",
=======
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         # bidirectional lstm:\n",
    "#         self.lstm = nn.LSTM(5, hp.encoder_hidden_size, bidirectional=True)\n",
    "#         # create mu and sigma from lstm's last output:\n",
    "#         self.fc_mu = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "#         self.fc_sigma = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "#         # active dropout:\n",
    "#         self.train()\n",
    "\n",
    "#     def forward(self, inputs, batch_size, hidden_cell=None):\n",
    "#         if hidden_cell is None:\n",
    "#             # then must init with zeros\n",
    "#             if use_cuda:\n",
    "#                 hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "#                 cell = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "#             else:\n",
    "#                 hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "#                 cell = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "#             hidden_cell = (hidden, cell)\n",
    "#         _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
    "#         # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "#         hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
    "#         hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
    "#         # mu and sigma:\n",
    "#         mu = self.fc_mu(hidden_cat)\n",
    "#         sigma_hat = self.fc_sigma(hidden_cat)\n",
    "#         sigma = torch.exp(sigma_hat/2.)\n",
    "#         # N ~ N(0,1)\n",
    "#         z_size = mu.size()\n",
    "                                   \n",
>>>>>>> parent of 96df3e5... commenting code for sampling
    "#         if use_cuda:\n",
    "#             N = torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda()\n",
    "#         else:\n",
    "#             N = torch.normal(torch.zeros(z_size),torch.ones(z_size))\n",
    "#         z = mu + sigma*N\n",
    "#         # mu and sigma_hat are needed for LKL loss\n",
    "#         return z, mu, sigma_hat"
<<<<<<< HEAD
>>>>>>> parent of 96df3e5... commenting code for sampling
=======
>>>>>>> parent of 96df3e5... commenting code for sampling
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
<<<<<<< HEAD
   "source": [
<<<<<<< HEAD
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # to init hidden and cell from z:\n",
    "        self.fc_hc = nn.Linear(hp.Nz, 2*hp.dec_hidden_size)\n",
    "        # unidirectional lstm:\n",
    "        self.lstm = nn.LSTM(hp.Nz+5, hp.dec_hidden_size, dropout=hp.dropout)\n",
    "        # create probability distribution parameters from hiddens: 5M + M + 3\n",
    "        self.fc_params = nn.Linear(hp.dec_hidden_size,6*hp.M+3) \n",
    "        # these values are fed to each correspondending decoder layer\n",
    "\n",
    "    def forward(self, inputs, z, hidden_cell=None):\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forms part of the Model class\n",
    "\n",
    "\"\"\"\n",
    "This for after training, the paper says we can sample sketches for the model.\n",
    "During the sampling process we generate parameters for both GMM and categorical\n",
    "distributions at each time step, and sample an outcome S'i for that time step.\n",
    "\n",
    "GMM ->\n",
    "Categorical distributions ->\n",
    "\n",
    "We continue to sample until p3, so the end of sketch, is 1 or the number of iterations\n",
    "has reached Nmax. Sampling the output is not deterministic, it's a random sequence,\n",
    "conditioned on the input latent vector z. We can control the level of randomness we'd like\n",
    "our samples to have during the sampling process by using a temperature parameter.\n",
    "\n",
    "- state_q_hat tends to state_q_hat / temperature\n",
    "- prime_k tends to prime_k / temperature\n",
    "- sigma_squared_x tends to sigma_squared_temperature_x\n",
    "- sigma_squared_y tends to sigma_squared_temperature_y\n",
    "\n",
    "- the softmax parameters can be scaled, of the categorical distribution and also sigma\n",
    "parameters of the bivariate normal distribution by a temperature parameter temperature.\n",
    "To control the level of randomness in our samples.\n",
    "- temperature is usually between 0 and 1, so limit T -> 0, model becomes deterministic and\n",
    "samples will consist of the most likely point in the probability density function.\n",
    "\n",
    "- Unconditinal generation uses this temperature methodology to control randomness of the\n",
    "distribution\n",
    "\n",
    "Unconditional generation -> train model to generate sketches unconditionally where train\n",
    "decoder RNN module\n",
    "-> Initial hidden states and cell states of decoder RNN are initialized to zero, inputs xi\n",
    "of decoder RNN at each step is only Si-1 or Si-1', don't need to concatenate latent vectorZ\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def sample_sketch_state():\n",
    "    \n",
    "def sample_bivariate_normal():\n",
    "    \n",
    "def make_image():"
=======
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hp.Nz + 5, hp.decoder_hidden_size)\n",
    "        \n",
    "    def forward(self):\n",
    "        \n"
>>>>>>> 3c1bfd74e7076060122b8453984ba8354cbf0065
   ]
=======
   "source": []
>>>>>>> parent of 96df3e5... commenting code for sampling
=======
   "source": []
>>>>>>> parent of 96df3e5... commenting code for sampling
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
