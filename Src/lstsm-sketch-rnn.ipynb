{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\acer\\anaconda3\\lib\\site-packages (1.7.0+cpu)\n",
      "Requirement already satisfied: dataclasses in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (0.6)\n",
      "Requirement already satisfied: future in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\acer\\anaconda3\\lib\\site-packages (from torch) (1.19.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']=\"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams():\n",
    "    def __init__(self):\n",
    "        self.data_location = '../Datasets/sketchrnn_chair.npz'\n",
    "        self.enc_hidden_size = 256\n",
    "        self.dec_hidden_size = 512\n",
    "        self.Nz = 128\n",
    "        self.M = 20\n",
    "        self.dropout = 0.9\n",
    "        self.batch_size = 100\n",
    "        self.eta_min = 0.01\n",
    "        self.R = 0.99995\n",
    "        self.KL_min = 0.2\n",
    "        self.wKL = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.lr_decay = 0.9999\n",
    "        self.min_lr = 0.00001\n",
    "        self.grad_clip = 1.\n",
    "        self.temperature = 0.001\n",
    "        self.max_seq_length = 200\n",
    "\n",
    "hp = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_size(data):\n",
    "    \"\"\"larger sequence length in the data set\"\"\"\n",
    "    sizes = [len(seq) for seq in data]\n",
    "    return max(sizes)\n",
    "\n",
    "def purify(strokes):\n",
    "    \"\"\"removes to small or too long sequences + removes large gaps\"\"\"\n",
    "    data = []\n",
    "    for seq in strokes:\n",
    "        if seq.shape[0] <= hp.max_seq_length and seq.shape[0] > 10:\n",
    "            seq = np.minimum(seq, 1000)\n",
    "            seq = np.maximum(seq, -1000)\n",
    "            seq = np.array(seq, dtype=np.float32)\n",
    "            data.append(seq)\n",
    "    return data\n",
    "\n",
    "def calculate_normalizing_scale_factor(strokes):\n",
    "    \"\"\"Calculate the normalizing factor explained in appendix of sketch-rnn.\"\"\"\n",
    "    data = []\n",
    "    for i in range(len(strokes)):\n",
    "        for j in range(len(strokes[i])):\n",
    "            data.append(strokes[i][j, 0])\n",
    "            data.append(strokes[i][j, 1])\n",
    "    data = np.array(data)\n",
    "    return np.std(data)\n",
    "\n",
    "def normalize(strokes):\n",
    "    \"\"\"Normalize entire dataset (delta_x, delta_y) by the scaling factor.\"\"\"\n",
    "    data = []\n",
    "    scale_factor = calculate_normalizing_scale_factor(strokes)\n",
    "    for seq in strokes:\n",
    "        seq[:, 0:2] /= scale_factor\n",
    "        data.append(seq)\n",
    "    return data\n",
    "\n",
    "dataset = np.load(hp.data_location, encoding='latin1')\n",
    "data = dataset['train']\n",
    "data = purify(data)\n",
    "data = normalize(data)\n",
<<<<<<< HEAD
    "Nmax = max_size(data)\n",
    "\n",
    "# print(data.head())"
=======
    "Nmax = max_size(data)"
>>>>>>> master
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(batch_size):\n",
    "    batch_idx = np.random.choice(len(data),batch_size)\n",
    "    batch_sequences = [data[idx] for idx in batch_idx]\n",
    "    strokes = []\n",
    "    lengths = []\n",
    "    indice = 0\n",
    "    for seq in batch_sequences:\n",
    "        len_seq = len(seq[:,0])\n",
    "        new_seq = np.zeros((Nmax,5)) # 66,5\n",
    "        new_seq[:len_seq,:2] = seq[:,:2]\n",
    "        new_seq[:len_seq-1,2] = 1-seq[:-1,2]\n",
    "        new_seq[:len_seq,3] = seq[:,2]\n",
    "        new_seq[(len_seq-1):,4] = 1\n",
    "        new_seq[len_seq-1,2:4] = 0\n",
    "        lengths.append(len(seq[:,0]))\n",
    "        strokes.append(new_seq)\n",
    "        indice += 1\n",
    "\n",
    "    if use_cuda:\n",
    "        batch = Variable(torch.from_numpy(np.stack(strokes,1)).cuda().float())\n",
    "    else:\n",
    "        batch = Variable(torch.from_numpy(np.stack(strokes,1)).float())\n",
    "    return batch, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_decay(optimizer):\n",
    "    \"\"\"Decay learning rate by a factor of lr_decay\"\"\"\n",
    "    for param_group in optimizer.param_groups:\n",
    "        if param_group['lr']>hp.min_lr:\n",
    "            param_group['lr'] *= hp.lr_decay\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        # bidirectional lstm:\n",
    "        self.lstm = nn.LSTM(5, hp.enc_hidden_size, \\\n",
    "            dropout=hp.dropout, bidirectional=True)\n",
    "        # create mu and sigma from lstm's last output:\n",
    "        self.fc_mu = nn.Linear(2*hp.enc_hidden_size, hp.Nz)\n",
    "        self.fc_sigma = nn.Linear(2*hp.enc_hidden_size, hp.Nz)\n",
    "        # active dropout:\n",
    "        self.train()\n",
    "\n",
    "    def forward(self, inputs, batch_size, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            # then must init with zeros\n",
    "            if use_cuda:\n",
    "                hidden = torch.zeros(2, batch_size, hp.enc_hidden_size).cuda()\n",
    "                cell = torch.zeros(2, batch_size, hp.enc_hidden_size).cuda()\n",
    "            else:\n",
    "                hidden = torch.zeros(2, batch_size, hp.enc_hidden_size)\n",
    "                cell = torch.zeros(2, batch_size, hp.enc_hidden_size)\n",
    "            hidden_cell = (hidden, cell)\n",
    "        _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
    "        # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "        hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
    "        hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
    "        # mu and sigma:\n",
    "        mu = self.fc_mu(hidden_cat)\n",
    "        sigma_hat = self.fc_sigma(hidden_cat)\n",
    "        sigma = torch.exp(sigma_hat/2.)\n",
    "        # N ~ N(0,1)\n",
    "        z_size = mu.size()\n",
    "                                   \n",
    "        if use_cuda:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda()\n",
    "        else:\n",
    "            N = torch.normal(torch.zeros(z_size),torch.ones(z_size))\n",
    "        z = mu + sigma*N\n",
    "        # mu and sigma_hat are needed for LKL loss\n",
    "        return z, mu, sigma_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # to init hidden and cell from z:\n",
    "        self.fc_hc = nn.Linear(hp.Nz, 2*hp.dec_hidden_size)\n",
    "        # unidirectional lstm:\n",
    "        self.lstm = nn.LSTM(hp.Nz+5, hp.dec_hidden_size, dropout=hp.dropout)\n",
    "        # create proba distribution parameters from hiddens:\n",
    "        self.fc_params = nn.Linear(hp.dec_hidden_size,6*hp.M+3)\n",
    "\n",
    "    def forward(self, inputs, z, hidden_cell=None):\n",
    "        if hidden_cell is None:\n",
    "            # then we must init from z\n",
    "            hidden,cell = torch.split(F.tanh(self.fc_hc(z)),hp.dec_hidden_size,1)\n",
    "            hidden_cell = (hidden.unsqueeze(0).contiguous(), cell.unsqueeze(0).contiguous())\n",
    "        outputs,(hidden,cell) = self.lstm(inputs, hidden_cell)\n",
    "        # in training we feed the lstm with the whole input in one shot\n",
    "        # and use all outputs contained in 'outputs', while in generate\n",
    "        # mode we just feed with the last generated sample:\n",
    "        if self.training:\n",
    "            y = self.fc_params(outputs.view(-1, hp.dec_hidden_size))\n",
    "        else:\n",
    "            y = self.fc_params(hidden.view(-1, hp.dec_hidden_size))\n",
    "        # separate pen and mixture params:\n",
    "        params = torch.split(y,6,1)\n",
    "        params_mixture = torch.stack(params[:-1]) # trajectory\n",
    "        params_pen = params[-1] # pen up/down\n",
    "        # identify mixture params:\n",
    "        pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy = torch.split(params_mixture,1,2)\n",
    "        # preprocess params::\n",
    "        if self.training:\n",
    "            len_out = Nmax+1\n",
    "        else:\n",
    "            len_out = 1\n",
    "                                   \n",
    "        pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        sigma_x = torch.exp(sigma_x.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        sigma_y = torch.exp(sigma_y.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        rho_xy = torch.tanh(rho_xy.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
    "        mu_x = mu_x.transpose(0,1).squeeze().contiguous().view(len_out,-1,hp.M)\n",
    "        mu_y = mu_y.transpose(0,1).squeeze().contiguous().view(len_out,-1,hp.M)\n",
    "        q = F.softmax(params_pen).view(len_out,-1,3)\n",
    "        return pi,mu_x,mu_y,sigma_x,sigma_y,rho_xy,q,hidden,cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        if use_cuda:\n",
    "            self.encoder = EncoderRNN().cuda()\n",
    "            self.decoder = DecoderRNN().cuda()\n",
    "        else:\n",
    "            self.encoder = EncoderRNN()\n",
    "            self.decoder = DecoderRNN()\n",
    "        self.encoder_optimizer = optim.Adam(self.encoder.parameters(), hp.lr)\n",
    "        self.decoder_optimizer = optim.Adam(self.decoder.parameters(), hp.lr)\n",
    "        self.eta_step = hp.eta_min\n",
    "        self.lost_array = []\n",
    "\n",
    "    def make_target(self, batch, lengths):\n",
    "        if use_cuda:\n",
    "            eos = torch.stack([torch.Tensor([0,0,0,0,1])]*batch.size()[1]).cuda().unsqueeze(0)\n",
    "        else:\n",
    "            eos = torch.stack([torch.Tensor([0,0,0,0,1])]*batch.size()[1]).unsqueeze(0)\n",
    "        batch = torch.cat([batch, eos], 0)\n",
    "        mask = torch.zeros(Nmax+1, batch.size()[1])\n",
    "        for indice,length in enumerate(lengths):\n",
    "            mask[:length,indice] = 1\n",
    "        if use_cuda:\n",
    "            mask = mask.cuda()\n",
    "        dx = torch.stack([batch.data[:,:,0]]*hp.M,2)\n",
    "        dy = torch.stack([batch.data[:,:,1]]*hp.M,2)\n",
    "        p1 = batch.data[:,:,2]\n",
    "        p2 = batch.data[:,:,3]\n",
    "        p3 = batch.data[:,:,4]\n",
    "        p = torch.stack([p1,p2,p3],2)\n",
    "        return mask,dx,dy,p\n",
    "\n",
    "    def train(self, epoch):\n",
    "        self.encoder.train()\n",
    "        self.decoder.train()\n",
    "        batch, lengths = make_batch(hp.batch_size)\n",
    "        # encode:\n",
    "        z, self.mu, self.sigma = self.encoder(batch, hp.batch_size)\n",
    "        # create start of sequence:\n",
    "        if use_cuda:\n",
    "            sos = torch.stack([torch.Tensor([0,0,1,0,0])]*hp.batch_size).cuda().unsqueeze(0)\n",
    "        else:\n",
    "            sos = torch.stack([torch.Tensor([0,0,1,0,0])]*hp.batch_size).unsqueeze(0)\n",
    "        # had sos at the begining of the batch:\n",
    "        batch_init = torch.cat([sos, batch],0)\n",
    "        # expend z to be ready to concatenate with inputs:\n",
    "        z_stack = torch.stack([z]*(Nmax+1))\n",
    "        # inputs is concatenation of z and batch_inputs\n",
    "        inputs = torch.cat([batch_init, z_stack],2)\n",
    "        # decode:\n",
    "        self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
    "            self.rho_xy, self.q, _, _ = self.decoder(inputs, z)\n",
    "        # prepare targets:\n",
    "        mask,dx,dy,p = self.make_target(batch, lengths)\n",
    "        # prepare optimizers:\n",
    "        self.encoder_optimizer.zero_grad()\n",
    "        self.decoder_optimizer.zero_grad()\n",
    "        # update eta for LKL:\n",
    "        self.eta_step = 1-(1-hp.eta_min)*hp.R\n",
    "        # compute losses:\n",
    "        LKL = self.kullback_leibler_loss()\n",
    "        LR = self.reconstruction_loss(mask,dx,dy,p,epoch)\n",
    "        loss = LR + LKL\n",
    "        # gradient step\n",
    "        loss.backward()\n",
    "        # gradient cliping\n",
    "        nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
    "        nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n",
    "        # optim step\n",
    "        self.encoder_optimizer.step()\n",
    "        self.decoder_optimizer.step()\n",
    "        # some print and save:\n",
    "        print(f\"The number of epochs is: {epoch}\")\n",
    "        if epoch%1==0:\n",
    "#             print('epoch',epoch,'loss',loss.data[0],'LR',LR.data[0],'LKL',LKL.data[0])\n",
    "            print(f\"epoch: {epoch} loss is {loss}\")\n",
    "            self.encoder_optimizer = lr_decay(self.encoder_optimizer)\n",
    "            self.decoder_optimizer = lr_decay(self.decoder_optimizer)\n",
<<<<<<< HEAD
    "        if epoch%100==0:\n",
    "            #self.save(epoch)\n",
    "            self.lost_array.append(loss.item())\n",
=======
    "        if epoch%200==0:\n",
    "            self.save(epoch)\n",
>>>>>>> master
    "            self.conditional_generation(epoch)\n",
    "\n",
    "    def bivariate_normal_pdf(self, dx, dy):\n",
    "        z_x = ((dx-self.mu_x)/self.sigma_x)**2\n",
    "        z_y = ((dy-self.mu_y)/self.sigma_y)**2\n",
    "        z_xy = (dx-self.mu_x)*(dy-self.mu_y)/(self.sigma_x*self.sigma_y)\n",
    "        z = z_x + z_y -2*self.rho_xy*z_xy\n",
    "        exp = torch.exp(-z/(2*(1-self.rho_xy**2)))\n",
    "        norm = 2*np.pi*self.sigma_x*self.sigma_y*torch.sqrt(1-self.rho_xy**2)\n",
    "        return exp/norm\n",
    "\n",
    "    def reconstruction_loss(self, mask, dx, dy, p, epoch):\n",
    "        pdf = self.bivariate_normal_pdf(dx, dy)\n",
    "        LS = -torch.sum(mask*torch.log(1e-5+torch.sum(self.pi * pdf, 2)))\\\n",
    "            /float(Nmax*hp.batch_size)\n",
    "        LP = -torch.sum(p*torch.log(self.q))/float(Nmax*hp.batch_size)\n",
    "        return LS+LP\n",
    "\n",
    "    def kullback_leibler_loss(self):\n",
    "        LKL = -0.5*torch.sum(1+self.sigma-self.mu**2-torch.exp(self.sigma))\\\n",
    "            /float(hp.Nz*hp.batch_size)\n",
    "        if use_cuda:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min]).cuda()).detach()\n",
    "        else:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min])).detach()\n",
    "        return hp.wKL*self.eta_step * torch.max(LKL,KL_min)\n",
    "\n",
    "    def save(self, epoch):\n",
    "        random_number = np.random.rand()\n",
    "        enc_model_name = 'sketchRNN_encoder_%3f_%d.pt' % (random_number, epoch)\n",
    "        enc_path = F\"../Models_Chairs/{enc_model_name}\"\n",
    "        torch.save(self.encoder.state_dict(), enc_path)\n",
    "        dec_model_name = 'sketchRNN_decoder_%3f_%d.pt' % (random_number, epoch)\n",
    "        dec_path = F\"../Models_Chairs/{dec_model_name}\"\n",
    "        torch.save(self.decoder.state_dict(), dec_path)\n",
    "        \n",
    "#         torch.save(self.encoder.state_dict(), 'encoderRNN_sel_%3f_epoch_%d.pth' % (sel,epoch))\n",
    "#         torch.save(self.decoder.state_dict(), 'decoderRNN_sel_%3f_epoch_%d.pth' % (sel,epoch))\n",
    "\n",
    "    def load(self, encoder_name, decoder_name):\n",
    "        saved_encoder = torch.load(encoder_name)\n",
    "        saved_decoder = torch.load(decoder_name)\n",
    "        self.encoder.load_state_dict(saved_encoder)\n",
    "        self.decoder.load_state_dict(saved_decoder)\n",
    "\n",
    "    def conditional_generation(self, epoch):\n",
    "        batch,lengths = make_batch(1)\n",
    "        # should remove dropouts:\n",
    "        self.encoder.train(False)\n",
    "        self.decoder.train(False)\n",
    "        # encode:\n",
    "        z, _, _ = self.encoder(batch, 1)\n",
    "        if use_cuda:\n",
    "            sos = Variable(torch.Tensor([0,0,1,0,0]).view(1,1,-1).cuda())\n",
    "        else:\n",
    "            sos = Variable(torch.Tensor([0,0,1,0,0]).view(1,1,-1))\n",
    "        s = sos\n",
    "        seq_x = []\n",
    "        seq_y = []\n",
    "        seq_z = []\n",
    "        hidden_cell = None\n",
    "        for i in range(Nmax):\n",
    "            input = torch.cat([s,z.unsqueeze(0)],2)\n",
    "            # decode:\n",
    "            self.pi, self.mu_x, self.mu_y, self.sigma_x, self.sigma_y, \\\n",
    "                self.rho_xy, self.q, hidden, cell = \\\n",
    "                    self.decoder(input, z, hidden_cell)\n",
    "            hidden_cell = (hidden, cell)\n",
    "            # sample from parameters:\n",
    "            s, dx, dy, pen_down, eos = self.sample_next_state()\n",
    "            #------\n",
    "            seq_x.append(dx)\n",
    "            seq_y.append(dy)\n",
    "            seq_z.append(pen_down)\n",
    "            if eos:\n",
    "                print(i)\n",
    "                break\n",
    "        # visualize result:\n",
    "        x_sample = np.cumsum(seq_x, 0)\n",
    "        y_sample = np.cumsum(seq_y, 0)\n",
    "        z_sample = np.array(seq_z)\n",
    "        sequence = np.stack([x_sample,y_sample,z_sample]).T\n",
    "        make_image(sequence, epoch)\n",
    "\n",
    "    def sample_next_state(self):\n",
    "        def adjust_temp(pi_pdf):\n",
    "            pi_pdf = np.log(pi_pdf)/hp.temperature\n",
    "            pi_pdf -= pi_pdf.max()\n",
    "            pi_pdf = np.exp(pi_pdf)\n",
    "            pi_pdf /= pi_pdf.sum()\n",
    "            return pi_pdf\n",
    "\n",
    "        # get mixture indice:\n",
    "        pi = self.pi.data[0,0,:].cpu().numpy()\n",
    "        pi = adjust_temp(pi)\n",
    "        pi_idx = np.random.choice(hp.M, p=pi)\n",
    "        # get pen state:\n",
    "        q = self.q.data[0,0,:].cpu().numpy()\n",
    "        q = adjust_temp(q)\n",
    "        q_idx = np.random.choice(3, p=q)\n",
    "        # get mixture params:\n",
    "        mu_x = self.mu_x.data[0,0,pi_idx]\n",
    "        mu_y = self.mu_y.data[0,0,pi_idx]\n",
    "        sigma_x = self.sigma_x.data[0,0,pi_idx]\n",
    "        sigma_y = self.sigma_y.data[0,0,pi_idx]\n",
    "        rho_xy = self.rho_xy.data[0,0,pi_idx]\n",
    "        x,y = sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy,greedy=False)\n",
    "        next_state = torch.zeros(5)\n",
    "        next_state[0] = x\n",
    "        next_state[1] = y\n",
    "        next_state[q_idx+2] = 1\n",
    "        if use_cuda:\n",
    "            return Variable(next_state.cuda()).view(1,1,-1),x,y,q_idx==1,q_idx==2\n",
    "        else:\n",
    "            return Variable(next_state).view(1,1,-1),x,y,q_idx==1,q_idx==2\n",
    "\n",
    "def sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "    # inputs must be floats\n",
    "    if greedy:\n",
    "        return mu_x,mu_y\n",
    "    mean = [mu_x, mu_y]\n",
    "    sigma_x *= np.sqrt(hp.temperature)\n",
    "    sigma_y *= np.sqrt(hp.temperature)\n",
    "    cov = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],\\\n",
    "        [rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
    "    x = np.random.multivariate_normal(mean, cov, 1)\n",
    "    return x[0][0], x[0][1]\n",
    "\n",
    "def make_image(sequence, epoch, name='_output_'):\n",
    "    \"\"\"plot drawing with separated strokes\"\"\"\n",
    "    strokes = np.split(sequence, np.where(sequence[:,2]>0)[0]+1)\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "    for s in strokes:\n",
    "        plt.plot(s[:,0],-s[:,1])\n",
    "    print(\"Outputting sketch\")\n",
    "    plt.show()\n",
    "    name = str(epoch)+name+'.jpg'\n",
    "    plt.savefig(F\"./outputs_chair/{name}\")\n",
    "    \n",
    "#     canvas = plt.get_current_fig_manager().canvas\n",
    "#     canvas.draw()\n",
    "#     pil_image = PIL.Image.frombytes('RGB', canvas.get_width_height(),\n",
    "#                  canvas.tostring_rgb())\n",
    "#     name = str(epoch)+name+'.jpg'\n",
    "#     pil_image.save(F\"./outputs_chair/{name}\",\"JPEG\")\n",
    "#     plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
<<<<<<< HEAD
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = Model()\n",
    "# for epoch in range(50001):\n",
    "#     model.train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
=======
>>>>>>> master
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py:61: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.9 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 0 loss is tensor([2.3688], grad_fn=<AddBackward0>)\n",
      "0\n"
=======
      "The number of epochs is: 0\n",
      "0\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 1 loss is tensor([2.2534], grad_fn=<AddBackward0>)\n",
      "epoch: 2 loss is tensor([2.2996], grad_fn=<AddBackward0>)\n",
      "epoch: 3 loss is tensor([2.1165], grad_fn=<AddBackward0>)\n",
      "epoch: 4 loss is tensor([2.0429], grad_fn=<AddBackward0>)\n",
      "epoch: 5 loss is tensor([1.9262], grad_fn=<AddBackward0>)\n",
      "epoch: 6 loss is tensor([1.9035], grad_fn=<AddBackward0>)\n",
      "epoch: 7 loss is tensor([2.0001], grad_fn=<AddBackward0>)\n",
      "epoch: 8 loss is tensor([1.7129], grad_fn=<AddBackward0>)\n",
      "epoch: 9 loss is tensor([1.8701], grad_fn=<AddBackward0>)\n",
      "epoch: 10 loss is tensor([1.8235], grad_fn=<AddBackward0>)\n",
      "epoch: 11 loss is tensor([1.6834], grad_fn=<AddBackward0>)\n",
      "epoch: 12 loss is tensor([1.7313], grad_fn=<AddBackward0>)\n",
      "epoch: 13 loss is tensor([1.5631], grad_fn=<AddBackward0>)\n",
      "epoch: 14 loss is tensor([1.7253], grad_fn=<AddBackward0>)\n",
      "epoch: 15 loss is tensor([1.6629], grad_fn=<AddBackward0>)\n",
      "epoch: 16 loss is tensor([1.5541], grad_fn=<AddBackward0>)\n",
      "epoch: 17 loss is tensor([1.5736], grad_fn=<AddBackward0>)\n",
      "epoch: 18 loss is tensor([1.4983], grad_fn=<AddBackward0>)\n",
      "epoch: 19 loss is tensor([1.4507], grad_fn=<AddBackward0>)\n",
      "epoch: 20 loss is tensor([1.3620], grad_fn=<AddBackward0>)\n",
      "epoch: 21 loss is tensor([1.3588], grad_fn=<AddBackward0>)\n",
      "epoch: 22 loss is tensor([1.3497], grad_fn=<AddBackward0>)\n",
      "epoch: 23 loss is tensor([1.4262], grad_fn=<AddBackward0>)\n",
      "epoch: 24 loss is tensor([1.3455], grad_fn=<AddBackward0>)\n",
      "epoch: 25 loss is tensor([1.3829], grad_fn=<AddBackward0>)\n",
      "epoch: 26 loss is tensor([1.1925], grad_fn=<AddBackward0>)\n",
      "epoch: 27 loss is tensor([1.2816], grad_fn=<AddBackward0>)\n",
      "epoch: 28 loss is tensor([1.2353], grad_fn=<AddBackward0>)\n",
      "epoch: 29 loss is tensor([1.2659], grad_fn=<AddBackward0>)\n",
      "epoch: 30 loss is tensor([1.3608], grad_fn=<AddBackward0>)\n",
      "epoch: 31 loss is tensor([1.2373], grad_fn=<AddBackward0>)\n",
      "epoch: 32 loss is tensor([1.3339], grad_fn=<AddBackward0>)\n",
      "epoch: 33 loss is tensor([1.1652], grad_fn=<AddBackward0>)\n",
      "epoch: 34 loss is tensor([1.2840], grad_fn=<AddBackward0>)\n",
      "epoch: 35 loss is tensor([1.2373], grad_fn=<AddBackward0>)\n",
      "epoch: 36 loss is tensor([1.2009], grad_fn=<AddBackward0>)\n",
      "epoch: 37 loss is tensor([1.2201], grad_fn=<AddBackward0>)\n",
      "epoch: 38 loss is tensor([1.1763], grad_fn=<AddBackward0>)\n",
      "epoch: 39 loss is tensor([1.1937], grad_fn=<AddBackward0>)\n",
      "epoch: 40 loss is tensor([1.2154], grad_fn=<AddBackward0>)\n",
      "epoch: 41 loss is tensor([1.1598], grad_fn=<AddBackward0>)\n",
      "epoch: 42 loss is tensor([1.1292], grad_fn=<AddBackward0>)\n",
      "epoch: 43 loss is tensor([1.1951], grad_fn=<AddBackward0>)\n",
      "epoch: 44 loss is tensor([1.1527], grad_fn=<AddBackward0>)\n",
      "epoch: 45 loss is tensor([1.1190], grad_fn=<AddBackward0>)\n",
      "epoch: 46 loss is tensor([1.1283], grad_fn=<AddBackward0>)\n",
      "epoch: 47 loss is tensor([1.2165], grad_fn=<AddBackward0>)\n",
      "epoch: 48 loss is tensor([1.2067], grad_fn=<AddBackward0>)\n",
      "epoch: 49 loss is tensor([1.1480], grad_fn=<AddBackward0>)\n",
      "epoch: 50 loss is tensor([1.1594], grad_fn=<AddBackward0>)\n",
      "epoch: 51 loss is tensor([1.0872], grad_fn=<AddBackward0>)\n",
      "epoch: 52 loss is tensor([1.0319], grad_fn=<AddBackward0>)\n",
      "epoch: 53 loss is tensor([1.1694], grad_fn=<AddBackward0>)\n",
      "epoch: 54 loss is tensor([1.1072], grad_fn=<AddBackward0>)\n",
      "epoch: 55 loss is tensor([1.0480], grad_fn=<AddBackward0>)\n",
      "epoch: 56 loss is tensor([1.1878], grad_fn=<AddBackward0>)\n",
      "epoch: 57 loss is tensor([1.1306], grad_fn=<AddBackward0>)\n",
      "epoch: 58 loss is tensor([1.1113], grad_fn=<AddBackward0>)\n",
      "epoch: 59 loss is tensor([1.2072], grad_fn=<AddBackward0>)\n",
      "epoch: 60 loss is tensor([1.1152], grad_fn=<AddBackward0>)\n",
      "epoch: 61 loss is tensor([1.0595], grad_fn=<AddBackward0>)\n",
      "epoch: 62 loss is tensor([1.0946], grad_fn=<AddBackward0>)\n",
      "epoch: 63 loss is tensor([1.0142], grad_fn=<AddBackward0>)\n",
      "epoch: 64 loss is tensor([1.1334], grad_fn=<AddBackward0>)\n",
      "epoch: 65 loss is tensor([1.0459], grad_fn=<AddBackward0>)\n",
      "epoch: 66 loss is tensor([1.0737], grad_fn=<AddBackward0>)\n",
      "epoch: 67 loss is tensor([0.9786], grad_fn=<AddBackward0>)\n",
      "epoch: 68 loss is tensor([1.1105], grad_fn=<AddBackward0>)\n",
      "epoch: 69 loss is tensor([1.1656], grad_fn=<AddBackward0>)\n",
      "epoch: 70 loss is tensor([1.0961], grad_fn=<AddBackward0>)\n",
      "epoch: 71 loss is tensor([1.1646], grad_fn=<AddBackward0>)\n",
      "epoch: 72 loss is tensor([1.1106], grad_fn=<AddBackward0>)\n",
      "epoch: 73 loss is tensor([1.0676], grad_fn=<AddBackward0>)\n",
      "epoch: 74 loss is tensor([1.0946], grad_fn=<AddBackward0>)\n",
      "epoch: 75 loss is tensor([1.1367], grad_fn=<AddBackward0>)\n",
      "epoch: 76 loss is tensor([0.9778], grad_fn=<AddBackward0>)\n",
      "epoch: 77 loss is tensor([1.0243], grad_fn=<AddBackward0>)\n",
      "epoch: 78 loss is tensor([0.9807], grad_fn=<AddBackward0>)\n",
      "epoch: 79 loss is tensor([1.0865], grad_fn=<AddBackward0>)\n",
      "epoch: 80 loss is tensor([0.9801], grad_fn=<AddBackward0>)\n",
      "epoch: 81 loss is tensor([1.0596], grad_fn=<AddBackward0>)\n",
      "epoch: 82 loss is tensor([0.9847], grad_fn=<AddBackward0>)\n",
      "epoch: 83 loss is tensor([0.9843], grad_fn=<AddBackward0>)\n",
      "epoch: 84 loss is tensor([1.0144], grad_fn=<AddBackward0>)\n",
      "epoch: 85 loss is tensor([1.0521], grad_fn=<AddBackward0>)\n",
      "epoch: 86 loss is tensor([1.0988], grad_fn=<AddBackward0>)\n",
      "epoch: 87 loss is tensor([1.1112], grad_fn=<AddBackward0>)\n",
      "epoch: 88 loss is tensor([1.1150], grad_fn=<AddBackward0>)\n",
      "epoch: 89 loss is tensor([1.0801], grad_fn=<AddBackward0>)\n",
      "epoch: 90 loss is tensor([0.9378], grad_fn=<AddBackward0>)\n",
      "epoch: 91 loss is tensor([1.0837], grad_fn=<AddBackward0>)\n",
      "epoch: 92 loss is tensor([1.0067], grad_fn=<AddBackward0>)\n",
      "epoch: 93 loss is tensor([1.0883], grad_fn=<AddBackward0>)\n",
      "epoch: 94 loss is tensor([1.0424], grad_fn=<AddBackward0>)\n",
      "epoch: 95 loss is tensor([1.1278], grad_fn=<AddBackward0>)\n",
      "epoch: 96 loss is tensor([0.9952], grad_fn=<AddBackward0>)\n",
      "epoch: 97 loss is tensor([1.0066], grad_fn=<AddBackward0>)\n",
      "epoch: 98 loss is tensor([1.1542], grad_fn=<AddBackward0>)\n",
      "epoch: 99 loss is tensor([0.9965], grad_fn=<AddBackward0>)\n",
      "epoch: 100 loss is tensor([1.0040], grad_fn=<AddBackward0>)\n"
=======
      "The number of epochs is: 1\n",
      "The number of epochs is: 2\n",
      "The number of epochs is: 3\n",
      "The number of epochs is: 4\n",
      "The number of epochs is: 5\n",
      "The number of epochs is: 6\n",
      "The number of epochs is: 7\n",
      "The number of epochs is: 8\n",
      "The number of epochs is: 9\n",
      "The number of epochs is: 10\n",
      "The number of epochs is: 11\n",
      "The number of epochs is: 12\n",
      "The number of epochs is: 13\n",
      "The number of epochs is: 14\n",
      "The number of epochs is: 15\n",
      "The number of epochs is: 16\n",
      "The number of epochs is: 17\n",
      "The number of epochs is: 18\n",
      "The number of epochs is: 19\n",
      "The number of epochs is: 20\n",
      "The number of epochs is: 21\n",
      "The number of epochs is: 22\n",
      "The number of epochs is: 23\n",
      "The number of epochs is: 24\n",
      "The number of epochs is: 25\n",
      "The number of epochs is: 26\n",
      "The number of epochs is: 27\n",
      "The number of epochs is: 28\n",
      "The number of epochs is: 29\n",
      "The number of epochs is: 30\n",
      "The number of epochs is: 31\n",
      "The number of epochs is: 32\n",
      "The number of epochs is: 33\n",
      "The number of epochs is: 34\n",
      "The number of epochs is: 35\n",
      "The number of epochs is: 36\n",
      "The number of epochs is: 37\n",
      "The number of epochs is: 38\n",
      "The number of epochs is: 39\n",
      "The number of epochs is: 40\n",
      "The number of epochs is: 41\n",
      "The number of epochs is: 42\n",
      "The number of epochs is: 43\n",
      "The number of epochs is: 44\n",
      "The number of epochs is: 45\n",
      "The number of epochs is: 46\n",
      "The number of epochs is: 47\n",
      "The number of epochs is: 48\n",
      "The number of epochs is: 49\n",
      "The number of epochs is: 50\n",
      "The number of epochs is: 51\n",
      "The number of epochs is: 52\n",
      "The number of epochs is: 53\n",
      "The number of epochs is: 54\n",
      "The number of epochs is: 55\n",
      "The number of epochs is: 56\n",
      "The number of epochs is: 57\n",
      "The number of epochs is: 58\n",
      "The number of epochs is: 59\n",
      "The number of epochs is: 60\n",
      "The number of epochs is: 61\n",
      "The number of epochs is: 62\n",
      "The number of epochs is: 63\n",
      "The number of epochs is: 64\n",
      "The number of epochs is: 65\n",
      "The number of epochs is: 66\n",
      "The number of epochs is: 67\n",
      "The number of epochs is: 68\n",
      "The number of epochs is: 69\n",
      "The number of epochs is: 70\n",
      "The number of epochs is: 71\n",
      "The number of epochs is: 72\n",
      "The number of epochs is: 73\n",
      "The number of epochs is: 74\n",
      "The number of epochs is: 75\n",
      "The number of epochs is: 76\n",
      "The number of epochs is: 77\n",
      "The number of epochs is: 78\n",
      "The number of epochs is: 79\n",
      "The number of epochs is: 80\n",
      "The number of epochs is: 81\n",
      "The number of epochs is: 82\n",
      "The number of epochs is: 83\n",
      "The number of epochs is: 84\n",
      "The number of epochs is: 85\n",
      "The number of epochs is: 86\n",
      "The number of epochs is: 87\n",
      "The number of epochs is: 88\n",
      "The number of epochs is: 89\n",
      "The number of epochs is: 90\n",
      "The number of epochs is: 91\n",
      "The number of epochs is: 92\n",
      "The number of epochs is: 93\n",
      "The number of epochs is: 94\n",
      "The number of epochs is: 95\n",
      "The number of epochs is: 96\n",
      "The number of epochs is: 97\n",
      "The number of epochs is: 98\n",
      "The number of epochs is: 99\n",
      "The number of epochs is: 100\n",
      "The number of epochs is: 101\n",
      "The number of epochs is: 102\n",
      "The number of epochs is: 103\n",
      "The number of epochs is: 104\n",
      "The number of epochs is: 105\n",
      "The number of epochs is: 106\n",
      "The number of epochs is: 107\n",
      "The number of epochs is: 108\n",
      "The number of epochs is: 109\n",
      "The number of epochs is: 110\n",
      "The number of epochs is: 111\n",
      "The number of epochs is: 112\n",
      "The number of epochs is: 113\n",
      "The number of epochs is: 114\n",
      "The number of epochs is: 115\n",
      "The number of epochs is: 116\n",
      "The number of epochs is: 117\n",
      "The number of epochs is: 118\n",
      "The number of epochs is: 119\n",
      "The number of epochs is: 120\n",
      "The number of epochs is: 121\n",
      "The number of epochs is: 122\n",
      "The number of epochs is: 123\n",
      "The number of epochs is: 124\n",
      "The number of epochs is: 125\n",
      "The number of epochs is: 126\n",
      "The number of epochs is: 127\n",
      "The number of epochs is: 128\n",
      "The number of epochs is: 129\n",
      "The number of epochs is: 130\n",
      "The number of epochs is: 131\n",
      "The number of epochs is: 132\n",
      "The number of epochs is: 133\n",
      "The number of epochs is: 134\n",
      "The number of epochs is: 135\n",
      "The number of epochs is: 136\n",
      "The number of epochs is: 137\n",
      "The number of epochs is: 138\n",
      "The number of epochs is: 139\n",
      "The number of epochs is: 140\n",
      "The number of epochs is: 141\n",
      "The number of epochs is: 142\n",
      "The number of epochs is: 143\n",
      "The number of epochs is: 144\n",
      "The number of epochs is: 145\n",
      "The number of epochs is: 146\n",
      "The number of epochs is: 147\n",
      "The number of epochs is: 148\n",
      "The number of epochs is: 149\n",
      "The number of epochs is: 150\n",
      "The number of epochs is: 151\n",
      "The number of epochs is: 152\n",
      "The number of epochs is: 153\n",
      "The number of epochs is: 154\n",
      "The number of epochs is: 155\n",
      "The number of epochs is: 156\n",
      "The number of epochs is: 157\n",
      "The number of epochs is: 158\n",
      "The number of epochs is: 159\n",
      "The number of epochs is: 160\n",
      "The number of epochs is: 161\n",
      "The number of epochs is: 162\n",
      "The number of epochs is: 163\n",
      "The number of epochs is: 164\n",
      "The number of epochs is: 165\n",
      "The number of epochs is: 166\n",
      "The number of epochs is: 167\n",
      "The number of epochs is: 168\n",
      "The number of epochs is: 169\n",
      "The number of epochs is: 170\n",
      "The number of epochs is: 171\n",
      "The number of epochs is: 172\n",
      "The number of epochs is: 173\n",
      "The number of epochs is: 174\n",
      "The number of epochs is: 175\n",
      "The number of epochs is: 176\n",
      "The number of epochs is: 177\n",
      "The number of epochs is: 178\n",
      "The number of epochs is: 179\n",
      "The number of epochs is: 180\n",
      "The number of epochs is: 181\n",
      "The number of epochs is: 182\n",
      "The number of epochs is: 183\n",
      "The number of epochs is: 184\n",
      "The number of epochs is: 185\n",
      "The number of epochs is: 186\n",
      "The number of epochs is: 187\n",
      "The number of epochs is: 188\n",
      "The number of epochs is: 189\n",
      "The number of epochs is: 190\n",
      "The number of epochs is: 191\n",
      "The number of epochs is: 192\n",
      "The number of epochs is: 193\n",
      "The number of epochs is: 194\n",
      "The number of epochs is: 195\n",
      "The number of epochs is: 196\n",
      "The number of epochs is: 197\n",
      "The number of epochs is: 198\n",
      "The number of epochs is: 199\n",
      "The number of epochs is: 200\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 101 loss is tensor([0.9436], grad_fn=<AddBackward0>)\n",
      "epoch: 102 loss is tensor([0.8964], grad_fn=<AddBackward0>)\n",
      "epoch: 103 loss is tensor([1.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 104 loss is tensor([0.9640], grad_fn=<AddBackward0>)\n",
      "epoch: 105 loss is tensor([0.9399], grad_fn=<AddBackward0>)\n",
      "epoch: 106 loss is tensor([0.9746], grad_fn=<AddBackward0>)\n",
      "epoch: 107 loss is tensor([1.0172], grad_fn=<AddBackward0>)\n",
      "epoch: 108 loss is tensor([0.9633], grad_fn=<AddBackward0>)\n",
      "epoch: 109 loss is tensor([0.9731], grad_fn=<AddBackward0>)\n",
      "epoch: 110 loss is tensor([0.8795], grad_fn=<AddBackward0>)\n",
      "epoch: 111 loss is tensor([1.0078], grad_fn=<AddBackward0>)\n",
      "epoch: 112 loss is tensor([0.9841], grad_fn=<AddBackward0>)\n",
      "epoch: 113 loss is tensor([0.9619], grad_fn=<AddBackward0>)\n",
      "epoch: 114 loss is tensor([1.0270], grad_fn=<AddBackward0>)\n",
      "epoch: 115 loss is tensor([1.0086], grad_fn=<AddBackward0>)\n",
      "epoch: 116 loss is tensor([1.0884], grad_fn=<AddBackward0>)\n",
      "epoch: 117 loss is tensor([0.9844], grad_fn=<AddBackward0>)\n",
      "epoch: 118 loss is tensor([1.0457], grad_fn=<AddBackward0>)\n",
      "epoch: 119 loss is tensor([0.9149], grad_fn=<AddBackward0>)\n",
      "epoch: 120 loss is tensor([0.9829], grad_fn=<AddBackward0>)\n",
      "epoch: 121 loss is tensor([1.0395], grad_fn=<AddBackward0>)\n",
      "epoch: 122 loss is tensor([0.9720], grad_fn=<AddBackward0>)\n",
      "epoch: 123 loss is tensor([0.9350], grad_fn=<AddBackward0>)\n",
      "epoch: 124 loss is tensor([0.9595], grad_fn=<AddBackward0>)\n",
      "epoch: 125 loss is tensor([0.9180], grad_fn=<AddBackward0>)\n",
      "epoch: 126 loss is tensor([0.9204], grad_fn=<AddBackward0>)\n",
      "epoch: 127 loss is tensor([0.9549], grad_fn=<AddBackward0>)\n",
      "epoch: 128 loss is tensor([0.9364], grad_fn=<AddBackward0>)\n",
      "epoch: 129 loss is tensor([0.8919], grad_fn=<AddBackward0>)\n",
      "epoch: 130 loss is tensor([0.9274], grad_fn=<AddBackward0>)\n",
      "epoch: 131 loss is tensor([0.9385], grad_fn=<AddBackward0>)\n",
      "epoch: 132 loss is tensor([0.8255], grad_fn=<AddBackward0>)\n",
      "epoch: 133 loss is tensor([0.9149], grad_fn=<AddBackward0>)\n",
      "epoch: 134 loss is tensor([0.8699], grad_fn=<AddBackward0>)\n",
      "epoch: 135 loss is tensor([0.7982], grad_fn=<AddBackward0>)\n",
      "epoch: 136 loss is tensor([0.9995], grad_fn=<AddBackward0>)\n",
      "epoch: 137 loss is tensor([0.8863], grad_fn=<AddBackward0>)\n",
      "epoch: 138 loss is tensor([0.9118], grad_fn=<AddBackward0>)\n",
      "epoch: 139 loss is tensor([0.9026], grad_fn=<AddBackward0>)\n",
      "epoch: 140 loss is tensor([0.9690], grad_fn=<AddBackward0>)\n",
      "epoch: 141 loss is tensor([0.8254], grad_fn=<AddBackward0>)\n",
      "epoch: 142 loss is tensor([0.8890], grad_fn=<AddBackward0>)\n",
      "epoch: 143 loss is tensor([0.8573], grad_fn=<AddBackward0>)\n",
      "epoch: 144 loss is tensor([0.8705], grad_fn=<AddBackward0>)\n",
      "epoch: 145 loss is tensor([0.8475], grad_fn=<AddBackward0>)\n",
      "epoch: 146 loss is tensor([0.8728], grad_fn=<AddBackward0>)\n",
      "epoch: 147 loss is tensor([0.8745], grad_fn=<AddBackward0>)\n",
      "epoch: 148 loss is tensor([0.8663], grad_fn=<AddBackward0>)\n",
      "epoch: 149 loss is tensor([0.7924], grad_fn=<AddBackward0>)\n",
      "epoch: 150 loss is tensor([0.8617], grad_fn=<AddBackward0>)\n",
      "epoch: 151 loss is tensor([0.7491], grad_fn=<AddBackward0>)\n",
      "epoch: 152 loss is tensor([0.9291], grad_fn=<AddBackward0>)\n",
      "epoch: 153 loss is tensor([0.7350], grad_fn=<AddBackward0>)\n",
      "epoch: 154 loss is tensor([0.7598], grad_fn=<AddBackward0>)\n",
      "epoch: 155 loss is tensor([0.7706], grad_fn=<AddBackward0>)\n",
      "epoch: 156 loss is tensor([0.7606], grad_fn=<AddBackward0>)\n",
      "epoch: 157 loss is tensor([0.8293], grad_fn=<AddBackward0>)\n",
      "epoch: 158 loss is tensor([0.8536], grad_fn=<AddBackward0>)\n",
      "epoch: 159 loss is tensor([0.8755], grad_fn=<AddBackward0>)\n",
      "epoch: 160 loss is tensor([0.8459], grad_fn=<AddBackward0>)\n",
      "epoch: 161 loss is tensor([0.8319], grad_fn=<AddBackward0>)\n",
      "epoch: 162 loss is tensor([0.7984], grad_fn=<AddBackward0>)\n",
      "epoch: 163 loss is tensor([0.8192], grad_fn=<AddBackward0>)\n",
      "epoch: 164 loss is tensor([0.7812], grad_fn=<AddBackward0>)\n",
      "epoch: 165 loss is tensor([0.7434], grad_fn=<AddBackward0>)\n",
      "epoch: 166 loss is tensor([0.7688], grad_fn=<AddBackward0>)\n",
      "epoch: 167 loss is tensor([0.8149], grad_fn=<AddBackward0>)\n",
      "epoch: 168 loss is tensor([0.8237], grad_fn=<AddBackward0>)\n",
      "epoch: 169 loss is tensor([0.7669], grad_fn=<AddBackward0>)\n",
      "epoch: 170 loss is tensor([0.7717], grad_fn=<AddBackward0>)\n",
      "epoch: 171 loss is tensor([0.8177], grad_fn=<AddBackward0>)\n",
      "epoch: 172 loss is tensor([0.8074], grad_fn=<AddBackward0>)\n",
      "epoch: 173 loss is tensor([0.8987], grad_fn=<AddBackward0>)\n",
      "epoch: 174 loss is tensor([0.8385], grad_fn=<AddBackward0>)\n",
      "epoch: 175 loss is tensor([0.6956], grad_fn=<AddBackward0>)\n",
      "epoch: 176 loss is tensor([0.7479], grad_fn=<AddBackward0>)\n",
      "epoch: 177 loss is tensor([0.8064], grad_fn=<AddBackward0>)\n",
      "epoch: 178 loss is tensor([0.6836], grad_fn=<AddBackward0>)\n",
      "epoch: 179 loss is tensor([0.8081], grad_fn=<AddBackward0>)\n",
      "epoch: 180 loss is tensor([0.7496], grad_fn=<AddBackward0>)\n",
      "epoch: 181 loss is tensor([0.7392], grad_fn=<AddBackward0>)\n",
      "epoch: 182 loss is tensor([0.7077], grad_fn=<AddBackward0>)\n",
      "epoch: 183 loss is tensor([0.7456], grad_fn=<AddBackward0>)\n",
      "epoch: 184 loss is tensor([0.7764], grad_fn=<AddBackward0>)\n",
      "epoch: 185 loss is tensor([0.7601], grad_fn=<AddBackward0>)\n",
      "epoch: 186 loss is tensor([0.7798], grad_fn=<AddBackward0>)\n",
      "epoch: 187 loss is tensor([0.7636], grad_fn=<AddBackward0>)\n",
      "epoch: 188 loss is tensor([0.7497], grad_fn=<AddBackward0>)\n",
      "epoch: 189 loss is tensor([0.7813], grad_fn=<AddBackward0>)\n",
      "epoch: 190 loss is tensor([0.7256], grad_fn=<AddBackward0>)\n",
      "epoch: 191 loss is tensor([0.7891], grad_fn=<AddBackward0>)\n",
      "epoch: 192 loss is tensor([0.7293], grad_fn=<AddBackward0>)\n",
      "epoch: 193 loss is tensor([0.7709], grad_fn=<AddBackward0>)\n",
      "epoch: 194 loss is tensor([0.7116], grad_fn=<AddBackward0>)\n",
      "epoch: 195 loss is tensor([0.7341], grad_fn=<AddBackward0>)\n",
      "epoch: 196 loss is tensor([0.7357], grad_fn=<AddBackward0>)\n",
      "epoch: 197 loss is tensor([0.7501], grad_fn=<AddBackward0>)\n",
      "epoch: 198 loss is tensor([0.7713], grad_fn=<AddBackward0>)\n",
      "epoch: 199 loss is tensor([0.7047], grad_fn=<AddBackward0>)\n",
      "epoch: 200 loss is tensor([0.7070], grad_fn=<AddBackward0>)\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 201 loss is tensor([0.7760], grad_fn=<AddBackward0>)\n",
      "epoch: 202 loss is tensor([0.7604], grad_fn=<AddBackward0>)\n",
      "epoch: 203 loss is tensor([0.7551], grad_fn=<AddBackward0>)\n",
      "epoch: 204 loss is tensor([0.6732], grad_fn=<AddBackward0>)\n",
      "epoch: 205 loss is tensor([0.7009], grad_fn=<AddBackward0>)\n",
      "epoch: 206 loss is tensor([0.7487], grad_fn=<AddBackward0>)\n",
      "epoch: 207 loss is tensor([0.7454], grad_fn=<AddBackward0>)\n",
      "epoch: 208 loss is tensor([0.7760], grad_fn=<AddBackward0>)\n",
      "epoch: 209 loss is tensor([0.6924], grad_fn=<AddBackward0>)\n",
      "epoch: 210 loss is tensor([0.7876], grad_fn=<AddBackward0>)\n",
      "epoch: 211 loss is tensor([0.7290], grad_fn=<AddBackward0>)\n",
      "epoch: 212 loss is tensor([0.6795], grad_fn=<AddBackward0>)\n",
      "epoch: 213 loss is tensor([0.7193], grad_fn=<AddBackward0>)\n",
      "epoch: 214 loss is tensor([0.7648], grad_fn=<AddBackward0>)\n",
      "epoch: 215 loss is tensor([0.7341], grad_fn=<AddBackward0>)\n",
      "epoch: 216 loss is tensor([0.6543], grad_fn=<AddBackward0>)\n",
      "epoch: 217 loss is tensor([0.6784], grad_fn=<AddBackward0>)\n",
      "epoch: 218 loss is tensor([0.7191], grad_fn=<AddBackward0>)\n",
      "epoch: 219 loss is tensor([0.7218], grad_fn=<AddBackward0>)\n",
      "epoch: 220 loss is tensor([0.7340], grad_fn=<AddBackward0>)\n",
      "epoch: 221 loss is tensor([0.7171], grad_fn=<AddBackward0>)\n",
      "epoch: 222 loss is tensor([0.7347], grad_fn=<AddBackward0>)\n",
      "epoch: 223 loss is tensor([0.8019], grad_fn=<AddBackward0>)\n",
      "epoch: 224 loss is tensor([0.7495], grad_fn=<AddBackward0>)\n",
      "epoch: 225 loss is tensor([0.7532], grad_fn=<AddBackward0>)\n",
      "epoch: 226 loss is tensor([0.6473], grad_fn=<AddBackward0>)\n",
      "epoch: 227 loss is tensor([0.7215], grad_fn=<AddBackward0>)\n",
      "epoch: 228 loss is tensor([0.6179], grad_fn=<AddBackward0>)\n",
      "epoch: 229 loss is tensor([0.7064], grad_fn=<AddBackward0>)\n",
      "epoch: 230 loss is tensor([0.7270], grad_fn=<AddBackward0>)\n",
      "epoch: 231 loss is tensor([0.7414], grad_fn=<AddBackward0>)\n",
      "epoch: 232 loss is tensor([0.6865], grad_fn=<AddBackward0>)\n",
      "epoch: 233 loss is tensor([0.7097], grad_fn=<AddBackward0>)\n",
      "epoch: 234 loss is tensor([0.7415], grad_fn=<AddBackward0>)\n",
      "epoch: 235 loss is tensor([0.6542], grad_fn=<AddBackward0>)\n",
      "epoch: 236 loss is tensor([0.6908], grad_fn=<AddBackward0>)\n",
      "epoch: 237 loss is tensor([0.7841], grad_fn=<AddBackward0>)\n",
      "epoch: 238 loss is tensor([0.7107], grad_fn=<AddBackward0>)\n",
      "epoch: 239 loss is tensor([0.6775], grad_fn=<AddBackward0>)\n",
      "epoch: 240 loss is tensor([0.7303], grad_fn=<AddBackward0>)\n",
      "epoch: 241 loss is tensor([0.6850], grad_fn=<AddBackward0>)\n",
      "epoch: 242 loss is tensor([0.6346], grad_fn=<AddBackward0>)\n",
      "epoch: 243 loss is tensor([0.6538], grad_fn=<AddBackward0>)\n",
      "epoch: 244 loss is tensor([0.7033], grad_fn=<AddBackward0>)\n",
      "epoch: 245 loss is tensor([0.6865], grad_fn=<AddBackward0>)\n",
      "epoch: 246 loss is tensor([0.6585], grad_fn=<AddBackward0>)\n",
      "epoch: 247 loss is tensor([0.6857], grad_fn=<AddBackward0>)\n",
      "epoch: 248 loss is tensor([0.7458], grad_fn=<AddBackward0>)\n",
      "epoch: 249 loss is tensor([0.6513], grad_fn=<AddBackward0>)\n",
      "epoch: 250 loss is tensor([0.6774], grad_fn=<AddBackward0>)\n",
      "epoch: 251 loss is tensor([0.6445], grad_fn=<AddBackward0>)\n",
      "epoch: 252 loss is tensor([0.6033], grad_fn=<AddBackward0>)\n",
      "epoch: 253 loss is tensor([0.6507], grad_fn=<AddBackward0>)\n",
      "epoch: 254 loss is tensor([0.7159], grad_fn=<AddBackward0>)\n",
      "epoch: 255 loss is tensor([0.6759], grad_fn=<AddBackward0>)\n",
      "epoch: 256 loss is tensor([0.8037], grad_fn=<AddBackward0>)\n",
      "epoch: 257 loss is tensor([0.6356], grad_fn=<AddBackward0>)\n",
      "epoch: 258 loss is tensor([0.6708], grad_fn=<AddBackward0>)\n",
      "epoch: 259 loss is tensor([0.6816], grad_fn=<AddBackward0>)\n",
      "epoch: 260 loss is tensor([0.6313], grad_fn=<AddBackward0>)\n",
      "epoch: 261 loss is tensor([0.6760], grad_fn=<AddBackward0>)\n",
      "epoch: 262 loss is tensor([0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 263 loss is tensor([0.6640], grad_fn=<AddBackward0>)\n",
      "epoch: 264 loss is tensor([0.6351], grad_fn=<AddBackward0>)\n",
      "epoch: 265 loss is tensor([0.6734], grad_fn=<AddBackward0>)\n",
      "epoch: 266 loss is tensor([0.6092], grad_fn=<AddBackward0>)\n",
      "epoch: 267 loss is tensor([0.6648], grad_fn=<AddBackward0>)\n",
      "epoch: 268 loss is tensor([0.6639], grad_fn=<AddBackward0>)\n",
      "epoch: 269 loss is tensor([0.5668], grad_fn=<AddBackward0>)\n",
      "epoch: 270 loss is tensor([0.6273], grad_fn=<AddBackward0>)\n",
      "epoch: 271 loss is tensor([0.6157], grad_fn=<AddBackward0>)\n",
      "epoch: 272 loss is tensor([0.6987], grad_fn=<AddBackward0>)\n",
      "epoch: 273 loss is tensor([0.6504], grad_fn=<AddBackward0>)\n",
      "epoch: 274 loss is tensor([0.6875], grad_fn=<AddBackward0>)\n",
      "epoch: 275 loss is tensor([0.6338], grad_fn=<AddBackward0>)\n",
      "epoch: 276 loss is tensor([0.6168], grad_fn=<AddBackward0>)\n",
      "epoch: 277 loss is tensor([0.6923], grad_fn=<AddBackward0>)\n",
      "epoch: 278 loss is tensor([0.6478], grad_fn=<AddBackward0>)\n",
      "epoch: 279 loss is tensor([0.6215], grad_fn=<AddBackward0>)\n",
      "epoch: 280 loss is tensor([0.6402], grad_fn=<AddBackward0>)\n",
      "epoch: 281 loss is tensor([0.6373], grad_fn=<AddBackward0>)\n",
      "epoch: 282 loss is tensor([0.7040], grad_fn=<AddBackward0>)\n",
      "epoch: 283 loss is tensor([0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 284 loss is tensor([0.7005], grad_fn=<AddBackward0>)\n",
      "epoch: 285 loss is tensor([0.6357], grad_fn=<AddBackward0>)\n",
      "epoch: 286 loss is tensor([0.5919], grad_fn=<AddBackward0>)\n",
      "epoch: 287 loss is tensor([0.6369], grad_fn=<AddBackward0>)\n",
      "epoch: 288 loss is tensor([0.6681], grad_fn=<AddBackward0>)\n",
      "epoch: 289 loss is tensor([0.6331], grad_fn=<AddBackward0>)\n",
      "epoch: 290 loss is tensor([0.6510], grad_fn=<AddBackward0>)\n",
      "epoch: 291 loss is tensor([0.6177], grad_fn=<AddBackward0>)\n",
      "epoch: 292 loss is tensor([0.5950], grad_fn=<AddBackward0>)\n",
      "epoch: 293 loss is tensor([0.6406], grad_fn=<AddBackward0>)\n",
      "epoch: 294 loss is tensor([0.6345], grad_fn=<AddBackward0>)\n",
      "epoch: 295 loss is tensor([0.5830], grad_fn=<AddBackward0>)\n",
      "epoch: 296 loss is tensor([0.6947], grad_fn=<AddBackward0>)\n",
      "epoch: 297 loss is tensor([0.6077], grad_fn=<AddBackward0>)\n",
      "epoch: 298 loss is tensor([0.6229], grad_fn=<AddBackward0>)\n",
      "epoch: 299 loss is tensor([0.7002], grad_fn=<AddBackward0>)\n",
      "epoch: 300 loss is tensor([0.6366], grad_fn=<AddBackward0>)\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 301 loss is tensor([0.6520], grad_fn=<AddBackward0>)\n",
      "epoch: 302 loss is tensor([0.6321], grad_fn=<AddBackward0>)\n",
      "epoch: 303 loss is tensor([0.6473], grad_fn=<AddBackward0>)\n",
      "epoch: 304 loss is tensor([0.6274], grad_fn=<AddBackward0>)\n",
      "epoch: 305 loss is tensor([0.6345], grad_fn=<AddBackward0>)\n",
      "epoch: 306 loss is tensor([0.5647], grad_fn=<AddBackward0>)\n",
      "epoch: 307 loss is tensor([0.6821], grad_fn=<AddBackward0>)\n",
      "epoch: 308 loss is tensor([0.5284], grad_fn=<AddBackward0>)\n",
      "epoch: 309 loss is tensor([0.6945], grad_fn=<AddBackward0>)\n",
      "epoch: 310 loss is tensor([0.6266], grad_fn=<AddBackward0>)\n",
      "epoch: 311 loss is tensor([0.6372], grad_fn=<AddBackward0>)\n",
      "epoch: 312 loss is tensor([0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 313 loss is tensor([0.6397], grad_fn=<AddBackward0>)\n",
      "epoch: 314 loss is tensor([0.5683], grad_fn=<AddBackward0>)\n",
      "epoch: 315 loss is tensor([0.6382], grad_fn=<AddBackward0>)\n",
      "epoch: 316 loss is tensor([0.6158], grad_fn=<AddBackward0>)\n",
      "epoch: 317 loss is tensor([0.5978], grad_fn=<AddBackward0>)\n",
      "epoch: 318 loss is tensor([0.5514], grad_fn=<AddBackward0>)\n",
      "epoch: 319 loss is tensor([0.6278], grad_fn=<AddBackward0>)\n",
      "epoch: 320 loss is tensor([0.6341], grad_fn=<AddBackward0>)\n",
      "epoch: 321 loss is tensor([0.6946], grad_fn=<AddBackward0>)\n",
      "epoch: 322 loss is tensor([0.6166], grad_fn=<AddBackward0>)\n",
      "epoch: 323 loss is tensor([0.6575], grad_fn=<AddBackward0>)\n",
      "epoch: 324 loss is tensor([0.6017], grad_fn=<AddBackward0>)\n",
      "epoch: 325 loss is tensor([0.6272], grad_fn=<AddBackward0>)\n",
      "epoch: 326 loss is tensor([0.6210], grad_fn=<AddBackward0>)\n",
      "epoch: 327 loss is tensor([0.6056], grad_fn=<AddBackward0>)\n",
      "epoch: 328 loss is tensor([0.7160], grad_fn=<AddBackward0>)\n",
      "epoch: 329 loss is tensor([0.5759], grad_fn=<AddBackward0>)\n",
      "epoch: 330 loss is tensor([0.5526], grad_fn=<AddBackward0>)\n",
      "epoch: 331 loss is tensor([0.6611], grad_fn=<AddBackward0>)\n",
      "epoch: 332 loss is tensor([0.6384], grad_fn=<AddBackward0>)\n",
      "epoch: 333 loss is tensor([0.5935], grad_fn=<AddBackward0>)\n",
      "epoch: 334 loss is tensor([0.6585], grad_fn=<AddBackward0>)\n",
      "epoch: 335 loss is tensor([0.6021], grad_fn=<AddBackward0>)\n",
      "epoch: 336 loss is tensor([0.6939], grad_fn=<AddBackward0>)\n",
      "epoch: 337 loss is tensor([0.5958], grad_fn=<AddBackward0>)\n",
      "epoch: 338 loss is tensor([0.5934], grad_fn=<AddBackward0>)\n",
      "epoch: 339 loss is tensor([0.6457], grad_fn=<AddBackward0>)\n",
      "epoch: 340 loss is tensor([0.6450], grad_fn=<AddBackward0>)\n",
      "epoch: 341 loss is tensor([0.5690], grad_fn=<AddBackward0>)\n",
      "epoch: 342 loss is tensor([0.6118], grad_fn=<AddBackward0>)\n",
      "epoch: 343 loss is tensor([0.6218], grad_fn=<AddBackward0>)\n",
      "epoch: 344 loss is tensor([0.5948], grad_fn=<AddBackward0>)\n",
      "epoch: 345 loss is tensor([0.6224], grad_fn=<AddBackward0>)\n",
      "epoch: 346 loss is tensor([0.5323], grad_fn=<AddBackward0>)\n",
      "epoch: 347 loss is tensor([0.5675], grad_fn=<AddBackward0>)\n",
      "epoch: 348 loss is tensor([0.5466], grad_fn=<AddBackward0>)\n",
      "epoch: 349 loss is tensor([0.5959], grad_fn=<AddBackward0>)\n",
      "epoch: 350 loss is tensor([0.5376], grad_fn=<AddBackward0>)\n",
      "epoch: 351 loss is tensor([0.5702], grad_fn=<AddBackward0>)\n",
      "epoch: 352 loss is tensor([0.5765], grad_fn=<AddBackward0>)\n",
      "epoch: 353 loss is tensor([0.6201], grad_fn=<AddBackward0>)\n",
      "epoch: 354 loss is tensor([0.5327], grad_fn=<AddBackward0>)\n",
      "epoch: 355 loss is tensor([0.5985], grad_fn=<AddBackward0>)\n",
      "epoch: 356 loss is tensor([0.6291], grad_fn=<AddBackward0>)\n",
      "epoch: 357 loss is tensor([0.6244], grad_fn=<AddBackward0>)\n",
      "epoch: 358 loss is tensor([0.7494], grad_fn=<AddBackward0>)\n",
      "epoch: 359 loss is tensor([0.6192], grad_fn=<AddBackward0>)\n",
      "epoch: 360 loss is tensor([0.6060], grad_fn=<AddBackward0>)\n",
      "epoch: 361 loss is tensor([0.5105], grad_fn=<AddBackward0>)\n",
      "epoch: 362 loss is tensor([0.6069], grad_fn=<AddBackward0>)\n",
      "epoch: 363 loss is tensor([0.6209], grad_fn=<AddBackward0>)\n",
      "epoch: 364 loss is tensor([0.5937], grad_fn=<AddBackward0>)\n",
      "epoch: 365 loss is tensor([0.6417], grad_fn=<AddBackward0>)\n",
      "epoch: 366 loss is tensor([0.5834], grad_fn=<AddBackward0>)\n",
      "epoch: 367 loss is tensor([0.6734], grad_fn=<AddBackward0>)\n",
      "epoch: 368 loss is tensor([0.5992], grad_fn=<AddBackward0>)\n",
      "epoch: 369 loss is tensor([0.5070], grad_fn=<AddBackward0>)\n",
      "epoch: 370 loss is tensor([0.5771], grad_fn=<AddBackward0>)\n",
      "epoch: 371 loss is tensor([0.5634], grad_fn=<AddBackward0>)\n",
      "epoch: 372 loss is tensor([0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 373 loss is tensor([0.5939], grad_fn=<AddBackward0>)\n",
      "epoch: 374 loss is tensor([0.5573], grad_fn=<AddBackward0>)\n",
      "epoch: 375 loss is tensor([0.5678], grad_fn=<AddBackward0>)\n",
      "epoch: 376 loss is tensor([0.4937], grad_fn=<AddBackward0>)\n",
      "epoch: 377 loss is tensor([0.5541], grad_fn=<AddBackward0>)\n",
      "epoch: 378 loss is tensor([0.5597], grad_fn=<AddBackward0>)\n",
      "epoch: 379 loss is tensor([0.5866], grad_fn=<AddBackward0>)\n",
      "epoch: 380 loss is tensor([0.5901], grad_fn=<AddBackward0>)\n",
      "epoch: 381 loss is tensor([0.5822], grad_fn=<AddBackward0>)\n",
      "epoch: 382 loss is tensor([0.5308], grad_fn=<AddBackward0>)\n",
      "epoch: 383 loss is tensor([0.5362], grad_fn=<AddBackward0>)\n",
      "epoch: 384 loss is tensor([0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 385 loss is tensor([0.6018], grad_fn=<AddBackward0>)\n",
      "epoch: 386 loss is tensor([0.5682], grad_fn=<AddBackward0>)\n",
      "epoch: 387 loss is tensor([0.6222], grad_fn=<AddBackward0>)\n",
      "epoch: 388 loss is tensor([0.5386], grad_fn=<AddBackward0>)\n",
      "epoch: 389 loss is tensor([0.5619], grad_fn=<AddBackward0>)\n",
      "epoch: 390 loss is tensor([0.5694], grad_fn=<AddBackward0>)\n",
      "epoch: 391 loss is tensor([0.5826], grad_fn=<AddBackward0>)\n",
      "epoch: 392 loss is tensor([0.6168], grad_fn=<AddBackward0>)\n",
      "epoch: 393 loss is tensor([0.6455], grad_fn=<AddBackward0>)\n",
      "epoch: 394 loss is tensor([0.5469], grad_fn=<AddBackward0>)\n",
      "epoch: 395 loss is tensor([0.5953], grad_fn=<AddBackward0>)\n",
      "epoch: 396 loss is tensor([0.5877], grad_fn=<AddBackward0>)\n",
      "epoch: 397 loss is tensor([0.6253], grad_fn=<AddBackward0>)\n",
      "epoch: 398 loss is tensor([0.5644], grad_fn=<AddBackward0>)\n",
      "epoch: 399 loss is tensor([0.5470], grad_fn=<AddBackward0>)\n",
      "epoch: 400 loss is tensor([0.5908], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 401 loss is tensor([0.5507], grad_fn=<AddBackward0>)\n",
      "epoch: 402 loss is tensor([0.6228], grad_fn=<AddBackward0>)\n",
      "epoch: 403 loss is tensor([0.5532], grad_fn=<AddBackward0>)\n",
      "epoch: 404 loss is tensor([0.5587], grad_fn=<AddBackward0>)\n",
      "epoch: 405 loss is tensor([0.5616], grad_fn=<AddBackward0>)\n",
      "epoch: 406 loss is tensor([0.5808], grad_fn=<AddBackward0>)\n",
      "epoch: 407 loss is tensor([0.5840], grad_fn=<AddBackward0>)\n",
      "epoch: 408 loss is tensor([0.5900], grad_fn=<AddBackward0>)\n",
      "epoch: 409 loss is tensor([0.4918], grad_fn=<AddBackward0>)\n",
      "epoch: 410 loss is tensor([0.5763], grad_fn=<AddBackward0>)\n",
      "epoch: 411 loss is tensor([0.4934], grad_fn=<AddBackward0>)\n",
      "epoch: 412 loss is tensor([0.5235], grad_fn=<AddBackward0>)\n",
      "epoch: 413 loss is tensor([0.5506], grad_fn=<AddBackward0>)\n",
      "epoch: 414 loss is tensor([0.6247], grad_fn=<AddBackward0>)\n",
      "epoch: 415 loss is tensor([0.5282], grad_fn=<AddBackward0>)\n",
      "epoch: 416 loss is tensor([0.5452], grad_fn=<AddBackward0>)\n",
      "epoch: 417 loss is tensor([0.5840], grad_fn=<AddBackward0>)\n",
      "epoch: 418 loss is tensor([0.4998], grad_fn=<AddBackward0>)\n",
      "epoch: 419 loss is tensor([0.5397], grad_fn=<AddBackward0>)\n",
      "epoch: 420 loss is tensor([0.5464], grad_fn=<AddBackward0>)\n",
      "epoch: 421 loss is tensor([0.6009], grad_fn=<AddBackward0>)\n",
      "epoch: 422 loss is tensor([0.5698], grad_fn=<AddBackward0>)\n",
      "epoch: 423 loss is tensor([0.4991], grad_fn=<AddBackward0>)\n",
      "epoch: 424 loss is tensor([0.5784], grad_fn=<AddBackward0>)\n",
      "epoch: 425 loss is tensor([0.5895], grad_fn=<AddBackward0>)\n",
      "epoch: 426 loss is tensor([0.5392], grad_fn=<AddBackward0>)\n",
      "epoch: 427 loss is tensor([0.5792], grad_fn=<AddBackward0>)\n",
      "epoch: 428 loss is tensor([0.5627], grad_fn=<AddBackward0>)\n",
      "epoch: 429 loss is tensor([0.5106], grad_fn=<AddBackward0>)\n",
      "epoch: 430 loss is tensor([0.5042], grad_fn=<AddBackward0>)\n",
      "epoch: 431 loss is tensor([0.4926], grad_fn=<AddBackward0>)\n",
      "epoch: 432 loss is tensor([0.5324], grad_fn=<AddBackward0>)\n",
      "epoch: 433 loss is tensor([0.5297], grad_fn=<AddBackward0>)\n",
      "epoch: 434 loss is tensor([0.5203], grad_fn=<AddBackward0>)\n",
      "epoch: 435 loss is tensor([0.4930], grad_fn=<AddBackward0>)\n",
      "epoch: 436 loss is tensor([0.4981], grad_fn=<AddBackward0>)\n",
      "epoch: 437 loss is tensor([0.6018], grad_fn=<AddBackward0>)\n",
      "epoch: 438 loss is tensor([0.5410], grad_fn=<AddBackward0>)\n",
      "epoch: 439 loss is tensor([0.4462], grad_fn=<AddBackward0>)\n",
      "epoch: 440 loss is tensor([0.5152], grad_fn=<AddBackward0>)\n",
      "epoch: 441 loss is tensor([0.5816], grad_fn=<AddBackward0>)\n",
      "epoch: 442 loss is tensor([0.5224], grad_fn=<AddBackward0>)\n",
      "epoch: 443 loss is tensor([0.4540], grad_fn=<AddBackward0>)\n",
      "epoch: 444 loss is tensor([0.5448], grad_fn=<AddBackward0>)\n",
      "epoch: 445 loss is tensor([0.4546], grad_fn=<AddBackward0>)\n",
      "epoch: 446 loss is tensor([0.5651], grad_fn=<AddBackward0>)\n",
      "epoch: 447 loss is tensor([0.5176], grad_fn=<AddBackward0>)\n",
      "epoch: 448 loss is tensor([0.5165], grad_fn=<AddBackward0>)\n",
      "epoch: 449 loss is tensor([0.5495], grad_fn=<AddBackward0>)\n",
      "epoch: 450 loss is tensor([0.5595], grad_fn=<AddBackward0>)\n",
      "epoch: 451 loss is tensor([0.6142], grad_fn=<AddBackward0>)\n",
      "epoch: 452 loss is tensor([0.4728], grad_fn=<AddBackward0>)\n",
      "epoch: 453 loss is tensor([0.5107], grad_fn=<AddBackward0>)\n",
      "epoch: 454 loss is tensor([0.5908], grad_fn=<AddBackward0>)\n",
      "epoch: 455 loss is tensor([0.5098], grad_fn=<AddBackward0>)\n",
      "epoch: 456 loss is tensor([0.4739], grad_fn=<AddBackward0>)\n",
      "epoch: 457 loss is tensor([0.5183], grad_fn=<AddBackward0>)\n",
      "epoch: 458 loss is tensor([0.5800], grad_fn=<AddBackward0>)\n",
      "epoch: 459 loss is tensor([0.5838], grad_fn=<AddBackward0>)\n",
      "epoch: 460 loss is tensor([0.5854], grad_fn=<AddBackward0>)\n",
      "epoch: 461 loss is tensor([0.4993], grad_fn=<AddBackward0>)\n",
      "epoch: 462 loss is tensor([0.5539], grad_fn=<AddBackward0>)\n",
      "epoch: 463 loss is tensor([0.5168], grad_fn=<AddBackward0>)\n",
      "epoch: 464 loss is tensor([0.5335], grad_fn=<AddBackward0>)\n",
      "epoch: 465 loss is tensor([0.5149], grad_fn=<AddBackward0>)\n",
      "epoch: 466 loss is tensor([0.5500], grad_fn=<AddBackward0>)\n",
      "epoch: 467 loss is tensor([0.4971], grad_fn=<AddBackward0>)\n",
      "epoch: 468 loss is tensor([0.5876], grad_fn=<AddBackward0>)\n",
      "epoch: 469 loss is tensor([0.5543], grad_fn=<AddBackward0>)\n",
      "epoch: 470 loss is tensor([0.5872], grad_fn=<AddBackward0>)\n",
      "epoch: 471 loss is tensor([0.4589], grad_fn=<AddBackward0>)\n",
      "epoch: 472 loss is tensor([0.4760], grad_fn=<AddBackward0>)\n",
      "epoch: 473 loss is tensor([0.5899], grad_fn=<AddBackward0>)\n",
      "epoch: 474 loss is tensor([0.4919], grad_fn=<AddBackward0>)\n",
      "epoch: 475 loss is tensor([0.4934], grad_fn=<AddBackward0>)\n",
      "epoch: 476 loss is tensor([0.4996], grad_fn=<AddBackward0>)\n",
      "epoch: 477 loss is tensor([0.4668], grad_fn=<AddBackward0>)\n",
      "epoch: 478 loss is tensor([0.4894], grad_fn=<AddBackward0>)\n",
      "epoch: 479 loss is tensor([0.4567], grad_fn=<AddBackward0>)\n",
      "epoch: 480 loss is tensor([0.5008], grad_fn=<AddBackward0>)\n",
      "epoch: 481 loss is tensor([0.4870], grad_fn=<AddBackward0>)\n",
      "epoch: 482 loss is tensor([0.5213], grad_fn=<AddBackward0>)\n",
      "epoch: 483 loss is tensor([0.5424], grad_fn=<AddBackward0>)\n",
      "epoch: 484 loss is tensor([0.5142], grad_fn=<AddBackward0>)\n",
      "epoch: 485 loss is tensor([0.4734], grad_fn=<AddBackward0>)\n",
      "epoch: 486 loss is tensor([0.4550], grad_fn=<AddBackward0>)\n",
      "epoch: 487 loss is tensor([0.4697], grad_fn=<AddBackward0>)\n",
      "epoch: 488 loss is tensor([0.4509], grad_fn=<AddBackward0>)\n",
      "epoch: 489 loss is tensor([0.4574], grad_fn=<AddBackward0>)\n",
      "epoch: 490 loss is tensor([0.5324], grad_fn=<AddBackward0>)\n",
      "epoch: 491 loss is tensor([0.4596], grad_fn=<AddBackward0>)\n",
      "epoch: 492 loss is tensor([0.4623], grad_fn=<AddBackward0>)\n",
      "epoch: 493 loss is tensor([0.5828], grad_fn=<AddBackward0>)\n",
      "epoch: 494 loss is tensor([0.5433], grad_fn=<AddBackward0>)\n",
      "epoch: 495 loss is tensor([0.5023], grad_fn=<AddBackward0>)\n",
      "epoch: 496 loss is tensor([0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 497 loss is tensor([0.5139], grad_fn=<AddBackward0>)\n",
      "epoch: 498 loss is tensor([0.4974], grad_fn=<AddBackward0>)\n",
      "epoch: 499 loss is tensor([0.4798], grad_fn=<AddBackward0>)\n",
      "epoch: 500 loss is tensor([0.4630], grad_fn=<AddBackward0>)\n",
      "26\n"
=======
      "The number of epochs is: 201\n",
      "The number of epochs is: 202\n",
      "The number of epochs is: 203\n",
      "The number of epochs is: 204\n",
      "The number of epochs is: 205\n",
      "The number of epochs is: 206\n",
      "The number of epochs is: 207\n",
      "The number of epochs is: 208\n",
      "The number of epochs is: 209\n",
      "The number of epochs is: 210\n",
      "The number of epochs is: 211\n",
      "The number of epochs is: 212\n",
      "The number of epochs is: 213\n",
      "The number of epochs is: 214\n",
      "The number of epochs is: 215\n",
      "The number of epochs is: 216\n",
      "The number of epochs is: 217\n",
      "The number of epochs is: 218\n",
      "The number of epochs is: 219\n",
      "The number of epochs is: 220\n",
      "The number of epochs is: 221\n",
      "The number of epochs is: 222\n",
      "The number of epochs is: 223\n",
      "The number of epochs is: 224\n",
      "The number of epochs is: 225\n",
      "The number of epochs is: 226\n",
      "The number of epochs is: 227\n",
      "The number of epochs is: 228\n",
      "The number of epochs is: 229\n",
      "The number of epochs is: 230\n",
      "The number of epochs is: 231\n",
      "The number of epochs is: 232\n",
      "The number of epochs is: 233\n",
      "The number of epochs is: 234\n",
      "The number of epochs is: 235\n",
      "The number of epochs is: 236\n",
      "The number of epochs is: 237\n",
      "The number of epochs is: 238\n",
      "The number of epochs is: 239\n",
      "The number of epochs is: 240\n",
      "The number of epochs is: 241\n",
      "The number of epochs is: 242\n",
      "The number of epochs is: 243\n",
      "The number of epochs is: 244\n",
      "The number of epochs is: 245\n",
      "The number of epochs is: 246\n",
      "The number of epochs is: 247\n",
      "The number of epochs is: 248\n",
      "The number of epochs is: 249\n",
      "The number of epochs is: 250\n",
      "The number of epochs is: 251\n",
      "The number of epochs is: 252\n",
      "The number of epochs is: 253\n",
      "The number of epochs is: 254\n",
      "The number of epochs is: 255\n",
      "The number of epochs is: 256\n",
      "The number of epochs is: 257\n",
      "The number of epochs is: 258\n",
      "The number of epochs is: 259\n",
      "The number of epochs is: 260\n",
      "The number of epochs is: 261\n",
      "The number of epochs is: 262\n",
      "The number of epochs is: 263\n",
      "The number of epochs is: 264\n",
      "The number of epochs is: 265\n",
      "The number of epochs is: 266\n",
      "The number of epochs is: 267\n",
      "The number of epochs is: 268\n",
      "The number of epochs is: 269\n",
      "The number of epochs is: 270\n",
      "The number of epochs is: 271\n",
      "The number of epochs is: 272\n",
      "The number of epochs is: 273\n",
      "The number of epochs is: 274\n",
      "The number of epochs is: 275\n",
      "The number of epochs is: 276\n",
      "The number of epochs is: 277\n",
      "The number of epochs is: 278\n",
      "The number of epochs is: 279\n",
      "The number of epochs is: 280\n",
      "The number of epochs is: 281\n",
      "The number of epochs is: 282\n",
      "The number of epochs is: 283\n",
      "The number of epochs is: 284\n",
      "The number of epochs is: 285\n",
      "The number of epochs is: 286\n",
      "The number of epochs is: 287\n",
      "The number of epochs is: 288\n",
      "The number of epochs is: 289\n",
      "The number of epochs is: 290\n",
      "The number of epochs is: 291\n",
      "The number of epochs is: 292\n",
      "The number of epochs is: 293\n",
      "The number of epochs is: 294\n",
      "The number of epochs is: 295\n",
      "The number of epochs is: 296\n",
      "The number of epochs is: 297\n",
      "The number of epochs is: 298\n",
      "The number of epochs is: 299\n",
      "The number of epochs is: 300\n",
      "The number of epochs is: 301\n",
      "The number of epochs is: 302\n",
      "The number of epochs is: 303\n",
      "The number of epochs is: 304\n",
      "The number of epochs is: 305\n",
      "The number of epochs is: 306\n",
      "The number of epochs is: 307\n",
      "The number of epochs is: 308\n",
      "The number of epochs is: 309\n",
      "The number of epochs is: 310\n",
      "The number of epochs is: 311\n",
      "The number of epochs is: 312\n",
      "The number of epochs is: 313\n",
      "The number of epochs is: 314\n",
      "The number of epochs is: 315\n",
      "The number of epochs is: 316\n",
      "The number of epochs is: 317\n",
      "The number of epochs is: 318\n",
      "The number of epochs is: 319\n",
      "The number of epochs is: 320\n",
      "The number of epochs is: 321\n",
      "The number of epochs is: 322\n",
      "The number of epochs is: 323\n",
      "The number of epochs is: 324\n",
      "The number of epochs is: 325\n",
      "The number of epochs is: 326\n",
      "The number of epochs is: 327\n",
      "The number of epochs is: 328\n",
      "The number of epochs is: 329\n",
      "The number of epochs is: 330\n",
      "The number of epochs is: 331\n",
      "The number of epochs is: 332\n",
      "The number of epochs is: 333\n",
      "The number of epochs is: 334\n",
      "The number of epochs is: 335\n",
      "The number of epochs is: 336\n",
      "The number of epochs is: 337\n",
      "The number of epochs is: 338\n",
      "The number of epochs is: 339\n",
      "The number of epochs is: 340\n",
      "The number of epochs is: 341\n",
      "The number of epochs is: 342\n",
      "The number of epochs is: 343\n",
      "The number of epochs is: 344\n",
      "The number of epochs is: 345\n",
      "The number of epochs is: 346\n",
      "The number of epochs is: 347\n",
      "The number of epochs is: 348\n",
      "The number of epochs is: 349\n",
      "The number of epochs is: 350\n",
      "The number of epochs is: 351\n",
      "The number of epochs is: 352\n",
      "The number of epochs is: 353\n",
      "The number of epochs is: 354\n",
      "The number of epochs is: 355\n",
      "The number of epochs is: 356\n",
      "The number of epochs is: 357\n",
      "The number of epochs is: 358\n",
      "The number of epochs is: 359\n",
      "The number of epochs is: 360\n",
      "The number of epochs is: 361\n",
      "The number of epochs is: 362\n",
      "The number of epochs is: 363\n",
      "The number of epochs is: 364\n",
      "The number of epochs is: 365\n",
      "The number of epochs is: 366\n",
      "The number of epochs is: 367\n",
      "The number of epochs is: 368\n",
      "The number of epochs is: 369\n",
      "The number of epochs is: 370\n",
      "The number of epochs is: 371\n",
      "The number of epochs is: 372\n",
      "The number of epochs is: 373\n",
      "The number of epochs is: 374\n",
      "The number of epochs is: 375\n",
      "The number of epochs is: 376\n",
      "The number of epochs is: 377\n",
      "The number of epochs is: 378\n",
      "The number of epochs is: 379\n",
      "The number of epochs is: 380\n",
      "The number of epochs is: 381\n",
      "The number of epochs is: 382\n",
      "The number of epochs is: 383\n",
      "The number of epochs is: 384\n",
      "The number of epochs is: 385\n",
      "The number of epochs is: 386\n",
      "The number of epochs is: 387\n",
      "The number of epochs is: 388\n",
      "The number of epochs is: 389\n",
      "The number of epochs is: 390\n",
      "The number of epochs is: 391\n",
      "The number of epochs is: 392\n",
      "The number of epochs is: 393\n",
      "The number of epochs is: 394\n",
      "The number of epochs is: 395\n",
      "The number of epochs is: 396\n",
      "The number of epochs is: 397\n",
      "The number of epochs is: 398\n",
      "The number of epochs is: 399\n",
      "The number of epochs is: 400\n",
      "33\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 501 loss is tensor([0.4962], grad_fn=<AddBackward0>)\n",
      "epoch: 502 loss is tensor([0.4174], grad_fn=<AddBackward0>)\n",
      "epoch: 503 loss is tensor([0.4273], grad_fn=<AddBackward0>)\n",
      "epoch: 504 loss is tensor([0.4475], grad_fn=<AddBackward0>)\n",
      "epoch: 505 loss is tensor([0.5138], grad_fn=<AddBackward0>)\n",
      "epoch: 506 loss is tensor([0.4657], grad_fn=<AddBackward0>)\n",
      "epoch: 507 loss is tensor([0.5155], grad_fn=<AddBackward0>)\n",
      "epoch: 508 loss is tensor([0.4364], grad_fn=<AddBackward0>)\n",
      "epoch: 509 loss is tensor([0.4752], grad_fn=<AddBackward0>)\n",
      "epoch: 510 loss is tensor([0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 511 loss is tensor([0.4879], grad_fn=<AddBackward0>)\n",
      "epoch: 512 loss is tensor([0.5168], grad_fn=<AddBackward0>)\n",
      "epoch: 513 loss is tensor([0.4819], grad_fn=<AddBackward0>)\n",
      "epoch: 514 loss is tensor([0.4685], grad_fn=<AddBackward0>)\n",
      "epoch: 515 loss is tensor([0.4646], grad_fn=<AddBackward0>)\n",
      "epoch: 516 loss is tensor([0.5199], grad_fn=<AddBackward0>)\n",
      "epoch: 517 loss is tensor([0.4870], grad_fn=<AddBackward0>)\n",
      "epoch: 518 loss is tensor([0.4765], grad_fn=<AddBackward0>)\n",
      "epoch: 519 loss is tensor([0.5016], grad_fn=<AddBackward0>)\n",
      "epoch: 520 loss is tensor([0.4586], grad_fn=<AddBackward0>)\n",
      "epoch: 521 loss is tensor([0.4778], grad_fn=<AddBackward0>)\n",
      "epoch: 522 loss is tensor([0.4746], grad_fn=<AddBackward0>)\n",
      "epoch: 523 loss is tensor([0.4728], grad_fn=<AddBackward0>)\n",
      "epoch: 524 loss is tensor([0.4905], grad_fn=<AddBackward0>)\n",
      "epoch: 525 loss is tensor([0.3948], grad_fn=<AddBackward0>)\n",
      "epoch: 526 loss is tensor([0.4009], grad_fn=<AddBackward0>)\n",
      "epoch: 527 loss is tensor([0.4684], grad_fn=<AddBackward0>)\n",
      "epoch: 528 loss is tensor([0.5275], grad_fn=<AddBackward0>)\n",
      "epoch: 529 loss is tensor([0.4915], grad_fn=<AddBackward0>)\n",
      "epoch: 530 loss is tensor([0.4585], grad_fn=<AddBackward0>)\n",
      "epoch: 531 loss is tensor([0.4447], grad_fn=<AddBackward0>)\n",
      "epoch: 532 loss is tensor([0.4280], grad_fn=<AddBackward0>)\n",
      "epoch: 533 loss is tensor([0.4615], grad_fn=<AddBackward0>)\n",
      "epoch: 534 loss is tensor([0.4791], grad_fn=<AddBackward0>)\n",
      "epoch: 535 loss is tensor([0.4833], grad_fn=<AddBackward0>)\n",
      "epoch: 536 loss is tensor([0.4346], grad_fn=<AddBackward0>)\n",
      "epoch: 537 loss is tensor([0.5181], grad_fn=<AddBackward0>)\n",
      "epoch: 538 loss is tensor([0.4504], grad_fn=<AddBackward0>)\n",
      "epoch: 539 loss is tensor([0.4695], grad_fn=<AddBackward0>)\n",
      "epoch: 540 loss is tensor([0.4647], grad_fn=<AddBackward0>)\n",
      "epoch: 541 loss is tensor([0.4449], grad_fn=<AddBackward0>)\n",
      "epoch: 542 loss is tensor([0.5221], grad_fn=<AddBackward0>)\n",
      "epoch: 543 loss is tensor([0.3923], grad_fn=<AddBackward0>)\n",
      "epoch: 544 loss is tensor([0.4437], grad_fn=<AddBackward0>)\n",
      "epoch: 545 loss is tensor([0.4331], grad_fn=<AddBackward0>)\n",
      "epoch: 546 loss is tensor([0.4031], grad_fn=<AddBackward0>)\n",
      "epoch: 547 loss is tensor([0.4863], grad_fn=<AddBackward0>)\n",
      "epoch: 548 loss is tensor([0.4387], grad_fn=<AddBackward0>)\n",
      "epoch: 549 loss is tensor([0.4602], grad_fn=<AddBackward0>)\n",
      "epoch: 550 loss is tensor([0.4896], grad_fn=<AddBackward0>)\n",
      "epoch: 551 loss is tensor([0.4498], grad_fn=<AddBackward0>)\n",
      "epoch: 552 loss is tensor([0.4582], grad_fn=<AddBackward0>)\n",
      "epoch: 553 loss is tensor([0.5065], grad_fn=<AddBackward0>)\n",
      "epoch: 554 loss is tensor([0.5433], grad_fn=<AddBackward0>)\n",
      "epoch: 555 loss is tensor([0.5319], grad_fn=<AddBackward0>)\n",
      "epoch: 556 loss is tensor([0.5279], grad_fn=<AddBackward0>)\n",
      "epoch: 557 loss is tensor([0.4173], grad_fn=<AddBackward0>)\n",
      "epoch: 558 loss is tensor([0.4989], grad_fn=<AddBackward0>)\n",
      "epoch: 559 loss is tensor([0.5408], grad_fn=<AddBackward0>)\n",
      "epoch: 560 loss is tensor([0.4911], grad_fn=<AddBackward0>)\n",
      "epoch: 561 loss is tensor([0.4844], grad_fn=<AddBackward0>)\n",
      "epoch: 562 loss is tensor([0.4877], grad_fn=<AddBackward0>)\n",
      "epoch: 563 loss is tensor([0.4447], grad_fn=<AddBackward0>)\n",
      "epoch: 564 loss is tensor([0.4587], grad_fn=<AddBackward0>)\n",
      "epoch: 565 loss is tensor([0.4357], grad_fn=<AddBackward0>)\n",
      "epoch: 566 loss is tensor([0.5041], grad_fn=<AddBackward0>)\n",
      "epoch: 567 loss is tensor([0.4351], grad_fn=<AddBackward0>)\n",
      "epoch: 568 loss is tensor([0.4643], grad_fn=<AddBackward0>)\n",
      "epoch: 569 loss is tensor([0.5014], grad_fn=<AddBackward0>)\n",
      "epoch: 570 loss is tensor([0.5010], grad_fn=<AddBackward0>)\n",
      "epoch: 571 loss is tensor([0.4633], grad_fn=<AddBackward0>)\n",
      "epoch: 572 loss is tensor([0.4070], grad_fn=<AddBackward0>)\n",
      "epoch: 573 loss is tensor([0.4583], grad_fn=<AddBackward0>)\n",
      "epoch: 574 loss is tensor([0.5019], grad_fn=<AddBackward0>)\n",
      "epoch: 575 loss is tensor([0.4517], grad_fn=<AddBackward0>)\n",
      "epoch: 576 loss is tensor([0.4270], grad_fn=<AddBackward0>)\n",
      "epoch: 577 loss is tensor([0.4088], grad_fn=<AddBackward0>)\n",
      "epoch: 578 loss is tensor([0.5160], grad_fn=<AddBackward0>)\n",
      "epoch: 579 loss is tensor([0.4724], grad_fn=<AddBackward0>)\n",
      "epoch: 580 loss is tensor([0.4895], grad_fn=<AddBackward0>)\n",
      "epoch: 581 loss is tensor([0.5339], grad_fn=<AddBackward0>)\n",
      "epoch: 582 loss is tensor([0.4571], grad_fn=<AddBackward0>)\n",
      "epoch: 583 loss is tensor([0.4357], grad_fn=<AddBackward0>)\n",
      "epoch: 584 loss is tensor([0.4227], grad_fn=<AddBackward0>)\n",
      "epoch: 585 loss is tensor([0.4087], grad_fn=<AddBackward0>)\n",
      "epoch: 586 loss is tensor([0.4340], grad_fn=<AddBackward0>)\n",
      "epoch: 587 loss is tensor([0.4803], grad_fn=<AddBackward0>)\n",
      "epoch: 588 loss is tensor([0.4439], grad_fn=<AddBackward0>)\n",
      "epoch: 589 loss is tensor([0.4622], grad_fn=<AddBackward0>)\n",
      "epoch: 590 loss is tensor([0.5037], grad_fn=<AddBackward0>)\n",
      "epoch: 591 loss is tensor([0.4164], grad_fn=<AddBackward0>)\n",
      "epoch: 592 loss is tensor([0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 593 loss is tensor([0.4792], grad_fn=<AddBackward0>)\n",
      "epoch: 594 loss is tensor([0.4432], grad_fn=<AddBackward0>)\n",
      "epoch: 595 loss is tensor([0.4577], grad_fn=<AddBackward0>)\n",
      "epoch: 596 loss is tensor([0.4255], grad_fn=<AddBackward0>)\n",
      "epoch: 597 loss is tensor([0.4746], grad_fn=<AddBackward0>)\n",
      "epoch: 598 loss is tensor([0.4519], grad_fn=<AddBackward0>)\n",
      "epoch: 599 loss is tensor([0.5110], grad_fn=<AddBackward0>)\n",
      "epoch: 600 loss is tensor([0.4564], grad_fn=<AddBackward0>)\n",
      "18\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAA5a0lEQVR4nO3dd3xb13338c8BuAFOgBvcpKw9qWlLtuzI1nAsybbileFmuEnspE3bPHGaNG3TJ2nSJk+ap5nOaJM+SZzIlmzZkrwdy0uyqWlZk6QGl8S9F8Z5/rggRMkaFAnxAsTv/XrxRVzgEvhdifzi4pxzz1Faa4QQQkx8FrMLEEIIMT4k8IUQIkJI4AshRISQwBdCiAghgS+EEBEiyuwCLsXpdOrCwkKzyxBCiLCye/fuZq11+sUeC9nALywspKKiwuwyhBAirCilTl3qMWnSEUKICCGBL4QQEUICXwghIoQEvhBCRAgJfCGEiBAS+EIIESEk8IUQIkKE7Dh8IYS4Gp39bk4293CiuYemrgHWzcnFaY81u6yQIoEvhAgbPQMeTrb0cLK5l5MtRrifaO7hZHMPLT2D5+07NSdJAv8CEvhCiJDS7/b6Q72HE829xnf/dmPXwHn7RlsVbu+5RZyc9hg2lOdx7/w8Chy28S495EngCyHG3YDHS01r7wcC/WRzD/Ud/eft67THUOiwsWxSOkVOG/lpCdS09fJWZQtvV7cAcEOpk/sW5LNiaiYxUdI1eSkS+EKIa8Lt9VHb1hdoVz/R3BNohqlv78M3bHXVlIRoCh02FhY7KHTYKEq3UeSwUeBMICkuGoDGzn7++G4N39l+hLr2Phy2GD6ztJh75+dR6JSz+ZGQwBdCjJrXp6lr6wucoQ+F+snmHmra+vAOS/XEuCiKnDbm5qdy51wXRc4EI9ydNlISYi76/IMeH88dbOBPFbW8dqwJr09zfamDr66ezK1Ts+Rs/ipJ4AshLsvn0zR09p8LdH+oVzf3UNPae14bekKMlUKHjWk5yayZmU2hw0Zxuo1Ch400WwxKqRG95vv1HTyxu5an99XT2jNIZlIsDy0r5iPleRTJ2fyoSeALIdBa09g1QHVTz7AOU+P2qZZeBjy+wL6xURaKnDYmZSRy69Ss887U0xNjRxzqF2rrGeTpfXX8qaKWQw2dxFgtrJiayd3lLpaWOomyytn8WAUl8JVSvwZuBxq11tMv8rgCfgisBnqBB7XWe4Lx2kKIkdFa09w9GGhHPxloU+/lVEsPvYPewL4xVgv5DiPIb5yUTqHTaFMvdNrISorDYhldqF/I4/Xx+vFmNu6u4aVDjQx6fUzPTeKf75jGHbNySLVdvKlHjE6wzvD/G/gR8NtLPL4KKPN/LQR+6v8uhAiytp7B80a9VA+dqTf30jXgCewXZVHkpSVQ5LSxuNhhnKk7jeaXnJR4rEEK9YupaupmY0Utm/bU0tg1QJotho8uKmBDuYsp2UnX7HUjXVACX2u9QylVeJld1gK/1VprYKdSKkUpla21bgjG6wsRaTr63MPO0P1NMC3GEMeOPndgP4sCV6oR5PPyU41A95+tu1Ljx7WZpKvfzbMHGthYUcOe0+1YLYrl16Vz97w8bp6cIR2w42C82vBzgZph27X++84LfKXUQ8BDAPn5+eNUmhChqWfAc96olxP+q0svvKpUKchJjqfQmcDtM7Mpchrt6YVOG3mpCaYGqc+n2VndwsbdtWw/2EC/20dZhp2/Xz2ZdXNyyUiMM622SBRSnbZa68eAxwDKy8v1FXYXIux5vD6qm3uobuo2ml6ajWkDTrQY88EMl5UUR6EzgVunZVLob08fuhApLtpq0hFcXE1rL0/sruXJPbXUtvWRGBfFXXNd3D3Pxey8lFF37IqxGa/ArwPyhm27/PcJETG01pxu7WV/bQf7a9o5UNvOwbpO+tznOkud9liKnAncNNRR6v8qcCSQEBNS52cf0DfoZfvBBjZW1PJ2dQtKwfUlTr5823XcNi0r5N6UItF4/QZtAR5RSj2O0VnbIe33YqJr7OrnQE0H+2vb2V/bwYHadtp7jfb12CgL03OTuXdBHjNdyZRlJFLgSCDRf1VpuNBas+d0Gxsrann2QAPdAx7y0xL4mxWTuGuei9yUeLNLFMMEa1jmH4CbAKdSqhb4RyAaQGv9M2AbxpDMSoxhmX8RjNcVIlR09rs5WNvBvtr2QMg3+OeEsVoUZRl2Vk7LYqYrhVl5yUzKTCQ6jMeVn+3s58k9tTyxu5bqph7io62snpHNhnIXCwrTgjZsUwRXsEbp3HeFxzXwcDBeSwiz9bu9HG7o9DfLGOFe1dQTeLzAkcD8wjRmupKZnZfCtJxk4mPCvzljwOPlpUONbNxdw45jTfg0zC9M5bPLSlg9Mxt7bGg3OYkQ67QVItR4fZrKxm7217Szv9YI+CNnOgPTCaQnxjLLlcK62bnMzEthZm7yhLtY6GBdBxsranh6fz3tvW6ykuL43E0l3D1PpjkINxL4Qvhpralt62Ofv0N1f20HB+s6AlegJsZGMTMvmU8vLWaWK5lZeSlkJcVNyBEnLd0DPLWvnid213K4oZOYKAu3Ts1kQ3keN5Q6r+lFWeLakcAXEau5e4ADte3sqzE6VA/UdtDqH98eE2VhanYSHyk3OlVn5aVQ5LBN6LZpj9fHa8ea+FNFDa8cacTt1cx0JfMva6dxx6xckhPCq0NZfJAEvogI3QMe3vO3tx+obWd/TQd17X2AcTVqWUYiH5qSwUxXCrPzUpiUmRgxV35WNnYZ0xzsraOpawCHLYZPLC5kQ3ke12Ulml2eCCIJfDHhDHi8HGnoMoZD1gx1qnaj/Zfy5aXFMyc/hQeXFDLTlcz03GRsEdbh2Nnv5pn99WysqGVfTTtRFsXyyRlsmOdi+eSMsB5BJC4tsn7LxYTj9Wmqm7r97e5GuB9uONep6rTHMMuVwodn5jAzL5lZrhTSJlin6kj5fJq3qlrYuLuG5w6eYcDj47rMRL6+Zgrr5uTKgt8RQAJfhA2tNXXtfez3t7nvr23nvdoOevydqvbYKKbnJvHJG4qY5UphVl4KOckTs1P1apxu6eWJ3TU8uaeOuvY+kuKi+Eh5HhvKXczITY74f59IIoEvQlZL90DgrP2AfzqCoUnDYqwWpmQnctc8l3ExkyuZ4nS7jB7x6x30sO29M2ysqGHXiVaUgqVl6Ty6ajIrpmbKNAcRSgJfhISeAQ/v1XUEhkPur2mnts3oVFUKStPtLJ+cwSxXMjNdKUzOTiQ2SkJrOK9P81ZVM5v31PHc+2foHfRS6Ejgy7ddx51zc8lOlmkOIp0Evhh3gx4fR890+achMJpmKhu7GVrvOjclntl5KXxsUQEzXSnMcCXLVZyXoLXmUEMnm/fUsWV/PY1dAyTGRbF2dg7r57iYX5gqTTYiQP6KxDXl82mqm3sCs0Puq+3gcH0ng15jjdQ0WwwzXcmsmp7NrDzj7F06D6+svr2Pp/bV8dTeOo6d7SbaqrjpugzunJPL8skZ0mQjLkoCXwSN1pqGjn7/NARGs8zBuo7AsnoJMVam5ybz4PXGcMhZrhRcqfFyBjpCnf1unnvvDJv21rLrRCtaw7yCVP73uumsmZE94aZ0EMEngS9Gra1nMNChOnTFanO3sWhHtFUxOSuJtXNy/J2qKZRmSKfq1Rr0+NhxrInN++p46dBZBjw+ipw2vvShSaybnUu+I8HsEkUYkcAXI9I76OH9+s7zzt5Pt/YGHi9Jt7FskpNZrhRmupKZkp0kzQqjpLVmb007T+2t45n99bT1ukmzxXDv/DzWz3UxyyVDKcXoSOCLD3B7jU7V/cPmdj92tivQqZqTHMdMVwr3LchnliuZ6a5kksJs4Y5QdLK5J9Auf7Kll9goCyumZnLn3FyWlqXL1a9izCTwI5zPpznZ0nPeNASH6jsZ8BidqikJ0cx0pXDr1ExmulKYmZcsC08HUWvPIFsP1LN5bx17TrejFCwudvDw8lJWTs8KuxWwRGiTwI9ANa29vH68mR3HmnirqpnOfqNTNT7ayvTcJGM4ZJ5xMVN+WoI0HwRZv9vLy4cb2by3jj8fbcTj01yXmcijqyazdnaOjJcPIQMeL3tPt/N2VQvvnmyld9BLlEVhtSiirRasFkWURRFlVURZ/NtW5d/HQrRVDdvHcvGftSis/seGnqu8II28tOD3z0jgR4CeAQ87q1vYcayJ1483U91srM6UnRzHyulZzCtIZaYrhbIMO1HSbHBN+HyaXSdaeWpvHdvea6BrwENmUiyfvKGIdbNzmZqTZHaJAmOK6AN1Hbxd1cLbVS1UnGql3+3DomBqThKpCTF4fRqPT9M76Anc9ng1Hp8Pr0/j9urA/V6fz/+YcZ/b5wtM4nc5a2fn8MN75wT9+CTwJyCfz7gY57VjTbx+vIndp9pwezVx0RYWFTv46KIClk1yUpJul7P3a+z42S427a3j6b111Hf0Y4uxsnJ6Nuvn5LK4xCGjlkzm82kOn+nk7aoW3qpq4Z0TrXT7hxFPzkrkvgX5LC52sLDIcVXrAfQMeDjR3EN1cw8nmno40dwd2O7yf6IGY4qQAkcCRU4bRek2ip02ipx2ZuQmB/1YQQJ/wmjs7DeaaY438cbx5sCcM1Oyk/jk9UUsm5TOvIJUGTkzDho7+9my32iXf7++E6tFsazMyVdWTebWqVkTYn3bcKW1pqqpm7eqWnirsoWdJ1po73UDUOy0sXZ2DotLHCwqdlzxAsBBj4+atl5/oPvD3R/sZzsHAvspBTnJ8RSn21g/J9cId6eNYqed3NT4cX3Tl8APU/1uLxUn29hxvIkdx5o4cqYLAIcthqVlTpZNSueGMqd0sI6TngEPLxw6w6Y9dbxZ2YxPwyxXMv/44ancPjOH9ES5etgMWmtqWvt4q6qZt6paeLu6haYuI4xzU+JZMSWTxSUOFpc4RtR3UtPay//bdYoX3j/L6dZevL5z7TNpthiKnDaWlqX7A91GcbqdAkdCyJxoSeCHCa2NxbR3+Dtbd51ood/tI9rfwfOVlZNZWuZkanbShF6GL5R4vD7erGph855ann//LH1uL67UeB5eXsra2bmUZtjNLjEiNXT0BZpo3q5qCaxslp4Yy+JiB0tKHCwpcZKXNrKrvH0+zeuVzfz2rZO8crQRBdw4KZ01M7IpTrcFzthTEkL/SmcJ/BDW3jvIG5XNgc7Who5+AIrTbdw7P59lk5wsLHJE3GpNZtJa8359J5v8k5U1dw+QFBfF+rm5rJ+Ty7z8VHnDHWfN3QPsrDYCfmdVS2BQQkpCNIuKHPzljcUsKXFcdZ9VR6+bjbtr+H87T3GypRenPYaHbyrl/oX55KSE50gqSYoQ4vb62FfTzuvHmnjteDMHatvRGpLiori+1MkXb0lnaZkTV6pcTj/eatt6eXqf0S5f2dhNjNXCzZMzWDcnl+WT02Wq5nHU0edm11DAV7cEmjPtsVEsKErj/oX5LC5xMCVrdJ9236/v4H/ePsVT++rod/uYV5DKl1ZMYuX0rLD/f5bADxFfeeJAYLieRcHsvBT+6pYylpalM8uVLMMlTdDR52b7ew1s3lvHrhOtAMwvTOXb62ewekZWWHyEnwh6Bjy8e7KVt6uNJpqDdR34NMRGWZhfmMaXb8thSYmDGbmj/zsZ9PjYfrCB3759it2n2oiLtrB2Vi4fW1zA9Gs0YsYMEvghQGvNc++fwZkYy7/dPZMlpU6S4+UKSzMMenz8+ahxUdTLRxoZ9Pgodtr42xWTWDcn95pcDCPO1+/2sud0Gzv97fD7atrx+DTRVsWcvFS+cHMZS0oczM5PGfMZd0NHH7/fdZo/vFNDc/cABY4Evr5mChvm5V3VMMxwIYEfApRSLJuUzttVzdw2LUvagMeZ1po9p9vYvLeOZw800N7rxmGL4f4F+dw5N1fWfb3G3F4fB2o7eNs/kqbiVBuDHuNipxmuFD6zzGiDn1eQSkLM2CNLa83bVS389u1TvHj4LD6tufm6DD62uIBlZekT+u9PAj9E3DI5g2f21/NeXQez8lLMLicinGjuYfNeY7Ky0629xEVbuHVqFuvn5nJDqVMmK7tGvD7N4YbOwFDJd0+0Bhain5JtTO2xpMTB/KK0oE/K9+Khs3z3uSNUNnaTkhDNp5cW8dGFBRHzyU0CP0TcOCkdi4JXjjRK4F9DLd0DPHvAaJffV2NMVnZ9iZMv3lLGyulZspTiNaC15nhjN29VNgc6WofmbypJt3HnXBdLShwsLHaQNsJFXHoHPXx722FirFaykmPJTIojIzGOrOQ4MpNiL/lJYPvBBiobu0mMjeJnH53HomJH0I4zHMhvd4hItcUwNz+VV4408qUVk8wuZ0Lpd3t58dBZntpbx2vHmvD4NFOyk/j71ZO5Y1YuWclycVowaa051dJrXM1a1czO6haau40rv/PS4lk1PZslpcbVrJlJo/u37x308vTe+sBqahdKjI0i0x/+mUlxxldiLLdMziQ2yspzBxu497GdrJ2dw/9aOZncMB1mebUk8EPI8skZ/PvzR2ns7CdjlH8IwuDzaXZWt7B5bx3bD56he8BDVlIcn1paxPo5uUzOksnKgqm+ve9cwFe1UO+/ZiQzKZalZenG1azFjqA1nTjtsTz28XI+8V/vUOy08b0Ns+jsc3Oms5+znQOc7ewPfO2qbuVsZz+eYVfFRluNdvqn99Xz3MEzPLSsmM/eWDLhr2lReiRTt13pSZRaCfwQsAK/1Fp/54LHHwT+Hajz3/UjrfUvL/ec5eXluqKiYsy1hZPDDZ2s+uHrfPeuGdwzP9/scsLS0TNdbNpby5Z99TR09GOPjWLV9CzWz8llYbFMVhYsTV0D/mGSzbxd1cLJFmP1szRbDIuLHYHpCoqdtmva4f3cwQY+/7s9LJuUzi8+Xn7JfhefT9PaO8jZzn5OtfTy0qGzvHj47HkTmaUnxvLlW6/jrnmusP49UUrt1lqXX/SxsQa+UsoKHANWALXAu8B9WutDw/Z5ECjXWj8y0ueNxMDXWnP9d15hhiuZn3/sov9f4iLOdvbz9L46Nu+t53BDJ1EWxY2T0lk3J5cPTcmUycrGQGvN2c4BKhu7qWzs4lhjNxUnWzl2thswmk4W+gN+SYmD6zITx32Uyx/eOc1XN73H+jm5fH/DrBG//qDHx5tVzWx/r4EXDp0NTKI2NTuJ/7x/DiXp4Tk1xuUCPxifXxYAlVrrav+LPQ6sBQ5d9qfEByilWD45g8176xjweMP+qr5rqXvAw/MHz7B5bx1vVjWjtXGx2j/fMY3bZ2bjuMJMh+J8Xp+mprXXCPambn/Ad1PV2H1eO3lSXBSz81NZP8foaJ2Wk2T6RYH3LcinpXuA771wjDRbDF9fM2VEnypioiwsvy6D5ddl8C2vj13VrWw/2MCfjzZRcbI1bAP/coIR+LlAzbDtWmDhRfa7Sym1DOPTwJe01jUX2Sfi3TIlg9/tOs07J1pZWpZudjkhxeP18XplM5v31PHCoTP0u33kpyXwhZvLWDc7h+IJ+AcabAMeLyebjWA/3tgVCPbq5h4G/ctaAmQkxlKaYefOucYkcCUZdkoz7KTbY0PymoSHl5fS3D3Ir944gdMey+duKrmqn4+2WrihzMkNZc5rVGFoGK8eimeAP2itB5RSfwn8Brj5wp2UUg8BDwHk50dmG/biYiexURZeOdIogY/RpPBeXQeb99bxzP56mrsHSUmI5u55LtbPyWVufmpIBpDZugc8VPnDfPgZ+/ApfZWCvNQESjPsLJuUTmm6ndJMOyXp9rC70lspxTdun0przyDffe4IDlsMH5mfZ3ZZIScYgV8HDP+XdXGucxYArXXLsM1fAv92sSfSWj8GPAZGG34Qags7de19eHyamAi/6KemtdffLl9HVVMPMVYLt0zJYP2cXG66LoOYqMj+9xnS2jP4gbP1qsbuwCgZMEakFDpsTMlO5MMzswNn6yXp9pCZpz0YLBbF9zbMor3PzaObDpBqi2HF1EyzywopwQj8d4EypVQRRtDfC9w/fAelVLbWusG/eQdwOAivOyF9Z/th4qOtfGZZsdmljLuOXjdb32tg895a3j3ZBsCCojQ+s7SYVTOyw+6sM1i01jR09AcC/bg/1Cubumn1r2wGkBBjpSTdzsJiB6X+UC/NsJOflhAxVw3HRFn46QNzuf+Xu3jk93v4n08tZEFRmtllhYwxB77W2qOUegR4HmNY5q+11u8rpb4JVGittwBfVErdAXiAVuDBsb7uRPRWVTMvHW7kf6287orLq00UAx4vrx5p4qm9dbxypJFBr4+SdBtfvu061s7OiaipoD1eHzVtfRw/2xVohhlqlhmaegCMed7LMuzcNi2TknQj1MsyE8lOipvQ88CMlC02iv96cD53/+wtPvWbd/nTXy5mSrZcdwFBGod/LUTasEyfT3PHj9+grcfNy39744T6qH0hrTUVp4zJyrYeaKCjz43THssds3K4c24u03KSJnS7fL/by4nmnvPP1huNtVAHvec6TrOS4s47Ux/6cthiJvS/T7DUtfdx10/ewqs1mz63JGLmy7nWwzJFEGzeW8fBuk7+457ZEzbsq5q6eWpvHU/tq6OmtY/4aCu3Tctk/VwX15c4TB/eF2xd/e5AM0xlkxHsxxu7qWntZeiiT4uC/DSj4/SmyUbHaVlmIiXpNhKDPHFYpMlNiecH98zmvl/s5BO/foeX/ubGiP8EJIEfAvoGvXzvhaPMdCVzx6wcs8sJqubuAZ7ZX89Te+vYX9uBRcH1pU6+9KFJ3DYtK+wvZdda0xLoOD13tl7Z2M2ZznMdpzFWC0VOG9NzklnnX++2NMNOkdM2Yd/gzaK15u3qFn6/6zTPv38GgEKnDa/WWJDAFyb71RvVNHT08x/3zJ4QZyB9g15ePHyWzXtq2XG8Ga9PMy0nia+vmcIds3LCcp4gn09T39F3biRMUzfHzxpn7kNXaALYYqyUZthZUmp0nJZlJFKaYScvNX7CfYIJNe29gzyxu5bfv3Oa6qYekuOj+fjiQu5bkC8LyvtJ4JusqWuAn/65ilunZrIwjKdq9fonK9u0p47nDjbQM+glJzmOh5YVs35OLpMyE80ucUTcXh+nWnoDoV457Iy9z32u4zTNFkNpup3VM7KN8ev+M/bs5DhpXx9HxuI17fxu1ymePdDAoMfH3PwUvr9hFmtmZsunpwtI4JvsBy8dY8Dj49FVk80uZVQON3SyeW8dT++r42znAImxUdw+M4d1c3JZWJQWsp9Y+t3eDwR6ZWM3J1t6cHvPDWTISY6jJMPOvQvyAmfrpRn2Ec/bLq6Nrn43T+2t43e7TnPkTBf22CjuKc/j/oX5MiLnMiTwTXTsbBePv3Oajy8uDKtpARo6+tiyr57Ne+s4cqaLKIvipusy+MbtudwyJSOkzqo6+tznhjc2dQcuUqpt60MP6zgtcNgozbDzoamZgTP2kgy7LIgSYt6r7eD375zi6X319A56mZ6bxL/eOYM7ZuWEfX/QeJB/IRP967bD2GKj+OItZWaXckVd/W6e809W9nZ1C1rD3PwU/mXtNNbMzDH1jFdrTVP3wAfO1isbu2nsGgjsFxNlodhpY5Yrhbvn5gXO1gudCTJRXQjrHfTwzP56frfrNAdqO4iLtrB2Vi4PLMpnpivF7PLCigS+Sd443syrR5v46qrJIds84Pb6eP14E5v21PHiobMMeHwUOBL4q1vKWDc7l0KnbVzr8fk0de19w644PTedQOewec3tsVHn5ofJsPuHOtpxpSaE9TznkebomS5+v+sUm/bU0TXgYVKmnX++Yxrr5uRG7FXXYyWBbwKvT/O/tx7ClRrPJ5YUml3OebTW7K/t4Cn/ZGUtPYOkJkRzz/w81s3JZU5eyjXvlDQ6TnuMUTDDmmKqmrrpd5+7MMlpj6Ek3c4ds3P8zTBGG3tmUmjO6CiurN/tZfvBBn638zQVp9qIibKwZkY2DyzMZ16BTJQ3VhL4JnhyTy1HznTxn/fNCZn27tMtvTy1r46n9tZR3dxDTJSFFVMzWT87l2WT0q/JZGW9gx6qm3o+cLZ+qqX3vOXoclPiKc2ws6h4aKij0RSTkhCan4zE1atu6uYP75xm4+5a2nvdFDltfG31FO6a5wrZT8DhSAJ/nPUOevje80eZnZfC7TOzzS4HgK9uOsAf3jGWJ1hUnMZnbyxh5YwskoJ0pWd77+D5bev+Mex17X2BfawWRYEjgbIMOyunZ/mbYhIpybCRECO/phPRoMfHi4fO8rtdp3irqoUoi+K2aVk8sDCfRcWOkB3hFc7kL2mc/WLHCRq7BvjpR+eGzMfThUXG4tJrZ+eSmxI/qufQWtPYZXScDp/8q7Kxh+bucx2nsVEWStLtzCtI5Z75eYGz9QKHTaY8jhA1rb08/u5p/vhuLc3dA+SmxPPl265jQ7mLjMTwuygvnEjgj6PGzn5+vqOK1TOymFcQOlO2rpuTO+J9vT5NbVvveVP1XmwpvMS4KMoy7Nw8OT0wGqYsI5HclHg5c4tArT2DvHOihT++W8OfjzWhgJsnZ/LAonyWlaVLZ/o4kcAfR99/4Rhur4+vrAz9i6wGPT5OXqTjtLqpm4FhS+GlJ8ZSmm5n3ZxcyjLtgTHs6YnScRqp+t1eDtZ1sK+mnf21Heyvaed0ay8AmUmxfOHmMu6dn0fOKD9NitGTwB8nhxs6+dPuGj55fREFjvEdzng5PQOewBWnw8/WT12wFJ4rNZ7SdDs3lA4trpFIabqd5AQZHhfJvD5NZWM3+2va2Vfbzv6ado6c6Qr87uQkxzE7P4UHFuYzKy+FeQWpEbMYSyiSwB8n3952mKS4aL5wc6kprz+0FN7wjtPKs13nLYUXZVEUOm1cl5XImpnZgWXwStLtxMeExmgiYZ6hlbeGwn3f6Xbeq+ug1784S2JcFLPzUvjcjSXMykthlis5LCfKm8gk8MfBa8eaeP14M19fM+WaDiXUWnOms9/fcdp93qpJLcOWwouPtlKSYWNBUdq5s/UMOwWOyFkKT1xZR5+b92o72FfTxr6aDvbXttPkv3I5xmphSk4SG+a5jHDPS6HIYZP+mRAngX+NeX2ab289TIEjgY8vLgzac55u7b1gGoEuqpp66B7WcZocbyyFt2JqZmBumNJ0u3Scig8Y8Hg53NDF/pr2wBl8dVNP4PHidBtLS52BcJ+SnSjTUYQhCfxr7E8VNRw928VPHph71cMOBzzGUnjDO06rGrupbjp/KbzMpFhKM+zcNTeX0szEQMep0y5L4YkP8vk0J1p6zoV7TTuHGjoDs4Q67bHMzkvhzjm5zM5LZYYrWaYymCAk8K+hngEP33/hGOUFqayanjXin/v+C0d5Zn89p4cthaeGlsJLt3PjpHTjbN3/FawLpMTE1NjVz/4ao2lmv79ppss/91BCjJWZrmQ+eUMRs13G2bvM6T9xSeBfQz9/rYrm7gF+8fF5V/UHFBdtZWpOEncMLYWXbqc4XZbCE1fWPeDhvVoj1IfO4Ic65q0WxeSsRD48KycQ7qUZdhkDH0Ek8K+RMx39PPZ6NbfPzGZOfupV/ezDy80ZySPCi9vr4+iZrkC476tp53hjd2Ce//y0BOYVpvFJVzKz81KYlpMso60inAT+hSpfhpw5kDC2K2G/98JRfD7C4iIrEdp8Pk1DZz+nmns46V9+cX9tOwfrOgIXwaUmRDMrL4VV07OZnZ/CLFeKTDomPkACf7i+NvjjR2H6XbD2R6N+mvfrO3hyTy0PLS0mLy0hiAWKicrr09S393GyxQj1oXA/1dLDqdZeBodd3RwbZWF6bjIfXVTArLwUZrtSyEuLl3Z3cUUS+MPt+S24e2HhX476KbTWfGvrYVLio/m8NM2IYTxeH3XtfYEgP9Hcw6mWXk629FDT2nveWrqxURYKHTaKnDaWT86gwJFAocNGgSOB7OR4aXcXoyKBP8TrgV2PQeFSyJox6qd59Wgjb1W18E8fnipD2SLQoMdHbVtvIMhPtfT6g72H2ra+8+b5j4+2UuBIYFJGIrdOzaLQkUCBw0ahM4HMxDi5VkIEnQT+kMNboLMWVv/7qJ/C4/Xx7W1HKHLauH9hQRCLE6FkwOOlprWPk809gVAf+l7X3heYRwbAFmOl0GljWk4yq2dkU+iwUei0UehIkAnmxLiTwB+y86eQWgSTbhv1Uzz+bg2Vjd38/GPzZG73MNfv9nK6tTcQ6kPNMCebe6nv6AuMhAFjDpkip41ZeSmsnZ1jnKU7Eih02nDY5OI3ETok8AFqK6D2HVj5XbCMbthaV7+bH7x4jAVFadw6NTPIBYproXfQw6mhIL+gXb1h2KRyACkJ0RQ4bJQXplLocFHo9De/OGykJkRLqIuwIIEPxtl9bBLMeWDUT/Gz16po6Rnkv9ZMkT/+ENI94OFk8/Bml3PhfrZz4Lx9HbYYChwJLC52BNrShzpKZf1cMRFI4HfUwaGnYOFnITZxVE/R7/byy9dPoBT85f/sJtpqIdqqiImyEmNV/m0L0VGW87etFmKiLtgeejxq6PGR/IyFaP99MYH7lf/1jO2JPKqjs9/NqWYj0E8OG854sqX3vOUVwViwpdCRwNKy9HOdpA4bBc4EmaJCTHgS+O/+ArQPFnxm1E8RbbXwxVvKqG/vw+314fZqBr0+3B7fuW2Pj74+t3/73H3nbXt95423DiarRRlvAsPfFC72JuF/k7lwO2bYG0x0lDp/26qG/Yx/+7w3ucu/SZ2rQWG1qIt+QmrvHTyvHf1USw8n/B2lrcOmfgZjMrkCh41bJmdQMOwsvcBhwx4rv/IicgXlt18ptRL4IWAFfqm1/s4Fj8cCvwXmAS3APVrrk8F47TEZ7IWK/4LJayC1cNRPY7WooE2HoLXG69Pn3jT8X0NvDoMefe4+/xvF0BtLYPu8nzl/O/Cc/ucZHPaGM7RPz4Dn3Paw/YfexAb99w/vuAwWpTjvDSDaaqHf7aWz33PefjnJcRQ4bNw2LdMf6EYTTH5aAgkxEupCXMyY/zKUUlbgx8AKoBZ4Vym1RWt9aNhunwLatNalSql7ge8C94z1tcfswOPQ3w6LPm92JQFKKaKsiigrxBPa854Yb0y+YZ9mLvEmceGb1rA3qcD2sE9Eg8PepNxeH1aLoiDNOEsvctrIS0uQieSEGIVgnAotACq11tUASqnHgbXA8MBfC/yT//YTwI+UUkrra3GOOEI+n9FZmz0b8hebVkY4s1oUVotVwleIMBGMweK5QM2w7Vr/fRfdR2vtAToAx4VPpJR6SClVoZSqaGpqCkJpl1H1CjQfM87uZVSNECIChNTVQVrrx7TW5Vrr8vT09Gv7Yjt/AvZMmLb+2r6OEEKEiGAEfh2QN2zb5b/vovsopaKAZIzOW3M0HoGql2H+ZyBKxlcLISJDMAL/XaBMKVWklIoB7gW2XLDPFuAT/tt3A6+Y2n6/62dgjYXyvzCtBCGEGG9j7rTVWnuUUo8Az2MMy/y11vp9pdQ3gQqt9RbgV8D/KKUqgVaMNwVz9LXB/sdh1j1gc5pWhhBCjLegDFjWWm8Dtl1w3zeG3e4HNgTjtcbs2PPg6YO5D5pdiRBCjKuQ6rQdF4efgcQcYxlDIYSIIJEV+IO9xpq1k1eDJbIOXQghIiv1qv9sNOdMvt3sSoQQEWbP2T009jaaWkNkBf6RZyEuGQpvMLsSIUQE8fq8PPr6o3z9ja+bWkfkBL7XA0e3w6SVYJVpcIUQ4+fN+jdp6Gngrkl3mVpH5AR+zU7oazVmxhRCiHG08dhG0uLSuDnvZlPriJzAP/yscbFVyS1mVyKEiCBnes6wo3YH60vXE21y60JkBL7WcGQrlNwMsXazqxFCRJDNlZvxaZ/pzTkQKYF/5j3oOC3NOUKIceX1edl0fBNLcpaQl5h35R+4xiIj8I88C8oC160yuxIhRAR5o+4NzvSc4e5Jd5tdChAxgb/VWORE5s4RQoyjjcc24ox3clPeTWaXAkRC4LeegLMHpTlHCDGuzvSc4fW6143OWktoDAWf+IF/ZKvxXQJfCDGOnjz+JFrrkOisHRIZgZ85A1ILza5ECBEhPD4Pm45tYknuEnLtF674ap6JHfjdTcYFV3J2L4QYRztqd9DY18iGSaExK/yQiR34x7aD9kngCyHG1cZjG8mIz+BG141ml3KeiR34R7ZCSj5kzTC7EiFEhKjrruPNujdZX7aeKEtQ1pgKmokb+APdUPWqMRWyUmZXI4SIEJuOb0IpxV1lodNZO2TiBn7lS+AdkLnvhRDjxu1zs/n4Zm7IvYFse7bZ5XzAxA38I1shwQF5C82uRAgRIXbU7KCpr4m7y0LjytoLTczA97qNxconrQJraLWhCSEmro3HNpKRkMFS11KzS7moiRn4J1+HgQ6YIs05QojxUdtVy1v1b3FX2V0h11k7ZGIG/pGtEJ0AxTeZXYkQIkI8efxJlFLcWXan2aVc0sQLfJ8Pjmwz5r6Pjje7GiFEBBjqrF2Wu4wsW5bZ5VzSBAx8t/F1eifU7ja7GiFEBHj19Ku09Lew4brQurL2QhMv8KNi4S+egxgb/OZ2o/NWCCGuoY3HNpJty+b6nOvNLuWyJl7gAzhL4dMvgXMS/OE+2P0bsysSQkxQpztPs7NhJ3eW3YnVYjW7nMuamIEPYM+AB7dCyXJ45ovw6r8aa9sKIUQQPXn8SazKyvrS9WaXckUTN/DBWLD8vsdh9gPw2ndgyxfA6zG7KiHEBOH2unmq8imWuZaRacs0u5wrCs3BosFkjYa1P4akXNjxb9B9Fjb8t9HGL4QQY/Byzcu09reG3DTIlzKxz/CHKAU3fw1u/4Exx85/327MlS+EEGPwxNEnyLHlsCRnidmljMiYAl8plaaUelEpddz/PfUS+3mVUvv8X1vG8ppjUv5JuOd30HgYNn5C2vSFEKN2qvMUu87s4q5Jd4V8Z+2QsTbpPAq8rLX+jlLqUf/2Vy6yX5/WevYYXys4Jq+GTzxjXJQl0yYLIUbpiWNPEKWiwqKzdshYm3TWAkNjHn8DrBvj842PvPmQNd3sKoQQYWrQO8jTlU9zU95NpCekm13OiI018DO11g3+22eAS3VTxymlKpRSO5VS68b4mkIIYaqXTr1E20Bb2HTWDrlik45S6iXgYpNDfG34htZaK6Uu1SheoLWuU0oVA68opd7TWldd5LUeAh4CyM/Pv2LxQghhhs2Vm3HZXSzKWWR2KVflioGvtf7QpR5TSp1VSmVrrRuUUtlA4yWeo87/vVop9WdgDvCBwNdaPwY8BlBeXi49qkKIkFTdXs2S3CVYVHgNdBxrtVuAT/hvfwJ4+sIdlFKpSqlY/20ncD1waIyvK4QQptBa0zrQiiPOYXYpV22sgf8dYIVS6jjwIf82SqlypdQv/ftMASqUUvuBV4HvaK0l8EVEGDhxgq6XXza7DBFEXe4uPD4PqXEXHYUe0sY0LFNr3QLccpH7K4BP+2+/BcwYy+sIEa5afv4YXS+/TNmbb2CJiTG7HBEEbf1tAKTFpZlcydULrwYoIcJM0prV+Lq66HnjDbNLEUHS2t8KSOALIS5gW7QIa0oKnc9uNbsUESQS+EKIi1LR0STedhtdr76Kr7fX7HJEEAwFfji24UvgC3GNJa1Zje7ro/vPfza7FBEE0oYvhLikhHnziMrIoGPbNrNLEUHQ2t+KPdpOjDX8OuEl8IW4xpTVStKqlfS8tgNvV5fZ5Ygxau1vDcuze5DAF2JcJK1ejXa76XrxJbNLEWPU2t8alu33IIEvxLiImzmTaJeLTmnWCXtt/W1yhi+EuDSlFEmrV9Pz9tt4WlvNLkeMgTTpCCGuKGnNavB66XrhBbNLEaPk0z7a+9sl8IUQlxc7aRIxJSV0bpVmnXDVNdiFR4fnPDoggS/EuDGadVbRW1GB++xZs8sRoxDOV9mCBL4Q4ypp1WrQms7t280uRYxCOF9lCxL4Qoyr2OIiYqdOoXObBH44GrrKNhznwgcJfCHGXfKaNfQfOMBgTY3ZpYirJGf4QoirkrRyJYCc5YehQODHSuALIUYgOjeX+Dlz5CKsMNTa30piTCLR1mizSxkVCXwhTJC0ejUDR48yUFlpdiniKrT1t4Vt+z1I4AthiqSVt4HFImf5YSac59EBCXwhTBGVnk7CwgV0bt2G1trscsQIhfO0CiCBL4RpklavZvDUKfoPHTK7FDFCcoYvhBiVpBUrICpKmnXChE/7aB8I33l0QAJfCNNYU1KwX389ndu2o30+s8sRV9Ax0IFP+yTwhRCjk7RmNZ6GBvr27TO7FHEF4T6PDkjgC2Eq+823oGJjZQbNMBDuV9mCBL4QprLabdhvuonO555DezxmlyMuQ87whRBjlrR6Nd6WFnrffdfsUsRlDE2cJoEvhBg1+43LsCQk0LF1q9mliMsYOsNPiU0xt5AxkMAXwmSWuDjsH7qFrhdeRA8Oml2OuITW/lbio+I503PG7FJGTQJfiBCQtHo1vs5Out980+xSxCXk2nPp8/SxatMqHtj2AL87/DuaepvMLuuqqFC9rLu8vFxXVFSYXYYQ40IPDnJ86TJsy5aR++//ZnY54hLquut47sRzbD+xnaNtR7EoC/Mz57OyaCUrClaQHJtsdokopXZrrcsv+pgEvhChoeEfvkHn1q2UvfkGlvh4s8sRV1DdXs32k9vZfmI7pzpPEaWiWJK7hFVFq1ietxxbtM2Uui4X+GNq0lFKbVBKva+U8imlLvoC/v1WKqWOKqUqlVKPjuU1hZioktasxtfbS/drO8wuRYxAcUoxD89+mGfWPcMfb/8jH5v6MY61HeOrr3+Vm/54E99957tml/gBUWP8+YPAncDPL7WDUsoK/BhYAdQC7yqltmitZcYoIYZJmD8fq9NJx7PPGNMni7CglGKqYypTHVP563l/zf6m/Wyr3kZ6QrrZpX3AmAJfa30YjAO+jAVApda62r/v48BaQAJfiGGU1UrKnXfS8thjdDy7leTb15hdkrhKFmVhTsYc5mTMMbuUixqPUTq5wPDVmmv9932AUuohpVSFUqqiqSm8er+FCIb0Rx4mft48Gr72Nfree8/scsQEc8XAV0q9pJQ6eJGvtcEuRmv9mNa6XGtdnp4eeh+HhLjWVEwMrv/7Q6yONGoffgT32UazSxITyBUDX2v9Ia319It8PT3C16gD8oZtu/z3CSEuIsrhIO8nP8Hb3U3tF76Ar7/f7JLEBDEeTTrvAmVKqSKlVAxwL7BlHF5XiLAVN3kyOd/9Dv0HDtDwD9+QZRBFUIx1WOZ6pVQtsBjYqpR63n9/jlJqG4DW2gM8AjwPHAb+pLV+f2xlCzHxJa1YQfpffZHOZ56h5Ze/NLscMQGMdZTOZmDzRe6vB1YP294GyITfQlwlx2c/y8Dx4zT9nx8QW1JK4s3LzS5JhDGZS0eIEKaUIvtb3yJu6lTq/+7vGDh+3OySRBiTwBcixFni43H9+EcoWwI1n38YT1ub2SWJMCWBL0QYiM7KIu8//xPP2bPU/dVfo91us0sSYUgCX4gwET97Ntn/8k1633mHM9/6ltnliDA01rl0hBDjKHntWvqPHaP1V78mdtIk0u6/3+ySRBiRM3whwkzG3/wN9htv5Oy3vk3Pzp1mlyPCiAS+EGFGWa3kfP97xBQWUvdXf83g6dNmlyTChAS+EGHIareT95Mfo4Gaz30eb3e32SWJMCCBL0SYiikowPXD/2Dw5Enq//bv0F6v2SWJECeBL0QYsy1aRObff5Xu116j6Qc/MLscEeJklI4QYS71/vsZOHacll/+itiyMpLXBn3mcjFByBm+EGFOKUXW179GwoIFNPzDN+jbv9/skkSIksAXYgJQ0dHk/vA/iMrIoOaRR3CfOWN2SSIESeALMUFEpaaS99OfoHt6qX34EXx9fWaXJEKMBL4QE0hsWRk53/se/YcO0fC1r8vCKeI8EvhCTDCJNy8n/UtfonPbNlp+/nOzyxEhREbpCDEBOT7zaWPhlP/4ITElJSStWGF2SSIEyBm+EBOQUorsf/kmcTNmUP+VR+k/etTskkQIUKHaxldeXq4rKirMLkOIsOY+28jJDRtQUVEUbvwTUQ7HVf289vnA4zG+e71o/xc+n/Hd60V7feDznr+tfcRNnnyNjkpcjlJqt9a6/GKPSZOOEBNYdGYGrh//iFMf/RjVH74DS6IdvMPC+sJAv2B71JRiyuFDwTsQERQS+EJMcPEzZuD60Y9o3/QkymIFq+Xcd2tUYFtFWcFiRVktYI0yvl94v8WKsg79rBWs1vOea2gfrBa01iilzD58MYwEvhARwL70BuxLbzC7DGEy6bQVQogIIYEvhBARQgJfCCEihAS+EEJECAl8IYSIEBL4QggRISTwhRAiQkjgCyFEhAjZuXSUUk3AKbPruAwn0Gx2EdeIHFv4msjHJ8c2MgVa6/SLPRCygR/qlFIVl5qgKNzJsYWviXx8cmxjJ006QggRISTwhRAiQkjgj95jZhdwDcmxha+JfHxybGMkbfhCCBEh5AxfCCEihAS+EEJECAn8y1BKrVRKHVVKVSqlHr3I47FKqT/6H9+llCo0ocxRG8Hx/Y1S6pBS6oBS6mWlVIEZdY7GlY5t2H53KaW0UipshvuN5NiUUh/x/9+9r5T6/XjXOBYj+L3MV0q9qpTa6//dXG1GnVdLKfVrpVSjUurgJR5XSqn/6z/uA0qpuUEvQmstXxf5AqxAFVAMxAD7gakX7PN54Gf+2/cCfzS77iAf33IgwX/7c+FyfCM5Nv9+icAOYCdQbnbdQfx/KwP2Aqn+7Qyz6w7y8T0GfM5/eypw0uy6R3hsy4C5wMFLPL4a2A4oYBGwK9g1yBn+pS0AKrXW1VrrQeBxYO0F+6wFfuO//QRwiwqfRTyveHxa61e11r3+zZ2Aa5xrHK2R/N8B/AvwXaB/PIsbo5Ec22eAH2ut2wC01o3jXONYjOT4NJDkv50M1I9jfaOmtd4BtF5ml7XAb7VhJ5CilMoOZg0S+JeWC9QM267133fRfbTWHqADcIxLdWM3kuMb7lMYZx/h4IrH5v+4nKe13jqehQXBSP7fJgGTlFJvKqV2KqVWjlt1YzeS4/sn4KNKqVpgG/CF8Sntmrvav8mrJouYiytSSn0UKAduNLuWYFBKWYD/AzxocinXShRGs85NGJ/KdiilZmit280sKojuA/5ba/19pdRi4H+UUtO11j6zCwt1coZ/aXVA3rBtl/++i+6jlIrC+HjZMi7Vjd1Ijg+l1IeArwF3aK0Hxqm2sbrSsSUC04E/K6VOYrSXbgmTjtuR/L/VAlu01m6t9QngGMYbQDgYyfF9CvgTgNb6bSAOY/KxcDeiv8mxkMC/tHeBMqVUkVIqBqNTdssF+2wBPuG/fTfwivb3voSBKx6fUmoO8HOMsA+nduDLHpvWukNr7dRaF2qtCzH6J+7QWleYU+5VGcnv5VMYZ/copZwYTTzV41jjWIzk+E4DtwAopaZgBH7TuFZ5bWwBPu4frbMI6NBaNwTzBaRJ5xK01h6l1CPA8xgjB36ttX5fKfVNoEJrvQX4FcbHyUqMzph7zav46ozw+P4dsAMb/X3Rp7XWd5hW9AiN8NjC0giP7XngVqXUIcALfFlrHRafPEd4fH8L/EIp9SWMDtwHw+FESyn1B4w3Yqe//+EfgWgArfXPMPojVgOVQC/wF0GvIQz+nYQQQgSBNOkIIUSEkMAXQogIIYEvhBARQgJfCCEihAS+EEJECAl8IYSIEBL4QggRIf4/ue97EehL4csAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 601 loss is tensor([0.4500], grad_fn=<AddBackward0>)\n",
      "epoch: 602 loss is tensor([0.4375], grad_fn=<AddBackward0>)\n",
      "epoch: 603 loss is tensor([0.4322], grad_fn=<AddBackward0>)\n",
      "epoch: 604 loss is tensor([0.4746], grad_fn=<AddBackward0>)\n",
      "epoch: 605 loss is tensor([0.4154], grad_fn=<AddBackward0>)\n",
      "epoch: 606 loss is tensor([0.3948], grad_fn=<AddBackward0>)\n",
      "epoch: 607 loss is tensor([0.3884], grad_fn=<AddBackward0>)\n",
      "epoch: 608 loss is tensor([0.4213], grad_fn=<AddBackward0>)\n",
      "epoch: 609 loss is tensor([0.4346], grad_fn=<AddBackward0>)\n",
      "epoch: 610 loss is tensor([0.4103], grad_fn=<AddBackward0>)\n",
      "epoch: 611 loss is tensor([0.4107], grad_fn=<AddBackward0>)\n",
      "epoch: 612 loss is tensor([0.5083], grad_fn=<AddBackward0>)\n",
      "epoch: 613 loss is tensor([0.4757], grad_fn=<AddBackward0>)\n",
      "epoch: 614 loss is tensor([0.4774], grad_fn=<AddBackward0>)\n",
      "epoch: 615 loss is tensor([0.5071], grad_fn=<AddBackward0>)\n",
      "epoch: 616 loss is tensor([0.4906], grad_fn=<AddBackward0>)\n",
      "epoch: 617 loss is tensor([0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 618 loss is tensor([0.5259], grad_fn=<AddBackward0>)\n",
      "epoch: 619 loss is tensor([0.4254], grad_fn=<AddBackward0>)\n",
      "epoch: 620 loss is tensor([0.4903], grad_fn=<AddBackward0>)\n",
      "epoch: 621 loss is tensor([0.4746], grad_fn=<AddBackward0>)\n",
      "epoch: 622 loss is tensor([0.4616], grad_fn=<AddBackward0>)\n",
      "epoch: 623 loss is tensor([0.4528], grad_fn=<AddBackward0>)\n",
      "epoch: 624 loss is tensor([0.4534], grad_fn=<AddBackward0>)\n",
      "epoch: 625 loss is tensor([0.4401], grad_fn=<AddBackward0>)\n",
      "epoch: 626 loss is tensor([0.3958], grad_fn=<AddBackward0>)\n",
      "epoch: 627 loss is tensor([0.4662], grad_fn=<AddBackward0>)\n",
      "epoch: 628 loss is tensor([0.4136], grad_fn=<AddBackward0>)\n",
      "epoch: 629 loss is tensor([0.4120], grad_fn=<AddBackward0>)\n",
      "epoch: 630 loss is tensor([0.4230], grad_fn=<AddBackward0>)\n",
      "epoch: 631 loss is tensor([0.4194], grad_fn=<AddBackward0>)\n",
      "epoch: 632 loss is tensor([0.4132], grad_fn=<AddBackward0>)\n",
      "epoch: 633 loss is tensor([0.3981], grad_fn=<AddBackward0>)\n",
      "epoch: 634 loss is tensor([0.4187], grad_fn=<AddBackward0>)\n",
      "epoch: 635 loss is tensor([0.4162], grad_fn=<AddBackward0>)\n",
      "epoch: 636 loss is tensor([0.4522], grad_fn=<AddBackward0>)\n",
      "epoch: 637 loss is tensor([0.4284], grad_fn=<AddBackward0>)\n",
      "epoch: 638 loss is tensor([0.3962], grad_fn=<AddBackward0>)\n",
      "epoch: 639 loss is tensor([0.3880], grad_fn=<AddBackward0>)\n",
      "epoch: 640 loss is tensor([0.4880], grad_fn=<AddBackward0>)\n",
      "epoch: 641 loss is tensor([0.4866], grad_fn=<AddBackward0>)\n",
      "epoch: 642 loss is tensor([0.4202], grad_fn=<AddBackward0>)\n",
      "epoch: 643 loss is tensor([0.3717], grad_fn=<AddBackward0>)\n",
      "epoch: 644 loss is tensor([0.3791], grad_fn=<AddBackward0>)\n",
      "epoch: 645 loss is tensor([0.4445], grad_fn=<AddBackward0>)\n",
      "epoch: 646 loss is tensor([0.4183], grad_fn=<AddBackward0>)\n",
      "epoch: 647 loss is tensor([0.4982], grad_fn=<AddBackward0>)\n",
      "epoch: 648 loss is tensor([0.4223], grad_fn=<AddBackward0>)\n",
      "epoch: 649 loss is tensor([0.3693], grad_fn=<AddBackward0>)\n",
      "epoch: 650 loss is tensor([0.4658], grad_fn=<AddBackward0>)\n",
      "epoch: 651 loss is tensor([0.4201], grad_fn=<AddBackward0>)\n",
      "epoch: 652 loss is tensor([0.4004], grad_fn=<AddBackward0>)\n",
      "epoch: 653 loss is tensor([0.4190], grad_fn=<AddBackward0>)\n",
      "epoch: 654 loss is tensor([0.4208], grad_fn=<AddBackward0>)\n",
      "epoch: 655 loss is tensor([0.3843], grad_fn=<AddBackward0>)\n",
      "epoch: 656 loss is tensor([0.3888], grad_fn=<AddBackward0>)\n",
      "epoch: 657 loss is tensor([0.4484], grad_fn=<AddBackward0>)\n",
      "epoch: 658 loss is tensor([0.4675], grad_fn=<AddBackward0>)\n",
      "epoch: 659 loss is tensor([0.3947], grad_fn=<AddBackward0>)\n",
      "epoch: 660 loss is tensor([0.3712], grad_fn=<AddBackward0>)\n",
      "epoch: 661 loss is tensor([0.4317], grad_fn=<AddBackward0>)\n",
      "epoch: 662 loss is tensor([0.3827], grad_fn=<AddBackward0>)\n",
      "epoch: 663 loss is tensor([0.4109], grad_fn=<AddBackward0>)\n",
      "epoch: 664 loss is tensor([0.3727], grad_fn=<AddBackward0>)\n",
      "epoch: 665 loss is tensor([0.4027], grad_fn=<AddBackward0>)\n",
      "epoch: 666 loss is tensor([0.3948], grad_fn=<AddBackward0>)\n",
      "epoch: 667 loss is tensor([0.3937], grad_fn=<AddBackward0>)\n",
      "epoch: 668 loss is tensor([0.4859], grad_fn=<AddBackward0>)\n",
      "epoch: 669 loss is tensor([0.4629], grad_fn=<AddBackward0>)\n",
      "epoch: 670 loss is tensor([0.4708], grad_fn=<AddBackward0>)\n",
      "epoch: 671 loss is tensor([0.5374], grad_fn=<AddBackward0>)\n",
      "epoch: 672 loss is tensor([0.4180], grad_fn=<AddBackward0>)\n",
      "epoch: 673 loss is tensor([0.4922], grad_fn=<AddBackward0>)\n",
      "epoch: 674 loss is tensor([0.5790], grad_fn=<AddBackward0>)\n",
      "epoch: 675 loss is tensor([0.4612], grad_fn=<AddBackward0>)\n",
      "epoch: 676 loss is tensor([0.4599], grad_fn=<AddBackward0>)\n",
      "epoch: 677 loss is tensor([0.4634], grad_fn=<AddBackward0>)\n",
      "epoch: 678 loss is tensor([0.3897], grad_fn=<AddBackward0>)\n",
      "epoch: 679 loss is tensor([0.4389], grad_fn=<AddBackward0>)\n",
      "epoch: 680 loss is tensor([0.3875], grad_fn=<AddBackward0>)\n",
      "epoch: 681 loss is tensor([0.4183], grad_fn=<AddBackward0>)\n",
      "epoch: 682 loss is tensor([0.4655], grad_fn=<AddBackward0>)\n",
      "epoch: 683 loss is tensor([0.4118], grad_fn=<AddBackward0>)\n",
      "epoch: 684 loss is tensor([0.4143], grad_fn=<AddBackward0>)\n",
      "epoch: 685 loss is tensor([0.4578], grad_fn=<AddBackward0>)\n",
      "epoch: 686 loss is tensor([0.3940], grad_fn=<AddBackward0>)\n",
      "epoch: 687 loss is tensor([0.4262], grad_fn=<AddBackward0>)\n",
      "epoch: 688 loss is tensor([0.3745], grad_fn=<AddBackward0>)\n",
      "epoch: 689 loss is tensor([0.4466], grad_fn=<AddBackward0>)\n",
      "epoch: 690 loss is tensor([0.4705], grad_fn=<AddBackward0>)\n",
      "epoch: 691 loss is tensor([0.3950], grad_fn=<AddBackward0>)\n",
      "epoch: 692 loss is tensor([0.3923], grad_fn=<AddBackward0>)\n",
      "epoch: 693 loss is tensor([0.4490], grad_fn=<AddBackward0>)\n",
      "epoch: 694 loss is tensor([0.4744], grad_fn=<AddBackward0>)\n",
      "epoch: 695 loss is tensor([0.3515], grad_fn=<AddBackward0>)\n",
      "epoch: 696 loss is tensor([0.4256], grad_fn=<AddBackward0>)\n",
      "epoch: 697 loss is tensor([0.5010], grad_fn=<AddBackward0>)\n",
      "epoch: 698 loss is tensor([0.3810], grad_fn=<AddBackward0>)\n",
      "epoch: 699 loss is tensor([0.4016], grad_fn=<AddBackward0>)\n",
      "epoch: 700 loss is tensor([0.4125], grad_fn=<AddBackward0>)\n",
      "32\n"
=======
      "The number of epochs is: 401\n",
      "The number of epochs is: 402\n",
      "The number of epochs is: 403\n",
      "The number of epochs is: 404\n",
      "The number of epochs is: 405\n",
      "The number of epochs is: 406\n",
      "The number of epochs is: 407\n",
      "The number of epochs is: 408\n",
      "The number of epochs is: 409\n",
      "The number of epochs is: 410\n",
      "The number of epochs is: 411\n",
      "The number of epochs is: 412\n",
      "The number of epochs is: 413\n",
      "The number of epochs is: 414\n",
      "The number of epochs is: 415\n",
      "The number of epochs is: 416\n",
      "The number of epochs is: 417\n",
      "The number of epochs is: 418\n",
      "The number of epochs is: 419\n",
      "The number of epochs is: 420\n",
      "The number of epochs is: 421\n",
      "The number of epochs is: 422\n",
      "The number of epochs is: 423\n",
      "The number of epochs is: 424\n",
      "The number of epochs is: 425\n",
      "The number of epochs is: 426\n",
      "The number of epochs is: 427\n",
      "The number of epochs is: 428\n",
      "The number of epochs is: 429\n",
      "The number of epochs is: 430\n",
      "The number of epochs is: 431\n",
      "The number of epochs is: 432\n",
      "The number of epochs is: 433\n",
      "The number of epochs is: 434\n",
      "The number of epochs is: 435\n",
      "The number of epochs is: 436\n",
      "The number of epochs is: 437\n",
      "The number of epochs is: 438\n",
      "The number of epochs is: 439\n",
      "The number of epochs is: 440\n",
      "The number of epochs is: 441\n",
      "The number of epochs is: 442\n",
      "The number of epochs is: 443\n",
      "The number of epochs is: 444\n",
      "The number of epochs is: 445\n",
      "The number of epochs is: 446\n",
      "The number of epochs is: 447\n",
      "The number of epochs is: 448\n",
      "The number of epochs is: 449\n",
      "The number of epochs is: 450\n",
      "The number of epochs is: 451\n",
      "The number of epochs is: 452\n",
      "The number of epochs is: 453\n",
      "The number of epochs is: 454\n",
      "The number of epochs is: 455\n",
      "The number of epochs is: 456\n",
      "The number of epochs is: 457\n",
      "The number of epochs is: 458\n",
      "The number of epochs is: 459\n",
      "The number of epochs is: 460\n",
      "The number of epochs is: 461\n",
      "The number of epochs is: 462\n",
      "The number of epochs is: 463\n",
      "The number of epochs is: 464\n",
      "The number of epochs is: 465\n",
      "The number of epochs is: 466\n",
      "The number of epochs is: 467\n",
      "The number of epochs is: 468\n",
      "The number of epochs is: 469\n",
      "The number of epochs is: 470\n",
      "The number of epochs is: 471\n",
      "The number of epochs is: 472\n",
      "The number of epochs is: 473\n",
      "The number of epochs is: 474\n",
      "The number of epochs is: 475\n",
      "The number of epochs is: 476\n",
      "The number of epochs is: 477\n",
      "The number of epochs is: 478\n",
      "The number of epochs is: 479\n",
      "The number of epochs is: 480\n",
      "The number of epochs is: 481\n",
      "The number of epochs is: 482\n",
      "The number of epochs is: 483\n",
      "The number of epochs is: 484\n",
      "The number of epochs is: 485\n",
      "The number of epochs is: 486\n",
      "The number of epochs is: 487\n",
      "The number of epochs is: 488\n",
      "The number of epochs is: 489\n",
      "The number of epochs is: 490\n",
      "The number of epochs is: 491\n",
      "The number of epochs is: 492\n",
      "The number of epochs is: 493\n",
      "The number of epochs is: 494\n",
      "The number of epochs is: 495\n",
      "The number of epochs is: 496\n",
      "The number of epochs is: 497\n",
      "The number of epochs is: 498\n",
      "The number of epochs is: 499\n",
      "The number of epochs is: 500\n",
      "The number of epochs is: 501\n",
      "The number of epochs is: 502\n",
      "The number of epochs is: 503\n",
      "The number of epochs is: 504\n",
      "The number of epochs is: 505\n",
      "The number of epochs is: 506\n",
      "The number of epochs is: 507\n",
      "The number of epochs is: 508\n",
      "The number of epochs is: 509\n",
      "The number of epochs is: 510\n",
      "The number of epochs is: 511\n",
      "The number of epochs is: 512\n",
      "The number of epochs is: 513\n",
      "The number of epochs is: 514\n",
      "The number of epochs is: 515\n",
      "The number of epochs is: 516\n",
      "The number of epochs is: 517\n",
      "The number of epochs is: 518\n",
      "The number of epochs is: 519\n",
      "The number of epochs is: 520\n",
      "The number of epochs is: 521\n",
      "The number of epochs is: 522\n",
      "The number of epochs is: 523\n",
      "The number of epochs is: 524\n",
      "The number of epochs is: 525\n",
      "The number of epochs is: 526\n",
      "The number of epochs is: 527\n",
      "The number of epochs is: 528\n",
      "The number of epochs is: 529\n",
      "The number of epochs is: 530\n",
      "The number of epochs is: 531\n",
      "The number of epochs is: 532\n",
      "The number of epochs is: 533\n",
      "The number of epochs is: 534\n",
      "The number of epochs is: 535\n",
      "The number of epochs is: 536\n",
      "The number of epochs is: 537\n",
      "The number of epochs is: 538\n",
      "The number of epochs is: 539\n",
      "The number of epochs is: 540\n",
      "The number of epochs is: 541\n",
      "The number of epochs is: 542\n",
      "The number of epochs is: 543\n",
      "The number of epochs is: 544\n",
      "The number of epochs is: 545\n",
      "The number of epochs is: 546\n",
      "The number of epochs is: 547\n",
      "The number of epochs is: 548\n",
      "The number of epochs is: 549\n",
      "The number of epochs is: 550\n",
      "The number of epochs is: 551\n",
      "The number of epochs is: 552\n",
      "The number of epochs is: 553\n",
      "The number of epochs is: 554\n",
      "The number of epochs is: 555\n",
      "The number of epochs is: 556\n",
      "The number of epochs is: 557\n",
      "The number of epochs is: 558\n",
      "The number of epochs is: 559\n",
      "The number of epochs is: 560\n",
      "The number of epochs is: 561\n",
      "The number of epochs is: 562\n",
      "The number of epochs is: 563\n",
      "The number of epochs is: 564\n",
      "The number of epochs is: 565\n",
      "The number of epochs is: 566\n",
      "The number of epochs is: 567\n",
      "The number of epochs is: 568\n",
      "The number of epochs is: 569\n",
      "The number of epochs is: 570\n",
      "The number of epochs is: 571\n",
      "The number of epochs is: 572\n",
      "The number of epochs is: 573\n",
      "The number of epochs is: 574\n",
      "The number of epochs is: 575\n",
      "The number of epochs is: 576\n",
      "The number of epochs is: 577\n",
      "The number of epochs is: 578\n",
      "The number of epochs is: 579\n",
      "The number of epochs is: 580\n",
      "The number of epochs is: 581\n",
      "The number of epochs is: 582\n",
      "The number of epochs is: 583\n",
      "The number of epochs is: 584\n",
      "The number of epochs is: 585\n",
      "The number of epochs is: 586\n",
      "The number of epochs is: 587\n",
      "The number of epochs is: 588\n",
      "The number of epochs is: 589\n",
      "The number of epochs is: 590\n",
      "The number of epochs is: 591\n",
      "The number of epochs is: 592\n",
      "The number of epochs is: 593\n",
      "The number of epochs is: 594\n",
      "The number of epochs is: 595\n",
      "The number of epochs is: 596\n",
      "The number of epochs is: 597\n",
      "The number of epochs is: 598\n",
      "The number of epochs is: 599\n",
      "The number of epochs is: 600\n",
      "14\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 701 loss is tensor([0.4495], grad_fn=<AddBackward0>)\n",
      "epoch: 702 loss is tensor([0.3952], grad_fn=<AddBackward0>)\n",
      "epoch: 703 loss is tensor([0.2912], grad_fn=<AddBackward0>)\n",
      "epoch: 704 loss is tensor([0.3727], grad_fn=<AddBackward0>)\n",
      "epoch: 705 loss is tensor([0.4628], grad_fn=<AddBackward0>)\n",
      "epoch: 706 loss is tensor([0.3762], grad_fn=<AddBackward0>)\n",
      "epoch: 707 loss is tensor([0.3988], grad_fn=<AddBackward0>)\n",
      "epoch: 708 loss is tensor([0.3964], grad_fn=<AddBackward0>)\n",
      "epoch: 709 loss is tensor([0.4202], grad_fn=<AddBackward0>)\n",
      "epoch: 710 loss is tensor([0.4594], grad_fn=<AddBackward0>)\n",
      "epoch: 711 loss is tensor([0.4381], grad_fn=<AddBackward0>)\n",
      "epoch: 712 loss is tensor([0.3954], grad_fn=<AddBackward0>)\n",
      "epoch: 713 loss is tensor([0.3951], grad_fn=<AddBackward0>)\n",
      "epoch: 714 loss is tensor([0.4578], grad_fn=<AddBackward0>)\n",
      "epoch: 715 loss is tensor([0.3858], grad_fn=<AddBackward0>)\n",
      "epoch: 716 loss is tensor([0.3781], grad_fn=<AddBackward0>)\n",
      "epoch: 717 loss is tensor([0.3896], grad_fn=<AddBackward0>)\n",
      "epoch: 718 loss is tensor([0.4519], grad_fn=<AddBackward0>)\n",
      "epoch: 719 loss is tensor([0.3627], grad_fn=<AddBackward0>)\n",
      "epoch: 720 loss is tensor([0.3401], grad_fn=<AddBackward0>)\n",
      "epoch: 721 loss is tensor([0.3771], grad_fn=<AddBackward0>)\n",
      "epoch: 722 loss is tensor([0.3604], grad_fn=<AddBackward0>)\n",
      "epoch: 723 loss is tensor([0.3638], grad_fn=<AddBackward0>)\n",
      "epoch: 724 loss is tensor([0.4083], grad_fn=<AddBackward0>)\n",
      "epoch: 725 loss is tensor([0.3738], grad_fn=<AddBackward0>)\n",
      "epoch: 726 loss is tensor([0.4435], grad_fn=<AddBackward0>)\n",
      "epoch: 727 loss is tensor([0.3910], grad_fn=<AddBackward0>)\n",
      "epoch: 728 loss is tensor([0.2827], grad_fn=<AddBackward0>)\n",
      "epoch: 729 loss is tensor([0.4256], grad_fn=<AddBackward0>)\n",
      "epoch: 730 loss is tensor([0.3511], grad_fn=<AddBackward0>)\n",
      "epoch: 731 loss is tensor([0.4106], grad_fn=<AddBackward0>)\n",
      "epoch: 732 loss is tensor([0.4015], grad_fn=<AddBackward0>)\n",
      "epoch: 733 loss is tensor([0.3824], grad_fn=<AddBackward0>)\n",
      "epoch: 734 loss is tensor([0.3977], grad_fn=<AddBackward0>)\n",
      "epoch: 735 loss is tensor([0.3976], grad_fn=<AddBackward0>)\n",
      "epoch: 736 loss is tensor([0.3778], grad_fn=<AddBackward0>)\n",
      "epoch: 737 loss is tensor([0.4020], grad_fn=<AddBackward0>)\n",
      "epoch: 738 loss is tensor([0.3595], grad_fn=<AddBackward0>)\n",
      "epoch: 739 loss is tensor([0.3921], grad_fn=<AddBackward0>)\n",
      "epoch: 740 loss is tensor([0.4006], grad_fn=<AddBackward0>)\n",
      "epoch: 741 loss is tensor([0.4006], grad_fn=<AddBackward0>)\n",
      "epoch: 742 loss is tensor([0.3622], grad_fn=<AddBackward0>)\n",
      "epoch: 743 loss is tensor([0.3818], grad_fn=<AddBackward0>)\n",
      "epoch: 744 loss is tensor([0.3510], grad_fn=<AddBackward0>)\n",
      "epoch: 745 loss is tensor([0.4357], grad_fn=<AddBackward0>)\n",
      "epoch: 746 loss is tensor([0.3983], grad_fn=<AddBackward0>)\n",
      "epoch: 747 loss is tensor([0.3762], grad_fn=<AddBackward0>)\n",
      "epoch: 748 loss is tensor([0.4194], grad_fn=<AddBackward0>)\n",
      "epoch: 749 loss is tensor([0.3615], grad_fn=<AddBackward0>)\n",
      "epoch: 750 loss is tensor([0.3600], grad_fn=<AddBackward0>)\n",
      "epoch: 751 loss is tensor([0.4141], grad_fn=<AddBackward0>)\n",
      "epoch: 752 loss is tensor([0.4042], grad_fn=<AddBackward0>)\n",
      "epoch: 753 loss is tensor([0.3768], grad_fn=<AddBackward0>)\n",
      "epoch: 754 loss is tensor([0.3942], grad_fn=<AddBackward0>)\n",
      "epoch: 755 loss is tensor([0.4733], grad_fn=<AddBackward0>)\n",
      "epoch: 756 loss is tensor([0.3628], grad_fn=<AddBackward0>)\n",
      "epoch: 757 loss is tensor([0.3965], grad_fn=<AddBackward0>)\n",
      "epoch: 758 loss is tensor([0.3542], grad_fn=<AddBackward0>)\n",
      "epoch: 759 loss is tensor([0.3587], grad_fn=<AddBackward0>)\n",
      "epoch: 760 loss is tensor([0.3729], grad_fn=<AddBackward0>)\n",
      "epoch: 761 loss is tensor([0.3700], grad_fn=<AddBackward0>)\n",
      "epoch: 762 loss is tensor([0.3363], grad_fn=<AddBackward0>)\n",
      "epoch: 763 loss is tensor([0.4110], grad_fn=<AddBackward0>)\n",
      "epoch: 764 loss is tensor([0.3769], grad_fn=<AddBackward0>)\n",
      "epoch: 765 loss is tensor([0.4277], grad_fn=<AddBackward0>)\n",
      "epoch: 766 loss is tensor([0.3629], grad_fn=<AddBackward0>)\n",
      "epoch: 767 loss is tensor([0.3927], grad_fn=<AddBackward0>)\n",
      "epoch: 768 loss is tensor([0.4110], grad_fn=<AddBackward0>)\n",
      "epoch: 769 loss is tensor([0.3825], grad_fn=<AddBackward0>)\n",
      "epoch: 770 loss is tensor([0.3762], grad_fn=<AddBackward0>)\n",
      "epoch: 771 loss is tensor([0.4229], grad_fn=<AddBackward0>)\n",
      "epoch: 772 loss is tensor([0.4411], grad_fn=<AddBackward0>)\n",
      "epoch: 773 loss is tensor([0.3667], grad_fn=<AddBackward0>)\n",
      "epoch: 774 loss is tensor([0.4071], grad_fn=<AddBackward0>)\n",
      "epoch: 775 loss is tensor([0.4034], grad_fn=<AddBackward0>)\n",
      "epoch: 776 loss is tensor([0.4465], grad_fn=<AddBackward0>)\n",
      "epoch: 777 loss is tensor([0.3606], grad_fn=<AddBackward0>)\n",
      "epoch: 778 loss is tensor([0.3916], grad_fn=<AddBackward0>)\n",
      "epoch: 779 loss is tensor([0.4707], grad_fn=<AddBackward0>)\n",
      "epoch: 780 loss is tensor([0.3591], grad_fn=<AddBackward0>)\n",
      "epoch: 781 loss is tensor([0.3673], grad_fn=<AddBackward0>)\n",
      "epoch: 782 loss is tensor([0.3412], grad_fn=<AddBackward0>)\n",
      "epoch: 783 loss is tensor([0.3721], grad_fn=<AddBackward0>)\n",
      "epoch: 784 loss is tensor([0.3804], grad_fn=<AddBackward0>)\n",
      "epoch: 785 loss is tensor([0.3771], grad_fn=<AddBackward0>)\n",
      "epoch: 786 loss is tensor([0.3567], grad_fn=<AddBackward0>)\n",
      "epoch: 787 loss is tensor([0.3193], grad_fn=<AddBackward0>)\n",
      "epoch: 788 loss is tensor([0.3521], grad_fn=<AddBackward0>)\n",
      "epoch: 789 loss is tensor([0.3345], grad_fn=<AddBackward0>)\n",
      "epoch: 790 loss is tensor([0.4163], grad_fn=<AddBackward0>)\n",
      "epoch: 791 loss is tensor([0.3597], grad_fn=<AddBackward0>)\n",
      "epoch: 792 loss is tensor([0.3602], grad_fn=<AddBackward0>)\n",
      "epoch: 793 loss is tensor([0.3886], grad_fn=<AddBackward0>)\n",
      "epoch: 794 loss is tensor([0.3428], grad_fn=<AddBackward0>)\n",
      "epoch: 795 loss is tensor([0.3475], grad_fn=<AddBackward0>)\n",
      "epoch: 796 loss is tensor([0.4277], grad_fn=<AddBackward0>)\n",
      "epoch: 797 loss is tensor([0.3447], grad_fn=<AddBackward0>)\n",
      "epoch: 798 loss is tensor([0.3621], grad_fn=<AddBackward0>)\n",
      "epoch: 799 loss is tensor([0.3744], grad_fn=<AddBackward0>)\n",
      "epoch: 800 loss is tensor([0.4295], grad_fn=<AddBackward0>)\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 801 loss is tensor([0.3534], grad_fn=<AddBackward0>)\n",
      "epoch: 802 loss is tensor([0.3627], grad_fn=<AddBackward0>)\n",
      "epoch: 803 loss is tensor([0.3690], grad_fn=<AddBackward0>)\n",
      "epoch: 804 loss is tensor([0.3124], grad_fn=<AddBackward0>)\n",
      "epoch: 805 loss is tensor([0.2984], grad_fn=<AddBackward0>)\n",
      "epoch: 806 loss is tensor([0.4299], grad_fn=<AddBackward0>)\n",
      "epoch: 807 loss is tensor([0.2939], grad_fn=<AddBackward0>)\n",
      "epoch: 808 loss is tensor([0.3379], grad_fn=<AddBackward0>)\n",
      "epoch: 809 loss is tensor([0.3504], grad_fn=<AddBackward0>)\n",
      "epoch: 810 loss is tensor([0.3107], grad_fn=<AddBackward0>)\n",
      "epoch: 811 loss is tensor([0.3891], grad_fn=<AddBackward0>)\n",
      "epoch: 812 loss is tensor([0.4039], grad_fn=<AddBackward0>)\n",
      "epoch: 813 loss is tensor([0.3051], grad_fn=<AddBackward0>)\n",
      "epoch: 814 loss is tensor([0.3431], grad_fn=<AddBackward0>)\n",
      "epoch: 815 loss is tensor([0.3836], grad_fn=<AddBackward0>)\n",
      "epoch: 816 loss is tensor([0.4020], grad_fn=<AddBackward0>)\n",
      "epoch: 817 loss is tensor([0.3605], grad_fn=<AddBackward0>)\n",
      "epoch: 818 loss is tensor([0.3486], grad_fn=<AddBackward0>)\n",
      "epoch: 819 loss is tensor([0.3641], grad_fn=<AddBackward0>)\n",
      "epoch: 820 loss is tensor([0.3483], grad_fn=<AddBackward0>)\n",
      "epoch: 821 loss is tensor([0.2991], grad_fn=<AddBackward0>)\n",
      "epoch: 822 loss is tensor([0.3693], grad_fn=<AddBackward0>)\n",
      "epoch: 823 loss is tensor([0.3802], grad_fn=<AddBackward0>)\n",
      "epoch: 824 loss is tensor([0.3255], grad_fn=<AddBackward0>)\n",
      "epoch: 825 loss is tensor([0.3528], grad_fn=<AddBackward0>)\n",
      "epoch: 826 loss is tensor([0.3214], grad_fn=<AddBackward0>)\n",
      "epoch: 827 loss is tensor([0.3745], grad_fn=<AddBackward0>)\n",
      "epoch: 828 loss is tensor([0.3319], grad_fn=<AddBackward0>)\n",
      "epoch: 829 loss is tensor([0.3490], grad_fn=<AddBackward0>)\n",
      "epoch: 830 loss is tensor([0.3380], grad_fn=<AddBackward0>)\n",
      "epoch: 831 loss is tensor([0.4002], grad_fn=<AddBackward0>)\n",
      "epoch: 832 loss is tensor([0.3382], grad_fn=<AddBackward0>)\n",
      "epoch: 833 loss is tensor([0.3349], grad_fn=<AddBackward0>)\n",
      "epoch: 834 loss is tensor([0.3363], grad_fn=<AddBackward0>)\n",
      "epoch: 835 loss is tensor([0.3116], grad_fn=<AddBackward0>)\n",
      "epoch: 836 loss is tensor([0.3978], grad_fn=<AddBackward0>)\n",
      "epoch: 837 loss is tensor([0.3528], grad_fn=<AddBackward0>)\n",
      "epoch: 838 loss is tensor([0.3406], grad_fn=<AddBackward0>)\n",
      "epoch: 839 loss is tensor([0.3070], grad_fn=<AddBackward0>)\n",
      "epoch: 840 loss is tensor([0.3332], grad_fn=<AddBackward0>)\n",
      "epoch: 841 loss is tensor([0.2933], grad_fn=<AddBackward0>)\n",
      "epoch: 842 loss is tensor([0.3789], grad_fn=<AddBackward0>)\n",
      "epoch: 843 loss is tensor([0.3205], grad_fn=<AddBackward0>)\n",
      "epoch: 844 loss is tensor([0.3223], grad_fn=<AddBackward0>)\n",
      "epoch: 845 loss is tensor([0.3155], grad_fn=<AddBackward0>)\n",
      "epoch: 846 loss is tensor([0.3631], grad_fn=<AddBackward0>)\n",
      "epoch: 847 loss is tensor([0.3604], grad_fn=<AddBackward0>)\n",
      "epoch: 848 loss is tensor([0.3185], grad_fn=<AddBackward0>)\n",
      "epoch: 849 loss is tensor([0.3864], grad_fn=<AddBackward0>)\n",
      "epoch: 850 loss is tensor([0.3202], grad_fn=<AddBackward0>)\n",
      "epoch: 851 loss is tensor([0.3406], grad_fn=<AddBackward0>)\n",
      "epoch: 852 loss is tensor([0.3609], grad_fn=<AddBackward0>)\n",
      "epoch: 853 loss is tensor([0.3308], grad_fn=<AddBackward0>)\n",
      "epoch: 854 loss is tensor([0.3095], grad_fn=<AddBackward0>)\n",
      "epoch: 855 loss is tensor([0.3591], grad_fn=<AddBackward0>)\n",
      "epoch: 856 loss is tensor([0.3399], grad_fn=<AddBackward0>)\n",
      "epoch: 857 loss is tensor([0.3492], grad_fn=<AddBackward0>)\n",
      "epoch: 858 loss is tensor([0.3577], grad_fn=<AddBackward0>)\n",
      "epoch: 859 loss is tensor([0.3171], grad_fn=<AddBackward0>)\n",
      "epoch: 860 loss is tensor([0.3498], grad_fn=<AddBackward0>)\n",
      "epoch: 861 loss is tensor([0.3519], grad_fn=<AddBackward0>)\n",
      "epoch: 862 loss is tensor([0.3202], grad_fn=<AddBackward0>)\n",
      "epoch: 863 loss is tensor([0.3244], grad_fn=<AddBackward0>)\n",
      "epoch: 864 loss is tensor([0.2964], grad_fn=<AddBackward0>)\n",
      "epoch: 865 loss is tensor([0.3285], grad_fn=<AddBackward0>)\n",
      "epoch: 866 loss is tensor([0.3058], grad_fn=<AddBackward0>)\n",
      "epoch: 867 loss is tensor([0.3431], grad_fn=<AddBackward0>)\n",
      "epoch: 868 loss is tensor([0.3546], grad_fn=<AddBackward0>)\n",
      "epoch: 869 loss is tensor([0.3906], grad_fn=<AddBackward0>)\n",
      "epoch: 870 loss is tensor([0.3977], grad_fn=<AddBackward0>)\n",
      "epoch: 871 loss is tensor([0.2842], grad_fn=<AddBackward0>)\n",
      "epoch: 872 loss is tensor([0.4263], grad_fn=<AddBackward0>)\n",
      "epoch: 873 loss is tensor([0.3527], grad_fn=<AddBackward0>)\n",
      "epoch: 874 loss is tensor([0.3617], grad_fn=<AddBackward0>)\n",
      "epoch: 875 loss is tensor([0.3542], grad_fn=<AddBackward0>)\n",
      "epoch: 876 loss is tensor([0.3200], grad_fn=<AddBackward0>)\n",
      "epoch: 877 loss is tensor([0.3366], grad_fn=<AddBackward0>)\n",
      "epoch: 878 loss is tensor([0.3779], grad_fn=<AddBackward0>)\n",
      "epoch: 879 loss is tensor([0.3022], grad_fn=<AddBackward0>)\n",
      "epoch: 880 loss is tensor([0.3577], grad_fn=<AddBackward0>)\n",
      "epoch: 881 loss is tensor([0.3951], grad_fn=<AddBackward0>)\n",
      "epoch: 882 loss is tensor([0.3875], grad_fn=<AddBackward0>)\n",
      "epoch: 883 loss is tensor([0.2945], grad_fn=<AddBackward0>)\n",
      "epoch: 884 loss is tensor([0.3794], grad_fn=<AddBackward0>)\n",
      "epoch: 885 loss is tensor([0.2914], grad_fn=<AddBackward0>)\n",
      "epoch: 886 loss is tensor([0.3465], grad_fn=<AddBackward0>)\n",
      "epoch: 887 loss is tensor([0.3241], grad_fn=<AddBackward0>)\n",
      "epoch: 888 loss is tensor([0.3392], grad_fn=<AddBackward0>)\n",
      "epoch: 889 loss is tensor([0.2916], grad_fn=<AddBackward0>)\n",
      "epoch: 890 loss is tensor([0.2869], grad_fn=<AddBackward0>)\n",
      "epoch: 891 loss is tensor([0.3711], grad_fn=<AddBackward0>)\n",
      "epoch: 892 loss is tensor([0.2963], grad_fn=<AddBackward0>)\n",
      "epoch: 893 loss is tensor([0.3245], grad_fn=<AddBackward0>)\n",
      "epoch: 894 loss is tensor([0.3617], grad_fn=<AddBackward0>)\n",
      "epoch: 895 loss is tensor([0.3253], grad_fn=<AddBackward0>)\n",
      "epoch: 896 loss is tensor([0.2544], grad_fn=<AddBackward0>)\n",
      "epoch: 897 loss is tensor([0.3537], grad_fn=<AddBackward0>)\n",
      "epoch: 898 loss is tensor([0.3104], grad_fn=<AddBackward0>)\n",
      "epoch: 899 loss is tensor([0.3478], grad_fn=<AddBackward0>)\n",
      "epoch: 900 loss is tensor([0.2923], grad_fn=<AddBackward0>)\n",
      "26\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 901 loss is tensor([0.3597], grad_fn=<AddBackward0>)\n",
      "epoch: 902 loss is tensor([0.3295], grad_fn=<AddBackward0>)\n",
      "epoch: 903 loss is tensor([0.3408], grad_fn=<AddBackward0>)\n",
      "epoch: 904 loss is tensor([0.3739], grad_fn=<AddBackward0>)\n",
      "epoch: 905 loss is tensor([0.2613], grad_fn=<AddBackward0>)\n",
      "epoch: 906 loss is tensor([0.3051], grad_fn=<AddBackward0>)\n",
      "epoch: 907 loss is tensor([0.3791], grad_fn=<AddBackward0>)\n",
      "epoch: 908 loss is tensor([0.3797], grad_fn=<AddBackward0>)\n",
      "epoch: 909 loss is tensor([0.2779], grad_fn=<AddBackward0>)\n",
      "epoch: 910 loss is tensor([0.3450], grad_fn=<AddBackward0>)\n",
      "epoch: 911 loss is tensor([0.3677], grad_fn=<AddBackward0>)\n",
      "epoch: 912 loss is tensor([0.3215], grad_fn=<AddBackward0>)\n",
      "epoch: 913 loss is tensor([0.3432], grad_fn=<AddBackward0>)\n",
      "epoch: 914 loss is tensor([0.3164], grad_fn=<AddBackward0>)\n",
      "epoch: 915 loss is tensor([0.3173], grad_fn=<AddBackward0>)\n",
      "epoch: 916 loss is tensor([0.3470], grad_fn=<AddBackward0>)\n",
      "epoch: 917 loss is tensor([0.3458], grad_fn=<AddBackward0>)\n",
      "epoch: 918 loss is tensor([0.3716], grad_fn=<AddBackward0>)\n",
      "epoch: 919 loss is tensor([0.3334], grad_fn=<AddBackward0>)\n",
      "epoch: 920 loss is tensor([0.3203], grad_fn=<AddBackward0>)\n",
      "epoch: 921 loss is tensor([0.3447], grad_fn=<AddBackward0>)\n",
      "epoch: 922 loss is tensor([0.3236], grad_fn=<AddBackward0>)\n",
      "epoch: 923 loss is tensor([0.3524], grad_fn=<AddBackward0>)\n",
      "epoch: 924 loss is tensor([0.2426], grad_fn=<AddBackward0>)\n",
      "epoch: 925 loss is tensor([0.3130], grad_fn=<AddBackward0>)\n",
      "epoch: 926 loss is tensor([0.2698], grad_fn=<AddBackward0>)\n",
      "epoch: 927 loss is tensor([0.3803], grad_fn=<AddBackward0>)\n",
      "epoch: 928 loss is tensor([0.3140], grad_fn=<AddBackward0>)\n",
      "epoch: 929 loss is tensor([0.2901], grad_fn=<AddBackward0>)\n",
      "epoch: 930 loss is tensor([0.3196], grad_fn=<AddBackward0>)\n",
      "epoch: 931 loss is tensor([0.3247], grad_fn=<AddBackward0>)\n",
      "epoch: 932 loss is tensor([0.3289], grad_fn=<AddBackward0>)\n",
      "epoch: 933 loss is tensor([0.3059], grad_fn=<AddBackward0>)\n",
      "epoch: 934 loss is tensor([0.2956], grad_fn=<AddBackward0>)\n",
      "epoch: 935 loss is tensor([0.2893], grad_fn=<AddBackward0>)\n",
      "epoch: 936 loss is tensor([0.3356], grad_fn=<AddBackward0>)\n",
      "epoch: 937 loss is tensor([0.3077], grad_fn=<AddBackward0>)\n",
      "epoch: 938 loss is tensor([0.3923], grad_fn=<AddBackward0>)\n",
      "epoch: 939 loss is tensor([0.3560], grad_fn=<AddBackward0>)\n",
      "epoch: 940 loss is tensor([0.3239], grad_fn=<AddBackward0>)\n",
      "epoch: 941 loss is tensor([0.3134], grad_fn=<AddBackward0>)\n",
      "epoch: 942 loss is tensor([0.3392], grad_fn=<AddBackward0>)\n",
      "epoch: 943 loss is tensor([0.3511], grad_fn=<AddBackward0>)\n",
      "epoch: 944 loss is tensor([0.2861], grad_fn=<AddBackward0>)\n",
      "epoch: 945 loss is tensor([0.3248], grad_fn=<AddBackward0>)\n",
      "epoch: 946 loss is tensor([0.3103], grad_fn=<AddBackward0>)\n",
      "epoch: 947 loss is tensor([0.3171], grad_fn=<AddBackward0>)\n",
      "epoch: 948 loss is tensor([0.3169], grad_fn=<AddBackward0>)\n",
      "epoch: 949 loss is tensor([0.2914], grad_fn=<AddBackward0>)\n",
      "epoch: 950 loss is tensor([0.3252], grad_fn=<AddBackward0>)\n",
      "epoch: 951 loss is tensor([0.3082], grad_fn=<AddBackward0>)\n",
      "epoch: 952 loss is tensor([0.3090], grad_fn=<AddBackward0>)\n",
      "epoch: 953 loss is tensor([0.3090], grad_fn=<AddBackward0>)\n",
      "epoch: 954 loss is tensor([0.2573], grad_fn=<AddBackward0>)\n",
      "epoch: 955 loss is tensor([0.3440], grad_fn=<AddBackward0>)\n",
      "epoch: 956 loss is tensor([0.2949], grad_fn=<AddBackward0>)\n",
      "epoch: 957 loss is tensor([0.3084], grad_fn=<AddBackward0>)\n",
      "epoch: 958 loss is tensor([0.3017], grad_fn=<AddBackward0>)\n",
      "epoch: 959 loss is tensor([0.3193], grad_fn=<AddBackward0>)\n",
      "epoch: 960 loss is tensor([0.3199], grad_fn=<AddBackward0>)\n",
      "epoch: 961 loss is tensor([0.3210], grad_fn=<AddBackward0>)\n",
      "epoch: 962 loss is tensor([0.3150], grad_fn=<AddBackward0>)\n",
      "epoch: 963 loss is tensor([0.2011], grad_fn=<AddBackward0>)\n",
      "epoch: 964 loss is tensor([0.3322], grad_fn=<AddBackward0>)\n",
      "epoch: 965 loss is tensor([0.2905], grad_fn=<AddBackward0>)\n",
      "epoch: 966 loss is tensor([0.2913], grad_fn=<AddBackward0>)\n",
      "epoch: 967 loss is tensor([0.3030], grad_fn=<AddBackward0>)\n",
      "epoch: 968 loss is tensor([0.2661], grad_fn=<AddBackward0>)\n",
      "epoch: 969 loss is tensor([0.3572], grad_fn=<AddBackward0>)\n",
      "epoch: 970 loss is tensor([0.2989], grad_fn=<AddBackward0>)\n",
      "epoch: 971 loss is tensor([0.2716], grad_fn=<AddBackward0>)\n",
      "epoch: 972 loss is tensor([0.3443], grad_fn=<AddBackward0>)\n",
      "epoch: 973 loss is tensor([0.3157], grad_fn=<AddBackward0>)\n",
      "epoch: 974 loss is tensor([0.3522], grad_fn=<AddBackward0>)\n",
      "epoch: 975 loss is tensor([0.3206], grad_fn=<AddBackward0>)\n",
      "epoch: 976 loss is tensor([0.3219], grad_fn=<AddBackward0>)\n",
      "epoch: 977 loss is tensor([0.2789], grad_fn=<AddBackward0>)\n",
      "epoch: 978 loss is tensor([0.3261], grad_fn=<AddBackward0>)\n",
      "epoch: 979 loss is tensor([0.3344], grad_fn=<AddBackward0>)\n",
      "epoch: 980 loss is tensor([0.3233], grad_fn=<AddBackward0>)\n",
      "epoch: 981 loss is tensor([0.3469], grad_fn=<AddBackward0>)\n",
      "epoch: 982 loss is tensor([0.2527], grad_fn=<AddBackward0>)\n",
      "epoch: 983 loss is tensor([0.2808], grad_fn=<AddBackward0>)\n",
      "epoch: 984 loss is tensor([0.2858], grad_fn=<AddBackward0>)\n",
      "epoch: 985 loss is tensor([0.2353], grad_fn=<AddBackward0>)\n",
      "epoch: 986 loss is tensor([0.2811], grad_fn=<AddBackward0>)\n",
      "epoch: 987 loss is tensor([0.3563], grad_fn=<AddBackward0>)\n",
      "epoch: 988 loss is tensor([0.3103], grad_fn=<AddBackward0>)\n",
      "epoch: 989 loss is tensor([0.3630], grad_fn=<AddBackward0>)\n",
      "epoch: 990 loss is tensor([0.4150], grad_fn=<AddBackward0>)\n",
      "epoch: 991 loss is tensor([0.3127], grad_fn=<AddBackward0>)\n",
      "epoch: 992 loss is tensor([0.3259], grad_fn=<AddBackward0>)\n",
      "epoch: 993 loss is tensor([0.2728], grad_fn=<AddBackward0>)\n",
      "epoch: 994 loss is tensor([0.3383], grad_fn=<AddBackward0>)\n",
      "epoch: 995 loss is tensor([0.2863], grad_fn=<AddBackward0>)\n",
      "epoch: 996 loss is tensor([0.2639], grad_fn=<AddBackward0>)\n",
      "epoch: 997 loss is tensor([0.2744], grad_fn=<AddBackward0>)\n",
      "epoch: 998 loss is tensor([0.3136], grad_fn=<AddBackward0>)\n",
      "epoch: 999 loss is tensor([0.3591], grad_fn=<AddBackward0>)\n",
      "epoch: 1000 loss is tensor([0.3610], grad_fn=<AddBackward0>)\n",
      "25\n"
=======
      "The number of epochs is: 601\n",
      "The number of epochs is: 602\n",
      "The number of epochs is: 603\n",
      "The number of epochs is: 604\n",
      "The number of epochs is: 605\n",
      "The number of epochs is: 606\n",
      "The number of epochs is: 607\n",
      "The number of epochs is: 608\n",
      "The number of epochs is: 609\n",
      "The number of epochs is: 610\n",
      "The number of epochs is: 611\n",
      "The number of epochs is: 612\n",
      "The number of epochs is: 613\n",
      "The number of epochs is: 614\n",
      "The number of epochs is: 615\n",
      "The number of epochs is: 616\n",
      "The number of epochs is: 617\n",
      "The number of epochs is: 618\n",
      "The number of epochs is: 619\n",
      "The number of epochs is: 620\n",
      "The number of epochs is: 621\n",
      "The number of epochs is: 622\n",
      "The number of epochs is: 623\n",
      "The number of epochs is: 624\n",
      "The number of epochs is: 625\n",
      "The number of epochs is: 626\n",
      "The number of epochs is: 627\n",
      "The number of epochs is: 628\n",
      "The number of epochs is: 629\n",
      "The number of epochs is: 630\n",
      "The number of epochs is: 631\n",
      "The number of epochs is: 632\n",
      "The number of epochs is: 633\n",
      "The number of epochs is: 634\n",
      "The number of epochs is: 635\n",
      "The number of epochs is: 636\n",
      "The number of epochs is: 637\n",
      "The number of epochs is: 638\n",
      "The number of epochs is: 639\n",
      "The number of epochs is: 640\n",
      "The number of epochs is: 641\n",
      "The number of epochs is: 642\n",
      "The number of epochs is: 643\n",
      "The number of epochs is: 644\n",
      "The number of epochs is: 645\n",
      "The number of epochs is: 646\n",
      "The number of epochs is: 647\n",
      "The number of epochs is: 648\n",
      "The number of epochs is: 649\n",
      "The number of epochs is: 650\n",
      "The number of epochs is: 651\n",
      "The number of epochs is: 652\n",
      "The number of epochs is: 653\n",
      "The number of epochs is: 654\n",
      "The number of epochs is: 655\n",
      "The number of epochs is: 656\n",
      "The number of epochs is: 657\n",
      "The number of epochs is: 658\n",
      "The number of epochs is: 659\n",
      "The number of epochs is: 660\n",
      "The number of epochs is: 661\n",
      "The number of epochs is: 662\n",
      "The number of epochs is: 663\n",
      "The number of epochs is: 664\n",
      "The number of epochs is: 665\n",
      "The number of epochs is: 666\n",
      "The number of epochs is: 667\n",
      "The number of epochs is: 668\n",
      "The number of epochs is: 669\n",
      "The number of epochs is: 670\n",
      "The number of epochs is: 671\n",
      "The number of epochs is: 672\n",
      "The number of epochs is: 673\n",
      "The number of epochs is: 674\n",
      "The number of epochs is: 675\n",
      "The number of epochs is: 676\n",
      "The number of epochs is: 677\n",
      "The number of epochs is: 678\n",
      "The number of epochs is: 679\n",
      "The number of epochs is: 680\n",
      "The number of epochs is: 681\n",
      "The number of epochs is: 682\n",
      "The number of epochs is: 683\n",
      "The number of epochs is: 684\n",
      "The number of epochs is: 685\n",
      "The number of epochs is: 686\n",
      "The number of epochs is: 687\n",
      "The number of epochs is: 688\n",
      "The number of epochs is: 689\n",
      "The number of epochs is: 690\n",
      "The number of epochs is: 691\n",
      "The number of epochs is: 692\n",
      "The number of epochs is: 693\n",
      "The number of epochs is: 694\n",
      "The number of epochs is: 695\n",
      "The number of epochs is: 696\n",
      "The number of epochs is: 697\n",
      "The number of epochs is: 698\n",
      "The number of epochs is: 699\n",
      "The number of epochs is: 700\n",
      "The number of epochs is: 701\n",
      "The number of epochs is: 702\n",
      "The number of epochs is: 703\n",
      "The number of epochs is: 704\n",
      "The number of epochs is: 705\n",
      "The number of epochs is: 706\n",
      "The number of epochs is: 707\n",
      "The number of epochs is: 708\n",
      "The number of epochs is: 709\n",
      "The number of epochs is: 710\n",
      "The number of epochs is: 711\n",
      "The number of epochs is: 712\n",
      "The number of epochs is: 713\n",
      "The number of epochs is: 714\n",
      "The number of epochs is: 715\n",
      "The number of epochs is: 716\n",
      "The number of epochs is: 717\n",
      "The number of epochs is: 718\n",
      "The number of epochs is: 719\n",
      "The number of epochs is: 720\n",
      "The number of epochs is: 721\n",
      "The number of epochs is: 722\n",
      "The number of epochs is: 723\n",
      "The number of epochs is: 724\n",
      "The number of epochs is: 725\n",
      "The number of epochs is: 726\n",
      "The number of epochs is: 727\n",
      "The number of epochs is: 728\n",
      "The number of epochs is: 729\n",
      "The number of epochs is: 730\n",
      "The number of epochs is: 731\n",
      "The number of epochs is: 732\n",
      "The number of epochs is: 733\n",
      "The number of epochs is: 734\n",
      "The number of epochs is: 735\n",
      "The number of epochs is: 736\n",
      "The number of epochs is: 737\n",
      "The number of epochs is: 738\n",
      "The number of epochs is: 739\n",
      "The number of epochs is: 740\n",
      "The number of epochs is: 741\n",
      "The number of epochs is: 742\n",
      "The number of epochs is: 743\n",
      "The number of epochs is: 744\n",
      "The number of epochs is: 745\n",
      "The number of epochs is: 746\n",
      "The number of epochs is: 747\n",
      "The number of epochs is: 748\n",
      "The number of epochs is: 749\n",
      "The number of epochs is: 750\n",
      "The number of epochs is: 751\n",
      "The number of epochs is: 752\n",
      "The number of epochs is: 753\n",
      "The number of epochs is: 754\n",
      "The number of epochs is: 755\n",
      "The number of epochs is: 756\n",
      "The number of epochs is: 757\n",
      "The number of epochs is: 758\n",
      "The number of epochs is: 759\n",
      "The number of epochs is: 760\n",
      "The number of epochs is: 761\n",
      "The number of epochs is: 762\n",
      "The number of epochs is: 763\n",
      "The number of epochs is: 764\n",
      "The number of epochs is: 765\n",
      "The number of epochs is: 766\n",
      "The number of epochs is: 767\n",
      "The number of epochs is: 768\n",
      "The number of epochs is: 769\n",
      "The number of epochs is: 770\n",
      "The number of epochs is: 771\n",
      "The number of epochs is: 772\n",
      "The number of epochs is: 773\n",
      "The number of epochs is: 774\n",
      "The number of epochs is: 775\n",
      "The number of epochs is: 776\n",
      "The number of epochs is: 777\n",
      "The number of epochs is: 778\n",
      "The number of epochs is: 779\n",
      "The number of epochs is: 780\n",
      "The number of epochs is: 781\n",
      "The number of epochs is: 782\n",
      "The number of epochs is: 783\n",
      "The number of epochs is: 784\n",
      "The number of epochs is: 785\n",
      "The number of epochs is: 786\n",
      "The number of epochs is: 787\n",
      "The number of epochs is: 788\n",
      "The number of epochs is: 789\n",
      "The number of epochs is: 790\n",
      "The number of epochs is: 791\n",
      "The number of epochs is: 792\n",
      "The number of epochs is: 793\n",
      "The number of epochs is: 794\n",
      "The number of epochs is: 795\n",
      "The number of epochs is: 796\n",
      "The number of epochs is: 797\n",
      "The number of epochs is: 798\n",
      "The number of epochs is: 799\n",
      "The number of epochs is: 800\n",
      "46\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1001 loss is tensor([0.2732], grad_fn=<AddBackward0>)\n",
      "epoch: 1002 loss is tensor([0.3379], grad_fn=<AddBackward0>)\n",
      "epoch: 1003 loss is tensor([0.3077], grad_fn=<AddBackward0>)\n",
      "epoch: 1004 loss is tensor([0.3113], grad_fn=<AddBackward0>)\n",
      "epoch: 1005 loss is tensor([0.3149], grad_fn=<AddBackward0>)\n",
      "epoch: 1006 loss is tensor([0.3107], grad_fn=<AddBackward0>)\n",
      "epoch: 1007 loss is tensor([0.3219], grad_fn=<AddBackward0>)\n",
      "epoch: 1008 loss is tensor([0.3612], grad_fn=<AddBackward0>)\n",
      "epoch: 1009 loss is tensor([0.3084], grad_fn=<AddBackward0>)\n",
      "epoch: 1010 loss is tensor([0.3690], grad_fn=<AddBackward0>)\n",
      "epoch: 1011 loss is tensor([0.2166], grad_fn=<AddBackward0>)\n",
      "epoch: 1012 loss is tensor([0.2776], grad_fn=<AddBackward0>)\n",
      "epoch: 1013 loss is tensor([0.2720], grad_fn=<AddBackward0>)\n",
      "epoch: 1014 loss is tensor([0.2219], grad_fn=<AddBackward0>)\n",
      "epoch: 1015 loss is tensor([0.3026], grad_fn=<AddBackward0>)\n",
      "epoch: 1016 loss is tensor([0.2768], grad_fn=<AddBackward0>)\n",
      "epoch: 1017 loss is tensor([0.3185], grad_fn=<AddBackward0>)\n",
      "epoch: 1018 loss is tensor([0.3392], grad_fn=<AddBackward0>)\n",
      "epoch: 1019 loss is tensor([0.2882], grad_fn=<AddBackward0>)\n",
      "epoch: 1020 loss is tensor([0.3298], grad_fn=<AddBackward0>)\n",
      "epoch: 1021 loss is tensor([0.3278], grad_fn=<AddBackward0>)\n",
      "epoch: 1022 loss is tensor([0.3562], grad_fn=<AddBackward0>)\n",
      "epoch: 1023 loss is tensor([0.3114], grad_fn=<AddBackward0>)\n",
      "epoch: 1024 loss is tensor([0.3237], grad_fn=<AddBackward0>)\n",
      "epoch: 1025 loss is tensor([0.3091], grad_fn=<AddBackward0>)\n",
      "epoch: 1026 loss is tensor([0.2758], grad_fn=<AddBackward0>)\n",
      "epoch: 1027 loss is tensor([0.2871], grad_fn=<AddBackward0>)\n",
      "epoch: 1028 loss is tensor([0.2229], grad_fn=<AddBackward0>)\n",
      "epoch: 1029 loss is tensor([0.2473], grad_fn=<AddBackward0>)\n",
      "epoch: 1030 loss is tensor([0.3241], grad_fn=<AddBackward0>)\n",
      "epoch: 1031 loss is tensor([0.2540], grad_fn=<AddBackward0>)\n",
      "epoch: 1032 loss is tensor([0.2397], grad_fn=<AddBackward0>)\n",
      "epoch: 1033 loss is tensor([0.2231], grad_fn=<AddBackward0>)\n",
      "epoch: 1034 loss is tensor([0.2362], grad_fn=<AddBackward0>)\n",
      "epoch: 1035 loss is tensor([0.3187], grad_fn=<AddBackward0>)\n",
      "epoch: 1036 loss is tensor([0.3198], grad_fn=<AddBackward0>)\n",
      "epoch: 1037 loss is tensor([0.3235], grad_fn=<AddBackward0>)\n",
      "epoch: 1038 loss is tensor([0.3325], grad_fn=<AddBackward0>)\n",
      "epoch: 1039 loss is tensor([0.3288], grad_fn=<AddBackward0>)\n",
      "epoch: 1040 loss is tensor([0.2882], grad_fn=<AddBackward0>)\n",
      "epoch: 1041 loss is tensor([0.2814], grad_fn=<AddBackward0>)\n",
      "epoch: 1042 loss is tensor([0.2932], grad_fn=<AddBackward0>)\n",
      "epoch: 1043 loss is tensor([0.2712], grad_fn=<AddBackward0>)\n",
      "epoch: 1044 loss is tensor([0.2231], grad_fn=<AddBackward0>)\n",
      "epoch: 1045 loss is tensor([0.3474], grad_fn=<AddBackward0>)\n",
      "epoch: 1046 loss is tensor([0.2647], grad_fn=<AddBackward0>)\n",
      "epoch: 1047 loss is tensor([0.3301], grad_fn=<AddBackward0>)\n",
      "epoch: 1048 loss is tensor([0.3019], grad_fn=<AddBackward0>)\n",
      "epoch: 1049 loss is tensor([0.2934], grad_fn=<AddBackward0>)\n",
      "epoch: 1050 loss is tensor([0.3731], grad_fn=<AddBackward0>)\n",
      "epoch: 1051 loss is tensor([0.2596], grad_fn=<AddBackward0>)\n",
      "epoch: 1052 loss is tensor([0.2839], grad_fn=<AddBackward0>)\n",
      "epoch: 1053 loss is tensor([0.2723], grad_fn=<AddBackward0>)\n",
      "epoch: 1054 loss is tensor([0.2803], grad_fn=<AddBackward0>)\n",
      "epoch: 1055 loss is tensor([0.2948], grad_fn=<AddBackward0>)\n",
      "epoch: 1056 loss is tensor([0.2277], grad_fn=<AddBackward0>)\n",
      "epoch: 1057 loss is tensor([0.3229], grad_fn=<AddBackward0>)\n",
      "epoch: 1058 loss is tensor([0.3195], grad_fn=<AddBackward0>)\n",
      "epoch: 1059 loss is tensor([0.2382], grad_fn=<AddBackward0>)\n",
      "epoch: 1060 loss is tensor([0.2722], grad_fn=<AddBackward0>)\n",
      "epoch: 1061 loss is tensor([0.3120], grad_fn=<AddBackward0>)\n",
      "epoch: 1062 loss is tensor([0.2880], grad_fn=<AddBackward0>)\n",
      "epoch: 1063 loss is tensor([0.2737], grad_fn=<AddBackward0>)\n",
      "epoch: 1064 loss is tensor([0.2740], grad_fn=<AddBackward0>)\n",
      "epoch: 1065 loss is tensor([0.3236], grad_fn=<AddBackward0>)\n",
      "epoch: 1066 loss is tensor([0.2125], grad_fn=<AddBackward0>)\n",
      "epoch: 1067 loss is tensor([0.2686], grad_fn=<AddBackward0>)\n",
      "epoch: 1068 loss is tensor([0.2893], grad_fn=<AddBackward0>)\n",
      "epoch: 1069 loss is tensor([0.2868], grad_fn=<AddBackward0>)\n",
      "epoch: 1070 loss is tensor([0.2647], grad_fn=<AddBackward0>)\n",
      "epoch: 1071 loss is tensor([0.2796], grad_fn=<AddBackward0>)\n",
      "epoch: 1072 loss is tensor([0.2468], grad_fn=<AddBackward0>)\n",
      "epoch: 1073 loss is tensor([0.2253], grad_fn=<AddBackward0>)\n",
      "epoch: 1074 loss is tensor([0.2512], grad_fn=<AddBackward0>)\n",
      "epoch: 1075 loss is tensor([0.3578], grad_fn=<AddBackward0>)\n",
      "epoch: 1076 loss is tensor([0.2853], grad_fn=<AddBackward0>)\n",
      "epoch: 1077 loss is tensor([0.1957], grad_fn=<AddBackward0>)\n",
      "epoch: 1078 loss is tensor([0.3042], grad_fn=<AddBackward0>)\n",
      "epoch: 1079 loss is tensor([0.2668], grad_fn=<AddBackward0>)\n",
      "epoch: 1080 loss is tensor([0.2737], grad_fn=<AddBackward0>)\n",
      "epoch: 1081 loss is tensor([0.2277], grad_fn=<AddBackward0>)\n",
      "epoch: 1082 loss is tensor([0.2813], grad_fn=<AddBackward0>)\n",
      "epoch: 1083 loss is tensor([0.2546], grad_fn=<AddBackward0>)\n",
      "epoch: 1084 loss is tensor([0.2618], grad_fn=<AddBackward0>)\n",
      "epoch: 1085 loss is tensor([0.2480], grad_fn=<AddBackward0>)\n",
      "epoch: 1086 loss is tensor([0.2884], grad_fn=<AddBackward0>)\n",
      "epoch: 1087 loss is tensor([0.2247], grad_fn=<AddBackward0>)\n",
      "epoch: 1088 loss is tensor([0.2447], grad_fn=<AddBackward0>)\n",
      "epoch: 1089 loss is tensor([0.3007], grad_fn=<AddBackward0>)\n",
      "epoch: 1090 loss is tensor([0.2150], grad_fn=<AddBackward0>)\n",
      "epoch: 1091 loss is tensor([0.3537], grad_fn=<AddBackward0>)\n",
      "epoch: 1092 loss is tensor([0.2806], grad_fn=<AddBackward0>)\n",
      "epoch: 1093 loss is tensor([0.2582], grad_fn=<AddBackward0>)\n",
      "epoch: 1094 loss is tensor([0.2686], grad_fn=<AddBackward0>)\n",
      "epoch: 1095 loss is tensor([0.3236], grad_fn=<AddBackward0>)\n",
      "epoch: 1096 loss is tensor([0.2494], grad_fn=<AddBackward0>)\n",
      "epoch: 1097 loss is tensor([0.2426], grad_fn=<AddBackward0>)\n",
      "epoch: 1098 loss is tensor([0.3208], grad_fn=<AddBackward0>)\n",
      "epoch: 1099 loss is tensor([0.2548], grad_fn=<AddBackward0>)\n",
      "epoch: 1100 loss is tensor([0.2441], grad_fn=<AddBackward0>)\n",
      "12\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 1101 loss is tensor([0.2070], grad_fn=<AddBackward0>)\n",
      "epoch: 1102 loss is tensor([0.2609], grad_fn=<AddBackward0>)\n",
      "epoch: 1103 loss is tensor([0.2462], grad_fn=<AddBackward0>)\n",
      "epoch: 1104 loss is tensor([0.2372], grad_fn=<AddBackward0>)\n",
      "epoch: 1105 loss is tensor([0.2648], grad_fn=<AddBackward0>)\n",
      "epoch: 1106 loss is tensor([0.2455], grad_fn=<AddBackward0>)\n",
      "epoch: 1107 loss is tensor([0.2344], grad_fn=<AddBackward0>)\n",
      "epoch: 1108 loss is tensor([0.2991], grad_fn=<AddBackward0>)\n",
      "epoch: 1109 loss is tensor([0.2168], grad_fn=<AddBackward0>)\n",
      "epoch: 1110 loss is tensor([0.3485], grad_fn=<AddBackward0>)\n",
      "epoch: 1111 loss is tensor([0.2537], grad_fn=<AddBackward0>)\n",
      "epoch: 1112 loss is tensor([0.2918], grad_fn=<AddBackward0>)\n",
      "epoch: 1113 loss is tensor([0.2926], grad_fn=<AddBackward0>)\n",
      "epoch: 1114 loss is tensor([0.2515], grad_fn=<AddBackward0>)\n",
      "epoch: 1115 loss is tensor([0.2460], grad_fn=<AddBackward0>)\n",
      "epoch: 1116 loss is tensor([0.2270], grad_fn=<AddBackward0>)\n",
      "epoch: 1117 loss is tensor([0.2746], grad_fn=<AddBackward0>)\n",
      "epoch: 1118 loss is tensor([0.2831], grad_fn=<AddBackward0>)\n",
      "epoch: 1119 loss is tensor([0.3186], grad_fn=<AddBackward0>)\n",
      "epoch: 1120 loss is tensor([0.2202], grad_fn=<AddBackward0>)\n",
      "epoch: 1121 loss is tensor([0.1894], grad_fn=<AddBackward0>)\n",
      "epoch: 1122 loss is tensor([0.2196], grad_fn=<AddBackward0>)\n",
      "epoch: 1123 loss is tensor([0.1940], grad_fn=<AddBackward0>)\n",
      "epoch: 1124 loss is tensor([0.2503], grad_fn=<AddBackward0>)\n",
      "epoch: 1125 loss is tensor([0.2334], grad_fn=<AddBackward0>)\n",
      "epoch: 1126 loss is tensor([0.2854], grad_fn=<AddBackward0>)\n",
      "epoch: 1127 loss is tensor([0.2414], grad_fn=<AddBackward0>)\n",
      "epoch: 1128 loss is tensor([0.2746], grad_fn=<AddBackward0>)\n",
      "epoch: 1129 loss is tensor([0.2463], grad_fn=<AddBackward0>)\n",
      "epoch: 1130 loss is tensor([0.2704], grad_fn=<AddBackward0>)\n",
      "epoch: 1131 loss is tensor([0.2517], grad_fn=<AddBackward0>)\n",
      "epoch: 1132 loss is tensor([0.3163], grad_fn=<AddBackward0>)\n",
      "epoch: 1133 loss is tensor([0.2569], grad_fn=<AddBackward0>)\n",
      "epoch: 1134 loss is tensor([0.2883], grad_fn=<AddBackward0>)\n",
      "epoch: 1135 loss is tensor([0.3048], grad_fn=<AddBackward0>)\n",
      "epoch: 1136 loss is tensor([0.2698], grad_fn=<AddBackward0>)\n",
      "epoch: 1137 loss is tensor([0.2658], grad_fn=<AddBackward0>)\n",
      "epoch: 1138 loss is tensor([0.2827], grad_fn=<AddBackward0>)\n",
      "epoch: 1139 loss is tensor([0.2815], grad_fn=<AddBackward0>)\n",
      "epoch: 1140 loss is tensor([0.2416], grad_fn=<AddBackward0>)\n",
      "epoch: 1141 loss is tensor([0.2558], grad_fn=<AddBackward0>)\n",
      "epoch: 1142 loss is tensor([0.2878], grad_fn=<AddBackward0>)\n",
      "epoch: 1143 loss is tensor([0.2343], grad_fn=<AddBackward0>)\n",
      "epoch: 1144 loss is tensor([0.2334], grad_fn=<AddBackward0>)\n",
      "epoch: 1145 loss is tensor([0.2554], grad_fn=<AddBackward0>)\n",
      "epoch: 1146 loss is tensor([0.2306], grad_fn=<AddBackward0>)\n",
      "epoch: 1147 loss is tensor([0.2255], grad_fn=<AddBackward0>)\n",
      "epoch: 1148 loss is tensor([0.2651], grad_fn=<AddBackward0>)\n",
      "epoch: 1149 loss is tensor([0.1982], grad_fn=<AddBackward0>)\n",
      "epoch: 1150 loss is tensor([0.2021], grad_fn=<AddBackward0>)\n",
      "epoch: 1151 loss is tensor([0.1959], grad_fn=<AddBackward0>)\n",
      "epoch: 1152 loss is tensor([0.3061], grad_fn=<AddBackward0>)\n",
      "epoch: 1153 loss is tensor([0.2380], grad_fn=<AddBackward0>)\n",
      "epoch: 1154 loss is tensor([0.3307], grad_fn=<AddBackward0>)\n",
      "epoch: 1155 loss is tensor([0.1984], grad_fn=<AddBackward0>)\n",
      "epoch: 1156 loss is tensor([0.2934], grad_fn=<AddBackward0>)\n",
      "epoch: 1157 loss is tensor([0.2264], grad_fn=<AddBackward0>)\n",
      "epoch: 1158 loss is tensor([0.2510], grad_fn=<AddBackward0>)\n",
      "epoch: 1159 loss is tensor([0.2340], grad_fn=<AddBackward0>)\n",
      "epoch: 1160 loss is tensor([0.2252], grad_fn=<AddBackward0>)\n",
      "epoch: 1161 loss is tensor([0.2184], grad_fn=<AddBackward0>)\n",
      "epoch: 1162 loss is tensor([0.1970], grad_fn=<AddBackward0>)\n",
      "epoch: 1163 loss is tensor([0.2716], grad_fn=<AddBackward0>)\n",
      "epoch: 1164 loss is tensor([0.2306], grad_fn=<AddBackward0>)\n",
      "epoch: 1165 loss is tensor([0.2705], grad_fn=<AddBackward0>)\n",
      "epoch: 1166 loss is tensor([0.1905], grad_fn=<AddBackward0>)\n",
      "epoch: 1167 loss is tensor([0.1986], grad_fn=<AddBackward0>)\n",
      "epoch: 1168 loss is tensor([0.2462], grad_fn=<AddBackward0>)\n",
      "epoch: 1169 loss is tensor([0.2580], grad_fn=<AddBackward0>)\n",
      "epoch: 1170 loss is tensor([0.2890], grad_fn=<AddBackward0>)\n",
      "epoch: 1171 loss is tensor([0.2374], grad_fn=<AddBackward0>)\n",
      "epoch: 1172 loss is tensor([0.2117], grad_fn=<AddBackward0>)\n",
      "epoch: 1173 loss is tensor([0.2559], grad_fn=<AddBackward0>)\n",
      "epoch: 1174 loss is tensor([0.2565], grad_fn=<AddBackward0>)\n",
      "epoch: 1175 loss is tensor([0.1671], grad_fn=<AddBackward0>)\n",
      "epoch: 1176 loss is tensor([0.2150], grad_fn=<AddBackward0>)\n",
      "epoch: 1177 loss is tensor([0.2657], grad_fn=<AddBackward0>)\n",
      "epoch: 1178 loss is tensor([0.2966], grad_fn=<AddBackward0>)\n",
      "epoch: 1179 loss is tensor([0.2553], grad_fn=<AddBackward0>)\n",
      "epoch: 1180 loss is tensor([0.2211], grad_fn=<AddBackward0>)\n",
      "epoch: 1181 loss is tensor([0.2211], grad_fn=<AddBackward0>)\n",
      "epoch: 1182 loss is tensor([0.2647], grad_fn=<AddBackward0>)\n",
      "epoch: 1183 loss is tensor([0.1747], grad_fn=<AddBackward0>)\n",
      "epoch: 1184 loss is tensor([0.2617], grad_fn=<AddBackward0>)\n",
      "epoch: 1185 loss is tensor([0.2201], grad_fn=<AddBackward0>)\n",
      "epoch: 1186 loss is tensor([0.2457], grad_fn=<AddBackward0>)\n",
      "epoch: 1187 loss is tensor([0.2838], grad_fn=<AddBackward0>)\n",
      "epoch: 1188 loss is tensor([0.2318], grad_fn=<AddBackward0>)\n",
      "epoch: 1189 loss is tensor([0.2447], grad_fn=<AddBackward0>)\n",
      "epoch: 1190 loss is tensor([0.1865], grad_fn=<AddBackward0>)\n",
      "epoch: 1191 loss is tensor([0.2420], grad_fn=<AddBackward0>)\n",
      "epoch: 1192 loss is tensor([0.2616], grad_fn=<AddBackward0>)\n",
      "epoch: 1193 loss is tensor([0.2768], grad_fn=<AddBackward0>)\n",
      "epoch: 1194 loss is tensor([0.2886], grad_fn=<AddBackward0>)\n",
      "epoch: 1195 loss is tensor([0.1800], grad_fn=<AddBackward0>)\n",
      "epoch: 1196 loss is tensor([0.1972], grad_fn=<AddBackward0>)\n",
      "epoch: 1197 loss is tensor([0.2037], grad_fn=<AddBackward0>)\n",
      "epoch: 1198 loss is tensor([0.1977], grad_fn=<AddBackward0>)\n",
      "epoch: 1199 loss is tensor([0.2214], grad_fn=<AddBackward0>)\n",
      "epoch: 1200 loss is tensor([0.2588], grad_fn=<AddBackward0>)\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1201 loss is tensor([0.1828], grad_fn=<AddBackward0>)\n",
      "epoch: 1202 loss is tensor([0.2359], grad_fn=<AddBackward0>)\n",
      "epoch: 1203 loss is tensor([0.2273], grad_fn=<AddBackward0>)\n",
      "epoch: 1204 loss is tensor([0.2521], grad_fn=<AddBackward0>)\n",
      "epoch: 1205 loss is tensor([0.2555], grad_fn=<AddBackward0>)\n",
      "epoch: 1206 loss is tensor([0.2056], grad_fn=<AddBackward0>)\n",
      "epoch: 1207 loss is tensor([0.2146], grad_fn=<AddBackward0>)\n",
      "epoch: 1208 loss is tensor([0.2072], grad_fn=<AddBackward0>)\n",
      "epoch: 1209 loss is tensor([0.2542], grad_fn=<AddBackward0>)\n",
      "epoch: 1210 loss is tensor([0.2297], grad_fn=<AddBackward0>)\n",
      "epoch: 1211 loss is tensor([0.2717], grad_fn=<AddBackward0>)\n",
      "epoch: 1212 loss is tensor([0.1874], grad_fn=<AddBackward0>)\n",
      "epoch: 1213 loss is tensor([0.2280], grad_fn=<AddBackward0>)\n",
      "epoch: 1214 loss is tensor([0.2228], grad_fn=<AddBackward0>)\n",
      "epoch: 1215 loss is tensor([0.2702], grad_fn=<AddBackward0>)\n",
      "epoch: 1216 loss is tensor([0.1586], grad_fn=<AddBackward0>)\n",
      "epoch: 1217 loss is tensor([0.2281], grad_fn=<AddBackward0>)\n",
      "epoch: 1218 loss is tensor([0.2243], grad_fn=<AddBackward0>)\n",
      "epoch: 1219 loss is tensor([0.2086], grad_fn=<AddBackward0>)\n",
      "epoch: 1220 loss is tensor([0.2127], grad_fn=<AddBackward0>)\n",
      "epoch: 1221 loss is tensor([0.2302], grad_fn=<AddBackward0>)\n",
      "epoch: 1222 loss is tensor([0.2579], grad_fn=<AddBackward0>)\n",
      "epoch: 1223 loss is tensor([0.2237], grad_fn=<AddBackward0>)\n",
      "epoch: 1224 loss is tensor([0.1975], grad_fn=<AddBackward0>)\n",
      "epoch: 1225 loss is tensor([0.2858], grad_fn=<AddBackward0>)\n",
      "epoch: 1226 loss is tensor([0.1875], grad_fn=<AddBackward0>)\n",
      "epoch: 1227 loss is tensor([0.2800], grad_fn=<AddBackward0>)\n",
      "epoch: 1228 loss is tensor([0.2661], grad_fn=<AddBackward0>)\n",
      "epoch: 1229 loss is tensor([0.2664], grad_fn=<AddBackward0>)\n",
      "epoch: 1230 loss is tensor([0.3201], grad_fn=<AddBackward0>)\n",
      "epoch: 1231 loss is tensor([0.3022], grad_fn=<AddBackward0>)\n",
      "epoch: 1232 loss is tensor([0.2398], grad_fn=<AddBackward0>)\n",
      "epoch: 1233 loss is tensor([0.2573], grad_fn=<AddBackward0>)\n",
      "epoch: 1234 loss is tensor([0.3255], grad_fn=<AddBackward0>)\n",
      "epoch: 1235 loss is tensor([0.2893], grad_fn=<AddBackward0>)\n",
      "epoch: 1236 loss is tensor([0.2310], grad_fn=<AddBackward0>)\n",
      "epoch: 1237 loss is tensor([0.2715], grad_fn=<AddBackward0>)\n",
      "epoch: 1238 loss is tensor([0.2290], grad_fn=<AddBackward0>)\n",
      "epoch: 1239 loss is tensor([0.1768], grad_fn=<AddBackward0>)\n",
      "epoch: 1240 loss is tensor([0.2212], grad_fn=<AddBackward0>)\n",
      "epoch: 1241 loss is tensor([0.2245], grad_fn=<AddBackward0>)\n",
      "epoch: 1242 loss is tensor([0.2104], grad_fn=<AddBackward0>)\n",
      "epoch: 1243 loss is tensor([0.2679], grad_fn=<AddBackward0>)\n",
      "epoch: 1244 loss is tensor([0.1928], grad_fn=<AddBackward0>)\n",
      "epoch: 1245 loss is tensor([0.2575], grad_fn=<AddBackward0>)\n",
      "epoch: 1246 loss is tensor([0.3187], grad_fn=<AddBackward0>)\n",
      "epoch: 1247 loss is tensor([0.2324], grad_fn=<AddBackward0>)\n",
      "epoch: 1248 loss is tensor([0.2836], grad_fn=<AddBackward0>)\n",
      "epoch: 1249 loss is tensor([0.2700], grad_fn=<AddBackward0>)\n",
      "epoch: 1250 loss is tensor([0.2109], grad_fn=<AddBackward0>)\n",
      "epoch: 1251 loss is tensor([0.2582], grad_fn=<AddBackward0>)\n",
      "epoch: 1252 loss is tensor([0.2607], grad_fn=<AddBackward0>)\n",
      "epoch: 1253 loss is tensor([0.2533], grad_fn=<AddBackward0>)\n",
      "epoch: 1254 loss is tensor([0.2777], grad_fn=<AddBackward0>)\n",
      "epoch: 1255 loss is tensor([0.2545], grad_fn=<AddBackward0>)\n",
      "epoch: 1256 loss is tensor([0.2655], grad_fn=<AddBackward0>)\n",
      "epoch: 1257 loss is tensor([0.2931], grad_fn=<AddBackward0>)\n",
      "epoch: 1258 loss is tensor([0.1871], grad_fn=<AddBackward0>)\n",
      "epoch: 1259 loss is tensor([0.2385], grad_fn=<AddBackward0>)\n",
      "epoch: 1260 loss is tensor([0.2817], grad_fn=<AddBackward0>)\n",
      "epoch: 1261 loss is tensor([0.2439], grad_fn=<AddBackward0>)\n",
      "epoch: 1262 loss is tensor([0.1923], grad_fn=<AddBackward0>)\n",
      "epoch: 1263 loss is tensor([0.2140], grad_fn=<AddBackward0>)\n",
      "epoch: 1264 loss is tensor([0.2419], grad_fn=<AddBackward0>)\n",
      "epoch: 1265 loss is tensor([0.2293], grad_fn=<AddBackward0>)\n",
      "epoch: 1266 loss is tensor([0.2825], grad_fn=<AddBackward0>)\n",
      "epoch: 1267 loss is tensor([0.2623], grad_fn=<AddBackward0>)\n",
      "epoch: 1268 loss is tensor([0.2300], grad_fn=<AddBackward0>)\n",
      "epoch: 1269 loss is tensor([0.2210], grad_fn=<AddBackward0>)\n",
      "epoch: 1270 loss is tensor([0.2180], grad_fn=<AddBackward0>)\n",
      "epoch: 1271 loss is tensor([0.2098], grad_fn=<AddBackward0>)\n",
      "epoch: 1272 loss is tensor([0.1865], grad_fn=<AddBackward0>)\n",
      "epoch: 1273 loss is tensor([0.2919], grad_fn=<AddBackward0>)\n",
      "epoch: 1274 loss is tensor([0.2882], grad_fn=<AddBackward0>)\n",
      "epoch: 1275 loss is tensor([0.3078], grad_fn=<AddBackward0>)\n",
      "epoch: 1276 loss is tensor([0.1918], grad_fn=<AddBackward0>)\n",
      "epoch: 1277 loss is tensor([0.2768], grad_fn=<AddBackward0>)\n",
      "epoch: 1278 loss is tensor([0.2649], grad_fn=<AddBackward0>)\n",
      "epoch: 1279 loss is tensor([0.2653], grad_fn=<AddBackward0>)\n",
      "epoch: 1280 loss is tensor([0.2880], grad_fn=<AddBackward0>)\n",
      "epoch: 1281 loss is tensor([0.2183], grad_fn=<AddBackward0>)\n",
      "epoch: 1282 loss is tensor([0.2144], grad_fn=<AddBackward0>)\n",
      "epoch: 1283 loss is tensor([0.2472], grad_fn=<AddBackward0>)\n",
      "epoch: 1284 loss is tensor([0.2313], grad_fn=<AddBackward0>)\n",
      "epoch: 1285 loss is tensor([0.2121], grad_fn=<AddBackward0>)\n",
      "epoch: 1286 loss is tensor([0.2566], grad_fn=<AddBackward0>)\n",
      "epoch: 1287 loss is tensor([0.2793], grad_fn=<AddBackward0>)\n",
      "epoch: 1288 loss is tensor([0.2482], grad_fn=<AddBackward0>)\n",
      "epoch: 1289 loss is tensor([0.2735], grad_fn=<AddBackward0>)\n",
      "epoch: 1290 loss is tensor([0.2873], grad_fn=<AddBackward0>)\n",
      "epoch: 1291 loss is tensor([0.2476], grad_fn=<AddBackward0>)\n",
      "epoch: 1292 loss is tensor([0.3221], grad_fn=<AddBackward0>)\n",
      "epoch: 1293 loss is tensor([0.2389], grad_fn=<AddBackward0>)\n",
      "epoch: 1294 loss is tensor([0.2055], grad_fn=<AddBackward0>)\n",
      "epoch: 1295 loss is tensor([0.3013], grad_fn=<AddBackward0>)\n",
      "epoch: 1296 loss is tensor([0.2158], grad_fn=<AddBackward0>)\n",
      "epoch: 1297 loss is tensor([0.2471], grad_fn=<AddBackward0>)\n",
      "epoch: 1298 loss is tensor([0.2343], grad_fn=<AddBackward0>)\n",
      "epoch: 1299 loss is tensor([0.2546], grad_fn=<AddBackward0>)\n",
      "epoch: 1300 loss is tensor([0.1516], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
=======
      "The number of epochs is: 801\n",
      "The number of epochs is: 802\n",
      "The number of epochs is: 803\n",
      "The number of epochs is: 804\n",
      "The number of epochs is: 805\n",
      "The number of epochs is: 806\n",
      "The number of epochs is: 807\n",
      "The number of epochs is: 808\n",
      "The number of epochs is: 809\n",
      "The number of epochs is: 810\n",
      "The number of epochs is: 811\n",
      "The number of epochs is: 812\n",
      "The number of epochs is: 813\n",
      "The number of epochs is: 814\n",
      "The number of epochs is: 815\n",
      "The number of epochs is: 816\n",
      "The number of epochs is: 817\n",
      "The number of epochs is: 818\n",
      "The number of epochs is: 819\n",
      "The number of epochs is: 820\n",
      "The number of epochs is: 821\n",
      "The number of epochs is: 822\n",
      "The number of epochs is: 823\n",
      "The number of epochs is: 824\n",
      "The number of epochs is: 825\n",
      "The number of epochs is: 826\n",
      "The number of epochs is: 827\n",
      "The number of epochs is: 828\n",
      "The number of epochs is: 829\n",
      "The number of epochs is: 830\n",
      "The number of epochs is: 831\n",
      "The number of epochs is: 832\n",
      "The number of epochs is: 833\n",
      "The number of epochs is: 834\n",
      "The number of epochs is: 835\n",
      "The number of epochs is: 836\n",
      "The number of epochs is: 837\n",
      "The number of epochs is: 838\n",
      "The number of epochs is: 839\n",
      "The number of epochs is: 840\n",
      "The number of epochs is: 841\n",
      "The number of epochs is: 842\n",
      "The number of epochs is: 843\n",
      "The number of epochs is: 844\n",
      "The number of epochs is: 845\n",
      "The number of epochs is: 846\n",
      "The number of epochs is: 847\n",
      "The number of epochs is: 848\n",
      "The number of epochs is: 849\n",
      "The number of epochs is: 850\n",
      "The number of epochs is: 851\n",
      "The number of epochs is: 852\n",
      "The number of epochs is: 853\n",
      "The number of epochs is: 854\n",
      "The number of epochs is: 855\n",
      "The number of epochs is: 856\n",
      "The number of epochs is: 857\n",
      "The number of epochs is: 858\n",
      "The number of epochs is: 859\n",
      "The number of epochs is: 860\n",
      "The number of epochs is: 861\n",
      "The number of epochs is: 862\n",
      "The number of epochs is: 863\n",
      "The number of epochs is: 864\n",
      "The number of epochs is: 865\n",
      "The number of epochs is: 866\n",
      "The number of epochs is: 867\n",
      "The number of epochs is: 868\n",
      "The number of epochs is: 869\n",
      "The number of epochs is: 870\n",
      "The number of epochs is: 871\n",
      "The number of epochs is: 872\n",
      "The number of epochs is: 873\n",
      "The number of epochs is: 874\n",
      "The number of epochs is: 875\n",
      "The number of epochs is: 876\n",
      "The number of epochs is: 877\n",
      "The number of epochs is: 878\n",
      "The number of epochs is: 879\n",
      "The number of epochs is: 880\n",
      "The number of epochs is: 881\n",
      "The number of epochs is: 882\n",
      "The number of epochs is: 883\n",
      "The number of epochs is: 884\n",
      "The number of epochs is: 885\n",
      "The number of epochs is: 886\n",
      "The number of epochs is: 887\n",
      "The number of epochs is: 888\n",
      "The number of epochs is: 889\n",
      "The number of epochs is: 890\n",
      "The number of epochs is: 891\n",
      "The number of epochs is: 892\n",
      "The number of epochs is: 893\n",
      "The number of epochs is: 894\n",
      "The number of epochs is: 895\n",
      "The number of epochs is: 896\n",
      "The number of epochs is: 897\n",
      "The number of epochs is: 898\n",
      "The number of epochs is: 899\n",
      "The number of epochs is: 900\n",
      "The number of epochs is: 901\n",
      "The number of epochs is: 902\n",
      "The number of epochs is: 903\n",
      "The number of epochs is: 904\n",
      "The number of epochs is: 905\n",
      "The number of epochs is: 906\n",
      "The number of epochs is: 907\n",
      "The number of epochs is: 908\n",
      "The number of epochs is: 909\n",
      "The number of epochs is: 910\n",
      "The number of epochs is: 911\n",
      "The number of epochs is: 912\n",
      "The number of epochs is: 913\n",
      "The number of epochs is: 914\n",
      "The number of epochs is: 915\n",
      "The number of epochs is: 916\n",
      "The number of epochs is: 917\n",
      "The number of epochs is: 918\n",
      "The number of epochs is: 919\n",
      "The number of epochs is: 920\n",
      "The number of epochs is: 921\n",
      "The number of epochs is: 922\n",
      "The number of epochs is: 923\n",
      "The number of epochs is: 924\n",
      "The number of epochs is: 925\n",
      "The number of epochs is: 926\n",
      "The number of epochs is: 927\n",
      "The number of epochs is: 928\n",
      "The number of epochs is: 929\n",
      "The number of epochs is: 930\n",
      "The number of epochs is: 931\n",
      "The number of epochs is: 932\n",
      "The number of epochs is: 933\n",
      "The number of epochs is: 934\n",
      "The number of epochs is: 935\n",
      "The number of epochs is: 936\n",
      "The number of epochs is: 937\n",
      "The number of epochs is: 938\n",
      "The number of epochs is: 939\n",
      "The number of epochs is: 940\n",
      "The number of epochs is: 941\n",
      "The number of epochs is: 942\n",
      "The number of epochs is: 943\n",
      "The number of epochs is: 944\n",
      "The number of epochs is: 945\n",
      "The number of epochs is: 946\n",
      "The number of epochs is: 947\n",
      "The number of epochs is: 948\n",
      "The number of epochs is: 949\n",
      "The number of epochs is: 950\n",
      "The number of epochs is: 951\n",
      "The number of epochs is: 952\n",
      "The number of epochs is: 953\n",
      "The number of epochs is: 954\n",
      "The number of epochs is: 955\n",
      "The number of epochs is: 956\n",
      "The number of epochs is: 957\n",
      "The number of epochs is: 958\n",
      "The number of epochs is: 959\n",
      "The number of epochs is: 960\n",
      "The number of epochs is: 961\n",
      "The number of epochs is: 962\n",
      "The number of epochs is: 963\n",
      "The number of epochs is: 964\n",
      "The number of epochs is: 965\n",
      "The number of epochs is: 966\n",
      "The number of epochs is: 967\n",
      "The number of epochs is: 968\n",
      "The number of epochs is: 969\n",
      "The number of epochs is: 970\n",
      "The number of epochs is: 971\n",
      "The number of epochs is: 972\n",
      "The number of epochs is: 973\n",
      "The number of epochs is: 974\n",
      "The number of epochs is: 975\n",
      "The number of epochs is: 976\n",
      "The number of epochs is: 977\n",
      "The number of epochs is: 978\n",
      "The number of epochs is: 979\n",
      "The number of epochs is: 980\n",
      "The number of epochs is: 981\n",
      "The number of epochs is: 982\n",
      "The number of epochs is: 983\n",
      "The number of epochs is: 984\n",
      "The number of epochs is: 985\n",
      "The number of epochs is: 986\n",
      "The number of epochs is: 987\n",
      "The number of epochs is: 988\n",
      "The number of epochs is: 989\n",
      "The number of epochs is: 990\n",
      "The number of epochs is: 991\n",
      "The number of epochs is: 992\n",
      "The number of epochs is: 993\n",
      "The number of epochs is: 994\n",
      "The number of epochs is: 995\n",
      "The number of epochs is: 996\n",
      "The number of epochs is: 997\n",
      "The number of epochs is: 998\n",
      "The number of epochs is: 999\n",
      "The number of epochs is: 1000\n",
      "17\n",
      "Outputting sketch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 1301 loss is tensor([0.2056], grad_fn=<AddBackward0>)\n",
      "epoch: 1302 loss is tensor([0.2015], grad_fn=<AddBackward0>)\n",
      "epoch: 1303 loss is tensor([0.1596], grad_fn=<AddBackward0>)\n",
      "epoch: 1304 loss is tensor([0.2352], grad_fn=<AddBackward0>)\n",
      "epoch: 1305 loss is tensor([0.2870], grad_fn=<AddBackward0>)\n",
      "epoch: 1306 loss is tensor([0.2493], grad_fn=<AddBackward0>)\n",
      "epoch: 1307 loss is tensor([0.2953], grad_fn=<AddBackward0>)\n",
      "epoch: 1308 loss is tensor([0.2426], grad_fn=<AddBackward0>)\n",
      "epoch: 1309 loss is tensor([0.2000], grad_fn=<AddBackward0>)\n",
      "epoch: 1310 loss is tensor([0.2100], grad_fn=<AddBackward0>)\n",
      "epoch: 1311 loss is tensor([0.2237], grad_fn=<AddBackward0>)\n",
      "epoch: 1312 loss is tensor([0.2485], grad_fn=<AddBackward0>)\n",
      "epoch: 1313 loss is tensor([0.2392], grad_fn=<AddBackward0>)\n",
      "epoch: 1314 loss is tensor([0.2317], grad_fn=<AddBackward0>)\n",
      "epoch: 1315 loss is tensor([0.2077], grad_fn=<AddBackward0>)\n",
      "epoch: 1316 loss is tensor([0.2027], grad_fn=<AddBackward0>)\n",
      "epoch: 1317 loss is tensor([0.2042], grad_fn=<AddBackward0>)\n",
      "epoch: 1318 loss is tensor([0.2760], grad_fn=<AddBackward0>)\n",
      "epoch: 1319 loss is tensor([0.2527], grad_fn=<AddBackward0>)\n",
      "epoch: 1320 loss is tensor([0.2193], grad_fn=<AddBackward0>)\n",
      "epoch: 1321 loss is tensor([0.1715], grad_fn=<AddBackward0>)\n",
      "epoch: 1322 loss is tensor([0.1979], grad_fn=<AddBackward0>)\n",
      "epoch: 1323 loss is tensor([0.1814], grad_fn=<AddBackward0>)\n",
      "epoch: 1324 loss is tensor([0.2535], grad_fn=<AddBackward0>)\n",
      "epoch: 1325 loss is tensor([0.2849], grad_fn=<AddBackward0>)\n",
      "epoch: 1326 loss is tensor([0.2613], grad_fn=<AddBackward0>)\n",
      "epoch: 1327 loss is tensor([0.2177], grad_fn=<AddBackward0>)\n",
      "epoch: 1328 loss is tensor([0.2987], grad_fn=<AddBackward0>)\n",
      "epoch: 1329 loss is tensor([0.2281], grad_fn=<AddBackward0>)\n",
      "epoch: 1330 loss is tensor([0.2089], grad_fn=<AddBackward0>)\n",
      "epoch: 1331 loss is tensor([0.2242], grad_fn=<AddBackward0>)\n",
      "epoch: 1332 loss is tensor([0.2339], grad_fn=<AddBackward0>)\n",
      "epoch: 1333 loss is tensor([0.2187], grad_fn=<AddBackward0>)\n",
      "epoch: 1334 loss is tensor([0.2275], grad_fn=<AddBackward0>)\n",
      "epoch: 1335 loss is tensor([0.2343], grad_fn=<AddBackward0>)\n",
      "epoch: 1336 loss is tensor([0.2328], grad_fn=<AddBackward0>)\n",
      "epoch: 1337 loss is tensor([0.2690], grad_fn=<AddBackward0>)\n",
      "epoch: 1338 loss is tensor([0.3089], grad_fn=<AddBackward0>)\n",
      "epoch: 1339 loss is tensor([0.2131], grad_fn=<AddBackward0>)\n",
      "epoch: 1340 loss is tensor([0.2009], grad_fn=<AddBackward0>)\n",
      "epoch: 1341 loss is tensor([0.2153], grad_fn=<AddBackward0>)\n",
      "epoch: 1342 loss is tensor([0.2027], grad_fn=<AddBackward0>)\n",
      "epoch: 1343 loss is tensor([0.2295], grad_fn=<AddBackward0>)\n",
      "epoch: 1344 loss is tensor([0.2032], grad_fn=<AddBackward0>)\n",
      "epoch: 1345 loss is tensor([0.2111], grad_fn=<AddBackward0>)\n",
      "epoch: 1346 loss is tensor([0.2330], grad_fn=<AddBackward0>)\n",
      "epoch: 1347 loss is tensor([0.2116], grad_fn=<AddBackward0>)\n",
      "epoch: 1348 loss is tensor([0.1892], grad_fn=<AddBackward0>)\n",
      "epoch: 1349 loss is tensor([0.1952], grad_fn=<AddBackward0>)\n",
      "epoch: 1350 loss is tensor([0.2042], grad_fn=<AddBackward0>)\n",
      "epoch: 1351 loss is tensor([0.1937], grad_fn=<AddBackward0>)\n",
      "epoch: 1352 loss is tensor([0.2190], grad_fn=<AddBackward0>)\n",
      "epoch: 1353 loss is tensor([0.2504], grad_fn=<AddBackward0>)\n",
      "epoch: 1354 loss is tensor([0.2190], grad_fn=<AddBackward0>)\n",
      "epoch: 1355 loss is tensor([0.2496], grad_fn=<AddBackward0>)\n",
      "epoch: 1356 loss is tensor([0.2640], grad_fn=<AddBackward0>)\n",
      "epoch: 1357 loss is tensor([0.2190], grad_fn=<AddBackward0>)\n",
      "epoch: 1358 loss is tensor([0.1577], grad_fn=<AddBackward0>)\n",
      "epoch: 1359 loss is tensor([0.1948], grad_fn=<AddBackward0>)\n",
      "epoch: 1360 loss is tensor([0.2037], grad_fn=<AddBackward0>)\n",
      "epoch: 1361 loss is tensor([0.2479], grad_fn=<AddBackward0>)\n",
      "epoch: 1362 loss is tensor([0.2116], grad_fn=<AddBackward0>)\n",
      "epoch: 1363 loss is tensor([0.2106], grad_fn=<AddBackward0>)\n",
      "epoch: 1364 loss is tensor([0.2081], grad_fn=<AddBackward0>)\n",
      "epoch: 1365 loss is tensor([0.2197], grad_fn=<AddBackward0>)\n",
      "epoch: 1366 loss is tensor([0.3141], grad_fn=<AddBackward0>)\n",
      "epoch: 1367 loss is tensor([0.1914], grad_fn=<AddBackward0>)\n",
      "epoch: 1368 loss is tensor([0.2042], grad_fn=<AddBackward0>)\n",
      "epoch: 1369 loss is tensor([0.2322], grad_fn=<AddBackward0>)\n",
      "epoch: 1370 loss is tensor([0.1993], grad_fn=<AddBackward0>)\n",
      "epoch: 1371 loss is tensor([0.2386], grad_fn=<AddBackward0>)\n",
      "epoch: 1372 loss is tensor([0.2064], grad_fn=<AddBackward0>)\n",
      "epoch: 1373 loss is tensor([0.2331], grad_fn=<AddBackward0>)\n",
      "epoch: 1374 loss is tensor([0.2408], grad_fn=<AddBackward0>)\n",
      "epoch: 1375 loss is tensor([0.2367], grad_fn=<AddBackward0>)\n",
      "epoch: 1376 loss is tensor([0.2253], grad_fn=<AddBackward0>)\n",
      "epoch: 1377 loss is tensor([0.2721], grad_fn=<AddBackward0>)\n",
      "epoch: 1378 loss is tensor([0.2902], grad_fn=<AddBackward0>)\n",
      "epoch: 1379 loss is tensor([0.2291], grad_fn=<AddBackward0>)\n",
      "epoch: 1380 loss is tensor([0.1935], grad_fn=<AddBackward0>)\n",
      "epoch: 1381 loss is tensor([0.2548], grad_fn=<AddBackward0>)\n",
      "epoch: 1382 loss is tensor([0.2798], grad_fn=<AddBackward0>)\n",
      "epoch: 1383 loss is tensor([0.1595], grad_fn=<AddBackward0>)\n",
      "epoch: 1384 loss is tensor([0.2450], grad_fn=<AddBackward0>)\n",
      "epoch: 1385 loss is tensor([0.3680], grad_fn=<AddBackward0>)\n",
      "epoch: 1386 loss is tensor([0.2539], grad_fn=<AddBackward0>)\n",
      "epoch: 1387 loss is tensor([0.1689], grad_fn=<AddBackward0>)\n",
      "epoch: 1388 loss is tensor([0.2149], grad_fn=<AddBackward0>)\n",
      "epoch: 1389 loss is tensor([0.2376], grad_fn=<AddBackward0>)\n",
      "epoch: 1390 loss is tensor([0.1822], grad_fn=<AddBackward0>)\n",
      "epoch: 1391 loss is tensor([0.2078], grad_fn=<AddBackward0>)\n",
      "epoch: 1392 loss is tensor([0.3157], grad_fn=<AddBackward0>)\n",
      "epoch: 1393 loss is tensor([0.2604], grad_fn=<AddBackward0>)\n",
      "epoch: 1394 loss is tensor([0.2223], grad_fn=<AddBackward0>)\n",
      "epoch: 1395 loss is tensor([0.2708], grad_fn=<AddBackward0>)\n",
      "epoch: 1396 loss is tensor([0.2519], grad_fn=<AddBackward0>)\n",
      "epoch: 1397 loss is tensor([0.1903], grad_fn=<AddBackward0>)\n",
      "epoch: 1398 loss is tensor([0.2664], grad_fn=<AddBackward0>)\n",
      "epoch: 1399 loss is tensor([0.2721], grad_fn=<AddBackward0>)\n",
      "epoch: 1400 loss is tensor([0.2477], grad_fn=<AddBackward0>)\n",
      "13\n"
=======
      "The number of epochs is: 1001\n",
      "The number of epochs is: 1002\n",
      "The number of epochs is: 1003\n",
      "The number of epochs is: 1004\n",
      "The number of epochs is: 1005\n",
      "The number of epochs is: 1006\n",
      "The number of epochs is: 1007\n",
      "The number of epochs is: 1008\n",
      "The number of epochs is: 1009\n",
      "The number of epochs is: 1010\n",
      "The number of epochs is: 1011\n",
      "The number of epochs is: 1012\n",
      "The number of epochs is: 1013\n",
      "The number of epochs is: 1014\n",
      "The number of epochs is: 1015\n",
      "The number of epochs is: 1016\n",
      "The number of epochs is: 1017\n",
      "The number of epochs is: 1018\n",
      "The number of epochs is: 1019\n",
      "The number of epochs is: 1020\n",
      "The number of epochs is: 1021\n",
      "The number of epochs is: 1022\n",
      "The number of epochs is: 1023\n",
      "The number of epochs is: 1024\n",
      "The number of epochs is: 1025\n",
      "The number of epochs is: 1026\n",
      "The number of epochs is: 1027\n",
      "The number of epochs is: 1028\n",
      "The number of epochs is: 1029\n",
      "The number of epochs is: 1030\n",
      "The number of epochs is: 1031\n",
      "The number of epochs is: 1032\n",
      "The number of epochs is: 1033\n",
      "The number of epochs is: 1034\n",
      "The number of epochs is: 1035\n",
      "The number of epochs is: 1036\n",
      "The number of epochs is: 1037\n",
      "The number of epochs is: 1038\n",
      "The number of epochs is: 1039\n",
      "The number of epochs is: 1040\n",
      "The number of epochs is: 1041\n",
      "The number of epochs is: 1042\n",
      "The number of epochs is: 1043\n",
      "The number of epochs is: 1044\n",
      "The number of epochs is: 1045\n",
      "The number of epochs is: 1046\n",
      "The number of epochs is: 1047\n",
      "The number of epochs is: 1048\n",
      "The number of epochs is: 1049\n",
      "The number of epochs is: 1050\n",
      "The number of epochs is: 1051\n",
      "The number of epochs is: 1052\n",
      "The number of epochs is: 1053\n",
      "The number of epochs is: 1054\n",
      "The number of epochs is: 1055\n",
      "The number of epochs is: 1056\n",
      "The number of epochs is: 1057\n",
      "The number of epochs is: 1058\n",
      "The number of epochs is: 1059\n",
      "The number of epochs is: 1060\n",
      "The number of epochs is: 1061\n",
      "The number of epochs is: 1062\n",
      "The number of epochs is: 1063\n",
      "The number of epochs is: 1064\n",
      "The number of epochs is: 1065\n",
      "The number of epochs is: 1066\n",
      "The number of epochs is: 1067\n",
      "The number of epochs is: 1068\n",
      "The number of epochs is: 1069\n",
      "The number of epochs is: 1070\n",
      "The number of epochs is: 1071\n",
      "The number of epochs is: 1072\n",
      "The number of epochs is: 1073\n",
      "The number of epochs is: 1074\n",
      "The number of epochs is: 1075\n",
      "The number of epochs is: 1076\n",
      "The number of epochs is: 1077\n",
      "The number of epochs is: 1078\n",
      "The number of epochs is: 1079\n",
      "The number of epochs is: 1080\n",
      "The number of epochs is: 1081\n",
      "The number of epochs is: 1082\n",
      "The number of epochs is: 1083\n",
      "The number of epochs is: 1084\n",
      "The number of epochs is: 1085\n",
      "The number of epochs is: 1086\n",
      "The number of epochs is: 1087\n",
      "The number of epochs is: 1088\n",
      "The number of epochs is: 1089\n",
      "The number of epochs is: 1090\n",
      "The number of epochs is: 1091\n",
      "The number of epochs is: 1092\n",
      "The number of epochs is: 1093\n",
      "The number of epochs is: 1094\n",
      "The number of epochs is: 1095\n",
      "The number of epochs is: 1096\n",
      "The number of epochs is: 1097\n",
      "The number of epochs is: 1098\n",
      "The number of epochs is: 1099\n",
      "The number of epochs is: 1100\n",
      "The number of epochs is: 1101\n",
      "The number of epochs is: 1102\n",
      "The number of epochs is: 1103\n",
      "The number of epochs is: 1104\n",
      "The number of epochs is: 1105\n",
      "The number of epochs is: 1106\n",
      "The number of epochs is: 1107\n",
      "The number of epochs is: 1108\n",
      "The number of epochs is: 1109\n",
      "The number of epochs is: 1110\n",
      "The number of epochs is: 1111\n",
      "The number of epochs is: 1112\n",
      "The number of epochs is: 1113\n",
      "The number of epochs is: 1114\n",
      "The number of epochs is: 1115\n",
      "The number of epochs is: 1116\n",
      "The number of epochs is: 1117\n",
      "The number of epochs is: 1118\n",
      "The number of epochs is: 1119\n",
      "The number of epochs is: 1120\n",
      "The number of epochs is: 1121\n",
      "The number of epochs is: 1122\n",
      "The number of epochs is: 1123\n",
      "The number of epochs is: 1124\n",
      "The number of epochs is: 1125\n",
      "The number of epochs is: 1126\n",
      "The number of epochs is: 1127\n",
      "The number of epochs is: 1128\n",
      "The number of epochs is: 1129\n",
      "The number of epochs is: 1130\n",
      "The number of epochs is: 1131\n",
      "The number of epochs is: 1132\n",
      "The number of epochs is: 1133\n",
      "The number of epochs is: 1134\n",
      "The number of epochs is: 1135\n",
      "The number of epochs is: 1136\n",
      "The number of epochs is: 1137\n",
      "The number of epochs is: 1138\n",
      "The number of epochs is: 1139\n",
      "The number of epochs is: 1140\n",
      "The number of epochs is: 1141\n",
      "The number of epochs is: 1142\n",
      "The number of epochs is: 1143\n",
      "The number of epochs is: 1144\n",
      "The number of epochs is: 1145\n",
      "The number of epochs is: 1146\n",
      "The number of epochs is: 1147\n",
      "The number of epochs is: 1148\n",
      "The number of epochs is: 1149\n",
      "The number of epochs is: 1150\n",
      "The number of epochs is: 1151\n",
      "The number of epochs is: 1152\n",
      "The number of epochs is: 1153\n",
      "The number of epochs is: 1154\n",
      "The number of epochs is: 1155\n",
      "The number of epochs is: 1156\n",
      "The number of epochs is: 1157\n",
      "The number of epochs is: 1158\n",
      "The number of epochs is: 1159\n",
      "The number of epochs is: 1160\n",
      "The number of epochs is: 1161\n",
      "The number of epochs is: 1162\n",
      "The number of epochs is: 1163\n",
      "The number of epochs is: 1164\n",
      "The number of epochs is: 1165\n",
      "The number of epochs is: 1166\n",
      "The number of epochs is: 1167\n",
      "The number of epochs is: 1168\n",
      "The number of epochs is: 1169\n",
      "The number of epochs is: 1170\n",
      "The number of epochs is: 1171\n",
      "The number of epochs is: 1172\n",
      "The number of epochs is: 1173\n",
      "The number of epochs is: 1174\n",
      "The number of epochs is: 1175\n",
      "The number of epochs is: 1176\n",
      "The number of epochs is: 1177\n",
      "The number of epochs is: 1178\n",
      "The number of epochs is: 1179\n",
      "The number of epochs is: 1180\n",
      "The number of epochs is: 1181\n",
      "The number of epochs is: 1182\n",
      "The number of epochs is: 1183\n",
      "The number of epochs is: 1184\n",
      "The number of epochs is: 1185\n",
      "The number of epochs is: 1186\n",
      "The number of epochs is: 1187\n",
      "The number of epochs is: 1188\n",
      "The number of epochs is: 1189\n",
      "The number of epochs is: 1190\n",
      "The number of epochs is: 1191\n",
      "The number of epochs is: 1192\n",
      "The number of epochs is: 1193\n",
      "The number of epochs is: 1194\n",
      "The number of epochs is: 1195\n",
      "The number of epochs is: 1196\n",
      "The number of epochs is: 1197\n",
      "The number of epochs is: 1198\n",
      "The number of epochs is: 1199\n",
      "The number of epochs is: 1200\n",
      "14\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1401 loss is tensor([0.2106], grad_fn=<AddBackward0>)\n",
      "epoch: 1402 loss is tensor([0.2440], grad_fn=<AddBackward0>)\n",
      "epoch: 1403 loss is tensor([0.2531], grad_fn=<AddBackward0>)\n",
      "epoch: 1404 loss is tensor([0.1842], grad_fn=<AddBackward0>)\n",
      "epoch: 1405 loss is tensor([0.2638], grad_fn=<AddBackward0>)\n",
      "epoch: 1406 loss is tensor([0.2030], grad_fn=<AddBackward0>)\n",
      "epoch: 1407 loss is tensor([0.2190], grad_fn=<AddBackward0>)\n",
      "epoch: 1408 loss is tensor([0.2419], grad_fn=<AddBackward0>)\n",
      "epoch: 1409 loss is tensor([0.2236], grad_fn=<AddBackward0>)\n",
      "epoch: 1410 loss is tensor([0.2623], grad_fn=<AddBackward0>)\n",
      "epoch: 1411 loss is tensor([0.2666], grad_fn=<AddBackward0>)\n",
      "epoch: 1412 loss is tensor([0.2479], grad_fn=<AddBackward0>)\n",
      "epoch: 1413 loss is tensor([0.2259], grad_fn=<AddBackward0>)\n",
      "epoch: 1414 loss is tensor([0.2112], grad_fn=<AddBackward0>)\n",
      "epoch: 1415 loss is tensor([0.2468], grad_fn=<AddBackward0>)\n",
      "epoch: 1416 loss is tensor([0.1821], grad_fn=<AddBackward0>)\n",
      "epoch: 1417 loss is tensor([0.2129], grad_fn=<AddBackward0>)\n",
      "epoch: 1418 loss is tensor([0.2702], grad_fn=<AddBackward0>)\n",
      "epoch: 1419 loss is tensor([0.2063], grad_fn=<AddBackward0>)\n",
      "epoch: 1420 loss is tensor([0.2471], grad_fn=<AddBackward0>)\n",
      "epoch: 1421 loss is tensor([0.2934], grad_fn=<AddBackward0>)\n",
      "epoch: 1422 loss is tensor([0.2615], grad_fn=<AddBackward0>)\n",
      "epoch: 1423 loss is tensor([0.2914], grad_fn=<AddBackward0>)\n",
      "epoch: 1424 loss is tensor([0.1801], grad_fn=<AddBackward0>)\n",
      "epoch: 1425 loss is tensor([0.2333], grad_fn=<AddBackward0>)\n",
      "epoch: 1426 loss is tensor([0.2089], grad_fn=<AddBackward0>)\n",
      "epoch: 1427 loss is tensor([0.2510], grad_fn=<AddBackward0>)\n",
      "epoch: 1428 loss is tensor([0.2521], grad_fn=<AddBackward0>)\n",
      "epoch: 1429 loss is tensor([0.2372], grad_fn=<AddBackward0>)\n",
      "epoch: 1430 loss is tensor([0.2335], grad_fn=<AddBackward0>)\n",
      "epoch: 1431 loss is tensor([0.2775], grad_fn=<AddBackward0>)\n",
      "epoch: 1432 loss is tensor([0.2607], grad_fn=<AddBackward0>)\n",
      "epoch: 1433 loss is tensor([0.2004], grad_fn=<AddBackward0>)\n",
      "epoch: 1434 loss is tensor([0.2286], grad_fn=<AddBackward0>)\n",
      "epoch: 1435 loss is tensor([0.1647], grad_fn=<AddBackward0>)\n",
      "epoch: 1436 loss is tensor([0.2090], grad_fn=<AddBackward0>)\n",
      "epoch: 1437 loss is tensor([0.1867], grad_fn=<AddBackward0>)\n",
      "epoch: 1438 loss is tensor([0.2474], grad_fn=<AddBackward0>)\n",
      "epoch: 1439 loss is tensor([0.2614], grad_fn=<AddBackward0>)\n",
      "epoch: 1440 loss is tensor([0.2010], grad_fn=<AddBackward0>)\n",
      "epoch: 1441 loss is tensor([0.2259], grad_fn=<AddBackward0>)\n",
      "epoch: 1442 loss is tensor([0.3132], grad_fn=<AddBackward0>)\n",
      "epoch: 1443 loss is tensor([0.2519], grad_fn=<AddBackward0>)\n",
      "epoch: 1444 loss is tensor([0.2313], grad_fn=<AddBackward0>)\n",
      "epoch: 1445 loss is tensor([0.1871], grad_fn=<AddBackward0>)\n",
      "epoch: 1446 loss is tensor([0.1728], grad_fn=<AddBackward0>)\n",
      "epoch: 1447 loss is tensor([0.1438], grad_fn=<AddBackward0>)\n",
      "epoch: 1448 loss is tensor([0.2182], grad_fn=<AddBackward0>)\n",
      "epoch: 1449 loss is tensor([0.2614], grad_fn=<AddBackward0>)\n",
      "epoch: 1450 loss is tensor([0.2151], grad_fn=<AddBackward0>)\n",
      "epoch: 1451 loss is tensor([0.1946], grad_fn=<AddBackward0>)\n",
      "epoch: 1452 loss is tensor([0.2004], grad_fn=<AddBackward0>)\n",
      "epoch: 1453 loss is tensor([0.1907], grad_fn=<AddBackward0>)\n",
      "epoch: 1454 loss is tensor([0.2059], grad_fn=<AddBackward0>)\n",
      "epoch: 1455 loss is tensor([0.1269], grad_fn=<AddBackward0>)\n",
      "epoch: 1456 loss is tensor([0.1961], grad_fn=<AddBackward0>)\n",
      "epoch: 1457 loss is tensor([0.2541], grad_fn=<AddBackward0>)\n",
      "epoch: 1458 loss is tensor([0.2488], grad_fn=<AddBackward0>)\n",
      "epoch: 1459 loss is tensor([0.2249], grad_fn=<AddBackward0>)\n",
      "epoch: 1460 loss is tensor([0.1827], grad_fn=<AddBackward0>)\n",
      "epoch: 1461 loss is tensor([0.1489], grad_fn=<AddBackward0>)\n",
      "epoch: 1462 loss is tensor([0.2292], grad_fn=<AddBackward0>)\n",
      "epoch: 1463 loss is tensor([0.1827], grad_fn=<AddBackward0>)\n",
      "epoch: 1464 loss is tensor([0.1948], grad_fn=<AddBackward0>)\n",
      "epoch: 1465 loss is tensor([0.2468], grad_fn=<AddBackward0>)\n",
      "epoch: 1466 loss is tensor([0.2202], grad_fn=<AddBackward0>)\n",
      "epoch: 1467 loss is tensor([0.2355], grad_fn=<AddBackward0>)\n",
      "epoch: 1468 loss is tensor([0.2010], grad_fn=<AddBackward0>)\n",
      "epoch: 1469 loss is tensor([0.2165], grad_fn=<AddBackward0>)\n",
      "epoch: 1470 loss is tensor([0.1929], grad_fn=<AddBackward0>)\n",
      "epoch: 1471 loss is tensor([0.1542], grad_fn=<AddBackward0>)\n",
      "epoch: 1472 loss is tensor([0.2218], grad_fn=<AddBackward0>)\n",
      "epoch: 1473 loss is tensor([0.1483], grad_fn=<AddBackward0>)\n",
      "epoch: 1474 loss is tensor([0.1852], grad_fn=<AddBackward0>)\n",
      "epoch: 1475 loss is tensor([0.1990], grad_fn=<AddBackward0>)\n",
      "epoch: 1476 loss is tensor([0.1965], grad_fn=<AddBackward0>)\n",
      "epoch: 1477 loss is tensor([0.1770], grad_fn=<AddBackward0>)\n",
      "epoch: 1478 loss is tensor([0.1108], grad_fn=<AddBackward0>)\n",
      "epoch: 1479 loss is tensor([0.2140], grad_fn=<AddBackward0>)\n",
      "epoch: 1480 loss is tensor([0.1613], grad_fn=<AddBackward0>)\n",
      "epoch: 1481 loss is tensor([0.2048], grad_fn=<AddBackward0>)\n",
      "epoch: 1482 loss is tensor([0.2239], grad_fn=<AddBackward0>)\n",
      "epoch: 1483 loss is tensor([0.1991], grad_fn=<AddBackward0>)\n",
      "epoch: 1484 loss is tensor([0.2067], grad_fn=<AddBackward0>)\n",
      "epoch: 1485 loss is tensor([0.1470], grad_fn=<AddBackward0>)\n",
      "epoch: 1486 loss is tensor([0.1921], grad_fn=<AddBackward0>)\n",
      "epoch: 1487 loss is tensor([0.1680], grad_fn=<AddBackward0>)\n",
      "epoch: 1488 loss is tensor([0.1449], grad_fn=<AddBackward0>)\n",
      "epoch: 1489 loss is tensor([0.1812], grad_fn=<AddBackward0>)\n",
      "epoch: 1490 loss is tensor([0.1233], grad_fn=<AddBackward0>)\n",
      "epoch: 1491 loss is tensor([0.1231], grad_fn=<AddBackward0>)\n",
      "epoch: 1492 loss is tensor([0.1497], grad_fn=<AddBackward0>)\n",
      "epoch: 1493 loss is tensor([0.2408], grad_fn=<AddBackward0>)\n",
      "epoch: 1494 loss is tensor([0.1519], grad_fn=<AddBackward0>)\n",
      "epoch: 1495 loss is tensor([0.1608], grad_fn=<AddBackward0>)\n",
      "epoch: 1496 loss is tensor([0.1413], grad_fn=<AddBackward0>)\n",
      "epoch: 1497 loss is tensor([0.1856], grad_fn=<AddBackward0>)\n",
      "epoch: 1498 loss is tensor([0.1843], grad_fn=<AddBackward0>)\n",
      "epoch: 1499 loss is tensor([0.2145], grad_fn=<AddBackward0>)\n",
      "epoch: 1500 loss is tensor([0.1903], grad_fn=<AddBackward0>)\n",
      "31\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 1501 loss is tensor([0.1503], grad_fn=<AddBackward0>)\n",
      "epoch: 1502 loss is tensor([0.1990], grad_fn=<AddBackward0>)\n",
      "epoch: 1503 loss is tensor([0.2108], grad_fn=<AddBackward0>)\n",
      "epoch: 1504 loss is tensor([0.1808], grad_fn=<AddBackward0>)\n",
      "epoch: 1505 loss is tensor([0.1727], grad_fn=<AddBackward0>)\n",
      "epoch: 1506 loss is tensor([0.1727], grad_fn=<AddBackward0>)\n",
      "epoch: 1507 loss is tensor([0.2015], grad_fn=<AddBackward0>)\n",
      "epoch: 1508 loss is tensor([0.2119], grad_fn=<AddBackward0>)\n",
      "epoch: 1509 loss is tensor([0.2304], grad_fn=<AddBackward0>)\n",
      "epoch: 1510 loss is tensor([0.1933], grad_fn=<AddBackward0>)\n",
      "epoch: 1511 loss is tensor([0.1452], grad_fn=<AddBackward0>)\n",
      "epoch: 1512 loss is tensor([0.1997], grad_fn=<AddBackward0>)\n",
      "epoch: 1513 loss is tensor([0.1268], grad_fn=<AddBackward0>)\n",
      "epoch: 1514 loss is tensor([0.1921], grad_fn=<AddBackward0>)\n",
      "epoch: 1515 loss is tensor([0.1760], grad_fn=<AddBackward0>)\n",
      "epoch: 1516 loss is tensor([0.1557], grad_fn=<AddBackward0>)\n",
      "epoch: 1517 loss is tensor([0.1969], grad_fn=<AddBackward0>)\n",
      "epoch: 1518 loss is tensor([0.2033], grad_fn=<AddBackward0>)\n",
      "epoch: 1519 loss is tensor([0.1787], grad_fn=<AddBackward0>)\n",
      "epoch: 1520 loss is tensor([0.1434], grad_fn=<AddBackward0>)\n",
      "epoch: 1521 loss is tensor([0.1723], grad_fn=<AddBackward0>)\n",
      "epoch: 1522 loss is tensor([0.2039], grad_fn=<AddBackward0>)\n",
      "epoch: 1523 loss is tensor([0.1662], grad_fn=<AddBackward0>)\n",
      "epoch: 1524 loss is tensor([0.1913], grad_fn=<AddBackward0>)\n",
      "epoch: 1525 loss is tensor([0.2362], grad_fn=<AddBackward0>)\n",
      "epoch: 1526 loss is tensor([0.2082], grad_fn=<AddBackward0>)\n",
      "epoch: 1527 loss is tensor([0.2795], grad_fn=<AddBackward0>)\n",
      "epoch: 1528 loss is tensor([0.3129], grad_fn=<AddBackward0>)\n",
      "epoch: 1529 loss is tensor([0.2454], grad_fn=<AddBackward0>)\n",
      "epoch: 1530 loss is tensor([0.2800], grad_fn=<AddBackward0>)\n",
      "epoch: 1531 loss is tensor([0.2769], grad_fn=<AddBackward0>)\n",
      "epoch: 1532 loss is tensor([0.1720], grad_fn=<AddBackward0>)\n",
      "epoch: 1533 loss is tensor([0.1543], grad_fn=<AddBackward0>)\n",
      "epoch: 1534 loss is tensor([0.1776], grad_fn=<AddBackward0>)\n",
      "epoch: 1535 loss is tensor([0.2466], grad_fn=<AddBackward0>)\n",
      "epoch: 1536 loss is tensor([0.1753], grad_fn=<AddBackward0>)\n",
      "epoch: 1537 loss is tensor([0.2207], grad_fn=<AddBackward0>)\n",
      "epoch: 1538 loss is tensor([0.2212], grad_fn=<AddBackward0>)\n",
      "epoch: 1539 loss is tensor([0.2435], grad_fn=<AddBackward0>)\n",
      "epoch: 1540 loss is tensor([0.2234], grad_fn=<AddBackward0>)\n",
      "epoch: 1541 loss is tensor([0.1901], grad_fn=<AddBackward0>)\n",
      "epoch: 1542 loss is tensor([0.1874], grad_fn=<AddBackward0>)\n",
      "epoch: 1543 loss is tensor([0.2240], grad_fn=<AddBackward0>)\n",
      "epoch: 1544 loss is tensor([0.2247], grad_fn=<AddBackward0>)\n",
      "epoch: 1545 loss is tensor([0.1773], grad_fn=<AddBackward0>)\n",
      "epoch: 1546 loss is tensor([0.1569], grad_fn=<AddBackward0>)\n",
      "epoch: 1547 loss is tensor([0.1541], grad_fn=<AddBackward0>)\n",
      "epoch: 1548 loss is tensor([0.1605], grad_fn=<AddBackward0>)\n",
      "epoch: 1549 loss is tensor([0.2351], grad_fn=<AddBackward0>)\n",
      "epoch: 1550 loss is tensor([0.1685], grad_fn=<AddBackward0>)\n",
      "epoch: 1551 loss is tensor([0.1640], grad_fn=<AddBackward0>)\n",
      "epoch: 1552 loss is tensor([0.2133], grad_fn=<AddBackward0>)\n",
      "epoch: 1553 loss is tensor([0.1507], grad_fn=<AddBackward0>)\n",
      "epoch: 1554 loss is tensor([0.2197], grad_fn=<AddBackward0>)\n",
      "epoch: 1555 loss is tensor([0.2215], grad_fn=<AddBackward0>)\n",
      "epoch: 1556 loss is tensor([0.2107], grad_fn=<AddBackward0>)\n",
      "epoch: 1557 loss is tensor([0.1941], grad_fn=<AddBackward0>)\n",
      "epoch: 1558 loss is tensor([0.1811], grad_fn=<AddBackward0>)\n",
      "epoch: 1559 loss is tensor([0.2628], grad_fn=<AddBackward0>)\n",
      "epoch: 1560 loss is tensor([0.2606], grad_fn=<AddBackward0>)\n",
      "epoch: 1561 loss is tensor([0.1709], grad_fn=<AddBackward0>)\n",
      "epoch: 1562 loss is tensor([0.1088], grad_fn=<AddBackward0>)\n",
      "epoch: 1563 loss is tensor([0.1687], grad_fn=<AddBackward0>)\n",
      "epoch: 1564 loss is tensor([0.2134], grad_fn=<AddBackward0>)\n",
      "epoch: 1565 loss is tensor([0.2055], grad_fn=<AddBackward0>)\n",
      "epoch: 1566 loss is tensor([0.1986], grad_fn=<AddBackward0>)\n",
      "epoch: 1567 loss is tensor([0.1861], grad_fn=<AddBackward0>)\n",
      "epoch: 1568 loss is tensor([0.2053], grad_fn=<AddBackward0>)\n",
      "epoch: 1569 loss is tensor([0.1924], grad_fn=<AddBackward0>)\n",
      "epoch: 1570 loss is tensor([0.1472], grad_fn=<AddBackward0>)\n",
      "epoch: 1571 loss is tensor([0.1668], grad_fn=<AddBackward0>)\n",
      "epoch: 1572 loss is tensor([0.2124], grad_fn=<AddBackward0>)\n",
      "epoch: 1573 loss is tensor([0.2229], grad_fn=<AddBackward0>)\n",
      "epoch: 1574 loss is tensor([0.1487], grad_fn=<AddBackward0>)\n",
      "epoch: 1575 loss is tensor([0.1861], grad_fn=<AddBackward0>)\n",
      "epoch: 1576 loss is tensor([0.2217], grad_fn=<AddBackward0>)\n",
      "epoch: 1577 loss is tensor([0.1588], grad_fn=<AddBackward0>)\n",
      "epoch: 1578 loss is tensor([0.1322], grad_fn=<AddBackward0>)\n",
      "epoch: 1579 loss is tensor([0.1448], grad_fn=<AddBackward0>)\n",
      "epoch: 1580 loss is tensor([0.1896], grad_fn=<AddBackward0>)\n",
      "epoch: 1581 loss is tensor([0.1326], grad_fn=<AddBackward0>)\n",
      "epoch: 1582 loss is tensor([0.1406], grad_fn=<AddBackward0>)\n",
      "epoch: 1583 loss is tensor([0.1835], grad_fn=<AddBackward0>)\n",
      "epoch: 1584 loss is tensor([0.1911], grad_fn=<AddBackward0>)\n",
      "epoch: 1585 loss is tensor([0.2299], grad_fn=<AddBackward0>)\n",
      "epoch: 1586 loss is tensor([0.1858], grad_fn=<AddBackward0>)\n",
      "epoch: 1587 loss is tensor([0.1735], grad_fn=<AddBackward0>)\n",
      "epoch: 1588 loss is tensor([0.1649], grad_fn=<AddBackward0>)\n",
      "epoch: 1589 loss is tensor([0.1643], grad_fn=<AddBackward0>)\n",
      "epoch: 1590 loss is tensor([0.1739], grad_fn=<AddBackward0>)\n",
      "epoch: 1591 loss is tensor([0.2302], grad_fn=<AddBackward0>)\n",
      "epoch: 1592 loss is tensor([0.1785], grad_fn=<AddBackward0>)\n",
      "epoch: 1593 loss is tensor([0.1462], grad_fn=<AddBackward0>)\n",
      "epoch: 1594 loss is tensor([0.1903], grad_fn=<AddBackward0>)\n",
      "epoch: 1595 loss is tensor([0.1887], grad_fn=<AddBackward0>)\n",
      "epoch: 1596 loss is tensor([0.1885], grad_fn=<AddBackward0>)\n",
      "epoch: 1597 loss is tensor([0.1534], grad_fn=<AddBackward0>)\n",
      "epoch: 1598 loss is tensor([0.1540], grad_fn=<AddBackward0>)\n",
      "epoch: 1599 loss is tensor([0.1685], grad_fn=<AddBackward0>)\n",
      "epoch: 1600 loss is tensor([0.1031], grad_fn=<AddBackward0>)\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1601 loss is tensor([0.1821], grad_fn=<AddBackward0>)\n",
      "epoch: 1602 loss is tensor([0.2128], grad_fn=<AddBackward0>)\n",
      "epoch: 1603 loss is tensor([0.1375], grad_fn=<AddBackward0>)\n",
      "epoch: 1604 loss is tensor([0.1578], grad_fn=<AddBackward0>)\n",
      "epoch: 1605 loss is tensor([0.1502], grad_fn=<AddBackward0>)\n",
      "epoch: 1606 loss is tensor([0.1499], grad_fn=<AddBackward0>)\n",
      "epoch: 1607 loss is tensor([0.2092], grad_fn=<AddBackward0>)\n",
      "epoch: 1608 loss is tensor([0.1553], grad_fn=<AddBackward0>)\n",
      "epoch: 1609 loss is tensor([0.1675], grad_fn=<AddBackward0>)\n",
      "epoch: 1610 loss is tensor([0.2144], grad_fn=<AddBackward0>)\n",
      "epoch: 1611 loss is tensor([0.1420], grad_fn=<AddBackward0>)\n",
      "epoch: 1612 loss is tensor([0.1294], grad_fn=<AddBackward0>)\n",
      "epoch: 1613 loss is tensor([0.1769], grad_fn=<AddBackward0>)\n",
      "epoch: 1614 loss is tensor([0.1628], grad_fn=<AddBackward0>)\n",
      "epoch: 1615 loss is tensor([0.1307], grad_fn=<AddBackward0>)\n",
      "epoch: 1616 loss is tensor([0.1072], grad_fn=<AddBackward0>)\n",
      "epoch: 1617 loss is tensor([0.1370], grad_fn=<AddBackward0>)\n",
      "epoch: 1618 loss is tensor([0.2243], grad_fn=<AddBackward0>)\n",
      "epoch: 1619 loss is tensor([0.1266], grad_fn=<AddBackward0>)\n",
      "epoch: 1620 loss is tensor([0.2035], grad_fn=<AddBackward0>)\n",
      "epoch: 1621 loss is tensor([0.1611], grad_fn=<AddBackward0>)\n",
      "epoch: 1622 loss is tensor([0.1706], grad_fn=<AddBackward0>)\n",
      "epoch: 1623 loss is tensor([0.1888], grad_fn=<AddBackward0>)\n",
      "epoch: 1624 loss is tensor([0.1091], grad_fn=<AddBackward0>)\n",
      "epoch: 1625 loss is tensor([0.1515], grad_fn=<AddBackward0>)\n",
      "epoch: 1626 loss is tensor([0.1645], grad_fn=<AddBackward0>)\n",
      "epoch: 1627 loss is tensor([0.1744], grad_fn=<AddBackward0>)\n",
      "epoch: 1628 loss is tensor([0.1521], grad_fn=<AddBackward0>)\n",
      "epoch: 1629 loss is tensor([0.0838], grad_fn=<AddBackward0>)\n",
      "epoch: 1630 loss is tensor([0.1191], grad_fn=<AddBackward0>)\n",
      "epoch: 1631 loss is tensor([0.1311], grad_fn=<AddBackward0>)\n",
      "epoch: 1632 loss is tensor([0.2082], grad_fn=<AddBackward0>)\n",
      "epoch: 1633 loss is tensor([0.1584], grad_fn=<AddBackward0>)\n",
      "epoch: 1634 loss is tensor([0.1866], grad_fn=<AddBackward0>)\n",
      "epoch: 1635 loss is tensor([0.1889], grad_fn=<AddBackward0>)\n",
      "epoch: 1636 loss is tensor([0.2176], grad_fn=<AddBackward0>)\n",
      "epoch: 1637 loss is tensor([0.2079], grad_fn=<AddBackward0>)\n",
      "epoch: 1638 loss is tensor([0.1739], grad_fn=<AddBackward0>)\n",
      "epoch: 1639 loss is tensor([0.2010], grad_fn=<AddBackward0>)\n",
      "epoch: 1640 loss is tensor([0.1287], grad_fn=<AddBackward0>)\n",
      "epoch: 1641 loss is tensor([0.2041], grad_fn=<AddBackward0>)\n",
      "epoch: 1642 loss is tensor([0.1292], grad_fn=<AddBackward0>)\n",
      "epoch: 1643 loss is tensor([0.1651], grad_fn=<AddBackward0>)\n",
      "epoch: 1644 loss is tensor([0.2001], grad_fn=<AddBackward0>)\n",
      "epoch: 1645 loss is tensor([0.1774], grad_fn=<AddBackward0>)\n",
      "epoch: 1646 loss is tensor([0.1152], grad_fn=<AddBackward0>)\n",
      "epoch: 1647 loss is tensor([0.2118], grad_fn=<AddBackward0>)\n",
      "epoch: 1648 loss is tensor([0.1534], grad_fn=<AddBackward0>)\n",
      "epoch: 1649 loss is tensor([0.1437], grad_fn=<AddBackward0>)\n",
      "epoch: 1650 loss is tensor([0.1649], grad_fn=<AddBackward0>)\n",
      "epoch: 1651 loss is tensor([0.1708], grad_fn=<AddBackward0>)\n",
      "epoch: 1652 loss is tensor([0.2043], grad_fn=<AddBackward0>)\n",
      "epoch: 1653 loss is tensor([0.1812], grad_fn=<AddBackward0>)\n",
      "epoch: 1654 loss is tensor([0.1551], grad_fn=<AddBackward0>)\n",
      "epoch: 1655 loss is tensor([0.1957], grad_fn=<AddBackward0>)\n",
      "epoch: 1656 loss is tensor([0.1753], grad_fn=<AddBackward0>)\n",
      "epoch: 1657 loss is tensor([0.2152], grad_fn=<AddBackward0>)\n",
      "epoch: 1658 loss is tensor([0.2560], grad_fn=<AddBackward0>)\n",
      "epoch: 1659 loss is tensor([0.2155], grad_fn=<AddBackward0>)\n",
      "epoch: 1660 loss is tensor([0.1939], grad_fn=<AddBackward0>)\n",
      "epoch: 1661 loss is tensor([0.2027], grad_fn=<AddBackward0>)\n",
      "epoch: 1662 loss is tensor([0.1602], grad_fn=<AddBackward0>)\n",
      "epoch: 1663 loss is tensor([0.1951], grad_fn=<AddBackward0>)\n",
      "epoch: 1664 loss is tensor([0.1867], grad_fn=<AddBackward0>)\n",
      "epoch: 1665 loss is tensor([0.1911], grad_fn=<AddBackward0>)\n",
      "epoch: 1666 loss is tensor([0.1645], grad_fn=<AddBackward0>)\n",
      "epoch: 1667 loss is tensor([0.1644], grad_fn=<AddBackward0>)\n",
      "epoch: 1668 loss is tensor([0.1933], grad_fn=<AddBackward0>)\n",
      "epoch: 1669 loss is tensor([0.1891], grad_fn=<AddBackward0>)\n",
      "epoch: 1670 loss is tensor([0.1805], grad_fn=<AddBackward0>)\n",
      "epoch: 1671 loss is tensor([0.1989], grad_fn=<AddBackward0>)\n",
      "epoch: 1672 loss is tensor([0.1844], grad_fn=<AddBackward0>)\n",
      "epoch: 1673 loss is tensor([0.1953], grad_fn=<AddBackward0>)\n",
      "epoch: 1674 loss is tensor([0.1997], grad_fn=<AddBackward0>)\n",
      "epoch: 1675 loss is tensor([0.1841], grad_fn=<AddBackward0>)\n",
      "epoch: 1676 loss is tensor([0.1982], grad_fn=<AddBackward0>)\n",
      "epoch: 1677 loss is tensor([0.1146], grad_fn=<AddBackward0>)\n",
      "epoch: 1678 loss is tensor([0.1135], grad_fn=<AddBackward0>)\n",
      "epoch: 1679 loss is tensor([0.1664], grad_fn=<AddBackward0>)\n",
      "epoch: 1680 loss is tensor([0.1054], grad_fn=<AddBackward0>)\n",
      "epoch: 1681 loss is tensor([0.1014], grad_fn=<AddBackward0>)\n",
      "epoch: 1682 loss is tensor([0.1369], grad_fn=<AddBackward0>)\n",
      "epoch: 1683 loss is tensor([0.1299], grad_fn=<AddBackward0>)\n",
      "epoch: 1684 loss is tensor([0.1577], grad_fn=<AddBackward0>)\n",
      "epoch: 1685 loss is tensor([0.1932], grad_fn=<AddBackward0>)\n",
      "epoch: 1686 loss is tensor([0.1471], grad_fn=<AddBackward0>)\n",
      "epoch: 1687 loss is tensor([0.1707], grad_fn=<AddBackward0>)\n",
      "epoch: 1688 loss is tensor([0.1367], grad_fn=<AddBackward0>)\n",
      "epoch: 1689 loss is tensor([0.1530], grad_fn=<AddBackward0>)\n",
      "epoch: 1690 loss is tensor([0.1487], grad_fn=<AddBackward0>)\n",
      "epoch: 1691 loss is tensor([0.1780], grad_fn=<AddBackward0>)\n",
      "epoch: 1692 loss is tensor([0.0930], grad_fn=<AddBackward0>)\n",
      "epoch: 1693 loss is tensor([0.1843], grad_fn=<AddBackward0>)\n",
      "epoch: 1694 loss is tensor([0.1598], grad_fn=<AddBackward0>)\n",
      "epoch: 1695 loss is tensor([0.1817], grad_fn=<AddBackward0>)\n",
      "epoch: 1696 loss is tensor([0.1219], grad_fn=<AddBackward0>)\n",
      "epoch: 1697 loss is tensor([0.1364], grad_fn=<AddBackward0>)\n",
      "epoch: 1698 loss is tensor([0.1742], grad_fn=<AddBackward0>)\n",
      "epoch: 1699 loss is tensor([0.1399], grad_fn=<AddBackward0>)\n",
      "epoch: 1700 loss is tensor([0.1108], grad_fn=<AddBackward0>)\n",
      "31\n"
     ]
=======
      "The number of epochs is: 1201\n",
      "The number of epochs is: 1202\n",
      "The number of epochs is: 1203\n",
      "The number of epochs is: 1204\n",
      "The number of epochs is: 1205\n",
      "The number of epochs is: 1206\n",
      "The number of epochs is: 1207\n",
      "The number of epochs is: 1208\n",
      "The number of epochs is: 1209\n",
      "The number of epochs is: 1210\n",
      "The number of epochs is: 1211\n",
      "The number of epochs is: 1212\n",
      "The number of epochs is: 1213\n",
      "The number of epochs is: 1214\n",
      "The number of epochs is: 1215\n",
      "The number of epochs is: 1216\n",
      "The number of epochs is: 1217\n",
      "The number of epochs is: 1218\n",
      "The number of epochs is: 1219\n",
      "The number of epochs is: 1220\n",
      "The number of epochs is: 1221\n",
      "The number of epochs is: 1222\n",
      "The number of epochs is: 1223\n",
      "The number of epochs is: 1224\n",
      "The number of epochs is: 1225\n",
      "The number of epochs is: 1226\n",
      "The number of epochs is: 1227\n",
      "The number of epochs is: 1228\n",
      "The number of epochs is: 1229\n",
      "The number of epochs is: 1230\n",
      "The number of epochs is: 1231\n",
      "The number of epochs is: 1232\n",
      "The number of epochs is: 1233\n",
      "The number of epochs is: 1234\n",
      "The number of epochs is: 1235\n",
      "The number of epochs is: 1236\n",
      "The number of epochs is: 1237\n",
      "The number of epochs is: 1238\n",
      "The number of epochs is: 1239\n",
      "The number of epochs is: 1240\n",
      "The number of epochs is: 1241\n",
      "The number of epochs is: 1242\n",
      "The number of epochs is: 1243\n",
      "The number of epochs is: 1244\n",
      "The number of epochs is: 1245\n",
      "The number of epochs is: 1246\n",
      "The number of epochs is: 1247\n",
      "The number of epochs is: 1248\n",
      "The number of epochs is: 1249\n",
      "The number of epochs is: 1250\n",
      "The number of epochs is: 1251\n",
      "The number of epochs is: 1252\n",
      "The number of epochs is: 1253\n",
      "The number of epochs is: 1254\n",
      "The number of epochs is: 1255\n",
      "The number of epochs is: 1256\n",
      "The number of epochs is: 1257\n",
      "The number of epochs is: 1258\n",
      "The number of epochs is: 1259\n",
      "The number of epochs is: 1260\n",
      "The number of epochs is: 1261\n",
      "The number of epochs is: 1262\n",
      "The number of epochs is: 1263\n",
      "The number of epochs is: 1264\n",
      "The number of epochs is: 1265\n",
      "The number of epochs is: 1266\n",
      "The number of epochs is: 1267\n",
      "The number of epochs is: 1268\n",
      "The number of epochs is: 1269\n",
      "The number of epochs is: 1270\n",
      "The number of epochs is: 1271\n",
      "The number of epochs is: 1272\n",
      "The number of epochs is: 1273\n",
      "The number of epochs is: 1274\n",
      "The number of epochs is: 1275\n",
      "The number of epochs is: 1276\n",
      "The number of epochs is: 1277\n",
      "The number of epochs is: 1278\n",
      "The number of epochs is: 1279\n",
      "The number of epochs is: 1280\n",
      "The number of epochs is: 1281\n",
      "The number of epochs is: 1282\n",
      "The number of epochs is: 1283\n",
      "The number of epochs is: 1284\n",
      "The number of epochs is: 1285\n",
      "The number of epochs is: 1286\n",
      "The number of epochs is: 1287\n",
      "The number of epochs is: 1288\n",
      "The number of epochs is: 1289\n",
      "The number of epochs is: 1290\n",
      "The number of epochs is: 1291\n",
      "The number of epochs is: 1292\n",
      "The number of epochs is: 1293\n",
      "The number of epochs is: 1294\n",
      "The number of epochs is: 1295\n",
      "The number of epochs is: 1296\n",
      "The number of epochs is: 1297\n",
      "The number of epochs is: 1298\n",
      "The number of epochs is: 1299\n",
      "The number of epochs is: 1300\n",
      "The number of epochs is: 1301\n",
      "The number of epochs is: 1302\n",
      "The number of epochs is: 1303\n",
      "The number of epochs is: 1304\n",
      "The number of epochs is: 1305\n",
      "The number of epochs is: 1306\n",
      "The number of epochs is: 1307\n",
      "The number of epochs is: 1308\n",
      "The number of epochs is: 1309\n",
      "The number of epochs is: 1310\n",
      "The number of epochs is: 1311\n",
      "The number of epochs is: 1312\n",
      "The number of epochs is: 1313\n",
      "The number of epochs is: 1314\n",
      "The number of epochs is: 1315\n",
      "The number of epochs is: 1316\n",
      "The number of epochs is: 1317\n",
      "The number of epochs is: 1318\n",
      "The number of epochs is: 1319\n",
      "The number of epochs is: 1320\n",
      "The number of epochs is: 1321\n",
      "The number of epochs is: 1322\n",
      "The number of epochs is: 1323\n",
      "The number of epochs is: 1324\n",
      "The number of epochs is: 1325\n",
      "The number of epochs is: 1326\n",
      "The number of epochs is: 1327\n",
      "The number of epochs is: 1328\n",
      "The number of epochs is: 1329\n",
      "The number of epochs is: 1330\n",
      "The number of epochs is: 1331\n",
      "The number of epochs is: 1332\n",
      "The number of epochs is: 1333\n",
      "The number of epochs is: 1334\n",
      "The number of epochs is: 1335\n",
      "The number of epochs is: 1336\n",
      "The number of epochs is: 1337\n",
      "The number of epochs is: 1338\n",
      "The number of epochs is: 1339\n",
      "The number of epochs is: 1340\n",
      "The number of epochs is: 1341\n",
      "The number of epochs is: 1342\n",
      "The number of epochs is: 1343\n",
      "The number of epochs is: 1344\n",
      "The number of epochs is: 1345\n",
      "The number of epochs is: 1346\n",
      "The number of epochs is: 1347\n",
      "The number of epochs is: 1348\n",
      "The number of epochs is: 1349\n",
      "The number of epochs is: 1350\n",
      "The number of epochs is: 1351\n",
      "The number of epochs is: 1352\n",
      "The number of epochs is: 1353\n",
      "The number of epochs is: 1354\n",
      "The number of epochs is: 1355\n",
      "The number of epochs is: 1356\n",
      "The number of epochs is: 1357\n",
      "The number of epochs is: 1358\n",
      "The number of epochs is: 1359\n",
      "The number of epochs is: 1360\n",
      "The number of epochs is: 1361\n",
      "The number of epochs is: 1362\n",
      "The number of epochs is: 1363\n",
      "The number of epochs is: 1364\n",
      "The number of epochs is: 1365\n",
      "The number of epochs is: 1366\n",
      "The number of epochs is: 1367\n",
      "The number of epochs is: 1368\n",
      "The number of epochs is: 1369\n",
      "The number of epochs is: 1370\n",
      "The number of epochs is: 1371\n",
      "The number of epochs is: 1372\n",
      "The number of epochs is: 1373\n",
      "The number of epochs is: 1374\n",
      "The number of epochs is: 1375\n",
      "The number of epochs is: 1376\n",
      "The number of epochs is: 1377\n",
      "The number of epochs is: 1378\n",
      "The number of epochs is: 1379\n",
      "The number of epochs is: 1380\n",
      "The number of epochs is: 1381\n",
      "The number of epochs is: 1382\n",
      "The number of epochs is: 1383\n",
      "The number of epochs is: 1384\n",
      "The number of epochs is: 1385\n",
      "The number of epochs is: 1386\n",
      "The number of epochs is: 1387\n",
      "The number of epochs is: 1388\n",
      "The number of epochs is: 1389\n",
      "The number of epochs is: 1390\n",
      "The number of epochs is: 1391\n",
      "The number of epochs is: 1392\n",
      "The number of epochs is: 1393\n",
      "The number of epochs is: 1394\n",
      "The number of epochs is: 1395\n",
      "The number of epochs is: 1396\n",
      "The number of epochs is: 1397\n",
      "The number of epochs is: 1398\n",
      "The number of epochs is: 1399\n",
      "The number of epochs is: 1400\n",
      "60\n",
      "Outputting sketch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 1701 loss is tensor([0.1266], grad_fn=<AddBackward0>)\n",
      "epoch: 1702 loss is tensor([0.1389], grad_fn=<AddBackward0>)\n",
      "epoch: 1703 loss is tensor([0.1233], grad_fn=<AddBackward0>)\n",
      "epoch: 1704 loss is tensor([0.1802], grad_fn=<AddBackward0>)\n",
      "epoch: 1705 loss is tensor([0.1457], grad_fn=<AddBackward0>)\n",
      "epoch: 1706 loss is tensor([0.1923], grad_fn=<AddBackward0>)\n",
      "epoch: 1707 loss is tensor([0.1958], grad_fn=<AddBackward0>)\n",
      "epoch: 1708 loss is tensor([0.1207], grad_fn=<AddBackward0>)\n",
      "epoch: 1709 loss is tensor([0.2276], grad_fn=<AddBackward0>)\n",
      "epoch: 1710 loss is tensor([0.1441], grad_fn=<AddBackward0>)\n",
      "epoch: 1711 loss is tensor([0.0759], grad_fn=<AddBackward0>)\n",
      "epoch: 1712 loss is tensor([0.1380], grad_fn=<AddBackward0>)\n",
      "epoch: 1713 loss is tensor([0.1226], grad_fn=<AddBackward0>)\n",
      "epoch: 1714 loss is tensor([0.1581], grad_fn=<AddBackward0>)\n",
      "epoch: 1715 loss is tensor([0.1037], grad_fn=<AddBackward0>)\n",
      "epoch: 1716 loss is tensor([0.1575], grad_fn=<AddBackward0>)\n",
      "epoch: 1717 loss is tensor([0.1235], grad_fn=<AddBackward0>)\n",
      "epoch: 1718 loss is tensor([0.1806], grad_fn=<AddBackward0>)\n",
      "epoch: 1719 loss is tensor([0.1625], grad_fn=<AddBackward0>)\n",
      "epoch: 1720 loss is tensor([0.1517], grad_fn=<AddBackward0>)\n",
      "epoch: 1721 loss is tensor([0.1166], grad_fn=<AddBackward0>)\n",
      "epoch: 1722 loss is tensor([0.1334], grad_fn=<AddBackward0>)\n",
      "epoch: 1723 loss is tensor([0.1224], grad_fn=<AddBackward0>)\n",
      "epoch: 1724 loss is tensor([0.1032], grad_fn=<AddBackward0>)\n",
      "epoch: 1725 loss is tensor([0.1697], grad_fn=<AddBackward0>)\n",
      "epoch: 1726 loss is tensor([0.1945], grad_fn=<AddBackward0>)\n",
      "epoch: 1727 loss is tensor([0.2059], grad_fn=<AddBackward0>)\n",
      "epoch: 1728 loss is tensor([0.1413], grad_fn=<AddBackward0>)\n",
      "epoch: 1729 loss is tensor([0.1659], grad_fn=<AddBackward0>)\n",
      "epoch: 1730 loss is tensor([0.0984], grad_fn=<AddBackward0>)\n",
      "epoch: 1731 loss is tensor([0.1671], grad_fn=<AddBackward0>)\n",
      "epoch: 1732 loss is tensor([0.1966], grad_fn=<AddBackward0>)\n",
      "epoch: 1733 loss is tensor([0.1783], grad_fn=<AddBackward0>)\n",
      "epoch: 1734 loss is tensor([0.1485], grad_fn=<AddBackward0>)\n",
      "epoch: 1735 loss is tensor([0.1569], grad_fn=<AddBackward0>)\n",
      "epoch: 1736 loss is tensor([0.1490], grad_fn=<AddBackward0>)\n",
      "epoch: 1737 loss is tensor([0.1756], grad_fn=<AddBackward0>)\n",
      "epoch: 1738 loss is tensor([0.1597], grad_fn=<AddBackward0>)\n",
      "epoch: 1739 loss is tensor([0.1834], grad_fn=<AddBackward0>)\n",
      "epoch: 1740 loss is tensor([0.1271], grad_fn=<AddBackward0>)\n",
      "epoch: 1741 loss is tensor([0.1520], grad_fn=<AddBackward0>)\n",
      "epoch: 1742 loss is tensor([0.1938], grad_fn=<AddBackward0>)\n",
      "epoch: 1743 loss is tensor([0.1235], grad_fn=<AddBackward0>)\n",
      "epoch: 1744 loss is tensor([0.1552], grad_fn=<AddBackward0>)\n",
      "epoch: 1745 loss is tensor([0.2195], grad_fn=<AddBackward0>)\n",
      "epoch: 1746 loss is tensor([0.1042], grad_fn=<AddBackward0>)\n",
      "epoch: 1747 loss is tensor([0.0824], grad_fn=<AddBackward0>)\n",
      "epoch: 1748 loss is tensor([0.0980], grad_fn=<AddBackward0>)\n",
      "epoch: 1749 loss is tensor([0.1450], grad_fn=<AddBackward0>)\n",
      "epoch: 1750 loss is tensor([0.1507], grad_fn=<AddBackward0>)\n",
      "epoch: 1751 loss is tensor([0.0983], grad_fn=<AddBackward0>)\n",
      "epoch: 1752 loss is tensor([0.0932], grad_fn=<AddBackward0>)\n",
      "epoch: 1753 loss is tensor([0.1253], grad_fn=<AddBackward0>)\n",
      "epoch: 1754 loss is tensor([0.1259], grad_fn=<AddBackward0>)\n",
      "epoch: 1755 loss is tensor([0.1128], grad_fn=<AddBackward0>)\n",
      "epoch: 1756 loss is tensor([0.1074], grad_fn=<AddBackward0>)\n",
      "epoch: 1757 loss is tensor([0.0959], grad_fn=<AddBackward0>)\n",
      "epoch: 1758 loss is tensor([0.1428], grad_fn=<AddBackward0>)\n",
      "epoch: 1759 loss is tensor([0.1582], grad_fn=<AddBackward0>)\n",
      "epoch: 1760 loss is tensor([0.0930], grad_fn=<AddBackward0>)\n",
      "epoch: 1761 loss is tensor([0.1008], grad_fn=<AddBackward0>)\n",
      "epoch: 1762 loss is tensor([0.1652], grad_fn=<AddBackward0>)\n",
      "epoch: 1763 loss is tensor([0.1530], grad_fn=<AddBackward0>)\n",
      "epoch: 1764 loss is tensor([0.2000], grad_fn=<AddBackward0>)\n",
      "epoch: 1765 loss is tensor([0.0910], grad_fn=<AddBackward0>)\n",
      "epoch: 1766 loss is tensor([0.0823], grad_fn=<AddBackward0>)\n",
      "epoch: 1767 loss is tensor([0.0798], grad_fn=<AddBackward0>)\n",
      "epoch: 1768 loss is tensor([0.1745], grad_fn=<AddBackward0>)\n",
      "epoch: 1769 loss is tensor([0.0235], grad_fn=<AddBackward0>)\n",
      "epoch: 1770 loss is tensor([0.1368], grad_fn=<AddBackward0>)\n",
      "epoch: 1771 loss is tensor([0.1139], grad_fn=<AddBackward0>)\n",
      "epoch: 1772 loss is tensor([0.1043], grad_fn=<AddBackward0>)\n",
      "epoch: 1773 loss is tensor([0.1473], grad_fn=<AddBackward0>)\n",
      "epoch: 1774 loss is tensor([0.1556], grad_fn=<AddBackward0>)\n",
      "epoch: 1775 loss is tensor([0.1267], grad_fn=<AddBackward0>)\n",
      "epoch: 1776 loss is tensor([0.1687], grad_fn=<AddBackward0>)\n",
      "epoch: 1777 loss is tensor([0.1382], grad_fn=<AddBackward0>)\n",
      "epoch: 1778 loss is tensor([0.1203], grad_fn=<AddBackward0>)\n",
      "epoch: 1779 loss is tensor([0.1314], grad_fn=<AddBackward0>)\n",
      "epoch: 1780 loss is tensor([0.1027], grad_fn=<AddBackward0>)\n",
      "epoch: 1781 loss is tensor([0.1289], grad_fn=<AddBackward0>)\n",
      "epoch: 1782 loss is tensor([0.1168], grad_fn=<AddBackward0>)\n",
      "epoch: 1783 loss is tensor([0.1481], grad_fn=<AddBackward0>)\n",
      "epoch: 1784 loss is tensor([0.1008], grad_fn=<AddBackward0>)\n",
      "epoch: 1785 loss is tensor([0.1481], grad_fn=<AddBackward0>)\n",
      "epoch: 1786 loss is tensor([0.1386], grad_fn=<AddBackward0>)\n",
      "epoch: 1787 loss is tensor([0.1088], grad_fn=<AddBackward0>)\n",
      "epoch: 1788 loss is tensor([0.1432], grad_fn=<AddBackward0>)\n",
      "epoch: 1789 loss is tensor([0.1301], grad_fn=<AddBackward0>)\n",
      "epoch: 1790 loss is tensor([0.1657], grad_fn=<AddBackward0>)\n",
      "epoch: 1791 loss is tensor([0.0756], grad_fn=<AddBackward0>)\n",
      "epoch: 1792 loss is tensor([0.1546], grad_fn=<AddBackward0>)\n",
      "epoch: 1793 loss is tensor([0.1623], grad_fn=<AddBackward0>)\n",
      "epoch: 1794 loss is tensor([0.0799], grad_fn=<AddBackward0>)\n",
      "epoch: 1795 loss is tensor([0.1028], grad_fn=<AddBackward0>)\n",
      "epoch: 1796 loss is tensor([0.1554], grad_fn=<AddBackward0>)\n",
      "epoch: 1797 loss is tensor([0.0812], grad_fn=<AddBackward0>)\n",
      "epoch: 1798 loss is tensor([0.0785], grad_fn=<AddBackward0>)\n",
      "epoch: 1799 loss is tensor([0.1966], grad_fn=<AddBackward0>)\n",
      "epoch: 1800 loss is tensor([0.1995], grad_fn=<AddBackward0>)\n",
      "12\n"
=======
      "The number of epochs is: 1401\n",
      "The number of epochs is: 1402\n",
      "The number of epochs is: 1403\n",
      "The number of epochs is: 1404\n",
      "The number of epochs is: 1405\n",
      "The number of epochs is: 1406\n",
      "The number of epochs is: 1407\n",
      "The number of epochs is: 1408\n",
      "The number of epochs is: 1409\n",
      "The number of epochs is: 1410\n",
      "The number of epochs is: 1411\n",
      "The number of epochs is: 1412\n",
      "The number of epochs is: 1413\n",
      "The number of epochs is: 1414\n",
      "The number of epochs is: 1415\n",
      "The number of epochs is: 1416\n",
      "The number of epochs is: 1417\n",
      "The number of epochs is: 1418\n",
      "The number of epochs is: 1419\n",
      "The number of epochs is: 1420\n",
      "The number of epochs is: 1421\n",
      "The number of epochs is: 1422\n",
      "The number of epochs is: 1423\n",
      "The number of epochs is: 1424\n",
      "The number of epochs is: 1425\n",
      "The number of epochs is: 1426\n",
      "The number of epochs is: 1427\n",
      "The number of epochs is: 1428\n",
      "The number of epochs is: 1429\n",
      "The number of epochs is: 1430\n",
      "The number of epochs is: 1431\n",
      "The number of epochs is: 1432\n",
      "The number of epochs is: 1433\n",
      "The number of epochs is: 1434\n",
      "The number of epochs is: 1435\n",
      "The number of epochs is: 1436\n",
      "The number of epochs is: 1437\n",
      "The number of epochs is: 1438\n",
      "The number of epochs is: 1439\n",
      "The number of epochs is: 1440\n",
      "The number of epochs is: 1441\n",
      "The number of epochs is: 1442\n",
      "The number of epochs is: 1443\n",
      "The number of epochs is: 1444\n",
      "The number of epochs is: 1445\n",
      "The number of epochs is: 1446\n",
      "The number of epochs is: 1447\n",
      "The number of epochs is: 1448\n",
      "The number of epochs is: 1449\n",
      "The number of epochs is: 1450\n",
      "The number of epochs is: 1451\n",
      "The number of epochs is: 1452\n",
      "The number of epochs is: 1453\n",
      "The number of epochs is: 1454\n",
      "The number of epochs is: 1455\n",
      "The number of epochs is: 1456\n",
      "The number of epochs is: 1457\n",
      "The number of epochs is: 1458\n",
      "The number of epochs is: 1459\n",
      "The number of epochs is: 1460\n",
      "The number of epochs is: 1461\n",
      "The number of epochs is: 1462\n",
      "The number of epochs is: 1463\n",
      "The number of epochs is: 1464\n",
      "The number of epochs is: 1465\n",
      "The number of epochs is: 1466\n",
      "The number of epochs is: 1467\n",
      "The number of epochs is: 1468\n",
      "The number of epochs is: 1469\n",
      "The number of epochs is: 1470\n",
      "The number of epochs is: 1471\n",
      "The number of epochs is: 1472\n",
      "The number of epochs is: 1473\n",
      "The number of epochs is: 1474\n",
      "The number of epochs is: 1475\n",
      "The number of epochs is: 1476\n",
      "The number of epochs is: 1477\n",
      "The number of epochs is: 1478\n",
      "The number of epochs is: 1479\n",
      "The number of epochs is: 1480\n",
      "The number of epochs is: 1481\n",
      "The number of epochs is: 1482\n",
      "The number of epochs is: 1483\n",
      "The number of epochs is: 1484\n",
      "The number of epochs is: 1485\n",
      "The number of epochs is: 1486\n",
      "The number of epochs is: 1487\n",
      "The number of epochs is: 1488\n",
      "The number of epochs is: 1489\n",
      "The number of epochs is: 1490\n",
      "The number of epochs is: 1491\n",
      "The number of epochs is: 1492\n",
      "The number of epochs is: 1493\n",
      "The number of epochs is: 1494\n",
      "The number of epochs is: 1495\n",
      "The number of epochs is: 1496\n",
      "The number of epochs is: 1497\n",
      "The number of epochs is: 1498\n",
      "The number of epochs is: 1499\n",
      "The number of epochs is: 1500\n",
      "The number of epochs is: 1501\n",
      "The number of epochs is: 1502\n",
      "The number of epochs is: 1503\n",
      "The number of epochs is: 1504\n",
      "The number of epochs is: 1505\n",
      "The number of epochs is: 1506\n",
      "The number of epochs is: 1507\n",
      "The number of epochs is: 1508\n",
      "The number of epochs is: 1509\n",
      "The number of epochs is: 1510\n",
      "The number of epochs is: 1511\n",
      "The number of epochs is: 1512\n",
      "The number of epochs is: 1513\n",
      "The number of epochs is: 1514\n",
      "The number of epochs is: 1515\n",
      "The number of epochs is: 1516\n",
      "The number of epochs is: 1517\n",
      "The number of epochs is: 1518\n",
      "The number of epochs is: 1519\n",
      "The number of epochs is: 1520\n",
      "The number of epochs is: 1521\n",
      "The number of epochs is: 1522\n",
      "The number of epochs is: 1523\n",
      "The number of epochs is: 1524\n",
      "The number of epochs is: 1525\n",
      "The number of epochs is: 1526\n",
      "The number of epochs is: 1527\n",
      "The number of epochs is: 1528\n",
      "The number of epochs is: 1529\n",
      "The number of epochs is: 1530\n",
      "The number of epochs is: 1531\n",
      "The number of epochs is: 1532\n",
      "The number of epochs is: 1533\n",
      "The number of epochs is: 1534\n",
      "The number of epochs is: 1535\n",
      "The number of epochs is: 1536\n",
      "The number of epochs is: 1537\n",
      "The number of epochs is: 1538\n",
      "The number of epochs is: 1539\n",
      "The number of epochs is: 1540\n",
      "The number of epochs is: 1541\n",
      "The number of epochs is: 1542\n",
      "The number of epochs is: 1543\n",
      "The number of epochs is: 1544\n",
      "The number of epochs is: 1545\n",
      "The number of epochs is: 1546\n",
      "The number of epochs is: 1547\n",
      "The number of epochs is: 1548\n",
      "The number of epochs is: 1549\n",
      "The number of epochs is: 1550\n",
      "The number of epochs is: 1551\n",
      "The number of epochs is: 1552\n",
      "The number of epochs is: 1553\n",
      "The number of epochs is: 1554\n",
      "The number of epochs is: 1555\n",
      "The number of epochs is: 1556\n",
      "The number of epochs is: 1557\n",
      "The number of epochs is: 1558\n",
      "The number of epochs is: 1559\n",
      "The number of epochs is: 1560\n",
      "The number of epochs is: 1561\n",
      "The number of epochs is: 1562\n",
      "The number of epochs is: 1563\n",
      "The number of epochs is: 1564\n",
      "The number of epochs is: 1565\n",
      "The number of epochs is: 1566\n",
      "The number of epochs is: 1567\n",
      "The number of epochs is: 1568\n",
      "The number of epochs is: 1569\n",
      "The number of epochs is: 1570\n",
      "The number of epochs is: 1571\n",
      "The number of epochs is: 1572\n",
      "The number of epochs is: 1573\n",
      "The number of epochs is: 1574\n",
      "The number of epochs is: 1575\n",
      "The number of epochs is: 1576\n",
      "The number of epochs is: 1577\n",
      "The number of epochs is: 1578\n",
      "The number of epochs is: 1579\n",
      "The number of epochs is: 1580\n",
      "The number of epochs is: 1581\n",
      "The number of epochs is: 1582\n",
      "The number of epochs is: 1583\n",
      "The number of epochs is: 1584\n",
      "The number of epochs is: 1585\n",
      "The number of epochs is: 1586\n",
      "The number of epochs is: 1587\n",
      "The number of epochs is: 1588\n",
      "The number of epochs is: 1589\n",
      "The number of epochs is: 1590\n",
      "The number of epochs is: 1591\n",
      "The number of epochs is: 1592\n",
      "The number of epochs is: 1593\n",
      "The number of epochs is: 1594\n",
      "The number of epochs is: 1595\n",
      "The number of epochs is: 1596\n",
      "The number of epochs is: 1597\n",
      "The number of epochs is: 1598\n",
      "The number of epochs is: 1599\n",
      "The number of epochs is: 1600\n",
      "11\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1801 loss is tensor([0.1312], grad_fn=<AddBackward0>)\n",
      "epoch: 1802 loss is tensor([0.1393], grad_fn=<AddBackward0>)\n",
      "epoch: 1803 loss is tensor([0.1082], grad_fn=<AddBackward0>)\n",
      "epoch: 1804 loss is tensor([0.1303], grad_fn=<AddBackward0>)\n",
      "epoch: 1805 loss is tensor([0.1665], grad_fn=<AddBackward0>)\n",
      "epoch: 1806 loss is tensor([0.1140], grad_fn=<AddBackward0>)\n",
      "epoch: 1807 loss is tensor([0.1434], grad_fn=<AddBackward0>)\n",
      "epoch: 1808 loss is tensor([0.1389], grad_fn=<AddBackward0>)\n",
      "epoch: 1809 loss is tensor([0.1864], grad_fn=<AddBackward0>)\n",
      "epoch: 1810 loss is tensor([0.2435], grad_fn=<AddBackward0>)\n",
      "epoch: 1811 loss is tensor([0.1415], grad_fn=<AddBackward0>)\n",
      "epoch: 1812 loss is tensor([0.1434], grad_fn=<AddBackward0>)\n",
      "epoch: 1813 loss is tensor([0.1131], grad_fn=<AddBackward0>)\n",
      "epoch: 1814 loss is tensor([0.2103], grad_fn=<AddBackward0>)\n",
      "epoch: 1815 loss is tensor([0.1756], grad_fn=<AddBackward0>)\n",
      "epoch: 1816 loss is tensor([0.1143], grad_fn=<AddBackward0>)\n",
      "epoch: 1817 loss is tensor([0.1550], grad_fn=<AddBackward0>)\n",
      "epoch: 1818 loss is tensor([0.1445], grad_fn=<AddBackward0>)\n",
      "epoch: 1819 loss is tensor([0.1567], grad_fn=<AddBackward0>)\n",
      "epoch: 1820 loss is tensor([0.1325], grad_fn=<AddBackward0>)\n",
      "epoch: 1821 loss is tensor([0.1257], grad_fn=<AddBackward0>)\n",
      "epoch: 1822 loss is tensor([0.1077], grad_fn=<AddBackward0>)\n",
      "epoch: 1823 loss is tensor([0.1391], grad_fn=<AddBackward0>)\n",
      "epoch: 1824 loss is tensor([0.1571], grad_fn=<AddBackward0>)\n",
      "epoch: 1825 loss is tensor([0.1381], grad_fn=<AddBackward0>)\n",
      "epoch: 1826 loss is tensor([0.0957], grad_fn=<AddBackward0>)\n",
      "epoch: 1827 loss is tensor([0.1060], grad_fn=<AddBackward0>)\n",
      "epoch: 1828 loss is tensor([0.1595], grad_fn=<AddBackward0>)\n",
      "epoch: 1829 loss is tensor([0.1089], grad_fn=<AddBackward0>)\n",
      "epoch: 1830 loss is tensor([0.1580], grad_fn=<AddBackward0>)\n",
      "epoch: 1831 loss is tensor([0.1502], grad_fn=<AddBackward0>)\n",
      "epoch: 1832 loss is tensor([0.0935], grad_fn=<AddBackward0>)\n",
      "epoch: 1833 loss is tensor([0.1544], grad_fn=<AddBackward0>)\n",
      "epoch: 1834 loss is tensor([0.0862], grad_fn=<AddBackward0>)\n",
      "epoch: 1835 loss is tensor([0.1439], grad_fn=<AddBackward0>)\n",
      "epoch: 1836 loss is tensor([0.0485], grad_fn=<AddBackward0>)\n",
      "epoch: 1837 loss is tensor([0.1336], grad_fn=<AddBackward0>)\n",
      "epoch: 1838 loss is tensor([0.1326], grad_fn=<AddBackward0>)\n",
      "epoch: 1839 loss is tensor([0.1116], grad_fn=<AddBackward0>)\n",
      "epoch: 1840 loss is tensor([0.0723], grad_fn=<AddBackward0>)\n",
      "epoch: 1841 loss is tensor([0.0590], grad_fn=<AddBackward0>)\n",
      "epoch: 1842 loss is tensor([0.1666], grad_fn=<AddBackward0>)\n",
      "epoch: 1843 loss is tensor([0.1701], grad_fn=<AddBackward0>)\n",
      "epoch: 1844 loss is tensor([0.1290], grad_fn=<AddBackward0>)\n",
      "epoch: 1845 loss is tensor([0.1503], grad_fn=<AddBackward0>)\n",
      "epoch: 1846 loss is tensor([0.0740], grad_fn=<AddBackward0>)\n",
      "epoch: 1847 loss is tensor([0.0797], grad_fn=<AddBackward0>)\n",
      "epoch: 1848 loss is tensor([0.1085], grad_fn=<AddBackward0>)\n",
      "epoch: 1849 loss is tensor([0.1198], grad_fn=<AddBackward0>)\n",
      "epoch: 1850 loss is tensor([0.1628], grad_fn=<AddBackward0>)\n",
      "epoch: 1851 loss is tensor([0.1314], grad_fn=<AddBackward0>)\n",
      "epoch: 1852 loss is tensor([0.1155], grad_fn=<AddBackward0>)\n",
      "epoch: 1853 loss is tensor([0.1250], grad_fn=<AddBackward0>)\n",
      "epoch: 1854 loss is tensor([0.0656], grad_fn=<AddBackward0>)\n",
      "epoch: 1855 loss is tensor([0.1473], grad_fn=<AddBackward0>)\n",
      "epoch: 1856 loss is tensor([0.0969], grad_fn=<AddBackward0>)\n",
      "epoch: 1857 loss is tensor([0.1863], grad_fn=<AddBackward0>)\n",
      "epoch: 1858 loss is tensor([0.0976], grad_fn=<AddBackward0>)\n",
      "epoch: 1859 loss is tensor([0.1277], grad_fn=<AddBackward0>)\n",
      "epoch: 1860 loss is tensor([0.1524], grad_fn=<AddBackward0>)\n",
      "epoch: 1861 loss is tensor([0.0818], grad_fn=<AddBackward0>)\n",
      "epoch: 1862 loss is tensor([0.0982], grad_fn=<AddBackward0>)\n",
      "epoch: 1863 loss is tensor([0.1318], grad_fn=<AddBackward0>)\n",
      "epoch: 1864 loss is tensor([0.0971], grad_fn=<AddBackward0>)\n",
      "epoch: 1865 loss is tensor([0.0669], grad_fn=<AddBackward0>)\n",
      "epoch: 1866 loss is tensor([0.1168], grad_fn=<AddBackward0>)\n",
      "epoch: 1867 loss is tensor([0.1415], grad_fn=<AddBackward0>)\n",
      "epoch: 1868 loss is tensor([0.0684], grad_fn=<AddBackward0>)\n",
      "epoch: 1869 loss is tensor([0.0893], grad_fn=<AddBackward0>)\n",
      "epoch: 1870 loss is tensor([0.1013], grad_fn=<AddBackward0>)\n",
      "epoch: 1871 loss is tensor([0.0880], grad_fn=<AddBackward0>)\n",
      "epoch: 1872 loss is tensor([0.1317], grad_fn=<AddBackward0>)\n",
      "epoch: 1873 loss is tensor([0.0699], grad_fn=<AddBackward0>)\n",
      "epoch: 1874 loss is tensor([0.1365], grad_fn=<AddBackward0>)\n",
      "epoch: 1875 loss is tensor([0.1675], grad_fn=<AddBackward0>)\n",
      "epoch: 1876 loss is tensor([0.1318], grad_fn=<AddBackward0>)\n",
      "epoch: 1877 loss is tensor([0.1730], grad_fn=<AddBackward0>)\n",
      "epoch: 1878 loss is tensor([0.1112], grad_fn=<AddBackward0>)\n",
      "epoch: 1879 loss is tensor([0.1362], grad_fn=<AddBackward0>)\n",
      "epoch: 1880 loss is tensor([0.1545], grad_fn=<AddBackward0>)\n",
      "epoch: 1881 loss is tensor([0.1105], grad_fn=<AddBackward0>)\n",
      "epoch: 1882 loss is tensor([0.1402], grad_fn=<AddBackward0>)\n",
      "epoch: 1883 loss is tensor([0.0815], grad_fn=<AddBackward0>)\n",
      "epoch: 1884 loss is tensor([0.0989], grad_fn=<AddBackward0>)\n",
      "epoch: 1885 loss is tensor([0.1381], grad_fn=<AddBackward0>)\n",
      "epoch: 1886 loss is tensor([0.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 1887 loss is tensor([0.1179], grad_fn=<AddBackward0>)\n",
      "epoch: 1888 loss is tensor([0.1544], grad_fn=<AddBackward0>)\n",
      "epoch: 1889 loss is tensor([0.0669], grad_fn=<AddBackward0>)\n",
      "epoch: 1890 loss is tensor([0.1275], grad_fn=<AddBackward0>)\n",
      "epoch: 1891 loss is tensor([0.1671], grad_fn=<AddBackward0>)\n",
      "epoch: 1892 loss is tensor([0.1025], grad_fn=<AddBackward0>)\n",
      "epoch: 1893 loss is tensor([0.1983], grad_fn=<AddBackward0>)\n",
      "epoch: 1894 loss is tensor([0.1386], grad_fn=<AddBackward0>)\n",
      "epoch: 1895 loss is tensor([0.1159], grad_fn=<AddBackward0>)\n",
      "epoch: 1896 loss is tensor([0.1724], grad_fn=<AddBackward0>)\n",
      "epoch: 1897 loss is tensor([0.1376], grad_fn=<AddBackward0>)\n",
      "epoch: 1898 loss is tensor([0.1285], grad_fn=<AddBackward0>)\n",
      "epoch: 1899 loss is tensor([0.1438], grad_fn=<AddBackward0>)\n",
      "epoch: 1900 loss is tensor([0.1262], grad_fn=<AddBackward0>)\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1901 loss is tensor([0.1982], grad_fn=<AddBackward0>)\n",
      "epoch: 1902 loss is tensor([0.1777], grad_fn=<AddBackward0>)\n",
      "epoch: 1903 loss is tensor([0.1382], grad_fn=<AddBackward0>)\n",
      "epoch: 1904 loss is tensor([0.1558], grad_fn=<AddBackward0>)\n",
      "epoch: 1905 loss is tensor([0.2148], grad_fn=<AddBackward0>)\n",
      "epoch: 1906 loss is tensor([0.1345], grad_fn=<AddBackward0>)\n",
      "epoch: 1907 loss is tensor([0.0867], grad_fn=<AddBackward0>)\n",
      "epoch: 1908 loss is tensor([0.1678], grad_fn=<AddBackward0>)\n",
      "epoch: 1909 loss is tensor([0.1902], grad_fn=<AddBackward0>)\n",
      "epoch: 1910 loss is tensor([0.1105], grad_fn=<AddBackward0>)\n",
      "epoch: 1911 loss is tensor([0.1230], grad_fn=<AddBackward0>)\n",
      "epoch: 1912 loss is tensor([0.0359], grad_fn=<AddBackward0>)\n",
      "epoch: 1913 loss is tensor([0.0817], grad_fn=<AddBackward0>)\n",
      "epoch: 1914 loss is tensor([0.1474], grad_fn=<AddBackward0>)\n",
      "epoch: 1915 loss is tensor([0.1594], grad_fn=<AddBackward0>)\n",
      "epoch: 1916 loss is tensor([0.1153], grad_fn=<AddBackward0>)\n",
      "epoch: 1917 loss is tensor([0.1028], grad_fn=<AddBackward0>)\n",
      "epoch: 1918 loss is tensor([0.1572], grad_fn=<AddBackward0>)\n",
      "epoch: 1919 loss is tensor([0.1436], grad_fn=<AddBackward0>)\n",
      "epoch: 1920 loss is tensor([0.1126], grad_fn=<AddBackward0>)\n",
      "epoch: 1921 loss is tensor([0.1339], grad_fn=<AddBackward0>)\n",
      "epoch: 1922 loss is tensor([0.0891], grad_fn=<AddBackward0>)\n",
      "epoch: 1923 loss is tensor([0.1275], grad_fn=<AddBackward0>)\n",
      "epoch: 1924 loss is tensor([0.1288], grad_fn=<AddBackward0>)\n",
      "epoch: 1925 loss is tensor([0.1217], grad_fn=<AddBackward0>)\n",
      "epoch: 1926 loss is tensor([0.1590], grad_fn=<AddBackward0>)\n",
      "epoch: 1927 loss is tensor([0.0679], grad_fn=<AddBackward0>)\n",
      "epoch: 1928 loss is tensor([0.1304], grad_fn=<AddBackward0>)\n",
      "epoch: 1929 loss is tensor([0.1037], grad_fn=<AddBackward0>)\n",
      "epoch: 1930 loss is tensor([0.1220], grad_fn=<AddBackward0>)\n",
      "epoch: 1931 loss is tensor([0.1017], grad_fn=<AddBackward0>)\n",
      "epoch: 1932 loss is tensor([0.1423], grad_fn=<AddBackward0>)\n",
      "epoch: 1933 loss is tensor([0.1480], grad_fn=<AddBackward0>)\n",
      "epoch: 1934 loss is tensor([0.0955], grad_fn=<AddBackward0>)\n",
      "epoch: 1935 loss is tensor([0.1266], grad_fn=<AddBackward0>)\n",
      "epoch: 1936 loss is tensor([0.1318], grad_fn=<AddBackward0>)\n",
      "epoch: 1937 loss is tensor([0.0214], grad_fn=<AddBackward0>)\n",
      "epoch: 1938 loss is tensor([0.0560], grad_fn=<AddBackward0>)\n",
      "epoch: 1939 loss is tensor([0.1169], grad_fn=<AddBackward0>)\n",
      "epoch: 1940 loss is tensor([0.1124], grad_fn=<AddBackward0>)\n",
      "epoch: 1941 loss is tensor([0.0723], grad_fn=<AddBackward0>)\n",
      "epoch: 1942 loss is tensor([0.1256], grad_fn=<AddBackward0>)\n",
      "epoch: 1943 loss is tensor([0.0698], grad_fn=<AddBackward0>)\n",
      "epoch: 1944 loss is tensor([0.0794], grad_fn=<AddBackward0>)\n",
      "epoch: 1945 loss is tensor([0.0731], grad_fn=<AddBackward0>)\n",
      "epoch: 1946 loss is tensor([0.0596], grad_fn=<AddBackward0>)\n",
      "epoch: 1947 loss is tensor([0.0573], grad_fn=<AddBackward0>)\n",
      "epoch: 1948 loss is tensor([0.0699], grad_fn=<AddBackward0>)\n",
      "epoch: 1949 loss is tensor([0.1257], grad_fn=<AddBackward0>)\n",
      "epoch: 1950 loss is tensor([0.1313], grad_fn=<AddBackward0>)\n",
      "epoch: 1951 loss is tensor([0.1015], grad_fn=<AddBackward0>)\n",
      "epoch: 1952 loss is tensor([0.1275], grad_fn=<AddBackward0>)\n",
      "epoch: 1953 loss is tensor([0.1188], grad_fn=<AddBackward0>)\n",
      "epoch: 1954 loss is tensor([0.1652], grad_fn=<AddBackward0>)\n",
      "epoch: 1955 loss is tensor([0.1406], grad_fn=<AddBackward0>)\n",
      "epoch: 1956 loss is tensor([0.1411], grad_fn=<AddBackward0>)\n",
      "epoch: 1957 loss is tensor([0.1335], grad_fn=<AddBackward0>)\n",
      "epoch: 1958 loss is tensor([0.1272], grad_fn=<AddBackward0>)\n",
      "epoch: 1959 loss is tensor([0.1685], grad_fn=<AddBackward0>)\n",
      "epoch: 1960 loss is tensor([0.0910], grad_fn=<AddBackward0>)\n",
      "epoch: 1961 loss is tensor([0.1068], grad_fn=<AddBackward0>)\n",
      "epoch: 1962 loss is tensor([0.0519], grad_fn=<AddBackward0>)\n",
      "epoch: 1963 loss is tensor([0.0452], grad_fn=<AddBackward0>)\n",
      "epoch: 1964 loss is tensor([0.0797], grad_fn=<AddBackward0>)\n",
      "epoch: 1965 loss is tensor([0.0837], grad_fn=<AddBackward0>)\n",
      "epoch: 1966 loss is tensor([0.0606], grad_fn=<AddBackward0>)\n",
      "epoch: 1967 loss is tensor([0.0844], grad_fn=<AddBackward0>)\n",
      "epoch: 1968 loss is tensor([0.1766], grad_fn=<AddBackward0>)\n",
      "epoch: 1969 loss is tensor([0.1485], grad_fn=<AddBackward0>)\n",
      "epoch: 1970 loss is tensor([0.1202], grad_fn=<AddBackward0>)\n",
      "epoch: 1971 loss is tensor([0.0797], grad_fn=<AddBackward0>)\n",
      "epoch: 1972 loss is tensor([0.1370], grad_fn=<AddBackward0>)\n",
      "epoch: 1973 loss is tensor([0.0918], grad_fn=<AddBackward0>)\n",
      "epoch: 1974 loss is tensor([0.1123], grad_fn=<AddBackward0>)\n",
      "epoch: 1975 loss is tensor([0.0644], grad_fn=<AddBackward0>)\n",
      "epoch: 1976 loss is tensor([0.1103], grad_fn=<AddBackward0>)\n",
      "epoch: 1977 loss is tensor([0.0835], grad_fn=<AddBackward0>)\n",
      "epoch: 1978 loss is tensor([0.0667], grad_fn=<AddBackward0>)\n",
      "epoch: 1979 loss is tensor([0.1467], grad_fn=<AddBackward0>)\n",
      "epoch: 1980 loss is tensor([0.0818], grad_fn=<AddBackward0>)\n",
      "epoch: 1981 loss is tensor([0.1167], grad_fn=<AddBackward0>)\n",
      "epoch: 1982 loss is tensor([0.1203], grad_fn=<AddBackward0>)\n",
      "epoch: 1983 loss is tensor([0.0668], grad_fn=<AddBackward0>)\n",
      "epoch: 1984 loss is tensor([0.1356], grad_fn=<AddBackward0>)\n",
      "epoch: 1985 loss is tensor([0.1311], grad_fn=<AddBackward0>)\n",
      "epoch: 1986 loss is tensor([0.1206], grad_fn=<AddBackward0>)\n",
      "epoch: 1987 loss is tensor([0.1069], grad_fn=<AddBackward0>)\n",
      "epoch: 1988 loss is tensor([0.0763], grad_fn=<AddBackward0>)\n",
      "epoch: 1989 loss is tensor([0.0779], grad_fn=<AddBackward0>)\n",
      "epoch: 1990 loss is tensor([0.1358], grad_fn=<AddBackward0>)\n",
      "epoch: 1991 loss is tensor([0.1490], grad_fn=<AddBackward0>)\n",
      "epoch: 1992 loss is tensor([0.0788], grad_fn=<AddBackward0>)\n",
      "epoch: 1993 loss is tensor([0.1380], grad_fn=<AddBackward0>)\n",
      "epoch: 1994 loss is tensor([0.1936], grad_fn=<AddBackward0>)\n",
      "epoch: 1995 loss is tensor([0.1157], grad_fn=<AddBackward0>)\n",
      "epoch: 1996 loss is tensor([0.0928], grad_fn=<AddBackward0>)\n",
      "epoch: 1997 loss is tensor([0.0920], grad_fn=<AddBackward0>)\n",
      "epoch: 1998 loss is tensor([0.1208], grad_fn=<AddBackward0>)\n",
      "epoch: 1999 loss is tensor([0.0707], grad_fn=<AddBackward0>)\n",
      "epoch: 2000 loss is tensor([0.0893], grad_fn=<AddBackward0>)\n",
      "28\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 2001 loss is tensor([0.0911], grad_fn=<AddBackward0>)\n",
      "epoch: 2002 loss is tensor([0.1288], grad_fn=<AddBackward0>)\n",
      "epoch: 2003 loss is tensor([0.1237], grad_fn=<AddBackward0>)\n",
      "epoch: 2004 loss is tensor([0.1504], grad_fn=<AddBackward0>)\n",
      "epoch: 2005 loss is tensor([0.1386], grad_fn=<AddBackward0>)\n",
      "epoch: 2006 loss is tensor([0.1155], grad_fn=<AddBackward0>)\n",
      "epoch: 2007 loss is tensor([0.1001], grad_fn=<AddBackward0>)\n",
      "epoch: 2008 loss is tensor([0.1199], grad_fn=<AddBackward0>)\n",
      "epoch: 2009 loss is tensor([0.1265], grad_fn=<AddBackward0>)\n",
      "epoch: 2010 loss is tensor([0.1348], grad_fn=<AddBackward0>)\n",
      "epoch: 2011 loss is tensor([0.1349], grad_fn=<AddBackward0>)\n",
      "epoch: 2012 loss is tensor([0.1351], grad_fn=<AddBackward0>)\n",
      "epoch: 2013 loss is tensor([0.1354], grad_fn=<AddBackward0>)\n",
      "epoch: 2014 loss is tensor([0.0856], grad_fn=<AddBackward0>)\n",
      "epoch: 2015 loss is tensor([0.0882], grad_fn=<AddBackward0>)\n",
      "epoch: 2016 loss is tensor([0.0505], grad_fn=<AddBackward0>)\n",
      "epoch: 2017 loss is tensor([0.1071], grad_fn=<AddBackward0>)\n",
      "epoch: 2018 loss is tensor([0.0624], grad_fn=<AddBackward0>)\n",
      "epoch: 2019 loss is tensor([0.0360], grad_fn=<AddBackward0>)\n",
      "epoch: 2020 loss is tensor([0.0714], grad_fn=<AddBackward0>)\n",
      "epoch: 2021 loss is tensor([0.1260], grad_fn=<AddBackward0>)\n",
      "epoch: 2022 loss is tensor([0.1310], grad_fn=<AddBackward0>)\n",
      "epoch: 2023 loss is tensor([0.0744], grad_fn=<AddBackward0>)\n",
      "epoch: 2024 loss is tensor([0.0773], grad_fn=<AddBackward0>)\n",
      "epoch: 2025 loss is tensor([0.0660], grad_fn=<AddBackward0>)\n",
      "epoch: 2026 loss is tensor([0.1178], grad_fn=<AddBackward0>)\n",
      "epoch: 2027 loss is tensor([0.1013], grad_fn=<AddBackward0>)\n",
      "epoch: 2028 loss is tensor([0.1139], grad_fn=<AddBackward0>)\n",
      "epoch: 2029 loss is tensor([0.1391], grad_fn=<AddBackward0>)\n",
      "epoch: 2030 loss is tensor([0.1300], grad_fn=<AddBackward0>)\n",
      "epoch: 2031 loss is tensor([0.1393], grad_fn=<AddBackward0>)\n",
      "epoch: 2032 loss is tensor([0.1045], grad_fn=<AddBackward0>)\n",
      "epoch: 2033 loss is tensor([0.1005], grad_fn=<AddBackward0>)\n",
      "epoch: 2034 loss is tensor([0.1311], grad_fn=<AddBackward0>)\n",
      "epoch: 2035 loss is tensor([0.1579], grad_fn=<AddBackward0>)\n",
      "epoch: 2036 loss is tensor([0.0854], grad_fn=<AddBackward0>)\n",
      "epoch: 2037 loss is tensor([0.1094], grad_fn=<AddBackward0>)\n",
      "epoch: 2038 loss is tensor([0.1166], grad_fn=<AddBackward0>)\n",
      "epoch: 2039 loss is tensor([0.1575], grad_fn=<AddBackward0>)\n",
      "epoch: 2040 loss is tensor([0.1291], grad_fn=<AddBackward0>)\n",
      "epoch: 2041 loss is tensor([0.1080], grad_fn=<AddBackward0>)\n",
      "epoch: 2042 loss is tensor([0.1366], grad_fn=<AddBackward0>)\n",
      "epoch: 2043 loss is tensor([0.1120], grad_fn=<AddBackward0>)\n",
      "epoch: 2044 loss is tensor([0.1320], grad_fn=<AddBackward0>)\n",
      "epoch: 2045 loss is tensor([0.1996], grad_fn=<AddBackward0>)\n",
      "epoch: 2046 loss is tensor([0.1752], grad_fn=<AddBackward0>)\n",
      "epoch: 2047 loss is tensor([0.1727], grad_fn=<AddBackward0>)\n",
      "epoch: 2048 loss is tensor([0.0681], grad_fn=<AddBackward0>)\n",
      "epoch: 2049 loss is tensor([0.1107], grad_fn=<AddBackward0>)\n",
      "epoch: 2050 loss is tensor([0.0793], grad_fn=<AddBackward0>)\n",
      "epoch: 2051 loss is tensor([0.0608], grad_fn=<AddBackward0>)\n",
      "epoch: 2052 loss is tensor([0.1041], grad_fn=<AddBackward0>)\n",
      "epoch: 2053 loss is tensor([0.0685], grad_fn=<AddBackward0>)\n",
      "epoch: 2054 loss is tensor([0.1280], grad_fn=<AddBackward0>)\n",
      "epoch: 2055 loss is tensor([0.1263], grad_fn=<AddBackward0>)\n",
      "epoch: 2056 loss is tensor([0.0969], grad_fn=<AddBackward0>)\n",
      "epoch: 2057 loss is tensor([0.0921], grad_fn=<AddBackward0>)\n",
      "epoch: 2058 loss is tensor([0.1647], grad_fn=<AddBackward0>)\n",
      "epoch: 2059 loss is tensor([0.0680], grad_fn=<AddBackward0>)\n",
      "epoch: 2060 loss is tensor([0.1058], grad_fn=<AddBackward0>)\n",
      "epoch: 2061 loss is tensor([0.1145], grad_fn=<AddBackward0>)\n",
      "epoch: 2062 loss is tensor([0.0881], grad_fn=<AddBackward0>)\n",
      "epoch: 2063 loss is tensor([0.0370], grad_fn=<AddBackward0>)\n",
      "epoch: 2064 loss is tensor([0.0380], grad_fn=<AddBackward0>)\n",
      "epoch: 2065 loss is tensor([0.0927], grad_fn=<AddBackward0>)\n",
      "epoch: 2066 loss is tensor([0.0352], grad_fn=<AddBackward0>)\n",
      "epoch: 2067 loss is tensor([0.0694], grad_fn=<AddBackward0>)\n",
      "epoch: 2068 loss is tensor([0.0374], grad_fn=<AddBackward0>)\n",
      "epoch: 2069 loss is tensor([0.1005], grad_fn=<AddBackward0>)\n",
      "epoch: 2070 loss is tensor([0.0644], grad_fn=<AddBackward0>)\n",
      "epoch: 2071 loss is tensor([0.0913], grad_fn=<AddBackward0>)\n",
      "epoch: 2072 loss is tensor([0.0487], grad_fn=<AddBackward0>)\n",
      "epoch: 2073 loss is tensor([0.1036], grad_fn=<AddBackward0>)\n",
      "epoch: 2074 loss is tensor([0.1062], grad_fn=<AddBackward0>)\n",
      "epoch: 2075 loss is tensor([0.0499], grad_fn=<AddBackward0>)\n",
      "epoch: 2076 loss is tensor([0.0440], grad_fn=<AddBackward0>)\n",
      "epoch: 2077 loss is tensor([0.0088], grad_fn=<AddBackward0>)\n",
      "epoch: 2078 loss is tensor([0.0638], grad_fn=<AddBackward0>)\n",
      "epoch: 2079 loss is tensor([0.0211], grad_fn=<AddBackward0>)\n",
      "epoch: 2080 loss is tensor([0.0877], grad_fn=<AddBackward0>)\n",
      "epoch: 2081 loss is tensor([0.1055], grad_fn=<AddBackward0>)\n",
      "epoch: 2082 loss is tensor([0.1025], grad_fn=<AddBackward0>)\n",
      "epoch: 2083 loss is tensor([0.1338], grad_fn=<AddBackward0>)\n",
      "epoch: 2084 loss is tensor([0.0555], grad_fn=<AddBackward0>)\n",
      "epoch: 2085 loss is tensor([0.0974], grad_fn=<AddBackward0>)\n",
      "epoch: 2086 loss is tensor([0.0618], grad_fn=<AddBackward0>)\n",
      "epoch: 2087 loss is tensor([0.1138], grad_fn=<AddBackward0>)\n",
      "epoch: 2088 loss is tensor([0.1786], grad_fn=<AddBackward0>)\n",
      "epoch: 2089 loss is tensor([0.0882], grad_fn=<AddBackward0>)\n",
      "epoch: 2090 loss is tensor([0.1175], grad_fn=<AddBackward0>)\n",
      "epoch: 2091 loss is tensor([0.0793], grad_fn=<AddBackward0>)\n",
      "epoch: 2092 loss is tensor([0.1369], grad_fn=<AddBackward0>)\n",
      "epoch: 2093 loss is tensor([0.1470], grad_fn=<AddBackward0>)\n",
      "epoch: 2094 loss is tensor([0.1052], grad_fn=<AddBackward0>)\n",
      "epoch: 2095 loss is tensor([0.0867], grad_fn=<AddBackward0>)\n",
      "epoch: 2096 loss is tensor([0.1118], grad_fn=<AddBackward0>)\n",
      "epoch: 2097 loss is tensor([0.0478], grad_fn=<AddBackward0>)\n",
      "epoch: 2098 loss is tensor([0.1022], grad_fn=<AddBackward0>)\n",
      "epoch: 2099 loss is tensor([0.1160], grad_fn=<AddBackward0>)\n",
      "epoch: 2100 loss is tensor([0.1005], grad_fn=<AddBackward0>)\n",
      "11\n"
=======
      "The number of epochs is: 1601\n",
      "The number of epochs is: 1602\n",
      "The number of epochs is: 1603\n",
      "The number of epochs is: 1604\n",
      "The number of epochs is: 1605\n",
      "The number of epochs is: 1606\n",
      "The number of epochs is: 1607\n",
      "The number of epochs is: 1608\n",
      "The number of epochs is: 1609\n",
      "The number of epochs is: 1610\n",
      "The number of epochs is: 1611\n",
      "The number of epochs is: 1612\n",
      "The number of epochs is: 1613\n",
      "The number of epochs is: 1614\n",
      "The number of epochs is: 1615\n",
      "The number of epochs is: 1616\n",
      "The number of epochs is: 1617\n",
      "The number of epochs is: 1618\n",
      "The number of epochs is: 1619\n",
      "The number of epochs is: 1620\n",
      "The number of epochs is: 1621\n",
      "The number of epochs is: 1622\n",
      "The number of epochs is: 1623\n",
      "The number of epochs is: 1624\n",
      "The number of epochs is: 1625\n",
      "The number of epochs is: 1626\n",
      "The number of epochs is: 1627\n",
      "The number of epochs is: 1628\n",
      "The number of epochs is: 1629\n",
      "The number of epochs is: 1630\n",
      "The number of epochs is: 1631\n",
      "The number of epochs is: 1632\n",
      "The number of epochs is: 1633\n",
      "The number of epochs is: 1634\n",
      "The number of epochs is: 1635\n",
      "The number of epochs is: 1636\n",
      "The number of epochs is: 1637\n",
      "The number of epochs is: 1638\n",
      "The number of epochs is: 1639\n",
      "The number of epochs is: 1640\n",
      "The number of epochs is: 1641\n",
      "The number of epochs is: 1642\n",
      "The number of epochs is: 1643\n",
      "The number of epochs is: 1644\n",
      "The number of epochs is: 1645\n",
      "The number of epochs is: 1646\n",
      "The number of epochs is: 1647\n",
      "The number of epochs is: 1648\n",
      "The number of epochs is: 1649\n",
      "The number of epochs is: 1650\n",
      "The number of epochs is: 1651\n",
      "The number of epochs is: 1652\n",
      "The number of epochs is: 1653\n",
      "The number of epochs is: 1654\n",
      "The number of epochs is: 1655\n",
      "The number of epochs is: 1656\n",
      "The number of epochs is: 1657\n",
      "The number of epochs is: 1658\n",
      "The number of epochs is: 1659\n",
      "The number of epochs is: 1660\n",
      "The number of epochs is: 1661\n",
      "The number of epochs is: 1662\n",
      "The number of epochs is: 1663\n",
      "The number of epochs is: 1664\n",
      "The number of epochs is: 1665\n",
      "The number of epochs is: 1666\n",
      "The number of epochs is: 1667\n",
      "The number of epochs is: 1668\n",
      "The number of epochs is: 1669\n",
      "The number of epochs is: 1670\n",
      "The number of epochs is: 1671\n",
      "The number of epochs is: 1672\n",
      "The number of epochs is: 1673\n",
      "The number of epochs is: 1674\n",
      "The number of epochs is: 1675\n",
      "The number of epochs is: 1676\n",
      "The number of epochs is: 1677\n",
      "The number of epochs is: 1678\n",
      "The number of epochs is: 1679\n",
      "The number of epochs is: 1680\n",
      "The number of epochs is: 1681\n",
      "The number of epochs is: 1682\n",
      "The number of epochs is: 1683\n",
      "The number of epochs is: 1684\n",
      "The number of epochs is: 1685\n",
      "The number of epochs is: 1686\n",
      "The number of epochs is: 1687\n",
      "The number of epochs is: 1688\n",
      "The number of epochs is: 1689\n",
      "The number of epochs is: 1690\n",
      "The number of epochs is: 1691\n",
      "The number of epochs is: 1692\n",
      "The number of epochs is: 1693\n",
      "The number of epochs is: 1694\n",
      "The number of epochs is: 1695\n",
      "The number of epochs is: 1696\n",
      "The number of epochs is: 1697\n",
      "The number of epochs is: 1698\n",
      "The number of epochs is: 1699\n",
      "The number of epochs is: 1700\n",
      "The number of epochs is: 1701\n",
      "The number of epochs is: 1702\n",
      "The number of epochs is: 1703\n",
      "The number of epochs is: 1704\n",
      "The number of epochs is: 1705\n",
      "The number of epochs is: 1706\n",
      "The number of epochs is: 1707\n",
      "The number of epochs is: 1708\n",
      "The number of epochs is: 1709\n",
      "The number of epochs is: 1710\n",
      "The number of epochs is: 1711\n",
      "The number of epochs is: 1712\n",
      "The number of epochs is: 1713\n",
      "The number of epochs is: 1714\n",
      "The number of epochs is: 1715\n",
      "The number of epochs is: 1716\n",
      "The number of epochs is: 1717\n",
      "The number of epochs is: 1718\n",
      "The number of epochs is: 1719\n",
      "The number of epochs is: 1720\n",
      "The number of epochs is: 1721\n",
      "The number of epochs is: 1722\n",
      "The number of epochs is: 1723\n",
      "The number of epochs is: 1724\n",
      "The number of epochs is: 1725\n",
      "The number of epochs is: 1726\n",
      "The number of epochs is: 1727\n",
      "The number of epochs is: 1728\n",
      "The number of epochs is: 1729\n",
      "The number of epochs is: 1730\n",
      "The number of epochs is: 1731\n",
      "The number of epochs is: 1732\n",
      "The number of epochs is: 1733\n",
      "The number of epochs is: 1734\n",
      "The number of epochs is: 1735\n",
      "The number of epochs is: 1736\n",
      "The number of epochs is: 1737\n",
      "The number of epochs is: 1738\n",
      "The number of epochs is: 1739\n",
      "The number of epochs is: 1740\n",
      "The number of epochs is: 1741\n",
      "The number of epochs is: 1742\n",
      "The number of epochs is: 1743\n",
      "The number of epochs is: 1744\n",
      "The number of epochs is: 1745\n",
      "The number of epochs is: 1746\n",
      "The number of epochs is: 1747\n",
      "The number of epochs is: 1748\n",
      "The number of epochs is: 1749\n",
      "The number of epochs is: 1750\n",
      "The number of epochs is: 1751\n",
      "The number of epochs is: 1752\n",
      "The number of epochs is: 1753\n",
      "The number of epochs is: 1754\n",
      "The number of epochs is: 1755\n",
      "The number of epochs is: 1756\n",
      "The number of epochs is: 1757\n",
      "The number of epochs is: 1758\n",
      "The number of epochs is: 1759\n",
      "The number of epochs is: 1760\n",
      "The number of epochs is: 1761\n",
      "The number of epochs is: 1762\n",
      "The number of epochs is: 1763\n",
      "The number of epochs is: 1764\n",
      "The number of epochs is: 1765\n",
      "The number of epochs is: 1766\n",
      "The number of epochs is: 1767\n",
      "The number of epochs is: 1768\n",
      "The number of epochs is: 1769\n",
      "The number of epochs is: 1770\n",
      "The number of epochs is: 1771\n",
      "The number of epochs is: 1772\n",
      "The number of epochs is: 1773\n",
      "The number of epochs is: 1774\n",
      "The number of epochs is: 1775\n",
      "The number of epochs is: 1776\n",
      "The number of epochs is: 1777\n",
      "The number of epochs is: 1778\n",
      "The number of epochs is: 1779\n",
      "The number of epochs is: 1780\n",
      "The number of epochs is: 1781\n",
      "The number of epochs is: 1782\n",
      "The number of epochs is: 1783\n",
      "The number of epochs is: 1784\n",
      "The number of epochs is: 1785\n",
      "The number of epochs is: 1786\n",
      "The number of epochs is: 1787\n",
      "The number of epochs is: 1788\n",
      "The number of epochs is: 1789\n",
      "The number of epochs is: 1790\n",
      "The number of epochs is: 1791\n",
      "The number of epochs is: 1792\n",
      "The number of epochs is: 1793\n",
      "The number of epochs is: 1794\n",
      "The number of epochs is: 1795\n",
      "The number of epochs is: 1796\n",
      "The number of epochs is: 1797\n",
      "The number of epochs is: 1798\n",
      "The number of epochs is: 1799\n",
      "The number of epochs is: 1800\n",
      "14\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2101 loss is tensor([0.1206], grad_fn=<AddBackward0>)\n",
      "epoch: 2102 loss is tensor([0.0776], grad_fn=<AddBackward0>)\n",
      "epoch: 2103 loss is tensor([0.0806], grad_fn=<AddBackward0>)\n",
      "epoch: 2104 loss is tensor([0.0731], grad_fn=<AddBackward0>)\n",
      "epoch: 2105 loss is tensor([0.1199], grad_fn=<AddBackward0>)\n",
      "epoch: 2106 loss is tensor([0.0441], grad_fn=<AddBackward0>)\n",
      "epoch: 2107 loss is tensor([0.1023], grad_fn=<AddBackward0>)\n",
      "epoch: 2108 loss is tensor([0.1100], grad_fn=<AddBackward0>)\n",
      "epoch: 2109 loss is tensor([0.1325], grad_fn=<AddBackward0>)\n",
      "epoch: 2110 loss is tensor([0.0819], grad_fn=<AddBackward0>)\n",
      "epoch: 2111 loss is tensor([0.1017], grad_fn=<AddBackward0>)\n",
      "epoch: 2112 loss is tensor([0.0450], grad_fn=<AddBackward0>)\n",
      "epoch: 2113 loss is tensor([0.1547], grad_fn=<AddBackward0>)\n",
      "epoch: 2114 loss is tensor([0.0804], grad_fn=<AddBackward0>)\n",
      "epoch: 2115 loss is tensor([0.1176], grad_fn=<AddBackward0>)\n",
      "epoch: 2116 loss is tensor([0.1635], grad_fn=<AddBackward0>)\n",
      "epoch: 2117 loss is tensor([0.0494], grad_fn=<AddBackward0>)\n",
      "epoch: 2118 loss is tensor([0.0967], grad_fn=<AddBackward0>)\n",
      "epoch: 2119 loss is tensor([0.0650], grad_fn=<AddBackward0>)\n",
      "epoch: 2120 loss is tensor([0.1561], grad_fn=<AddBackward0>)\n",
      "epoch: 2121 loss is tensor([0.0575], grad_fn=<AddBackward0>)\n",
      "epoch: 2122 loss is tensor([0.1031], grad_fn=<AddBackward0>)\n",
      "epoch: 2123 loss is tensor([0.1123], grad_fn=<AddBackward0>)\n",
      "epoch: 2124 loss is tensor([0.0501], grad_fn=<AddBackward0>)\n",
      "epoch: 2125 loss is tensor([0.0710], grad_fn=<AddBackward0>)\n",
      "epoch: 2126 loss is tensor([0.0856], grad_fn=<AddBackward0>)\n",
      "epoch: 2127 loss is tensor([0.0570], grad_fn=<AddBackward0>)\n",
      "epoch: 2128 loss is tensor([0.0943], grad_fn=<AddBackward0>)\n",
      "epoch: 2129 loss is tensor([0.1017], grad_fn=<AddBackward0>)\n",
      "epoch: 2130 loss is tensor([0.0776], grad_fn=<AddBackward0>)\n",
      "epoch: 2131 loss is tensor([0.0899], grad_fn=<AddBackward0>)\n",
      "epoch: 2132 loss is tensor([0.0205], grad_fn=<AddBackward0>)\n",
      "epoch: 2133 loss is tensor([0.0933], grad_fn=<AddBackward0>)\n",
      "epoch: 2134 loss is tensor([0.0654], grad_fn=<AddBackward0>)\n",
      "epoch: 2135 loss is tensor([0.0984], grad_fn=<AddBackward0>)\n",
      "epoch: 2136 loss is tensor([0.0729], grad_fn=<AddBackward0>)\n",
      "epoch: 2137 loss is tensor([0.1088], grad_fn=<AddBackward0>)\n",
      "epoch: 2138 loss is tensor([0.1013], grad_fn=<AddBackward0>)\n",
      "epoch: 2139 loss is tensor([0.0870], grad_fn=<AddBackward0>)\n",
      "epoch: 2140 loss is tensor([0.0339], grad_fn=<AddBackward0>)\n",
      "epoch: 2141 loss is tensor([0.0109], grad_fn=<AddBackward0>)\n",
      "epoch: 2142 loss is tensor([0.1272], grad_fn=<AddBackward0>)\n",
      "epoch: 2143 loss is tensor([0.1056], grad_fn=<AddBackward0>)\n",
      "epoch: 2144 loss is tensor([0.0678], grad_fn=<AddBackward0>)\n",
      "epoch: 2145 loss is tensor([0.0829], grad_fn=<AddBackward0>)\n",
      "epoch: 2146 loss is tensor([0.1001], grad_fn=<AddBackward0>)\n",
      "epoch: 2147 loss is tensor([0.0907], grad_fn=<AddBackward0>)\n",
      "epoch: 2148 loss is tensor([0.1556], grad_fn=<AddBackward0>)\n",
      "epoch: 2149 loss is tensor([0.0890], grad_fn=<AddBackward0>)\n",
      "epoch: 2150 loss is tensor([0.1373], grad_fn=<AddBackward0>)\n",
      "epoch: 2151 loss is tensor([0.1066], grad_fn=<AddBackward0>)\n",
      "epoch: 2152 loss is tensor([0.1348], grad_fn=<AddBackward0>)\n",
      "epoch: 2153 loss is tensor([0.1207], grad_fn=<AddBackward0>)\n",
      "epoch: 2154 loss is tensor([0.0307], grad_fn=<AddBackward0>)\n",
      "epoch: 2155 loss is tensor([0.1595], grad_fn=<AddBackward0>)\n",
      "epoch: 2156 loss is tensor([0.0826], grad_fn=<AddBackward0>)\n",
      "epoch: 2157 loss is tensor([0.0506], grad_fn=<AddBackward0>)\n",
      "epoch: 2158 loss is tensor([0.0746], grad_fn=<AddBackward0>)\n",
      "epoch: 2159 loss is tensor([0.0590], grad_fn=<AddBackward0>)\n",
      "epoch: 2160 loss is tensor([0.0794], grad_fn=<AddBackward0>)\n",
      "epoch: 2161 loss is tensor([0.0956], grad_fn=<AddBackward0>)\n",
      "epoch: 2162 loss is tensor([0.0625], grad_fn=<AddBackward0>)\n",
      "epoch: 2163 loss is tensor([0.1090], grad_fn=<AddBackward0>)\n",
      "epoch: 2164 loss is tensor([0.0734], grad_fn=<AddBackward0>)\n",
      "epoch: 2165 loss is tensor([0.1057], grad_fn=<AddBackward0>)\n",
      "epoch: 2166 loss is tensor([0.0516], grad_fn=<AddBackward0>)\n",
      "epoch: 2167 loss is tensor([0.0397], grad_fn=<AddBackward0>)\n",
      "epoch: 2168 loss is tensor([0.0805], grad_fn=<AddBackward0>)\n",
      "epoch: 2169 loss is tensor([0.0817], grad_fn=<AddBackward0>)\n",
      "epoch: 2170 loss is tensor([0.1065], grad_fn=<AddBackward0>)\n",
      "epoch: 2171 loss is tensor([0.0377], grad_fn=<AddBackward0>)\n",
      "epoch: 2172 loss is tensor([0.0612], grad_fn=<AddBackward0>)\n",
      "epoch: 2173 loss is tensor([0.0794], grad_fn=<AddBackward0>)\n",
      "epoch: 2174 loss is tensor([0.0654], grad_fn=<AddBackward0>)\n",
      "epoch: 2175 loss is tensor([0.1037], grad_fn=<AddBackward0>)\n",
      "epoch: 2176 loss is tensor([0.0209], grad_fn=<AddBackward0>)\n",
      "epoch: 2177 loss is tensor([0.0909], grad_fn=<AddBackward0>)\n",
      "epoch: 2178 loss is tensor([0.0037], grad_fn=<AddBackward0>)\n",
      "epoch: 2179 loss is tensor([0.0653], grad_fn=<AddBackward0>)\n",
      "epoch: 2180 loss is tensor([0.0888], grad_fn=<AddBackward0>)\n",
      "epoch: 2181 loss is tensor([0.0547], grad_fn=<AddBackward0>)\n",
      "epoch: 2182 loss is tensor([0.0447], grad_fn=<AddBackward0>)\n",
      "epoch: 2183 loss is tensor([0.0696], grad_fn=<AddBackward0>)\n",
      "epoch: 2184 loss is tensor([0.0937], grad_fn=<AddBackward0>)\n",
      "epoch: 2185 loss is tensor([0.0303], grad_fn=<AddBackward0>)\n",
      "epoch: 2186 loss is tensor([0.1000], grad_fn=<AddBackward0>)\n",
      "epoch: 2187 loss is tensor([0.0723], grad_fn=<AddBackward0>)\n",
      "epoch: 2188 loss is tensor([0.0987], grad_fn=<AddBackward0>)\n",
      "epoch: 2189 loss is tensor([0.0678], grad_fn=<AddBackward0>)\n",
      "epoch: 2190 loss is tensor([0.0541], grad_fn=<AddBackward0>)\n",
      "epoch: 2191 loss is tensor([0.0620], grad_fn=<AddBackward0>)\n",
      "epoch: 2192 loss is tensor([0.1101], grad_fn=<AddBackward0>)\n",
      "epoch: 2193 loss is tensor([0.0957], grad_fn=<AddBackward0>)\n",
      "epoch: 2194 loss is tensor([0.0484], grad_fn=<AddBackward0>)\n",
      "epoch: 2195 loss is tensor([0.1091], grad_fn=<AddBackward0>)\n",
      "epoch: 2196 loss is tensor([0.0947], grad_fn=<AddBackward0>)\n",
      "epoch: 2197 loss is tensor([0.1094], grad_fn=<AddBackward0>)\n",
      "epoch: 2198 loss is tensor([0.1135], grad_fn=<AddBackward0>)\n",
      "epoch: 2199 loss is tensor([0.0448], grad_fn=<AddBackward0>)\n",
      "epoch: 2200 loss is tensor([0.0676], grad_fn=<AddBackward0>)\n",
      "10\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 2201 loss is tensor([0.0820], grad_fn=<AddBackward0>)\n",
      "epoch: 2202 loss is tensor([0.0462], grad_fn=<AddBackward0>)\n",
      "epoch: 2203 loss is tensor([0.1306], grad_fn=<AddBackward0>)\n",
      "epoch: 2204 loss is tensor([0.0836], grad_fn=<AddBackward0>)\n",
      "epoch: 2205 loss is tensor([0.0435], grad_fn=<AddBackward0>)\n",
      "epoch: 2206 loss is tensor([0.0899], grad_fn=<AddBackward0>)\n",
      "epoch: 2207 loss is tensor([0.1394], grad_fn=<AddBackward0>)\n",
      "epoch: 2208 loss is tensor([0.0757], grad_fn=<AddBackward0>)\n",
      "epoch: 2209 loss is tensor([0.1459], grad_fn=<AddBackward0>)\n",
      "epoch: 2210 loss is tensor([0.0623], grad_fn=<AddBackward0>)\n",
      "epoch: 2211 loss is tensor([0.0200], grad_fn=<AddBackward0>)\n",
      "epoch: 2212 loss is tensor([0.0181], grad_fn=<AddBackward0>)\n",
      "epoch: 2213 loss is tensor([0.0905], grad_fn=<AddBackward0>)\n",
      "epoch: 2214 loss is tensor([0.0737], grad_fn=<AddBackward0>)\n",
      "epoch: 2215 loss is tensor([0.0974], grad_fn=<AddBackward0>)\n",
      "epoch: 2216 loss is tensor([0.1224], grad_fn=<AddBackward0>)\n",
      "epoch: 2217 loss is tensor([0.0259], grad_fn=<AddBackward0>)\n",
      "epoch: 2218 loss is tensor([0.0399], grad_fn=<AddBackward0>)\n",
      "epoch: 2219 loss is tensor([0.1319], grad_fn=<AddBackward0>)\n",
      "epoch: 2220 loss is tensor([0.1349], grad_fn=<AddBackward0>)\n",
      "epoch: 2221 loss is tensor([0.1605], grad_fn=<AddBackward0>)\n",
      "epoch: 2222 loss is tensor([0.1072], grad_fn=<AddBackward0>)\n",
      "epoch: 2223 loss is tensor([0.1282], grad_fn=<AddBackward0>)\n",
      "epoch: 2224 loss is tensor([0.0774], grad_fn=<AddBackward0>)\n",
      "epoch: 2225 loss is tensor([0.1029], grad_fn=<AddBackward0>)\n",
      "epoch: 2226 loss is tensor([0.0690], grad_fn=<AddBackward0>)\n",
      "epoch: 2227 loss is tensor([0.0965], grad_fn=<AddBackward0>)\n",
      "epoch: 2228 loss is tensor([0.0911], grad_fn=<AddBackward0>)\n",
      "epoch: 2229 loss is tensor([0.1559], grad_fn=<AddBackward0>)\n",
      "epoch: 2230 loss is tensor([0.0601], grad_fn=<AddBackward0>)\n",
      "epoch: 2231 loss is tensor([0.1691], grad_fn=<AddBackward0>)\n",
      "epoch: 2232 loss is tensor([0.0651], grad_fn=<AddBackward0>)\n",
      "epoch: 2233 loss is tensor([0.1351], grad_fn=<AddBackward0>)\n",
      "epoch: 2234 loss is tensor([0.1137], grad_fn=<AddBackward0>)\n",
      "epoch: 2235 loss is tensor([0.1040], grad_fn=<AddBackward0>)\n",
      "epoch: 2236 loss is tensor([0.0932], grad_fn=<AddBackward0>)\n",
      "epoch: 2237 loss is tensor([0.1675], grad_fn=<AddBackward0>)\n",
      "epoch: 2238 loss is tensor([0.0793], grad_fn=<AddBackward0>)\n",
      "epoch: 2239 loss is tensor([0.0975], grad_fn=<AddBackward0>)\n",
      "epoch: 2240 loss is tensor([0.0684], grad_fn=<AddBackward0>)\n",
      "epoch: 2241 loss is tensor([0.0610], grad_fn=<AddBackward0>)\n",
      "epoch: 2242 loss is tensor([0.0709], grad_fn=<AddBackward0>)\n",
      "epoch: 2243 loss is tensor([0.1130], grad_fn=<AddBackward0>)\n",
      "epoch: 2244 loss is tensor([0.1239], grad_fn=<AddBackward0>)\n",
      "epoch: 2245 loss is tensor([0.1314], grad_fn=<AddBackward0>)\n",
      "epoch: 2246 loss is tensor([0.0334], grad_fn=<AddBackward0>)\n",
      "epoch: 2247 loss is tensor([0.1212], grad_fn=<AddBackward0>)\n",
      "epoch: 2248 loss is tensor([0.0838], grad_fn=<AddBackward0>)\n",
      "epoch: 2249 loss is tensor([0.0933], grad_fn=<AddBackward0>)\n",
      "epoch: 2250 loss is tensor([0.0407], grad_fn=<AddBackward0>)\n",
      "epoch: 2251 loss is tensor([0.0897], grad_fn=<AddBackward0>)\n",
      "epoch: 2252 loss is tensor([0.0474], grad_fn=<AddBackward0>)\n",
      "epoch: 2253 loss is tensor([0.0494], grad_fn=<AddBackward0>)\n",
      "epoch: 2254 loss is tensor([0.0452], grad_fn=<AddBackward0>)\n",
      "epoch: 2255 loss is tensor([-0.0127], grad_fn=<AddBackward0>)\n",
      "epoch: 2256 loss is tensor([0.0835], grad_fn=<AddBackward0>)\n",
      "epoch: 2257 loss is tensor([0.0861], grad_fn=<AddBackward0>)\n",
      "epoch: 2258 loss is tensor([0.0657], grad_fn=<AddBackward0>)\n",
      "epoch: 2259 loss is tensor([0.0303], grad_fn=<AddBackward0>)\n",
      "epoch: 2260 loss is tensor([0.0903], grad_fn=<AddBackward0>)\n",
      "epoch: 2261 loss is tensor([0.0425], grad_fn=<AddBackward0>)\n",
      "epoch: 2262 loss is tensor([0.0732], grad_fn=<AddBackward0>)\n",
      "epoch: 2263 loss is tensor([0.0102], grad_fn=<AddBackward0>)\n",
      "epoch: 2264 loss is tensor([0.0592], grad_fn=<AddBackward0>)\n",
      "epoch: 2265 loss is tensor([0.0186], grad_fn=<AddBackward0>)\n",
      "epoch: 2266 loss is tensor([0.0912], grad_fn=<AddBackward0>)\n",
      "epoch: 2267 loss is tensor([0.0709], grad_fn=<AddBackward0>)\n",
      "epoch: 2268 loss is tensor([0.1632], grad_fn=<AddBackward0>)\n",
      "epoch: 2269 loss is tensor([0.0941], grad_fn=<AddBackward0>)\n",
      "epoch: 2270 loss is tensor([0.1428], grad_fn=<AddBackward0>)\n",
      "epoch: 2271 loss is tensor([0.0398], grad_fn=<AddBackward0>)\n",
      "epoch: 2272 loss is tensor([0.0676], grad_fn=<AddBackward0>)\n",
      "epoch: 2273 loss is tensor([0.1129], grad_fn=<AddBackward0>)\n",
      "epoch: 2274 loss is tensor([0.0287], grad_fn=<AddBackward0>)\n",
      "epoch: 2275 loss is tensor([0.0874], grad_fn=<AddBackward0>)\n",
      "epoch: 2276 loss is tensor([0.0655], grad_fn=<AddBackward0>)\n",
      "epoch: 2277 loss is tensor([0.0722], grad_fn=<AddBackward0>)\n",
      "epoch: 2278 loss is tensor([0.0679], grad_fn=<AddBackward0>)\n",
      "epoch: 2279 loss is tensor([0.0575], grad_fn=<AddBackward0>)\n",
      "epoch: 2280 loss is tensor([0.0762], grad_fn=<AddBackward0>)\n",
      "epoch: 2281 loss is tensor([0.1676], grad_fn=<AddBackward0>)\n",
      "epoch: 2282 loss is tensor([0.0842], grad_fn=<AddBackward0>)\n",
      "epoch: 2283 loss is tensor([0.0500], grad_fn=<AddBackward0>)\n",
      "epoch: 2284 loss is tensor([0.0371], grad_fn=<AddBackward0>)\n",
      "epoch: 2285 loss is tensor([0.1122], grad_fn=<AddBackward0>)\n",
      "epoch: 2286 loss is tensor([0.1104], grad_fn=<AddBackward0>)\n",
      "epoch: 2287 loss is tensor([0.1503], grad_fn=<AddBackward0>)\n",
      "epoch: 2288 loss is tensor([0.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 2289 loss is tensor([0.0937], grad_fn=<AddBackward0>)\n",
      "epoch: 2290 loss is tensor([0.0911], grad_fn=<AddBackward0>)\n",
      "epoch: 2291 loss is tensor([0.0240], grad_fn=<AddBackward0>)\n",
      "epoch: 2292 loss is tensor([0.0819], grad_fn=<AddBackward0>)\n",
      "epoch: 2293 loss is tensor([0.0459], grad_fn=<AddBackward0>)\n",
      "epoch: 2294 loss is tensor([0.0433], grad_fn=<AddBackward0>)\n",
      "epoch: 2295 loss is tensor([0.0170], grad_fn=<AddBackward0>)\n",
      "epoch: 2296 loss is tensor([0.0889], grad_fn=<AddBackward0>)\n",
      "epoch: 2297 loss is tensor([0.0129], grad_fn=<AddBackward0>)\n",
      "epoch: 2298 loss is tensor([0.0738], grad_fn=<AddBackward0>)\n",
      "epoch: 2299 loss is tensor([-0.0006], grad_fn=<AddBackward0>)\n",
      "epoch: 2300 loss is tensor([0.0346], grad_fn=<AddBackward0>)\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2301 loss is tensor([0.0439], grad_fn=<AddBackward0>)\n",
      "epoch: 2302 loss is tensor([0.0595], grad_fn=<AddBackward0>)\n",
      "epoch: 2303 loss is tensor([0.0363], grad_fn=<AddBackward0>)\n",
      "epoch: 2304 loss is tensor([-0.0003], grad_fn=<AddBackward0>)\n",
      "epoch: 2305 loss is tensor([0.0292], grad_fn=<AddBackward0>)\n",
      "epoch: 2306 loss is tensor([0.0240], grad_fn=<AddBackward0>)\n",
      "epoch: 2307 loss is tensor([0.1217], grad_fn=<AddBackward0>)\n",
      "epoch: 2308 loss is tensor([0.0484], grad_fn=<AddBackward0>)\n",
      "epoch: 2309 loss is tensor([-0.0040], grad_fn=<AddBackward0>)\n",
      "epoch: 2310 loss is tensor([0.1094], grad_fn=<AddBackward0>)\n",
      "epoch: 2311 loss is tensor([0.0816], grad_fn=<AddBackward0>)\n",
      "epoch: 2312 loss is tensor([0.0114], grad_fn=<AddBackward0>)\n",
      "epoch: 2313 loss is tensor([0.0716], grad_fn=<AddBackward0>)\n",
      "epoch: 2314 loss is tensor([0.0091], grad_fn=<AddBackward0>)\n",
      "epoch: 2315 loss is tensor([0.0224], grad_fn=<AddBackward0>)\n",
      "epoch: 2316 loss is tensor([0.0729], grad_fn=<AddBackward0>)\n",
      "epoch: 2317 loss is tensor([0.0380], grad_fn=<AddBackward0>)\n",
      "epoch: 2318 loss is tensor([0.0975], grad_fn=<AddBackward0>)\n",
      "epoch: 2319 loss is tensor([0.1015], grad_fn=<AddBackward0>)\n",
      "epoch: 2320 loss is tensor([0.1055], grad_fn=<AddBackward0>)\n",
      "epoch: 2321 loss is tensor([0.0958], grad_fn=<AddBackward0>)\n",
      "epoch: 2322 loss is tensor([0.0833], grad_fn=<AddBackward0>)\n",
      "epoch: 2323 loss is tensor([0.0556], grad_fn=<AddBackward0>)\n",
      "epoch: 2324 loss is tensor([0.0098], grad_fn=<AddBackward0>)\n",
      "epoch: 2325 loss is tensor([0.0985], grad_fn=<AddBackward0>)\n",
      "epoch: 2326 loss is tensor([0.0694], grad_fn=<AddBackward0>)\n",
      "epoch: 2327 loss is tensor([0.1095], grad_fn=<AddBackward0>)\n",
      "epoch: 2328 loss is tensor([0.0891], grad_fn=<AddBackward0>)\n",
      "epoch: 2329 loss is tensor([0.0940], grad_fn=<AddBackward0>)\n",
      "epoch: 2330 loss is tensor([0.0270], grad_fn=<AddBackward0>)\n",
      "epoch: 2331 loss is tensor([0.1294], grad_fn=<AddBackward0>)\n",
      "epoch: 2332 loss is tensor([0.0853], grad_fn=<AddBackward0>)\n",
      "epoch: 2333 loss is tensor([0.0741], grad_fn=<AddBackward0>)\n",
      "epoch: 2334 loss is tensor([0.0432], grad_fn=<AddBackward0>)\n",
      "epoch: 2335 loss is tensor([0.0739], grad_fn=<AddBackward0>)\n",
      "epoch: 2336 loss is tensor([0.0793], grad_fn=<AddBackward0>)\n",
      "epoch: 2337 loss is tensor([0.1028], grad_fn=<AddBackward0>)\n",
      "epoch: 2338 loss is tensor([0.0480], grad_fn=<AddBackward0>)\n",
      "epoch: 2339 loss is tensor([0.1580], grad_fn=<AddBackward0>)\n",
      "epoch: 2340 loss is tensor([0.0662], grad_fn=<AddBackward0>)\n",
      "epoch: 2341 loss is tensor([0.0507], grad_fn=<AddBackward0>)\n",
      "epoch: 2342 loss is tensor([0.1459], grad_fn=<AddBackward0>)\n",
      "epoch: 2343 loss is tensor([0.1687], grad_fn=<AddBackward0>)\n",
      "epoch: 2344 loss is tensor([0.0886], grad_fn=<AddBackward0>)\n",
      "epoch: 2345 loss is tensor([0.1175], grad_fn=<AddBackward0>)\n",
      "epoch: 2346 loss is tensor([0.1236], grad_fn=<AddBackward0>)\n",
      "epoch: 2347 loss is tensor([0.0549], grad_fn=<AddBackward0>)\n",
      "epoch: 2348 loss is tensor([0.0503], grad_fn=<AddBackward0>)\n",
      "epoch: 2349 loss is tensor([0.0791], grad_fn=<AddBackward0>)\n",
      "epoch: 2350 loss is tensor([0.0581], grad_fn=<AddBackward0>)\n",
      "epoch: 2351 loss is tensor([0.0657], grad_fn=<AddBackward0>)\n",
      "epoch: 2352 loss is tensor([0.0024], grad_fn=<AddBackward0>)\n",
      "epoch: 2353 loss is tensor([0.0414], grad_fn=<AddBackward0>)\n",
      "epoch: 2354 loss is tensor([0.1006], grad_fn=<AddBackward0>)\n",
      "epoch: 2355 loss is tensor([0.0424], grad_fn=<AddBackward0>)\n",
      "epoch: 2356 loss is tensor([0.0570], grad_fn=<AddBackward0>)\n",
      "epoch: 2357 loss is tensor([0.0290], grad_fn=<AddBackward0>)\n",
      "epoch: 2358 loss is tensor([0.0785], grad_fn=<AddBackward0>)\n",
      "epoch: 2359 loss is tensor([0.0620], grad_fn=<AddBackward0>)\n",
      "epoch: 2360 loss is tensor([0.1106], grad_fn=<AddBackward0>)\n",
      "epoch: 2361 loss is tensor([0.0884], grad_fn=<AddBackward0>)\n",
      "epoch: 2362 loss is tensor([0.0671], grad_fn=<AddBackward0>)\n",
      "epoch: 2363 loss is tensor([-0.0159], grad_fn=<AddBackward0>)\n",
      "epoch: 2364 loss is tensor([0.0788], grad_fn=<AddBackward0>)\n",
      "epoch: 2365 loss is tensor([-0.0679], grad_fn=<AddBackward0>)\n",
      "epoch: 2366 loss is tensor([0.0036], grad_fn=<AddBackward0>)\n",
      "epoch: 2367 loss is tensor([0.0791], grad_fn=<AddBackward0>)\n",
      "epoch: 2368 loss is tensor([0.0537], grad_fn=<AddBackward0>)\n",
      "epoch: 2369 loss is tensor([0.0779], grad_fn=<AddBackward0>)\n",
      "epoch: 2370 loss is tensor([0.0525], grad_fn=<AddBackward0>)\n",
      "epoch: 2371 loss is tensor([0.0523], grad_fn=<AddBackward0>)\n",
      "epoch: 2372 loss is tensor([0.0517], grad_fn=<AddBackward0>)\n",
      "epoch: 2373 loss is tensor([0.0408], grad_fn=<AddBackward0>)\n",
      "epoch: 2374 loss is tensor([0.0650], grad_fn=<AddBackward0>)\n",
      "epoch: 2375 loss is tensor([0.0506], grad_fn=<AddBackward0>)\n",
      "epoch: 2376 loss is tensor([0.0605], grad_fn=<AddBackward0>)\n",
      "epoch: 2377 loss is tensor([0.1162], grad_fn=<AddBackward0>)\n",
      "epoch: 2378 loss is tensor([0.0216], grad_fn=<AddBackward0>)\n",
      "epoch: 2379 loss is tensor([0.0596], grad_fn=<AddBackward0>)\n",
      "epoch: 2380 loss is tensor([0.0668], grad_fn=<AddBackward0>)\n",
      "epoch: 2381 loss is tensor([0.0485], grad_fn=<AddBackward0>)\n",
      "epoch: 2382 loss is tensor([0.0292], grad_fn=<AddBackward0>)\n",
      "epoch: 2383 loss is tensor([0.0966], grad_fn=<AddBackward0>)\n",
      "epoch: 2384 loss is tensor([0.0054], grad_fn=<AddBackward0>)\n",
      "epoch: 2385 loss is tensor([0.0353], grad_fn=<AddBackward0>)\n",
      "epoch: 2386 loss is tensor([0.0575], grad_fn=<AddBackward0>)\n",
      "epoch: 2387 loss is tensor([0.0333], grad_fn=<AddBackward0>)\n",
      "epoch: 2388 loss is tensor([0.0583], grad_fn=<AddBackward0>)\n",
      "epoch: 2389 loss is tensor([0.0811], grad_fn=<AddBackward0>)\n",
      "epoch: 2390 loss is tensor([0.1168], grad_fn=<AddBackward0>)\n",
      "epoch: 2391 loss is tensor([0.0055], grad_fn=<AddBackward0>)\n",
      "epoch: 2392 loss is tensor([0.0272], grad_fn=<AddBackward0>)\n",
      "epoch: 2393 loss is tensor([0.0430], grad_fn=<AddBackward0>)\n",
      "epoch: 2394 loss is tensor([0.0124], grad_fn=<AddBackward0>)\n",
      "epoch: 2395 loss is tensor([0.0382], grad_fn=<AddBackward0>)\n",
      "epoch: 2396 loss is tensor([0.0868], grad_fn=<AddBackward0>)\n",
      "epoch: 2397 loss is tensor([0.0851], grad_fn=<AddBackward0>)\n",
      "epoch: 2398 loss is tensor([0.0180], grad_fn=<AddBackward0>)\n",
      "epoch: 2399 loss is tensor([0.0631], grad_fn=<AddBackward0>)\n",
      "epoch: 2400 loss is tensor([0.0413], grad_fn=<AddBackward0>)\n",
      "38\n"
     ]
=======
      "The number of epochs is: 1801\n",
      "The number of epochs is: 1802\n",
      "The number of epochs is: 1803\n",
      "The number of epochs is: 1804\n",
      "The number of epochs is: 1805\n",
      "The number of epochs is: 1806\n",
      "The number of epochs is: 1807\n",
      "The number of epochs is: 1808\n",
      "The number of epochs is: 1809\n",
      "The number of epochs is: 1810\n",
      "The number of epochs is: 1811\n",
      "The number of epochs is: 1812\n",
      "The number of epochs is: 1813\n",
      "The number of epochs is: 1814\n",
      "The number of epochs is: 1815\n",
      "The number of epochs is: 1816\n",
      "The number of epochs is: 1817\n",
      "The number of epochs is: 1818\n",
      "The number of epochs is: 1819\n",
      "The number of epochs is: 1820\n",
      "The number of epochs is: 1821\n",
      "The number of epochs is: 1822\n",
      "The number of epochs is: 1823\n",
      "The number of epochs is: 1824\n",
      "The number of epochs is: 1825\n",
      "The number of epochs is: 1826\n",
      "The number of epochs is: 1827\n",
      "The number of epochs is: 1828\n",
      "The number of epochs is: 1829\n",
      "The number of epochs is: 1830\n",
      "The number of epochs is: 1831\n",
      "The number of epochs is: 1832\n",
      "The number of epochs is: 1833\n",
      "The number of epochs is: 1834\n",
      "The number of epochs is: 1835\n",
      "The number of epochs is: 1836\n",
      "The number of epochs is: 1837\n",
      "The number of epochs is: 1838\n",
      "The number of epochs is: 1839\n",
      "The number of epochs is: 1840\n",
      "The number of epochs is: 1841\n",
      "The number of epochs is: 1842\n",
      "The number of epochs is: 1843\n",
      "The number of epochs is: 1844\n",
      "The number of epochs is: 1845\n",
      "The number of epochs is: 1846\n",
      "The number of epochs is: 1847\n",
      "The number of epochs is: 1848\n",
      "The number of epochs is: 1849\n",
      "The number of epochs is: 1850\n",
      "The number of epochs is: 1851\n",
      "The number of epochs is: 1852\n",
      "The number of epochs is: 1853\n",
      "The number of epochs is: 1854\n",
      "The number of epochs is: 1855\n",
      "The number of epochs is: 1856\n",
      "The number of epochs is: 1857\n",
      "The number of epochs is: 1858\n",
      "The number of epochs is: 1859\n",
      "The number of epochs is: 1860\n",
      "The number of epochs is: 1861\n",
      "The number of epochs is: 1862\n",
      "The number of epochs is: 1863\n",
      "The number of epochs is: 1864\n",
      "The number of epochs is: 1865\n",
      "The number of epochs is: 1866\n",
      "The number of epochs is: 1867\n",
      "The number of epochs is: 1868\n",
      "The number of epochs is: 1869\n",
      "The number of epochs is: 1870\n",
      "The number of epochs is: 1871\n",
      "The number of epochs is: 1872\n",
      "The number of epochs is: 1873\n",
      "The number of epochs is: 1874\n",
      "The number of epochs is: 1875\n",
      "The number of epochs is: 1876\n",
      "The number of epochs is: 1877\n",
      "The number of epochs is: 1878\n",
      "The number of epochs is: 1879\n",
      "The number of epochs is: 1880\n",
      "The number of epochs is: 1881\n",
      "The number of epochs is: 1882\n",
      "The number of epochs is: 1883\n",
      "The number of epochs is: 1884\n",
      "The number of epochs is: 1885\n",
      "The number of epochs is: 1886\n",
      "The number of epochs is: 1887\n",
      "The number of epochs is: 1888\n",
      "The number of epochs is: 1889\n",
      "The number of epochs is: 1890\n",
      "The number of epochs is: 1891\n",
      "The number of epochs is: 1892\n",
      "The number of epochs is: 1893\n",
      "The number of epochs is: 1894\n",
      "The number of epochs is: 1895\n",
      "The number of epochs is: 1896\n",
      "The number of epochs is: 1897\n",
      "The number of epochs is: 1898\n",
      "The number of epochs is: 1899\n",
      "The number of epochs is: 1900\n",
      "The number of epochs is: 1901\n",
      "The number of epochs is: 1902\n",
      "The number of epochs is: 1903\n",
      "The number of epochs is: 1904\n",
      "The number of epochs is: 1905\n",
      "The number of epochs is: 1906\n",
      "The number of epochs is: 1907\n",
      "The number of epochs is: 1908\n",
      "The number of epochs is: 1909\n",
      "The number of epochs is: 1910\n",
      "The number of epochs is: 1911\n",
      "The number of epochs is: 1912\n",
      "The number of epochs is: 1913\n",
      "The number of epochs is: 1914\n",
      "The number of epochs is: 1915\n",
      "The number of epochs is: 1916\n",
      "The number of epochs is: 1917\n",
      "The number of epochs is: 1918\n",
      "The number of epochs is: 1919\n",
      "The number of epochs is: 1920\n",
      "The number of epochs is: 1921\n",
      "The number of epochs is: 1922\n",
      "The number of epochs is: 1923\n",
      "The number of epochs is: 1924\n",
      "The number of epochs is: 1925\n",
      "The number of epochs is: 1926\n",
      "The number of epochs is: 1927\n",
      "The number of epochs is: 1928\n",
      "The number of epochs is: 1929\n",
      "The number of epochs is: 1930\n",
      "The number of epochs is: 1931\n",
      "The number of epochs is: 1932\n",
      "The number of epochs is: 1933\n",
      "The number of epochs is: 1934\n",
      "The number of epochs is: 1935\n",
      "The number of epochs is: 1936\n",
      "The number of epochs is: 1937\n",
      "The number of epochs is: 1938\n",
      "The number of epochs is: 1939\n",
      "The number of epochs is: 1940\n",
      "The number of epochs is: 1941\n",
      "The number of epochs is: 1942\n",
      "The number of epochs is: 1943\n",
      "The number of epochs is: 1944\n",
      "The number of epochs is: 1945\n",
      "The number of epochs is: 1946\n",
      "The number of epochs is: 1947\n",
      "The number of epochs is: 1948\n",
      "The number of epochs is: 1949\n",
      "The number of epochs is: 1950\n",
      "The number of epochs is: 1951\n",
      "The number of epochs is: 1952\n",
      "The number of epochs is: 1953\n",
      "The number of epochs is: 1954\n",
      "The number of epochs is: 1955\n",
      "The number of epochs is: 1956\n",
      "The number of epochs is: 1957\n",
      "The number of epochs is: 1958\n",
      "The number of epochs is: 1959\n",
      "The number of epochs is: 1960\n",
      "The number of epochs is: 1961\n",
      "The number of epochs is: 1962\n",
      "The number of epochs is: 1963\n",
      "The number of epochs is: 1964\n",
      "The number of epochs is: 1965\n",
      "The number of epochs is: 1966\n",
      "The number of epochs is: 1967\n",
      "The number of epochs is: 1968\n",
      "The number of epochs is: 1969\n",
      "The number of epochs is: 1970\n",
      "The number of epochs is: 1971\n",
      "The number of epochs is: 1972\n",
      "The number of epochs is: 1973\n",
      "The number of epochs is: 1974\n",
      "The number of epochs is: 1975\n",
      "The number of epochs is: 1976\n",
      "The number of epochs is: 1977\n",
      "The number of epochs is: 1978\n",
      "The number of epochs is: 1979\n",
      "The number of epochs is: 1980\n",
      "The number of epochs is: 1981\n",
      "The number of epochs is: 1982\n",
      "The number of epochs is: 1983\n",
      "The number of epochs is: 1984\n",
      "The number of epochs is: 1985\n",
      "The number of epochs is: 1986\n",
      "The number of epochs is: 1987\n",
      "The number of epochs is: 1988\n",
      "The number of epochs is: 1989\n",
      "The number of epochs is: 1990\n",
      "The number of epochs is: 1991\n",
      "The number of epochs is: 1992\n",
      "The number of epochs is: 1993\n",
      "The number of epochs is: 1994\n",
      "The number of epochs is: 1995\n",
      "The number of epochs is: 1996\n",
      "The number of epochs is: 1997\n",
      "The number of epochs is: 1998\n",
      "The number of epochs is: 1999\n",
      "The number of epochs is: 2000\n",
      "18\n",
      "Outputting sketch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 2401 loss is tensor([0.0696], grad_fn=<AddBackward0>)\n",
      "epoch: 2402 loss is tensor([0.0650], grad_fn=<AddBackward0>)\n",
      "epoch: 2403 loss is tensor([0.1248], grad_fn=<AddBackward0>)\n",
      "epoch: 2404 loss is tensor([0.1055], grad_fn=<AddBackward0>)\n",
      "epoch: 2405 loss is tensor([0.0659], grad_fn=<AddBackward0>)\n",
      "epoch: 2406 loss is tensor([0.0753], grad_fn=<AddBackward0>)\n",
      "epoch: 2407 loss is tensor([0.0631], grad_fn=<AddBackward0>)\n",
      "epoch: 2408 loss is tensor([0.0492], grad_fn=<AddBackward0>)\n",
      "epoch: 2409 loss is tensor([0.0270], grad_fn=<AddBackward0>)\n",
      "epoch: 2410 loss is tensor([0.0709], grad_fn=<AddBackward0>)\n",
      "epoch: 2411 loss is tensor([0.0598], grad_fn=<AddBackward0>)\n",
      "epoch: 2412 loss is tensor([0.0479], grad_fn=<AddBackward0>)\n",
      "epoch: 2413 loss is tensor([0.0641], grad_fn=<AddBackward0>)\n",
      "epoch: 2414 loss is tensor([-0.0023], grad_fn=<AddBackward0>)\n",
      "epoch: 2415 loss is tensor([0.0950], grad_fn=<AddBackward0>)\n",
      "epoch: 2416 loss is tensor([0.0444], grad_fn=<AddBackward0>)\n",
      "epoch: 2417 loss is tensor([0.0700], grad_fn=<AddBackward0>)\n",
      "epoch: 2418 loss is tensor([0.0810], grad_fn=<AddBackward0>)\n",
      "epoch: 2419 loss is tensor([0.0392], grad_fn=<AddBackward0>)\n",
      "epoch: 2420 loss is tensor([0.0973], grad_fn=<AddBackward0>)\n",
      "epoch: 2421 loss is tensor([0.0937], grad_fn=<AddBackward0>)\n",
      "epoch: 2422 loss is tensor([0.0507], grad_fn=<AddBackward0>)\n",
      "epoch: 2423 loss is tensor([0.1228], grad_fn=<AddBackward0>)\n",
      "epoch: 2424 loss is tensor([0.1317], grad_fn=<AddBackward0>)\n",
      "epoch: 2425 loss is tensor([0.1674], grad_fn=<AddBackward0>)\n",
      "epoch: 2426 loss is tensor([0.0329], grad_fn=<AddBackward0>)\n",
      "epoch: 2427 loss is tensor([0.0703], grad_fn=<AddBackward0>)\n",
      "epoch: 2428 loss is tensor([0.0546], grad_fn=<AddBackward0>)\n",
      "epoch: 2429 loss is tensor([0.0893], grad_fn=<AddBackward0>)\n",
      "epoch: 2430 loss is tensor([0.0400], grad_fn=<AddBackward0>)\n",
      "epoch: 2431 loss is tensor([0.0208], grad_fn=<AddBackward0>)\n",
      "epoch: 2432 loss is tensor([0.1437], grad_fn=<AddBackward0>)\n",
      "epoch: 2433 loss is tensor([0.0713], grad_fn=<AddBackward0>)\n",
      "epoch: 2434 loss is tensor([0.0626], grad_fn=<AddBackward0>)\n",
      "epoch: 2435 loss is tensor([0.0721], grad_fn=<AddBackward0>)\n",
      "epoch: 2436 loss is tensor([0.0835], grad_fn=<AddBackward0>)\n",
      "epoch: 2437 loss is tensor([0.0416], grad_fn=<AddBackward0>)\n",
      "epoch: 2438 loss is tensor([0.0661], grad_fn=<AddBackward0>)\n",
      "epoch: 2439 loss is tensor([0.0464], grad_fn=<AddBackward0>)\n",
      "epoch: 2440 loss is tensor([0.0034], grad_fn=<AddBackward0>)\n",
      "epoch: 2441 loss is tensor([0.0966], grad_fn=<AddBackward0>)\n",
      "epoch: 2442 loss is tensor([0.0664], grad_fn=<AddBackward0>)\n",
      "epoch: 2443 loss is tensor([0.0305], grad_fn=<AddBackward0>)\n",
      "epoch: 2444 loss is tensor([-0.0116], grad_fn=<AddBackward0>)\n",
      "epoch: 2445 loss is tensor([0.0431], grad_fn=<AddBackward0>)\n",
      "epoch: 2446 loss is tensor([0.0584], grad_fn=<AddBackward0>)\n",
      "epoch: 2447 loss is tensor([0.0458], grad_fn=<AddBackward0>)\n",
      "epoch: 2448 loss is tensor([0.0441], grad_fn=<AddBackward0>)\n",
      "epoch: 2449 loss is tensor([0.0517], grad_fn=<AddBackward0>)\n",
      "epoch: 2450 loss is tensor([0.0923], grad_fn=<AddBackward0>)\n",
      "epoch: 2451 loss is tensor([0.1094], grad_fn=<AddBackward0>)\n",
      "epoch: 2452 loss is tensor([0.0648], grad_fn=<AddBackward0>)\n",
      "epoch: 2453 loss is tensor([0.0666], grad_fn=<AddBackward0>)\n",
      "epoch: 2454 loss is tensor([0.0097], grad_fn=<AddBackward0>)\n",
      "epoch: 2455 loss is tensor([0.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 2456 loss is tensor([0.0935], grad_fn=<AddBackward0>)\n",
      "epoch: 2457 loss is tensor([0.0729], grad_fn=<AddBackward0>)\n",
      "epoch: 2458 loss is tensor([0.0724], grad_fn=<AddBackward0>)\n",
      "epoch: 2459 loss is tensor([0.0858], grad_fn=<AddBackward0>)\n",
      "epoch: 2460 loss is tensor([0.0693], grad_fn=<AddBackward0>)\n",
      "epoch: 2461 loss is tensor([0.0642], grad_fn=<AddBackward0>)\n",
      "epoch: 2462 loss is tensor([0.0800], grad_fn=<AddBackward0>)\n",
      "epoch: 2463 loss is tensor([0.0390], grad_fn=<AddBackward0>)\n",
      "epoch: 2464 loss is tensor([0.0598], grad_fn=<AddBackward0>)\n",
      "epoch: 2465 loss is tensor([0.0320], grad_fn=<AddBackward0>)\n",
      "epoch: 2466 loss is tensor([-0.0516], grad_fn=<AddBackward0>)\n",
      "epoch: 2467 loss is tensor([0.0922], grad_fn=<AddBackward0>)\n",
      "epoch: 2468 loss is tensor([0.0300], grad_fn=<AddBackward0>)\n",
      "epoch: 2469 loss is tensor([0.0454], grad_fn=<AddBackward0>)\n",
      "epoch: 2470 loss is tensor([0.0360], grad_fn=<AddBackward0>)\n",
      "epoch: 2471 loss is tensor([0.0332], grad_fn=<AddBackward0>)\n",
      "epoch: 2472 loss is tensor([0.0385], grad_fn=<AddBackward0>)\n",
      "epoch: 2473 loss is tensor([-0.0039], grad_fn=<AddBackward0>)\n",
      "epoch: 2474 loss is tensor([0.0396], grad_fn=<AddBackward0>)\n",
      "epoch: 2475 loss is tensor([0.0337], grad_fn=<AddBackward0>)\n",
      "epoch: 2476 loss is tensor([0.0299], grad_fn=<AddBackward0>)\n",
      "epoch: 2477 loss is tensor([0.0553], grad_fn=<AddBackward0>)\n",
      "epoch: 2478 loss is tensor([-0.0083], grad_fn=<AddBackward0>)\n",
      "epoch: 2479 loss is tensor([0.0532], grad_fn=<AddBackward0>)\n",
      "epoch: 2480 loss is tensor([0.0060], grad_fn=<AddBackward0>)\n",
      "epoch: 2481 loss is tensor([0.0712], grad_fn=<AddBackward0>)\n",
      "epoch: 2482 loss is tensor([0.0334], grad_fn=<AddBackward0>)\n",
      "epoch: 2483 loss is tensor([-0.0121], grad_fn=<AddBackward0>)\n",
      "epoch: 2484 loss is tensor([-0.0211], grad_fn=<AddBackward0>)\n",
      "epoch: 2485 loss is tensor([0.0037], grad_fn=<AddBackward0>)\n",
      "epoch: 2486 loss is tensor([-0.0005], grad_fn=<AddBackward0>)\n",
      "epoch: 2487 loss is tensor([0.0104], grad_fn=<AddBackward0>)\n",
      "epoch: 2488 loss is tensor([-0.0117], grad_fn=<AddBackward0>)\n",
      "epoch: 2489 loss is tensor([0.0198], grad_fn=<AddBackward0>)\n",
      "epoch: 2490 loss is tensor([0.0315], grad_fn=<AddBackward0>)\n",
      "epoch: 2491 loss is tensor([-0.0435], grad_fn=<AddBackward0>)\n",
      "epoch: 2492 loss is tensor([-0.0024], grad_fn=<AddBackward0>)\n",
      "epoch: 2493 loss is tensor([0.0177], grad_fn=<AddBackward0>)\n",
      "epoch: 2494 loss is tensor([-0.0160], grad_fn=<AddBackward0>)\n",
      "epoch: 2495 loss is tensor([-0.0153], grad_fn=<AddBackward0>)\n",
      "epoch: 2496 loss is tensor([0.0306], grad_fn=<AddBackward0>)\n",
      "epoch: 2497 loss is tensor([-0.0035], grad_fn=<AddBackward0>)\n",
      "epoch: 2498 loss is tensor([-0.0008], grad_fn=<AddBackward0>)\n",
      "epoch: 2499 loss is tensor([-0.0146], grad_fn=<AddBackward0>)\n",
      "epoch: 2500 loss is tensor([0.0377], grad_fn=<AddBackward0>)\n",
      "25\n"
=======
      "The number of epochs is: 2001\n",
      "The number of epochs is: 2002\n",
      "The number of epochs is: 2003\n",
      "The number of epochs is: 2004\n",
      "The number of epochs is: 2005\n",
      "The number of epochs is: 2006\n",
      "The number of epochs is: 2007\n",
      "The number of epochs is: 2008\n",
      "The number of epochs is: 2009\n",
      "The number of epochs is: 2010\n",
      "The number of epochs is: 2011\n",
      "The number of epochs is: 2012\n",
      "The number of epochs is: 2013\n",
      "The number of epochs is: 2014\n",
      "The number of epochs is: 2015\n",
      "The number of epochs is: 2016\n",
      "The number of epochs is: 2017\n",
      "The number of epochs is: 2018\n",
      "The number of epochs is: 2019\n",
      "The number of epochs is: 2020\n",
      "The number of epochs is: 2021\n",
      "The number of epochs is: 2022\n",
      "The number of epochs is: 2023\n",
      "The number of epochs is: 2024\n",
      "The number of epochs is: 2025\n",
      "The number of epochs is: 2026\n",
      "The number of epochs is: 2027\n",
      "The number of epochs is: 2028\n",
      "The number of epochs is: 2029\n",
      "The number of epochs is: 2030\n",
      "The number of epochs is: 2031\n",
      "The number of epochs is: 2032\n",
      "The number of epochs is: 2033\n",
      "The number of epochs is: 2034\n",
      "The number of epochs is: 2035\n",
      "The number of epochs is: 2036\n",
      "The number of epochs is: 2037\n",
      "The number of epochs is: 2038\n",
      "The number of epochs is: 2039\n",
      "The number of epochs is: 2040\n",
      "The number of epochs is: 2041\n",
      "The number of epochs is: 2042\n",
      "The number of epochs is: 2043\n",
      "The number of epochs is: 2044\n",
      "The number of epochs is: 2045\n",
      "The number of epochs is: 2046\n",
      "The number of epochs is: 2047\n",
      "The number of epochs is: 2048\n",
      "The number of epochs is: 2049\n",
      "The number of epochs is: 2050\n",
      "The number of epochs is: 2051\n",
      "The number of epochs is: 2052\n",
      "The number of epochs is: 2053\n",
      "The number of epochs is: 2054\n",
      "The number of epochs is: 2055\n",
      "The number of epochs is: 2056\n",
      "The number of epochs is: 2057\n",
      "The number of epochs is: 2058\n",
      "The number of epochs is: 2059\n",
      "The number of epochs is: 2060\n",
      "The number of epochs is: 2061\n",
      "The number of epochs is: 2062\n",
      "The number of epochs is: 2063\n",
      "The number of epochs is: 2064\n",
      "The number of epochs is: 2065\n",
      "The number of epochs is: 2066\n",
      "The number of epochs is: 2067\n",
      "The number of epochs is: 2068\n",
      "The number of epochs is: 2069\n",
      "The number of epochs is: 2070\n",
      "The number of epochs is: 2071\n",
      "The number of epochs is: 2072\n",
      "The number of epochs is: 2073\n",
      "The number of epochs is: 2074\n",
      "The number of epochs is: 2075\n",
      "The number of epochs is: 2076\n",
      "The number of epochs is: 2077\n",
      "The number of epochs is: 2078\n",
      "The number of epochs is: 2079\n",
      "The number of epochs is: 2080\n",
      "The number of epochs is: 2081\n",
      "The number of epochs is: 2082\n",
      "The number of epochs is: 2083\n",
      "The number of epochs is: 2084\n",
      "The number of epochs is: 2085\n",
      "The number of epochs is: 2086\n",
      "The number of epochs is: 2087\n",
      "The number of epochs is: 2088\n",
      "The number of epochs is: 2089\n",
      "The number of epochs is: 2090\n",
      "The number of epochs is: 2091\n",
      "The number of epochs is: 2092\n",
      "The number of epochs is: 2093\n",
      "The number of epochs is: 2094\n",
      "The number of epochs is: 2095\n",
      "The number of epochs is: 2096\n",
      "The number of epochs is: 2097\n",
      "The number of epochs is: 2098\n",
      "The number of epochs is: 2099\n",
      "The number of epochs is: 2100\n",
      "The number of epochs is: 2101\n",
      "The number of epochs is: 2102\n",
      "The number of epochs is: 2103\n",
      "The number of epochs is: 2104\n",
      "The number of epochs is: 2105\n",
      "The number of epochs is: 2106\n",
      "The number of epochs is: 2107\n",
      "The number of epochs is: 2108\n",
      "The number of epochs is: 2109\n",
      "The number of epochs is: 2110\n",
      "The number of epochs is: 2111\n",
      "The number of epochs is: 2112\n",
      "The number of epochs is: 2113\n",
      "The number of epochs is: 2114\n",
      "The number of epochs is: 2115\n",
      "The number of epochs is: 2116\n",
      "The number of epochs is: 2117\n",
      "The number of epochs is: 2118\n",
      "The number of epochs is: 2119\n",
      "The number of epochs is: 2120\n",
      "The number of epochs is: 2121\n",
      "The number of epochs is: 2122\n",
      "The number of epochs is: 2123\n",
      "The number of epochs is: 2124\n",
      "The number of epochs is: 2125\n",
      "The number of epochs is: 2126\n",
      "The number of epochs is: 2127\n",
      "The number of epochs is: 2128\n",
      "The number of epochs is: 2129\n",
      "The number of epochs is: 2130\n",
      "The number of epochs is: 2131\n",
      "The number of epochs is: 2132\n",
      "The number of epochs is: 2133\n",
      "The number of epochs is: 2134\n",
      "The number of epochs is: 2135\n",
      "The number of epochs is: 2136\n",
      "The number of epochs is: 2137\n",
      "The number of epochs is: 2138\n",
      "The number of epochs is: 2139\n",
      "The number of epochs is: 2140\n",
      "The number of epochs is: 2141\n",
      "The number of epochs is: 2142\n",
      "The number of epochs is: 2143\n",
      "The number of epochs is: 2144\n",
      "The number of epochs is: 2145\n",
      "The number of epochs is: 2146\n",
      "The number of epochs is: 2147\n",
      "The number of epochs is: 2148\n",
      "The number of epochs is: 2149\n",
      "The number of epochs is: 2150\n",
      "The number of epochs is: 2151\n",
      "The number of epochs is: 2152\n",
      "The number of epochs is: 2153\n",
      "The number of epochs is: 2154\n",
      "The number of epochs is: 2155\n",
      "The number of epochs is: 2156\n",
      "The number of epochs is: 2157\n",
      "The number of epochs is: 2158\n",
      "The number of epochs is: 2159\n",
      "The number of epochs is: 2160\n",
      "The number of epochs is: 2161\n",
      "The number of epochs is: 2162\n",
      "The number of epochs is: 2163\n",
      "The number of epochs is: 2164\n",
      "The number of epochs is: 2165\n",
      "The number of epochs is: 2166\n",
      "The number of epochs is: 2167\n",
      "The number of epochs is: 2168\n",
      "The number of epochs is: 2169\n",
      "The number of epochs is: 2170\n",
      "The number of epochs is: 2171\n",
      "The number of epochs is: 2172\n",
      "The number of epochs is: 2173\n",
      "The number of epochs is: 2174\n",
      "The number of epochs is: 2175\n",
      "The number of epochs is: 2176\n",
      "The number of epochs is: 2177\n",
      "The number of epochs is: 2178\n",
      "The number of epochs is: 2179\n",
      "The number of epochs is: 2180\n",
      "The number of epochs is: 2181\n",
      "The number of epochs is: 2182\n",
      "The number of epochs is: 2183\n",
      "The number of epochs is: 2184\n",
      "The number of epochs is: 2185\n",
      "The number of epochs is: 2186\n",
      "The number of epochs is: 2187\n",
      "The number of epochs is: 2188\n",
      "The number of epochs is: 2189\n",
      "The number of epochs is: 2190\n",
      "The number of epochs is: 2191\n",
      "The number of epochs is: 2192\n",
      "The number of epochs is: 2193\n",
      "The number of epochs is: 2194\n",
      "The number of epochs is: 2195\n",
      "The number of epochs is: 2196\n",
      "The number of epochs is: 2197\n",
      "The number of epochs is: 2198\n",
      "The number of epochs is: 2199\n",
      "The number of epochs is: 2200\n",
      "13\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2501 loss is tensor([0.0308], grad_fn=<AddBackward0>)\n",
      "epoch: 2502 loss is tensor([0.0149], grad_fn=<AddBackward0>)\n",
      "epoch: 2503 loss is tensor([0.0989], grad_fn=<AddBackward0>)\n",
      "epoch: 2504 loss is tensor([0.0662], grad_fn=<AddBackward0>)\n",
      "epoch: 2505 loss is tensor([0.0709], grad_fn=<AddBackward0>)\n",
      "epoch: 2506 loss is tensor([0.0123], grad_fn=<AddBackward0>)\n",
      "epoch: 2507 loss is tensor([0.0609], grad_fn=<AddBackward0>)\n",
      "epoch: 2508 loss is tensor([-0.0030], grad_fn=<AddBackward0>)\n",
      "epoch: 2509 loss is tensor([0.0048], grad_fn=<AddBackward0>)\n",
      "epoch: 2510 loss is tensor([0.0157], grad_fn=<AddBackward0>)\n",
      "epoch: 2511 loss is tensor([-0.0048], grad_fn=<AddBackward0>)\n",
      "epoch: 2512 loss is tensor([0.0681], grad_fn=<AddBackward0>)\n",
      "epoch: 2513 loss is tensor([0.0162], grad_fn=<AddBackward0>)\n",
      "epoch: 2514 loss is tensor([0.0615], grad_fn=<AddBackward0>)\n",
      "epoch: 2515 loss is tensor([0.0294], grad_fn=<AddBackward0>)\n",
      "epoch: 2516 loss is tensor([0.0110], grad_fn=<AddBackward0>)\n",
      "epoch: 2517 loss is tensor([-0.0163], grad_fn=<AddBackward0>)\n",
      "epoch: 2518 loss is tensor([0.0388], grad_fn=<AddBackward0>)\n",
      "epoch: 2519 loss is tensor([0.0190], grad_fn=<AddBackward0>)\n",
      "epoch: 2520 loss is tensor([0.0438], grad_fn=<AddBackward0>)\n",
      "epoch: 2521 loss is tensor([-0.0013], grad_fn=<AddBackward0>)\n",
      "epoch: 2522 loss is tensor([0.0200], grad_fn=<AddBackward0>)\n",
      "epoch: 2523 loss is tensor([0.0353], grad_fn=<AddBackward0>)\n",
      "epoch: 2524 loss is tensor([0.0295], grad_fn=<AddBackward0>)\n",
      "epoch: 2525 loss is tensor([-0.0006], grad_fn=<AddBackward0>)\n",
      "epoch: 2526 loss is tensor([0.0166], grad_fn=<AddBackward0>)\n",
      "epoch: 2527 loss is tensor([0.0412], grad_fn=<AddBackward0>)\n",
      "epoch: 2528 loss is tensor([0.0604], grad_fn=<AddBackward0>)\n",
      "epoch: 2529 loss is tensor([0.0352], grad_fn=<AddBackward0>)\n",
      "epoch: 2530 loss is tensor([0.0473], grad_fn=<AddBackward0>)\n",
      "epoch: 2531 loss is tensor([0.1006], grad_fn=<AddBackward0>)\n",
      "epoch: 2532 loss is tensor([0.0254], grad_fn=<AddBackward0>)\n",
      "epoch: 2533 loss is tensor([0.0809], grad_fn=<AddBackward0>)\n",
      "epoch: 2534 loss is tensor([0.0236], grad_fn=<AddBackward0>)\n",
      "epoch: 2535 loss is tensor([0.0754], grad_fn=<AddBackward0>)\n",
      "epoch: 2536 loss is tensor([0.0930], grad_fn=<AddBackward0>)\n",
      "epoch: 2537 loss is tensor([0.1099], grad_fn=<AddBackward0>)\n",
      "epoch: 2538 loss is tensor([0.0652], grad_fn=<AddBackward0>)\n",
      "epoch: 2539 loss is tensor([0.0660], grad_fn=<AddBackward0>)\n",
      "epoch: 2540 loss is tensor([0.0572], grad_fn=<AddBackward0>)\n",
      "epoch: 2541 loss is tensor([0.0604], grad_fn=<AddBackward0>)\n",
      "epoch: 2542 loss is tensor([0.1228], grad_fn=<AddBackward0>)\n",
      "epoch: 2543 loss is tensor([0.0577], grad_fn=<AddBackward0>)\n",
      "epoch: 2544 loss is tensor([0.0798], grad_fn=<AddBackward0>)\n",
      "epoch: 2545 loss is tensor([0.0529], grad_fn=<AddBackward0>)\n",
      "epoch: 2546 loss is tensor([0.0533], grad_fn=<AddBackward0>)\n",
      "epoch: 2547 loss is tensor([0.0099], grad_fn=<AddBackward0>)\n",
      "epoch: 2548 loss is tensor([0.0299], grad_fn=<AddBackward0>)\n",
      "epoch: 2549 loss is tensor([0.0443], grad_fn=<AddBackward0>)\n",
      "epoch: 2550 loss is tensor([-0.0279], grad_fn=<AddBackward0>)\n",
      "epoch: 2551 loss is tensor([0.0506], grad_fn=<AddBackward0>)\n",
      "epoch: 2552 loss is tensor([0.0339], grad_fn=<AddBackward0>)\n",
      "epoch: 2553 loss is tensor([0.0500], grad_fn=<AddBackward0>)\n",
      "epoch: 2554 loss is tensor([0.0642], grad_fn=<AddBackward0>)\n",
      "epoch: 2555 loss is tensor([0.0815], grad_fn=<AddBackward0>)\n",
      "epoch: 2556 loss is tensor([0.1205], grad_fn=<AddBackward0>)\n",
      "epoch: 2557 loss is tensor([0.0518], grad_fn=<AddBackward0>)\n",
      "epoch: 2558 loss is tensor([0.0191], grad_fn=<AddBackward0>)\n",
      "epoch: 2559 loss is tensor([0.0306], grad_fn=<AddBackward0>)\n",
      "epoch: 2560 loss is tensor([0.0355], grad_fn=<AddBackward0>)\n",
      "epoch: 2561 loss is tensor([0.0484], grad_fn=<AddBackward0>)\n",
      "epoch: 2562 loss is tensor([0.0626], grad_fn=<AddBackward0>)\n",
      "epoch: 2563 loss is tensor([0.0532], grad_fn=<AddBackward0>)\n",
      "epoch: 2564 loss is tensor([0.0228], grad_fn=<AddBackward0>)\n",
      "epoch: 2565 loss is tensor([0.0168], grad_fn=<AddBackward0>)\n",
      "epoch: 2566 loss is tensor([0.0500], grad_fn=<AddBackward0>)\n",
      "epoch: 2567 loss is tensor([0.0338], grad_fn=<AddBackward0>)\n",
      "epoch: 2568 loss is tensor([0.0006], grad_fn=<AddBackward0>)\n",
      "epoch: 2569 loss is tensor([0.0083], grad_fn=<AddBackward0>)\n",
      "epoch: 2570 loss is tensor([0.0353], grad_fn=<AddBackward0>)\n",
      "epoch: 2571 loss is tensor([0.0569], grad_fn=<AddBackward0>)\n",
      "epoch: 2572 loss is tensor([0.0341], grad_fn=<AddBackward0>)\n",
      "epoch: 2573 loss is tensor([0.0840], grad_fn=<AddBackward0>)\n",
      "epoch: 2574 loss is tensor([0.0956], grad_fn=<AddBackward0>)\n",
      "epoch: 2575 loss is tensor([0.0427], grad_fn=<AddBackward0>)\n",
      "epoch: 2576 loss is tensor([0.0965], grad_fn=<AddBackward0>)\n",
      "epoch: 2577 loss is tensor([0.0907], grad_fn=<AddBackward0>)\n",
      "epoch: 2578 loss is tensor([0.0460], grad_fn=<AddBackward0>)\n",
      "epoch: 2579 loss is tensor([0.0988], grad_fn=<AddBackward0>)\n",
      "epoch: 2580 loss is tensor([0.0459], grad_fn=<AddBackward0>)\n",
      "epoch: 2581 loss is tensor([0.0326], grad_fn=<AddBackward0>)\n",
      "epoch: 2582 loss is tensor([0.0089], grad_fn=<AddBackward0>)\n",
      "epoch: 2583 loss is tensor([0.0668], grad_fn=<AddBackward0>)\n",
      "epoch: 2584 loss is tensor([0.0806], grad_fn=<AddBackward0>)\n",
      "epoch: 2585 loss is tensor([-0.0026], grad_fn=<AddBackward0>)\n",
      "epoch: 2586 loss is tensor([0.0573], grad_fn=<AddBackward0>)\n",
      "epoch: 2587 loss is tensor([-0.0324], grad_fn=<AddBackward0>)\n",
      "epoch: 2588 loss is tensor([-0.0105], grad_fn=<AddBackward0>)\n",
      "epoch: 2589 loss is tensor([-0.0167], grad_fn=<AddBackward0>)\n",
      "epoch: 2590 loss is tensor([0.0039], grad_fn=<AddBackward0>)\n",
      "epoch: 2591 loss is tensor([0.0373], grad_fn=<AddBackward0>)\n",
      "epoch: 2592 loss is tensor([-0.0041], grad_fn=<AddBackward0>)\n",
      "epoch: 2593 loss is tensor([0.0116], grad_fn=<AddBackward0>)\n",
      "epoch: 2594 loss is tensor([0.0135], grad_fn=<AddBackward0>)\n",
      "epoch: 2595 loss is tensor([0.0619], grad_fn=<AddBackward0>)\n",
      "epoch: 2596 loss is tensor([0.0049], grad_fn=<AddBackward0>)\n",
      "epoch: 2597 loss is tensor([0.0295], grad_fn=<AddBackward0>)\n",
      "epoch: 2598 loss is tensor([3.9123e-05], grad_fn=<AddBackward0>)\n",
      "epoch: 2599 loss is tensor([0.0570], grad_fn=<AddBackward0>)\n",
      "epoch: 2600 loss is tensor([0.0849], grad_fn=<AddBackward0>)\n",
      "47\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 2601 loss is tensor([0.0177], grad_fn=<AddBackward0>)\n",
      "epoch: 2602 loss is tensor([-0.0320], grad_fn=<AddBackward0>)\n",
      "epoch: 2603 loss is tensor([0.0354], grad_fn=<AddBackward0>)\n",
      "epoch: 2604 loss is tensor([0.0569], grad_fn=<AddBackward0>)\n",
      "epoch: 2605 loss is tensor([0.0144], grad_fn=<AddBackward0>)\n",
      "epoch: 2606 loss is tensor([-0.0007], grad_fn=<AddBackward0>)\n",
      "epoch: 2607 loss is tensor([0.0369], grad_fn=<AddBackward0>)\n",
      "epoch: 2608 loss is tensor([0.0716], grad_fn=<AddBackward0>)\n",
      "epoch: 2609 loss is tensor([0.0145], grad_fn=<AddBackward0>)\n",
      "epoch: 2610 loss is tensor([0.0127], grad_fn=<AddBackward0>)\n",
      "epoch: 2611 loss is tensor([0.0514], grad_fn=<AddBackward0>)\n",
      "epoch: 2612 loss is tensor([0.0683], grad_fn=<AddBackward0>)\n",
      "epoch: 2613 loss is tensor([0.0738], grad_fn=<AddBackward0>)\n",
      "epoch: 2614 loss is tensor([0.0075], grad_fn=<AddBackward0>)\n",
      "epoch: 2615 loss is tensor([0.0837], grad_fn=<AddBackward0>)\n",
      "epoch: 2616 loss is tensor([0.0570], grad_fn=<AddBackward0>)\n",
      "epoch: 2617 loss is tensor([0.1014], grad_fn=<AddBackward0>)\n",
      "epoch: 2618 loss is tensor([0.1001], grad_fn=<AddBackward0>)\n",
      "epoch: 2619 loss is tensor([0.0213], grad_fn=<AddBackward0>)\n",
      "epoch: 2620 loss is tensor([0.0676], grad_fn=<AddBackward0>)\n",
      "epoch: 2621 loss is tensor([0.0868], grad_fn=<AddBackward0>)\n",
      "epoch: 2622 loss is tensor([0.0720], grad_fn=<AddBackward0>)\n",
      "epoch: 2623 loss is tensor([0.0688], grad_fn=<AddBackward0>)\n",
      "epoch: 2624 loss is tensor([0.0381], grad_fn=<AddBackward0>)\n",
      "epoch: 2625 loss is tensor([0.0158], grad_fn=<AddBackward0>)\n",
      "epoch: 2626 loss is tensor([0.1021], grad_fn=<AddBackward0>)\n",
      "epoch: 2627 loss is tensor([0.0649], grad_fn=<AddBackward0>)\n",
      "epoch: 2628 loss is tensor([0.0424], grad_fn=<AddBackward0>)\n",
      "epoch: 2629 loss is tensor([0.0391], grad_fn=<AddBackward0>)\n",
      "epoch: 2630 loss is tensor([0.0411], grad_fn=<AddBackward0>)\n",
      "epoch: 2631 loss is tensor([-4.6674e-05], grad_fn=<AddBackward0>)\n",
      "epoch: 2632 loss is tensor([0.0376], grad_fn=<AddBackward0>)\n",
      "epoch: 2633 loss is tensor([0.0273], grad_fn=<AddBackward0>)\n",
      "epoch: 2634 loss is tensor([-0.0187], grad_fn=<AddBackward0>)\n",
      "epoch: 2635 loss is tensor([0.0670], grad_fn=<AddBackward0>)\n",
      "epoch: 2636 loss is tensor([0.0047], grad_fn=<AddBackward0>)\n",
      "epoch: 2637 loss is tensor([-0.0304], grad_fn=<AddBackward0>)\n",
      "epoch: 2638 loss is tensor([0.0483], grad_fn=<AddBackward0>)\n",
      "epoch: 2639 loss is tensor([0.0319], grad_fn=<AddBackward0>)\n",
      "epoch: 2640 loss is tensor([0.0475], grad_fn=<AddBackward0>)\n",
      "epoch: 2641 loss is tensor([-0.0123], grad_fn=<AddBackward0>)\n",
      "epoch: 2642 loss is tensor([-0.0114], grad_fn=<AddBackward0>)\n",
      "epoch: 2643 loss is tensor([0.0095], grad_fn=<AddBackward0>)\n",
      "epoch: 2644 loss is tensor([-0.0024], grad_fn=<AddBackward0>)\n",
      "epoch: 2645 loss is tensor([0.0189], grad_fn=<AddBackward0>)\n",
      "epoch: 2646 loss is tensor([0.0420], grad_fn=<AddBackward0>)\n",
      "epoch: 2647 loss is tensor([0.0217], grad_fn=<AddBackward0>)\n",
      "epoch: 2648 loss is tensor([0.0020], grad_fn=<AddBackward0>)\n",
      "epoch: 2649 loss is tensor([-0.0072], grad_fn=<AddBackward0>)\n",
      "epoch: 2650 loss is tensor([0.0400], grad_fn=<AddBackward0>)\n",
      "epoch: 2651 loss is tensor([0.0529], grad_fn=<AddBackward0>)\n",
      "epoch: 2652 loss is tensor([0.0395], grad_fn=<AddBackward0>)\n",
      "epoch: 2653 loss is tensor([0.0196], grad_fn=<AddBackward0>)\n",
      "epoch: 2654 loss is tensor([0.0196], grad_fn=<AddBackward0>)\n",
      "epoch: 2655 loss is tensor([-0.0142], grad_fn=<AddBackward0>)\n",
      "epoch: 2656 loss is tensor([-0.0138], grad_fn=<AddBackward0>)\n",
      "epoch: 2657 loss is tensor([-0.0607], grad_fn=<AddBackward0>)\n",
      "epoch: 2658 loss is tensor([0.0135], grad_fn=<AddBackward0>)\n",
      "epoch: 2659 loss is tensor([0.0528], grad_fn=<AddBackward0>)\n",
      "epoch: 2660 loss is tensor([0.0320], grad_fn=<AddBackward0>)\n",
      "epoch: 2661 loss is tensor([0.0574], grad_fn=<AddBackward0>)\n",
      "epoch: 2662 loss is tensor([0.0297], grad_fn=<AddBackward0>)\n",
      "epoch: 2663 loss is tensor([0.0729], grad_fn=<AddBackward0>)\n",
      "epoch: 2664 loss is tensor([0.0470], grad_fn=<AddBackward0>)\n",
      "epoch: 2665 loss is tensor([0.0331], grad_fn=<AddBackward0>)\n",
      "epoch: 2666 loss is tensor([0.0794], grad_fn=<AddBackward0>)\n",
      "epoch: 2667 loss is tensor([0.0506], grad_fn=<AddBackward0>)\n",
      "epoch: 2668 loss is tensor([0.0523], grad_fn=<AddBackward0>)\n",
      "epoch: 2669 loss is tensor([0.0591], grad_fn=<AddBackward0>)\n",
      "epoch: 2670 loss is tensor([0.0261], grad_fn=<AddBackward0>)\n",
      "epoch: 2671 loss is tensor([-0.0058], grad_fn=<AddBackward0>)\n",
      "epoch: 2672 loss is tensor([0.0145], grad_fn=<AddBackward0>)\n",
      "epoch: 2673 loss is tensor([0.0610], grad_fn=<AddBackward0>)\n",
      "epoch: 2674 loss is tensor([0.0354], grad_fn=<AddBackward0>)\n",
      "epoch: 2675 loss is tensor([0.0017], grad_fn=<AddBackward0>)\n",
      "epoch: 2676 loss is tensor([0.0196], grad_fn=<AddBackward0>)\n",
      "epoch: 2677 loss is tensor([-0.0076], grad_fn=<AddBackward0>)\n",
      "epoch: 2678 loss is tensor([0.0368], grad_fn=<AddBackward0>)\n",
      "epoch: 2679 loss is tensor([-0.0257], grad_fn=<AddBackward0>)\n",
      "epoch: 2680 loss is tensor([0.0388], grad_fn=<AddBackward0>)\n",
      "epoch: 2681 loss is tensor([0.0452], grad_fn=<AddBackward0>)\n",
      "epoch: 2682 loss is tensor([0.0302], grad_fn=<AddBackward0>)\n",
      "epoch: 2683 loss is tensor([0.0584], grad_fn=<AddBackward0>)\n",
      "epoch: 2684 loss is tensor([0.1051], grad_fn=<AddBackward0>)\n",
      "epoch: 2685 loss is tensor([-0.0444], grad_fn=<AddBackward0>)\n",
      "epoch: 2686 loss is tensor([-0.0123], grad_fn=<AddBackward0>)\n",
      "epoch: 2687 loss is tensor([0.0277], grad_fn=<AddBackward0>)\n",
      "epoch: 2688 loss is tensor([0.0323], grad_fn=<AddBackward0>)\n",
      "epoch: 2689 loss is tensor([0.0506], grad_fn=<AddBackward0>)\n",
      "epoch: 2690 loss is tensor([0.0051], grad_fn=<AddBackward0>)\n",
      "epoch: 2691 loss is tensor([0.0388], grad_fn=<AddBackward0>)\n",
      "epoch: 2692 loss is tensor([0.0039], grad_fn=<AddBackward0>)\n",
      "epoch: 2693 loss is tensor([-0.0826], grad_fn=<AddBackward0>)\n",
      "epoch: 2694 loss is tensor([-0.0623], grad_fn=<AddBackward0>)\n",
      "epoch: 2695 loss is tensor([-0.0178], grad_fn=<AddBackward0>)\n",
      "epoch: 2696 loss is tensor([0.0222], grad_fn=<AddBackward0>)\n",
      "epoch: 2697 loss is tensor([-0.0578], grad_fn=<AddBackward0>)\n",
      "epoch: 2698 loss is tensor([0.0387], grad_fn=<AddBackward0>)\n",
      "epoch: 2699 loss is tensor([-0.0017], grad_fn=<AddBackward0>)\n",
      "epoch: 2700 loss is tensor([0.0423], grad_fn=<AddBackward0>)\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2701 loss is tensor([-0.0198], grad_fn=<AddBackward0>)\n",
      "epoch: 2702 loss is tensor([0.0318], grad_fn=<AddBackward0>)\n",
      "epoch: 2703 loss is tensor([0.0244], grad_fn=<AddBackward0>)\n",
      "epoch: 2704 loss is tensor([0.0396], grad_fn=<AddBackward0>)\n",
      "epoch: 2705 loss is tensor([-0.0313], grad_fn=<AddBackward0>)\n",
      "epoch: 2706 loss is tensor([-0.0006], grad_fn=<AddBackward0>)\n",
      "epoch: 2707 loss is tensor([-0.0122], grad_fn=<AddBackward0>)\n",
      "epoch: 2708 loss is tensor([-0.0240], grad_fn=<AddBackward0>)\n",
      "epoch: 2709 loss is tensor([-0.0287], grad_fn=<AddBackward0>)\n",
      "epoch: 2710 loss is tensor([-0.0435], grad_fn=<AddBackward0>)\n",
      "epoch: 2711 loss is tensor([-0.0360], grad_fn=<AddBackward0>)\n",
      "epoch: 2712 loss is tensor([-0.0007], grad_fn=<AddBackward0>)\n",
      "epoch: 2713 loss is tensor([-0.0319], grad_fn=<AddBackward0>)\n",
      "epoch: 2714 loss is tensor([-0.0572], grad_fn=<AddBackward0>)\n",
      "epoch: 2715 loss is tensor([-0.0076], grad_fn=<AddBackward0>)\n",
      "epoch: 2716 loss is tensor([-0.0376], grad_fn=<AddBackward0>)\n",
      "epoch: 2717 loss is tensor([-0.0315], grad_fn=<AddBackward0>)\n",
      "epoch: 2718 loss is tensor([0.0404], grad_fn=<AddBackward0>)\n",
      "epoch: 2719 loss is tensor([-0.0314], grad_fn=<AddBackward0>)\n",
      "epoch: 2720 loss is tensor([-0.0301], grad_fn=<AddBackward0>)\n",
      "epoch: 2721 loss is tensor([0.0052], grad_fn=<AddBackward0>)\n",
      "epoch: 2722 loss is tensor([-0.0343], grad_fn=<AddBackward0>)\n",
      "epoch: 2723 loss is tensor([0.0018], grad_fn=<AddBackward0>)\n",
      "epoch: 2724 loss is tensor([0.0009], grad_fn=<AddBackward0>)\n",
      "epoch: 2725 loss is tensor([-0.0466], grad_fn=<AddBackward0>)\n",
      "epoch: 2726 loss is tensor([0.0324], grad_fn=<AddBackward0>)\n",
      "epoch: 2727 loss is tensor([-0.0042], grad_fn=<AddBackward0>)\n",
      "epoch: 2728 loss is tensor([0.0122], grad_fn=<AddBackward0>)\n",
      "epoch: 2729 loss is tensor([-0.0485], grad_fn=<AddBackward0>)\n",
      "epoch: 2730 loss is tensor([0.0951], grad_fn=<AddBackward0>)\n",
      "epoch: 2731 loss is tensor([-0.0115], grad_fn=<AddBackward0>)\n",
      "epoch: 2732 loss is tensor([0.0073], grad_fn=<AddBackward0>)\n",
      "epoch: 2733 loss is tensor([-0.0179], grad_fn=<AddBackward0>)\n",
      "epoch: 2734 loss is tensor([0.0541], grad_fn=<AddBackward0>)\n",
      "epoch: 2735 loss is tensor([0.0223], grad_fn=<AddBackward0>)\n",
      "epoch: 2736 loss is tensor([0.0152], grad_fn=<AddBackward0>)\n",
      "epoch: 2737 loss is tensor([-0.0028], grad_fn=<AddBackward0>)\n",
      "epoch: 2738 loss is tensor([-0.0031], grad_fn=<AddBackward0>)\n",
      "epoch: 2739 loss is tensor([-0.0560], grad_fn=<AddBackward0>)\n",
      "epoch: 2740 loss is tensor([-0.0022], grad_fn=<AddBackward0>)\n",
      "epoch: 2741 loss is tensor([0.0590], grad_fn=<AddBackward0>)\n",
      "epoch: 2742 loss is tensor([0.0361], grad_fn=<AddBackward0>)\n",
      "epoch: 2743 loss is tensor([0.0618], grad_fn=<AddBackward0>)\n",
      "epoch: 2744 loss is tensor([0.0099], grad_fn=<AddBackward0>)\n",
      "epoch: 2745 loss is tensor([0.0379], grad_fn=<AddBackward0>)\n",
      "epoch: 2746 loss is tensor([-0.0498], grad_fn=<AddBackward0>)\n",
      "epoch: 2747 loss is tensor([-0.0336], grad_fn=<AddBackward0>)\n",
      "epoch: 2748 loss is tensor([-0.0457], grad_fn=<AddBackward0>)\n",
      "epoch: 2749 loss is tensor([0.0424], grad_fn=<AddBackward0>)\n",
      "epoch: 2750 loss is tensor([-0.0250], grad_fn=<AddBackward0>)\n",
      "epoch: 2751 loss is tensor([-0.0579], grad_fn=<AddBackward0>)\n",
      "epoch: 2752 loss is tensor([-0.0715], grad_fn=<AddBackward0>)\n",
      "epoch: 2753 loss is tensor([-0.0136], grad_fn=<AddBackward0>)\n",
      "epoch: 2754 loss is tensor([-0.0273], grad_fn=<AddBackward0>)\n",
      "epoch: 2755 loss is tensor([0.0995], grad_fn=<AddBackward0>)\n",
      "epoch: 2756 loss is tensor([-0.0105], grad_fn=<AddBackward0>)\n",
      "epoch: 2757 loss is tensor([0.0373], grad_fn=<AddBackward0>)\n",
      "epoch: 2758 loss is tensor([0.0053], grad_fn=<AddBackward0>)\n",
      "epoch: 2759 loss is tensor([0.0748], grad_fn=<AddBackward0>)\n",
      "epoch: 2760 loss is tensor([-0.0159], grad_fn=<AddBackward0>)\n",
      "epoch: 2761 loss is tensor([-0.0116], grad_fn=<AddBackward0>)\n",
      "epoch: 2762 loss is tensor([0.0646], grad_fn=<AddBackward0>)\n",
      "epoch: 2763 loss is tensor([0.0338], grad_fn=<AddBackward0>)\n",
      "epoch: 2764 loss is tensor([0.1017], grad_fn=<AddBackward0>)\n",
      "epoch: 2765 loss is tensor([0.0353], grad_fn=<AddBackward0>)\n",
      "epoch: 2766 loss is tensor([0.0116], grad_fn=<AddBackward0>)\n",
      "epoch: 2767 loss is tensor([0.0009], grad_fn=<AddBackward0>)\n",
      "epoch: 2768 loss is tensor([0.0415], grad_fn=<AddBackward0>)\n",
      "epoch: 2769 loss is tensor([0.0153], grad_fn=<AddBackward0>)\n",
      "epoch: 2770 loss is tensor([0.0556], grad_fn=<AddBackward0>)\n",
      "epoch: 2771 loss is tensor([0.0088], grad_fn=<AddBackward0>)\n",
      "epoch: 2772 loss is tensor([-0.0076], grad_fn=<AddBackward0>)\n",
      "epoch: 2773 loss is tensor([-0.0033], grad_fn=<AddBackward0>)\n",
      "epoch: 2774 loss is tensor([0.0107], grad_fn=<AddBackward0>)\n",
      "epoch: 2775 loss is tensor([-0.0520], grad_fn=<AddBackward0>)\n",
      "epoch: 2776 loss is tensor([0.0348], grad_fn=<AddBackward0>)\n",
      "epoch: 2777 loss is tensor([-0.0047], grad_fn=<AddBackward0>)\n",
      "epoch: 2778 loss is tensor([0.0506], grad_fn=<AddBackward0>)\n",
      "epoch: 2779 loss is tensor([0.0012], grad_fn=<AddBackward0>)\n",
      "epoch: 2780 loss is tensor([0.0209], grad_fn=<AddBackward0>)\n",
      "epoch: 2781 loss is tensor([0.0161], grad_fn=<AddBackward0>)\n",
      "epoch: 2782 loss is tensor([0.0246], grad_fn=<AddBackward0>)\n",
      "epoch: 2783 loss is tensor([0.0015], grad_fn=<AddBackward0>)\n",
      "epoch: 2784 loss is tensor([-0.0129], grad_fn=<AddBackward0>)\n",
      "epoch: 2785 loss is tensor([-0.0227], grad_fn=<AddBackward0>)\n",
      "epoch: 2786 loss is tensor([0.0124], grad_fn=<AddBackward0>)\n",
      "epoch: 2787 loss is tensor([0.0145], grad_fn=<AddBackward0>)\n",
      "epoch: 2788 loss is tensor([0.0953], grad_fn=<AddBackward0>)\n",
      "epoch: 2789 loss is tensor([0.0052], grad_fn=<AddBackward0>)\n",
      "epoch: 2790 loss is tensor([0.0547], grad_fn=<AddBackward0>)\n",
      "epoch: 2791 loss is tensor([-0.0809], grad_fn=<AddBackward0>)\n",
      "epoch: 2792 loss is tensor([0.0414], grad_fn=<AddBackward0>)\n",
      "epoch: 2793 loss is tensor([0.0423], grad_fn=<AddBackward0>)\n",
      "epoch: 2794 loss is tensor([-0.0187], grad_fn=<AddBackward0>)\n",
      "epoch: 2795 loss is tensor([-4.3891e-05], grad_fn=<AddBackward0>)\n",
      "epoch: 2796 loss is tensor([0.0256], grad_fn=<AddBackward0>)\n",
      "epoch: 2797 loss is tensor([0.0043], grad_fn=<AddBackward0>)\n",
      "epoch: 2798 loss is tensor([0.0273], grad_fn=<AddBackward0>)\n",
      "epoch: 2799 loss is tensor([0.0020], grad_fn=<AddBackward0>)\n",
      "epoch: 2800 loss is tensor([-0.0251], grad_fn=<AddBackward0>)\n",
      "25\n"
     ]
=======
      "The number of epochs is: 2201\n",
      "The number of epochs is: 2202\n",
      "The number of epochs is: 2203\n",
      "The number of epochs is: 2204\n",
      "The number of epochs is: 2205\n",
      "The number of epochs is: 2206\n",
      "The number of epochs is: 2207\n",
      "The number of epochs is: 2208\n",
      "The number of epochs is: 2209\n",
      "The number of epochs is: 2210\n",
      "The number of epochs is: 2211\n",
      "The number of epochs is: 2212\n",
      "The number of epochs is: 2213\n",
      "The number of epochs is: 2214\n",
      "The number of epochs is: 2215\n",
      "The number of epochs is: 2216\n",
      "The number of epochs is: 2217\n",
      "The number of epochs is: 2218\n",
      "The number of epochs is: 2219\n",
      "The number of epochs is: 2220\n",
      "The number of epochs is: 2221\n",
      "The number of epochs is: 2222\n",
      "The number of epochs is: 2223\n",
      "The number of epochs is: 2224\n",
      "The number of epochs is: 2225\n",
      "The number of epochs is: 2226\n",
      "The number of epochs is: 2227\n",
      "The number of epochs is: 2228\n",
      "The number of epochs is: 2229\n",
      "The number of epochs is: 2230\n",
      "The number of epochs is: 2231\n",
      "The number of epochs is: 2232\n",
      "The number of epochs is: 2233\n",
      "The number of epochs is: 2234\n",
      "The number of epochs is: 2235\n",
      "The number of epochs is: 2236\n",
      "The number of epochs is: 2237\n",
      "The number of epochs is: 2238\n",
      "The number of epochs is: 2239\n",
      "The number of epochs is: 2240\n",
      "The number of epochs is: 2241\n",
      "The number of epochs is: 2242\n",
      "The number of epochs is: 2243\n",
      "The number of epochs is: 2244\n",
      "The number of epochs is: 2245\n",
      "The number of epochs is: 2246\n",
      "The number of epochs is: 2247\n",
      "The number of epochs is: 2248\n",
      "The number of epochs is: 2249\n",
      "The number of epochs is: 2250\n",
      "The number of epochs is: 2251\n",
      "The number of epochs is: 2252\n",
      "The number of epochs is: 2253\n",
      "The number of epochs is: 2254\n",
      "The number of epochs is: 2255\n",
      "The number of epochs is: 2256\n",
      "The number of epochs is: 2257\n",
      "The number of epochs is: 2258\n",
      "The number of epochs is: 2259\n",
      "The number of epochs is: 2260\n",
      "The number of epochs is: 2261\n",
      "The number of epochs is: 2262\n",
      "The number of epochs is: 2263\n",
      "The number of epochs is: 2264\n",
      "The number of epochs is: 2265\n",
      "The number of epochs is: 2266\n",
      "The number of epochs is: 2267\n",
      "The number of epochs is: 2268\n",
      "The number of epochs is: 2269\n",
      "The number of epochs is: 2270\n",
      "The number of epochs is: 2271\n",
      "The number of epochs is: 2272\n",
      "The number of epochs is: 2273\n",
      "The number of epochs is: 2274\n",
      "The number of epochs is: 2275\n",
      "The number of epochs is: 2276\n",
      "The number of epochs is: 2277\n",
      "The number of epochs is: 2278\n",
      "The number of epochs is: 2279\n",
      "The number of epochs is: 2280\n",
      "The number of epochs is: 2281\n",
      "The number of epochs is: 2282\n",
      "The number of epochs is: 2283\n",
      "The number of epochs is: 2284\n",
      "The number of epochs is: 2285\n",
      "The number of epochs is: 2286\n",
      "The number of epochs is: 2287\n",
      "The number of epochs is: 2288\n",
      "The number of epochs is: 2289\n",
      "The number of epochs is: 2290\n",
      "The number of epochs is: 2291\n",
      "The number of epochs is: 2292\n",
      "The number of epochs is: 2293\n",
      "The number of epochs is: 2294\n",
      "The number of epochs is: 2295\n",
      "The number of epochs is: 2296\n",
      "The number of epochs is: 2297\n",
      "The number of epochs is: 2298\n",
      "The number of epochs is: 2299\n",
      "The number of epochs is: 2300\n",
      "The number of epochs is: 2301\n",
      "The number of epochs is: 2302\n",
      "The number of epochs is: 2303\n",
      "The number of epochs is: 2304\n",
      "The number of epochs is: 2305\n",
      "The number of epochs is: 2306\n",
      "The number of epochs is: 2307\n",
      "The number of epochs is: 2308\n",
      "The number of epochs is: 2309\n",
      "The number of epochs is: 2310\n",
      "The number of epochs is: 2311\n",
      "The number of epochs is: 2312\n",
      "The number of epochs is: 2313\n",
      "The number of epochs is: 2314\n",
      "The number of epochs is: 2315\n",
      "The number of epochs is: 2316\n",
      "The number of epochs is: 2317\n",
      "The number of epochs is: 2318\n",
      "The number of epochs is: 2319\n",
      "The number of epochs is: 2320\n",
      "The number of epochs is: 2321\n",
      "The number of epochs is: 2322\n",
      "The number of epochs is: 2323\n",
      "The number of epochs is: 2324\n",
      "The number of epochs is: 2325\n",
      "The number of epochs is: 2326\n",
      "The number of epochs is: 2327\n",
      "The number of epochs is: 2328\n",
      "The number of epochs is: 2329\n",
      "The number of epochs is: 2330\n",
      "The number of epochs is: 2331\n",
      "The number of epochs is: 2332\n",
      "The number of epochs is: 2333\n",
      "The number of epochs is: 2334\n",
      "The number of epochs is: 2335\n",
      "The number of epochs is: 2336\n",
      "The number of epochs is: 2337\n",
      "The number of epochs is: 2338\n",
      "The number of epochs is: 2339\n",
      "The number of epochs is: 2340\n",
      "The number of epochs is: 2341\n",
      "The number of epochs is: 2342\n",
      "The number of epochs is: 2343\n",
      "The number of epochs is: 2344\n",
      "The number of epochs is: 2345\n",
      "The number of epochs is: 2346\n",
      "The number of epochs is: 2347\n",
      "The number of epochs is: 2348\n",
      "The number of epochs is: 2349\n",
      "The number of epochs is: 2350\n",
      "The number of epochs is: 2351\n",
      "The number of epochs is: 2352\n",
      "The number of epochs is: 2353\n",
      "The number of epochs is: 2354\n",
      "The number of epochs is: 2355\n",
      "The number of epochs is: 2356\n",
      "The number of epochs is: 2357\n",
      "The number of epochs is: 2358\n",
      "The number of epochs is: 2359\n",
      "The number of epochs is: 2360\n",
      "The number of epochs is: 2361\n",
      "The number of epochs is: 2362\n",
      "The number of epochs is: 2363\n",
      "The number of epochs is: 2364\n",
      "The number of epochs is: 2365\n",
      "The number of epochs is: 2366\n",
      "The number of epochs is: 2367\n",
      "The number of epochs is: 2368\n",
      "The number of epochs is: 2369\n",
      "The number of epochs is: 2370\n",
      "The number of epochs is: 2371\n",
      "The number of epochs is: 2372\n",
      "The number of epochs is: 2373\n",
      "The number of epochs is: 2374\n",
      "The number of epochs is: 2375\n",
      "The number of epochs is: 2376\n",
      "The number of epochs is: 2377\n",
      "The number of epochs is: 2378\n",
      "The number of epochs is: 2379\n",
      "The number of epochs is: 2380\n",
      "The number of epochs is: 2381\n",
      "The number of epochs is: 2382\n",
      "The number of epochs is: 2383\n",
      "The number of epochs is: 2384\n",
      "The number of epochs is: 2385\n",
      "The number of epochs is: 2386\n",
      "The number of epochs is: 2387\n",
      "The number of epochs is: 2388\n",
      "The number of epochs is: 2389\n",
      "The number of epochs is: 2390\n",
      "The number of epochs is: 2391\n",
      "The number of epochs is: 2392\n",
      "The number of epochs is: 2393\n",
      "The number of epochs is: 2394\n",
      "The number of epochs is: 2395\n",
      "The number of epochs is: 2396\n",
      "The number of epochs is: 2397\n",
      "The number of epochs is: 2398\n",
      "The number of epochs is: 2399\n",
      "The number of epochs is: 2400\n",
      "10\n",
      "Outputting sketch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 2801 loss is tensor([-0.0570], grad_fn=<AddBackward0>)\n",
      "epoch: 2802 loss is tensor([-0.0240], grad_fn=<AddBackward0>)\n",
      "epoch: 2803 loss is tensor([-0.0033], grad_fn=<AddBackward0>)\n",
      "epoch: 2804 loss is tensor([0.0098], grad_fn=<AddBackward0>)\n",
      "epoch: 2805 loss is tensor([0.0073], grad_fn=<AddBackward0>)\n",
      "epoch: 2806 loss is tensor([0.0814], grad_fn=<AddBackward0>)\n",
      "epoch: 2807 loss is tensor([0.0802], grad_fn=<AddBackward0>)\n",
      "epoch: 2808 loss is tensor([-0.0221], grad_fn=<AddBackward0>)\n",
      "epoch: 2809 loss is tensor([0.0004], grad_fn=<AddBackward0>)\n",
      "epoch: 2810 loss is tensor([0.0081], grad_fn=<AddBackward0>)\n",
      "epoch: 2811 loss is tensor([0.0647], grad_fn=<AddBackward0>)\n",
      "epoch: 2812 loss is tensor([0.0118], grad_fn=<AddBackward0>)\n",
      "epoch: 2813 loss is tensor([-0.0445], grad_fn=<AddBackward0>)\n",
      "epoch: 2814 loss is tensor([0.0154], grad_fn=<AddBackward0>)\n",
      "epoch: 2815 loss is tensor([0.0113], grad_fn=<AddBackward0>)\n",
      "epoch: 2816 loss is tensor([-0.0220], grad_fn=<AddBackward0>)\n",
      "epoch: 2817 loss is tensor([-0.0238], grad_fn=<AddBackward0>)\n",
      "epoch: 2818 loss is tensor([-0.0228], grad_fn=<AddBackward0>)\n",
      "epoch: 2819 loss is tensor([-0.0349], grad_fn=<AddBackward0>)\n",
      "epoch: 2820 loss is tensor([0.0109], grad_fn=<AddBackward0>)\n",
      "epoch: 2821 loss is tensor([0.0069], grad_fn=<AddBackward0>)\n",
      "epoch: 2822 loss is tensor([0.0317], grad_fn=<AddBackward0>)\n",
      "epoch: 2823 loss is tensor([0.0038], grad_fn=<AddBackward0>)\n",
      "epoch: 2824 loss is tensor([-0.0008], grad_fn=<AddBackward0>)\n",
      "epoch: 2825 loss is tensor([0.0149], grad_fn=<AddBackward0>)\n",
      "epoch: 2826 loss is tensor([-0.0613], grad_fn=<AddBackward0>)\n",
      "epoch: 2827 loss is tensor([-0.0054], grad_fn=<AddBackward0>)\n",
      "epoch: 2828 loss is tensor([-0.0043], grad_fn=<AddBackward0>)\n",
      "epoch: 2829 loss is tensor([-0.0321], grad_fn=<AddBackward0>)\n",
      "epoch: 2830 loss is tensor([-0.0571], grad_fn=<AddBackward0>)\n",
      "epoch: 2831 loss is tensor([-0.0066], grad_fn=<AddBackward0>)\n",
      "epoch: 2832 loss is tensor([-0.0640], grad_fn=<AddBackward0>)\n",
      "epoch: 2833 loss is tensor([-0.0838], grad_fn=<AddBackward0>)\n",
      "epoch: 2834 loss is tensor([-0.0748], grad_fn=<AddBackward0>)\n",
      "epoch: 2835 loss is tensor([-0.0167], grad_fn=<AddBackward0>)\n",
      "epoch: 2836 loss is tensor([0.0344], grad_fn=<AddBackward0>)\n",
      "epoch: 2837 loss is tensor([0.0323], grad_fn=<AddBackward0>)\n",
      "epoch: 2838 loss is tensor([-0.0205], grad_fn=<AddBackward0>)\n",
      "epoch: 2839 loss is tensor([-0.0211], grad_fn=<AddBackward0>)\n",
      "epoch: 2840 loss is tensor([-0.0292], grad_fn=<AddBackward0>)\n",
      "epoch: 2841 loss is tensor([-0.0147], grad_fn=<AddBackward0>)\n",
      "epoch: 2842 loss is tensor([0.0330], grad_fn=<AddBackward0>)\n",
      "epoch: 2843 loss is tensor([-0.0054], grad_fn=<AddBackward0>)\n",
      "epoch: 2844 loss is tensor([-0.0579], grad_fn=<AddBackward0>)\n",
      "epoch: 2845 loss is tensor([-0.0199], grad_fn=<AddBackward0>)\n",
      "epoch: 2846 loss is tensor([-0.0284], grad_fn=<AddBackward0>)\n",
      "epoch: 2847 loss is tensor([0.0393], grad_fn=<AddBackward0>)\n",
      "epoch: 2848 loss is tensor([-0.0002], grad_fn=<AddBackward0>)\n",
      "epoch: 2849 loss is tensor([-0.0066], grad_fn=<AddBackward0>)\n",
      "epoch: 2850 loss is tensor([0.0450], grad_fn=<AddBackward0>)\n",
      "epoch: 2851 loss is tensor([-0.0011], grad_fn=<AddBackward0>)\n",
      "epoch: 2852 loss is tensor([-0.0606], grad_fn=<AddBackward0>)\n",
      "epoch: 2853 loss is tensor([-0.0196], grad_fn=<AddBackward0>)\n",
      "epoch: 2854 loss is tensor([0.0399], grad_fn=<AddBackward0>)\n",
      "epoch: 2855 loss is tensor([0.0199], grad_fn=<AddBackward0>)\n",
      "epoch: 2856 loss is tensor([-0.0418], grad_fn=<AddBackward0>)\n",
      "epoch: 2857 loss is tensor([0.0530], grad_fn=<AddBackward0>)\n",
      "epoch: 2858 loss is tensor([-0.0248], grad_fn=<AddBackward0>)\n",
      "epoch: 2859 loss is tensor([-0.0107], grad_fn=<AddBackward0>)\n",
      "epoch: 2860 loss is tensor([-0.0154], grad_fn=<AddBackward0>)\n",
      "epoch: 2861 loss is tensor([-0.0062], grad_fn=<AddBackward0>)\n",
      "epoch: 2862 loss is tensor([-0.0159], grad_fn=<AddBackward0>)\n",
      "epoch: 2863 loss is tensor([-0.0063], grad_fn=<AddBackward0>)\n",
      "epoch: 2864 loss is tensor([0.0193], grad_fn=<AddBackward0>)\n",
      "epoch: 2865 loss is tensor([0.0331], grad_fn=<AddBackward0>)\n",
      "epoch: 2866 loss is tensor([0.0114], grad_fn=<AddBackward0>)\n",
      "epoch: 2867 loss is tensor([-0.0140], grad_fn=<AddBackward0>)\n",
      "epoch: 2868 loss is tensor([-0.0009], grad_fn=<AddBackward0>)\n",
      "epoch: 2869 loss is tensor([-0.0309], grad_fn=<AddBackward0>)\n",
      "epoch: 2870 loss is tensor([-0.0292], grad_fn=<AddBackward0>)\n",
      "epoch: 2871 loss is tensor([0.0266], grad_fn=<AddBackward0>)\n",
      "epoch: 2872 loss is tensor([0.0236], grad_fn=<AddBackward0>)\n",
      "epoch: 2873 loss is tensor([-0.0065], grad_fn=<AddBackward0>)\n",
      "epoch: 2874 loss is tensor([0.0972], grad_fn=<AddBackward0>)\n",
      "epoch: 2875 loss is tensor([0.0925], grad_fn=<AddBackward0>)\n",
      "epoch: 2876 loss is tensor([0.0638], grad_fn=<AddBackward0>)\n",
      "epoch: 2877 loss is tensor([0.1189], grad_fn=<AddBackward0>)\n",
      "epoch: 2878 loss is tensor([-0.0154], grad_fn=<AddBackward0>)\n",
      "epoch: 2879 loss is tensor([0.0404], grad_fn=<AddBackward0>)\n",
      "epoch: 2880 loss is tensor([0.0777], grad_fn=<AddBackward0>)\n",
      "epoch: 2881 loss is tensor([0.0514], grad_fn=<AddBackward0>)\n",
      "epoch: 2882 loss is tensor([0.0629], grad_fn=<AddBackward0>)\n",
      "epoch: 2883 loss is tensor([0.0998], grad_fn=<AddBackward0>)\n",
      "epoch: 2884 loss is tensor([0.0385], grad_fn=<AddBackward0>)\n",
      "epoch: 2885 loss is tensor([-0.0152], grad_fn=<AddBackward0>)\n",
      "epoch: 2886 loss is tensor([-0.0363], grad_fn=<AddBackward0>)\n",
      "epoch: 2887 loss is tensor([0.0491], grad_fn=<AddBackward0>)\n",
      "epoch: 2888 loss is tensor([0.0409], grad_fn=<AddBackward0>)\n",
      "epoch: 2889 loss is tensor([0.0299], grad_fn=<AddBackward0>)\n",
      "epoch: 2890 loss is tensor([-0.0137], grad_fn=<AddBackward0>)\n",
      "epoch: 2891 loss is tensor([0.0080], grad_fn=<AddBackward0>)\n",
      "epoch: 2892 loss is tensor([-0.0636], grad_fn=<AddBackward0>)\n",
      "epoch: 2893 loss is tensor([-0.0209], grad_fn=<AddBackward0>)\n",
      "epoch: 2894 loss is tensor([-0.0382], grad_fn=<AddBackward0>)\n",
      "epoch: 2895 loss is tensor([-0.0171], grad_fn=<AddBackward0>)\n",
      "epoch: 2896 loss is tensor([0.0207], grad_fn=<AddBackward0>)\n",
      "epoch: 2897 loss is tensor([-0.0534], grad_fn=<AddBackward0>)\n",
      "epoch: 2898 loss is tensor([-0.0434], grad_fn=<AddBackward0>)\n",
      "epoch: 2899 loss is tensor([-0.0731], grad_fn=<AddBackward0>)\n",
      "epoch: 2900 loss is tensor([-0.0289], grad_fn=<AddBackward0>)\n",
      "10\n"
=======
      "The number of epochs is: 2401\n",
      "The number of epochs is: 2402\n",
      "The number of epochs is: 2403\n",
      "The number of epochs is: 2404\n",
      "The number of epochs is: 2405\n",
      "The number of epochs is: 2406\n",
      "The number of epochs is: 2407\n",
      "The number of epochs is: 2408\n",
      "The number of epochs is: 2409\n",
      "The number of epochs is: 2410\n",
      "The number of epochs is: 2411\n",
      "The number of epochs is: 2412\n",
      "The number of epochs is: 2413\n",
      "The number of epochs is: 2414\n",
      "The number of epochs is: 2415\n",
      "The number of epochs is: 2416\n",
      "The number of epochs is: 2417\n",
      "The number of epochs is: 2418\n",
      "The number of epochs is: 2419\n",
      "The number of epochs is: 2420\n",
      "The number of epochs is: 2421\n",
      "The number of epochs is: 2422\n",
      "The number of epochs is: 2423\n",
      "The number of epochs is: 2424\n",
      "The number of epochs is: 2425\n",
      "The number of epochs is: 2426\n",
      "The number of epochs is: 2427\n",
      "The number of epochs is: 2428\n",
      "The number of epochs is: 2429\n",
      "The number of epochs is: 2430\n",
      "The number of epochs is: 2431\n",
      "The number of epochs is: 2432\n",
      "The number of epochs is: 2433\n",
      "The number of epochs is: 2434\n",
      "The number of epochs is: 2435\n",
      "The number of epochs is: 2436\n",
      "The number of epochs is: 2437\n",
      "The number of epochs is: 2438\n",
      "The number of epochs is: 2439\n",
      "The number of epochs is: 2440\n",
      "The number of epochs is: 2441\n",
      "The number of epochs is: 2442\n",
      "The number of epochs is: 2443\n",
      "The number of epochs is: 2444\n",
      "The number of epochs is: 2445\n",
      "The number of epochs is: 2446\n",
      "The number of epochs is: 2447\n",
      "The number of epochs is: 2448\n",
      "The number of epochs is: 2449\n",
      "The number of epochs is: 2450\n",
      "The number of epochs is: 2451\n",
      "The number of epochs is: 2452\n",
      "The number of epochs is: 2453\n",
      "The number of epochs is: 2454\n",
      "The number of epochs is: 2455\n",
      "The number of epochs is: 2456\n",
      "The number of epochs is: 2457\n",
      "The number of epochs is: 2458\n",
      "The number of epochs is: 2459\n",
      "The number of epochs is: 2460\n",
      "The number of epochs is: 2461\n",
      "The number of epochs is: 2462\n",
      "The number of epochs is: 2463\n",
      "The number of epochs is: 2464\n",
      "The number of epochs is: 2465\n",
      "The number of epochs is: 2466\n",
      "The number of epochs is: 2467\n",
      "The number of epochs is: 2468\n",
      "The number of epochs is: 2469\n",
      "The number of epochs is: 2470\n",
      "The number of epochs is: 2471\n",
      "The number of epochs is: 2472\n",
      "The number of epochs is: 2473\n",
      "The number of epochs is: 2474\n",
      "The number of epochs is: 2475\n",
      "The number of epochs is: 2476\n",
      "The number of epochs is: 2477\n",
      "The number of epochs is: 2478\n",
      "The number of epochs is: 2479\n",
      "The number of epochs is: 2480\n",
      "The number of epochs is: 2481\n",
      "The number of epochs is: 2482\n",
      "The number of epochs is: 2483\n",
      "The number of epochs is: 2484\n",
      "The number of epochs is: 2485\n",
      "The number of epochs is: 2486\n",
      "The number of epochs is: 2487\n",
      "The number of epochs is: 2488\n",
      "The number of epochs is: 2489\n",
      "The number of epochs is: 2490\n",
      "The number of epochs is: 2491\n",
      "The number of epochs is: 2492\n",
      "The number of epochs is: 2493\n",
      "The number of epochs is: 2494\n",
      "The number of epochs is: 2495\n",
      "The number of epochs is: 2496\n",
      "The number of epochs is: 2497\n",
      "The number of epochs is: 2498\n",
      "The number of epochs is: 2499\n",
      "The number of epochs is: 2500\n",
      "The number of epochs is: 2501\n",
      "The number of epochs is: 2502\n",
      "The number of epochs is: 2503\n",
      "The number of epochs is: 2504\n",
      "The number of epochs is: 2505\n",
      "The number of epochs is: 2506\n",
      "The number of epochs is: 2507\n",
      "The number of epochs is: 2508\n",
      "The number of epochs is: 2509\n",
      "The number of epochs is: 2510\n",
      "The number of epochs is: 2511\n",
      "The number of epochs is: 2512\n",
      "The number of epochs is: 2513\n",
      "The number of epochs is: 2514\n",
      "The number of epochs is: 2515\n",
      "The number of epochs is: 2516\n",
      "The number of epochs is: 2517\n",
      "The number of epochs is: 2518\n",
      "The number of epochs is: 2519\n",
      "The number of epochs is: 2520\n",
      "The number of epochs is: 2521\n",
      "The number of epochs is: 2522\n",
      "The number of epochs is: 2523\n",
      "The number of epochs is: 2524\n",
      "The number of epochs is: 2525\n",
      "The number of epochs is: 2526\n",
      "The number of epochs is: 2527\n",
      "The number of epochs is: 2528\n",
      "The number of epochs is: 2529\n",
      "The number of epochs is: 2530\n",
      "The number of epochs is: 2531\n",
      "The number of epochs is: 2532\n",
      "The number of epochs is: 2533\n",
      "The number of epochs is: 2534\n",
      "The number of epochs is: 2535\n",
      "The number of epochs is: 2536\n",
      "The number of epochs is: 2537\n",
      "The number of epochs is: 2538\n",
      "The number of epochs is: 2539\n",
      "The number of epochs is: 2540\n",
      "The number of epochs is: 2541\n",
      "The number of epochs is: 2542\n",
      "The number of epochs is: 2543\n",
      "The number of epochs is: 2544\n",
      "The number of epochs is: 2545\n",
      "The number of epochs is: 2546\n",
      "The number of epochs is: 2547\n",
      "The number of epochs is: 2548\n",
      "The number of epochs is: 2549\n",
      "The number of epochs is: 2550\n",
      "The number of epochs is: 2551\n",
      "The number of epochs is: 2552\n",
      "The number of epochs is: 2553\n",
      "The number of epochs is: 2554\n",
      "The number of epochs is: 2555\n",
      "The number of epochs is: 2556\n",
      "The number of epochs is: 2557\n",
      "The number of epochs is: 2558\n",
      "The number of epochs is: 2559\n",
      "The number of epochs is: 2560\n",
      "The number of epochs is: 2561\n",
      "The number of epochs is: 2562\n",
      "The number of epochs is: 2563\n",
      "The number of epochs is: 2564\n",
      "The number of epochs is: 2565\n",
      "The number of epochs is: 2566\n",
      "The number of epochs is: 2567\n",
      "The number of epochs is: 2568\n",
      "The number of epochs is: 2569\n",
      "The number of epochs is: 2570\n",
      "The number of epochs is: 2571\n",
      "The number of epochs is: 2572\n",
      "The number of epochs is: 2573\n",
      "The number of epochs is: 2574\n",
      "The number of epochs is: 2575\n",
      "The number of epochs is: 2576\n",
      "The number of epochs is: 2577\n",
      "The number of epochs is: 2578\n",
      "The number of epochs is: 2579\n",
      "The number of epochs is: 2580\n",
      "The number of epochs is: 2581\n",
      "The number of epochs is: 2582\n",
      "The number of epochs is: 2583\n",
      "The number of epochs is: 2584\n",
      "The number of epochs is: 2585\n",
      "The number of epochs is: 2586\n",
      "The number of epochs is: 2587\n",
      "The number of epochs is: 2588\n",
      "The number of epochs is: 2589\n",
      "The number of epochs is: 2590\n",
      "The number of epochs is: 2591\n",
      "The number of epochs is: 2592\n",
      "The number of epochs is: 2593\n",
      "The number of epochs is: 2594\n",
      "The number of epochs is: 2595\n",
      "The number of epochs is: 2596\n",
      "The number of epochs is: 2597\n",
      "The number of epochs is: 2598\n",
      "The number of epochs is: 2599\n",
      "The number of epochs is: 2600\n",
      "17\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2901 loss is tensor([-0.0413], grad_fn=<AddBackward0>)\n",
      "epoch: 2902 loss is tensor([-0.0421], grad_fn=<AddBackward0>)\n",
      "epoch: 2903 loss is tensor([0.0242], grad_fn=<AddBackward0>)\n",
      "epoch: 2904 loss is tensor([-0.0104], grad_fn=<AddBackward0>)\n",
      "epoch: 2905 loss is tensor([-0.0155], grad_fn=<AddBackward0>)\n",
      "epoch: 2906 loss is tensor([0.0393], grad_fn=<AddBackward0>)\n",
      "epoch: 2907 loss is tensor([-0.0367], grad_fn=<AddBackward0>)\n",
      "epoch: 2908 loss is tensor([0.0255], grad_fn=<AddBackward0>)\n",
      "epoch: 2909 loss is tensor([0.0174], grad_fn=<AddBackward0>)\n",
      "epoch: 2910 loss is tensor([-0.0082], grad_fn=<AddBackward0>)\n",
      "epoch: 2911 loss is tensor([-0.0078], grad_fn=<AddBackward0>)\n",
      "epoch: 2912 loss is tensor([0.0355], grad_fn=<AddBackward0>)\n",
      "epoch: 2913 loss is tensor([0.0050], grad_fn=<AddBackward0>)\n",
      "epoch: 2914 loss is tensor([-0.0319], grad_fn=<AddBackward0>)\n",
      "epoch: 2915 loss is tensor([-0.0449], grad_fn=<AddBackward0>)\n",
      "epoch: 2916 loss is tensor([-0.0363], grad_fn=<AddBackward0>)\n",
      "epoch: 2917 loss is tensor([0.0483], grad_fn=<AddBackward0>)\n",
      "epoch: 2918 loss is tensor([-0.0744], grad_fn=<AddBackward0>)\n",
      "epoch: 2919 loss is tensor([-0.0095], grad_fn=<AddBackward0>)\n",
      "epoch: 2920 loss is tensor([-0.0548], grad_fn=<AddBackward0>)\n",
      "epoch: 2921 loss is tensor([-0.0166], grad_fn=<AddBackward0>)\n",
      "epoch: 2922 loss is tensor([0.0217], grad_fn=<AddBackward0>)\n",
      "epoch: 2923 loss is tensor([-0.0153], grad_fn=<AddBackward0>)\n",
      "epoch: 2924 loss is tensor([-0.0663], grad_fn=<AddBackward0>)\n",
      "epoch: 2925 loss is tensor([-0.0316], grad_fn=<AddBackward0>)\n",
      "epoch: 2926 loss is tensor([-0.0200], grad_fn=<AddBackward0>)\n",
      "epoch: 2927 loss is tensor([0.0506], grad_fn=<AddBackward0>)\n",
      "epoch: 2928 loss is tensor([-0.0221], grad_fn=<AddBackward0>)\n",
      "epoch: 2929 loss is tensor([-0.0388], grad_fn=<AddBackward0>)\n",
      "epoch: 2930 loss is tensor([0.0247], grad_fn=<AddBackward0>)\n",
      "epoch: 2931 loss is tensor([-0.0241], grad_fn=<AddBackward0>)\n",
      "epoch: 2932 loss is tensor([0.0156], grad_fn=<AddBackward0>)\n",
      "epoch: 2933 loss is tensor([-0.0415], grad_fn=<AddBackward0>)\n",
      "epoch: 2934 loss is tensor([-0.0372], grad_fn=<AddBackward0>)\n",
      "epoch: 2935 loss is tensor([-0.0061], grad_fn=<AddBackward0>)\n",
      "epoch: 2936 loss is tensor([-0.0182], grad_fn=<AddBackward0>)\n",
      "epoch: 2937 loss is tensor([0.0014], grad_fn=<AddBackward0>)\n",
      "epoch: 2938 loss is tensor([0.0188], grad_fn=<AddBackward0>)\n",
      "epoch: 2939 loss is tensor([-0.0298], grad_fn=<AddBackward0>)\n",
      "epoch: 2940 loss is tensor([-0.0606], grad_fn=<AddBackward0>)\n",
      "epoch: 2941 loss is tensor([0.0628], grad_fn=<AddBackward0>)\n",
      "epoch: 2942 loss is tensor([-0.0330], grad_fn=<AddBackward0>)\n",
      "epoch: 2943 loss is tensor([-0.0748], grad_fn=<AddBackward0>)\n",
      "epoch: 2944 loss is tensor([-0.0240], grad_fn=<AddBackward0>)\n",
      "epoch: 2945 loss is tensor([0.0010], grad_fn=<AddBackward0>)\n",
      "epoch: 2946 loss is tensor([0.0041], grad_fn=<AddBackward0>)\n",
      "epoch: 2947 loss is tensor([-0.0270], grad_fn=<AddBackward0>)\n",
      "epoch: 2948 loss is tensor([0.0013], grad_fn=<AddBackward0>)\n",
      "epoch: 2949 loss is tensor([-0.0428], grad_fn=<AddBackward0>)\n",
      "epoch: 2950 loss is tensor([0.0052], grad_fn=<AddBackward0>)\n",
      "epoch: 2951 loss is tensor([-0.0773], grad_fn=<AddBackward0>)\n",
      "epoch: 2952 loss is tensor([-0.0352], grad_fn=<AddBackward0>)\n",
      "epoch: 2953 loss is tensor([-0.0076], grad_fn=<AddBackward0>)\n",
      "epoch: 2954 loss is tensor([0.0053], grad_fn=<AddBackward0>)\n",
      "epoch: 2955 loss is tensor([0.0078], grad_fn=<AddBackward0>)\n",
      "epoch: 2956 loss is tensor([-0.0436], grad_fn=<AddBackward0>)\n",
      "epoch: 2957 loss is tensor([-0.0443], grad_fn=<AddBackward0>)\n",
      "epoch: 2958 loss is tensor([-0.0275], grad_fn=<AddBackward0>)\n",
      "epoch: 2959 loss is tensor([-0.0116], grad_fn=<AddBackward0>)\n",
      "epoch: 2960 loss is tensor([-0.0351], grad_fn=<AddBackward0>)\n",
      "epoch: 2961 loss is tensor([0.0004], grad_fn=<AddBackward0>)\n",
      "epoch: 2962 loss is tensor([-0.0174], grad_fn=<AddBackward0>)\n",
      "epoch: 2963 loss is tensor([-0.0235], grad_fn=<AddBackward0>)\n",
      "epoch: 2964 loss is tensor([-0.0469], grad_fn=<AddBackward0>)\n",
      "epoch: 2965 loss is tensor([-0.0092], grad_fn=<AddBackward0>)\n",
      "epoch: 2966 loss is tensor([-0.0261], grad_fn=<AddBackward0>)\n",
      "epoch: 2967 loss is tensor([0.0307], grad_fn=<AddBackward0>)\n",
      "epoch: 2968 loss is tensor([-0.0344], grad_fn=<AddBackward0>)\n",
      "epoch: 2969 loss is tensor([-0.0289], grad_fn=<AddBackward0>)\n",
      "epoch: 2970 loss is tensor([-0.0398], grad_fn=<AddBackward0>)\n",
      "epoch: 2971 loss is tensor([-0.0074], grad_fn=<AddBackward0>)\n",
      "epoch: 2972 loss is tensor([-0.0002], grad_fn=<AddBackward0>)\n",
      "epoch: 2973 loss is tensor([0.0409], grad_fn=<AddBackward0>)\n",
      "epoch: 2974 loss is tensor([0.0169], grad_fn=<AddBackward0>)\n",
      "epoch: 2975 loss is tensor([-0.0416], grad_fn=<AddBackward0>)\n",
      "epoch: 2976 loss is tensor([0.0013], grad_fn=<AddBackward0>)\n",
      "epoch: 2977 loss is tensor([0.0650], grad_fn=<AddBackward0>)\n",
      "epoch: 2978 loss is tensor([0.0480], grad_fn=<AddBackward0>)\n",
      "epoch: 2979 loss is tensor([0.0696], grad_fn=<AddBackward0>)\n",
      "epoch: 2980 loss is tensor([0.0201], grad_fn=<AddBackward0>)\n",
      "epoch: 2981 loss is tensor([0.1096], grad_fn=<AddBackward0>)\n",
      "epoch: 2982 loss is tensor([-0.0053], grad_fn=<AddBackward0>)\n",
      "epoch: 2983 loss is tensor([0.0206], grad_fn=<AddBackward0>)\n",
      "epoch: 2984 loss is tensor([0.0257], grad_fn=<AddBackward0>)\n",
      "epoch: 2985 loss is tensor([0.0459], grad_fn=<AddBackward0>)\n",
      "epoch: 2986 loss is tensor([0.0213], grad_fn=<AddBackward0>)\n",
      "epoch: 2987 loss is tensor([0.0526], grad_fn=<AddBackward0>)\n",
      "epoch: 2988 loss is tensor([0.0813], grad_fn=<AddBackward0>)\n",
      "epoch: 2989 loss is tensor([0.0214], grad_fn=<AddBackward0>)\n",
      "epoch: 2990 loss is tensor([0.0296], grad_fn=<AddBackward0>)\n",
      "epoch: 2991 loss is tensor([-0.0130], grad_fn=<AddBackward0>)\n",
      "epoch: 2992 loss is tensor([-0.0012], grad_fn=<AddBackward0>)\n",
      "epoch: 2993 loss is tensor([0.0064], grad_fn=<AddBackward0>)\n",
      "epoch: 2994 loss is tensor([0.0528], grad_fn=<AddBackward0>)\n",
      "epoch: 2995 loss is tensor([0.0312], grad_fn=<AddBackward0>)\n",
      "epoch: 2996 loss is tensor([-0.0074], grad_fn=<AddBackward0>)\n",
      "epoch: 2997 loss is tensor([-0.0194], grad_fn=<AddBackward0>)\n",
      "epoch: 2998 loss is tensor([0.0129], grad_fn=<AddBackward0>)\n",
      "epoch: 2999 loss is tensor([-0.0311], grad_fn=<AddBackward0>)\n",
      "epoch: 3000 loss is tensor([-0.0326], grad_fn=<AddBackward0>)\n",
      "60\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 3001 loss is tensor([0.0253], grad_fn=<AddBackward0>)\n",
      "epoch: 3002 loss is tensor([0.0407], grad_fn=<AddBackward0>)\n",
      "epoch: 3003 loss is tensor([-0.0827], grad_fn=<AddBackward0>)\n",
      "epoch: 3004 loss is tensor([-0.0715], grad_fn=<AddBackward0>)\n",
      "epoch: 3005 loss is tensor([-0.0576], grad_fn=<AddBackward0>)\n",
      "epoch: 3006 loss is tensor([-0.0130], grad_fn=<AddBackward0>)\n",
      "epoch: 3007 loss is tensor([-0.0307], grad_fn=<AddBackward0>)\n",
      "epoch: 3008 loss is tensor([-0.0353], grad_fn=<AddBackward0>)\n",
      "epoch: 3009 loss is tensor([-0.0488], grad_fn=<AddBackward0>)\n",
      "epoch: 3010 loss is tensor([0.0060], grad_fn=<AddBackward0>)\n",
      "epoch: 3011 loss is tensor([-0.0092], grad_fn=<AddBackward0>)\n",
      "epoch: 3012 loss is tensor([-0.0377], grad_fn=<AddBackward0>)\n",
      "epoch: 3013 loss is tensor([0.0229], grad_fn=<AddBackward0>)\n",
      "epoch: 3014 loss is tensor([-0.0312], grad_fn=<AddBackward0>)\n",
      "epoch: 3015 loss is tensor([-0.0083], grad_fn=<AddBackward0>)\n",
      "epoch: 3016 loss is tensor([-0.0451], grad_fn=<AddBackward0>)\n",
      "epoch: 3017 loss is tensor([-0.0808], grad_fn=<AddBackward0>)\n",
      "epoch: 3018 loss is tensor([-0.0214], grad_fn=<AddBackward0>)\n",
      "epoch: 3019 loss is tensor([-0.0223], grad_fn=<AddBackward0>)\n",
      "epoch: 3020 loss is tensor([-0.0034], grad_fn=<AddBackward0>)\n",
      "epoch: 3021 loss is tensor([-0.0626], grad_fn=<AddBackward0>)\n",
      "epoch: 3022 loss is tensor([-0.0351], grad_fn=<AddBackward0>)\n",
      "epoch: 3023 loss is tensor([-0.0009], grad_fn=<AddBackward0>)\n",
      "epoch: 3024 loss is tensor([-0.0164], grad_fn=<AddBackward0>)\n",
      "epoch: 3025 loss is tensor([0.0046], grad_fn=<AddBackward0>)\n",
      "epoch: 3026 loss is tensor([0.0148], grad_fn=<AddBackward0>)\n",
      "epoch: 3027 loss is tensor([-0.0342], grad_fn=<AddBackward0>)\n",
      "epoch: 3028 loss is tensor([-0.0272], grad_fn=<AddBackward0>)\n",
      "epoch: 3029 loss is tensor([-0.0614], grad_fn=<AddBackward0>)\n",
      "epoch: 3030 loss is tensor([-0.0187], grad_fn=<AddBackward0>)\n",
      "epoch: 3031 loss is tensor([0.0610], grad_fn=<AddBackward0>)\n",
      "epoch: 3032 loss is tensor([-0.0347], grad_fn=<AddBackward0>)\n",
      "epoch: 3033 loss is tensor([0.0018], grad_fn=<AddBackward0>)\n",
      "epoch: 3034 loss is tensor([0.0006], grad_fn=<AddBackward0>)\n",
      "epoch: 3035 loss is tensor([0.0604], grad_fn=<AddBackward0>)\n",
      "epoch: 3036 loss is tensor([-0.0036], grad_fn=<AddBackward0>)\n",
      "epoch: 3037 loss is tensor([0.0092], grad_fn=<AddBackward0>)\n",
      "epoch: 3038 loss is tensor([0.0296], grad_fn=<AddBackward0>)\n",
      "epoch: 3039 loss is tensor([-0.0246], grad_fn=<AddBackward0>)\n",
      "epoch: 3040 loss is tensor([-0.0172], grad_fn=<AddBackward0>)\n",
      "epoch: 3041 loss is tensor([-0.0008], grad_fn=<AddBackward0>)\n",
      "epoch: 3042 loss is tensor([0.0016], grad_fn=<AddBackward0>)\n",
      "epoch: 3043 loss is tensor([-0.0479], grad_fn=<AddBackward0>)\n",
      "epoch: 3044 loss is tensor([-0.0428], grad_fn=<AddBackward0>)\n",
      "epoch: 3045 loss is tensor([-0.0254], grad_fn=<AddBackward0>)\n",
      "epoch: 3046 loss is tensor([-0.0292], grad_fn=<AddBackward0>)\n",
      "epoch: 3047 loss is tensor([-0.0071], grad_fn=<AddBackward0>)\n",
      "epoch: 3048 loss is tensor([-0.0718], grad_fn=<AddBackward0>)\n",
      "epoch: 3049 loss is tensor([-0.0624], grad_fn=<AddBackward0>)\n",
      "epoch: 3050 loss is tensor([-0.0224], grad_fn=<AddBackward0>)\n",
      "epoch: 3051 loss is tensor([-0.0847], grad_fn=<AddBackward0>)\n",
      "epoch: 3052 loss is tensor([-0.0707], grad_fn=<AddBackward0>)\n",
      "epoch: 3053 loss is tensor([-0.0848], grad_fn=<AddBackward0>)\n",
      "epoch: 3054 loss is tensor([-0.0410], grad_fn=<AddBackward0>)\n",
      "epoch: 3055 loss is tensor([-0.0351], grad_fn=<AddBackward0>)\n",
      "epoch: 3056 loss is tensor([-0.0630], grad_fn=<AddBackward0>)\n",
      "epoch: 3057 loss is tensor([-0.0453], grad_fn=<AddBackward0>)\n",
      "epoch: 3058 loss is tensor([-0.0463], grad_fn=<AddBackward0>)\n",
      "epoch: 3059 loss is tensor([-0.0004], grad_fn=<AddBackward0>)\n",
      "epoch: 3060 loss is tensor([-0.0851], grad_fn=<AddBackward0>)\n",
      "epoch: 3061 loss is tensor([-0.0356], grad_fn=<AddBackward0>)\n",
      "epoch: 3062 loss is tensor([-0.0357], grad_fn=<AddBackward0>)\n",
      "epoch: 3063 loss is tensor([0.0277], grad_fn=<AddBackward0>)\n",
      "epoch: 3064 loss is tensor([-0.0547], grad_fn=<AddBackward0>)\n",
      "epoch: 3065 loss is tensor([-0.0119], grad_fn=<AddBackward0>)\n",
      "epoch: 3066 loss is tensor([-0.0338], grad_fn=<AddBackward0>)\n",
      "epoch: 3067 loss is tensor([-0.0695], grad_fn=<AddBackward0>)\n",
      "epoch: 3068 loss is tensor([0.0255], grad_fn=<AddBackward0>)\n",
      "epoch: 3069 loss is tensor([-0.0154], grad_fn=<AddBackward0>)\n",
      "epoch: 3070 loss is tensor([0.0009], grad_fn=<AddBackward0>)\n",
      "epoch: 3071 loss is tensor([-0.0078], grad_fn=<AddBackward0>)\n",
      "epoch: 3072 loss is tensor([-0.0065], grad_fn=<AddBackward0>)\n",
      "epoch: 3073 loss is tensor([-0.1137], grad_fn=<AddBackward0>)\n",
      "epoch: 3074 loss is tensor([-0.0327], grad_fn=<AddBackward0>)\n",
      "epoch: 3075 loss is tensor([-0.0490], grad_fn=<AddBackward0>)\n",
      "epoch: 3076 loss is tensor([-0.0004], grad_fn=<AddBackward0>)\n",
      "epoch: 3077 loss is tensor([0.0452], grad_fn=<AddBackward0>)\n",
      "epoch: 3078 loss is tensor([0.0337], grad_fn=<AddBackward0>)\n",
      "epoch: 3079 loss is tensor([-0.0621], grad_fn=<AddBackward0>)\n",
      "epoch: 3080 loss is tensor([0.0270], grad_fn=<AddBackward0>)\n",
      "epoch: 3081 loss is tensor([-0.0503], grad_fn=<AddBackward0>)\n",
      "epoch: 3082 loss is tensor([-0.0129], grad_fn=<AddBackward0>)\n",
      "epoch: 3083 loss is tensor([0.0268], grad_fn=<AddBackward0>)\n",
      "epoch: 3084 loss is tensor([0.0027], grad_fn=<AddBackward0>)\n",
      "epoch: 3085 loss is tensor([0.0015], grad_fn=<AddBackward0>)\n",
      "epoch: 3086 loss is tensor([-0.0418], grad_fn=<AddBackward0>)\n",
      "epoch: 3087 loss is tensor([-0.0258], grad_fn=<AddBackward0>)\n",
      "epoch: 3088 loss is tensor([0.0141], grad_fn=<AddBackward0>)\n",
      "epoch: 3089 loss is tensor([0.0229], grad_fn=<AddBackward0>)\n",
      "epoch: 3090 loss is tensor([-0.0145], grad_fn=<AddBackward0>)\n",
      "epoch: 3091 loss is tensor([-0.0527], grad_fn=<AddBackward0>)\n",
      "epoch: 3092 loss is tensor([-0.0173], grad_fn=<AddBackward0>)\n",
      "epoch: 3093 loss is tensor([-0.0335], grad_fn=<AddBackward0>)\n",
      "epoch: 3094 loss is tensor([-0.0204], grad_fn=<AddBackward0>)\n",
      "epoch: 3095 loss is tensor([-0.0146], grad_fn=<AddBackward0>)\n",
      "epoch: 3096 loss is tensor([-0.0195], grad_fn=<AddBackward0>)\n",
      "epoch: 3097 loss is tensor([-0.0963], grad_fn=<AddBackward0>)\n",
      "epoch: 3098 loss is tensor([-0.0550], grad_fn=<AddBackward0>)\n",
      "epoch: 3099 loss is tensor([-0.0211], grad_fn=<AddBackward0>)\n",
      "epoch: 3100 loss is tensor([-0.0443], grad_fn=<AddBackward0>)\n",
      "49\n"
=======
      "The number of epochs is: 2601\n",
      "The number of epochs is: 2602\n",
      "The number of epochs is: 2603\n",
      "The number of epochs is: 2604\n",
      "The number of epochs is: 2605\n",
      "The number of epochs is: 2606\n",
      "The number of epochs is: 2607\n",
      "The number of epochs is: 2608\n",
      "The number of epochs is: 2609\n",
      "The number of epochs is: 2610\n",
      "The number of epochs is: 2611\n",
      "The number of epochs is: 2612\n",
      "The number of epochs is: 2613\n",
      "The number of epochs is: 2614\n",
      "The number of epochs is: 2615\n",
      "The number of epochs is: 2616\n",
      "The number of epochs is: 2617\n",
      "The number of epochs is: 2618\n",
      "The number of epochs is: 2619\n",
      "The number of epochs is: 2620\n",
      "The number of epochs is: 2621\n",
      "The number of epochs is: 2622\n",
      "The number of epochs is: 2623\n",
      "The number of epochs is: 2624\n",
      "The number of epochs is: 2625\n",
      "The number of epochs is: 2626\n",
      "The number of epochs is: 2627\n",
      "The number of epochs is: 2628\n",
      "The number of epochs is: 2629\n",
      "The number of epochs is: 2630\n",
      "The number of epochs is: 2631\n",
      "The number of epochs is: 2632\n",
      "The number of epochs is: 2633\n",
      "The number of epochs is: 2634\n",
      "The number of epochs is: 2635\n",
      "The number of epochs is: 2636\n",
      "The number of epochs is: 2637\n",
      "The number of epochs is: 2638\n",
      "The number of epochs is: 2639\n",
      "The number of epochs is: 2640\n",
      "The number of epochs is: 2641\n",
      "The number of epochs is: 2642\n",
      "The number of epochs is: 2643\n",
      "The number of epochs is: 2644\n",
      "The number of epochs is: 2645\n",
      "The number of epochs is: 2646\n",
      "The number of epochs is: 2647\n",
      "The number of epochs is: 2648\n",
      "The number of epochs is: 2649\n",
      "The number of epochs is: 2650\n",
      "The number of epochs is: 2651\n",
      "The number of epochs is: 2652\n",
      "The number of epochs is: 2653\n",
      "The number of epochs is: 2654\n",
      "The number of epochs is: 2655\n",
      "The number of epochs is: 2656\n",
      "The number of epochs is: 2657\n",
      "The number of epochs is: 2658\n",
      "The number of epochs is: 2659\n",
      "The number of epochs is: 2660\n",
      "The number of epochs is: 2661\n",
      "The number of epochs is: 2662\n",
      "The number of epochs is: 2663\n",
      "The number of epochs is: 2664\n",
      "The number of epochs is: 2665\n",
      "The number of epochs is: 2666\n",
      "The number of epochs is: 2667\n",
      "The number of epochs is: 2668\n",
      "The number of epochs is: 2669\n",
      "The number of epochs is: 2670\n",
      "The number of epochs is: 2671\n",
      "The number of epochs is: 2672\n",
      "The number of epochs is: 2673\n",
      "The number of epochs is: 2674\n",
      "The number of epochs is: 2675\n",
      "The number of epochs is: 2676\n",
      "The number of epochs is: 2677\n",
      "The number of epochs is: 2678\n",
      "The number of epochs is: 2679\n",
      "The number of epochs is: 2680\n",
      "The number of epochs is: 2681\n",
      "The number of epochs is: 2682\n",
      "The number of epochs is: 2683\n",
      "The number of epochs is: 2684\n",
      "The number of epochs is: 2685\n",
      "The number of epochs is: 2686\n",
      "The number of epochs is: 2687\n",
      "The number of epochs is: 2688\n",
      "The number of epochs is: 2689\n",
      "The number of epochs is: 2690\n",
      "The number of epochs is: 2691\n",
      "The number of epochs is: 2692\n",
      "The number of epochs is: 2693\n",
      "The number of epochs is: 2694\n",
      "The number of epochs is: 2695\n",
      "The number of epochs is: 2696\n",
      "The number of epochs is: 2697\n",
      "The number of epochs is: 2698\n",
      "The number of epochs is: 2699\n",
      "The number of epochs is: 2700\n",
      "The number of epochs is: 2701\n",
      "The number of epochs is: 2702\n",
      "The number of epochs is: 2703\n",
      "The number of epochs is: 2704\n",
      "The number of epochs is: 2705\n",
      "The number of epochs is: 2706\n",
      "The number of epochs is: 2707\n",
      "The number of epochs is: 2708\n",
      "The number of epochs is: 2709\n",
      "The number of epochs is: 2710\n",
      "The number of epochs is: 2711\n",
      "The number of epochs is: 2712\n",
      "The number of epochs is: 2713\n",
      "The number of epochs is: 2714\n",
      "The number of epochs is: 2715\n",
      "The number of epochs is: 2716\n",
      "The number of epochs is: 2717\n",
      "The number of epochs is: 2718\n",
      "The number of epochs is: 2719\n",
      "The number of epochs is: 2720\n",
      "The number of epochs is: 2721\n",
      "The number of epochs is: 2722\n",
      "The number of epochs is: 2723\n",
      "The number of epochs is: 2724\n",
      "The number of epochs is: 2725\n",
      "The number of epochs is: 2726\n",
      "The number of epochs is: 2727\n",
      "The number of epochs is: 2728\n",
      "The number of epochs is: 2729\n",
      "The number of epochs is: 2730\n",
      "The number of epochs is: 2731\n",
      "The number of epochs is: 2732\n",
      "The number of epochs is: 2733\n",
      "The number of epochs is: 2734\n",
      "The number of epochs is: 2735\n",
      "The number of epochs is: 2736\n",
      "The number of epochs is: 2737\n",
      "The number of epochs is: 2738\n",
      "The number of epochs is: 2739\n",
      "The number of epochs is: 2740\n",
      "The number of epochs is: 2741\n",
      "The number of epochs is: 2742\n",
      "The number of epochs is: 2743\n",
      "The number of epochs is: 2744\n",
      "The number of epochs is: 2745\n",
      "The number of epochs is: 2746\n",
      "The number of epochs is: 2747\n",
      "The number of epochs is: 2748\n",
      "The number of epochs is: 2749\n",
      "The number of epochs is: 2750\n",
      "The number of epochs is: 2751\n",
      "The number of epochs is: 2752\n",
      "The number of epochs is: 2753\n",
      "The number of epochs is: 2754\n",
      "The number of epochs is: 2755\n",
      "The number of epochs is: 2756\n",
      "The number of epochs is: 2757\n",
      "The number of epochs is: 2758\n",
      "The number of epochs is: 2759\n",
      "The number of epochs is: 2760\n",
      "The number of epochs is: 2761\n",
      "The number of epochs is: 2762\n",
      "The number of epochs is: 2763\n",
      "The number of epochs is: 2764\n",
      "The number of epochs is: 2765\n",
      "The number of epochs is: 2766\n",
      "The number of epochs is: 2767\n",
      "The number of epochs is: 2768\n",
      "The number of epochs is: 2769\n",
      "The number of epochs is: 2770\n",
      "The number of epochs is: 2771\n",
      "The number of epochs is: 2772\n",
      "The number of epochs is: 2773\n",
      "The number of epochs is: 2774\n",
      "The number of epochs is: 2775\n",
      "The number of epochs is: 2776\n",
      "The number of epochs is: 2777\n",
      "The number of epochs is: 2778\n",
      "The number of epochs is: 2779\n",
      "The number of epochs is: 2780\n",
      "The number of epochs is: 2781\n",
      "The number of epochs is: 2782\n",
      "The number of epochs is: 2783\n",
      "The number of epochs is: 2784\n",
      "The number of epochs is: 2785\n",
      "The number of epochs is: 2786\n",
      "The number of epochs is: 2787\n",
      "The number of epochs is: 2788\n",
      "The number of epochs is: 2789\n",
      "The number of epochs is: 2790\n",
      "The number of epochs is: 2791\n",
      "The number of epochs is: 2792\n",
      "The number of epochs is: 2793\n",
      "The number of epochs is: 2794\n",
      "The number of epochs is: 2795\n",
      "The number of epochs is: 2796\n",
      "The number of epochs is: 2797\n",
      "The number of epochs is: 2798\n",
      "The number of epochs is: 2799\n",
      "The number of epochs is: 2800\n",
      "15\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 3101 loss is tensor([0.0048], grad_fn=<AddBackward0>)\n",
      "epoch: 3102 loss is tensor([-0.0545], grad_fn=<AddBackward0>)\n",
      "epoch: 3103 loss is tensor([-0.0138], grad_fn=<AddBackward0>)\n",
      "epoch: 3104 loss is tensor([-0.0616], grad_fn=<AddBackward0>)\n",
      "epoch: 3105 loss is tensor([-0.0153], grad_fn=<AddBackward0>)\n",
      "epoch: 3106 loss is tensor([-0.0621], grad_fn=<AddBackward0>)\n",
      "epoch: 3107 loss is tensor([0.0070], grad_fn=<AddBackward0>)\n",
      "epoch: 3108 loss is tensor([0.0060], grad_fn=<AddBackward0>)\n",
      "epoch: 3109 loss is tensor([-0.0662], grad_fn=<AddBackward0>)\n",
      "epoch: 3110 loss is tensor([-0.0346], grad_fn=<AddBackward0>)\n",
      "epoch: 3111 loss is tensor([0.0149], grad_fn=<AddBackward0>)\n",
      "epoch: 3112 loss is tensor([-0.0883], grad_fn=<AddBackward0>)\n",
      "epoch: 3113 loss is tensor([-0.0304], grad_fn=<AddBackward0>)\n",
      "epoch: 3114 loss is tensor([-0.0530], grad_fn=<AddBackward0>)\n",
      "epoch: 3115 loss is tensor([-0.0176], grad_fn=<AddBackward0>)\n",
      "epoch: 3116 loss is tensor([-0.0170], grad_fn=<AddBackward0>)\n",
      "epoch: 3117 loss is tensor([-0.0385], grad_fn=<AddBackward0>)\n",
      "epoch: 3118 loss is tensor([0.0288], grad_fn=<AddBackward0>)\n",
      "epoch: 3119 loss is tensor([-0.0049], grad_fn=<AddBackward0>)\n",
      "epoch: 3120 loss is tensor([0.0488], grad_fn=<AddBackward0>)\n",
      "epoch: 3121 loss is tensor([-0.0916], grad_fn=<AddBackward0>)\n",
      "epoch: 3122 loss is tensor([-0.0328], grad_fn=<AddBackward0>)\n",
      "epoch: 3123 loss is tensor([-0.0377], grad_fn=<AddBackward0>)\n",
      "epoch: 3124 loss is tensor([-0.0386], grad_fn=<AddBackward0>)\n",
      "epoch: 3125 loss is tensor([-0.0413], grad_fn=<AddBackward0>)\n",
      "epoch: 3126 loss is tensor([-0.0424], grad_fn=<AddBackward0>)\n",
      "epoch: 3127 loss is tensor([-0.0910], grad_fn=<AddBackward0>)\n",
      "epoch: 3128 loss is tensor([-0.0648], grad_fn=<AddBackward0>)\n",
      "epoch: 3129 loss is tensor([-0.0458], grad_fn=<AddBackward0>)\n",
      "epoch: 3130 loss is tensor([0.0120], grad_fn=<AddBackward0>)\n",
      "epoch: 3131 loss is tensor([-0.0675], grad_fn=<AddBackward0>)\n",
      "epoch: 3132 loss is tensor([-0.0881], grad_fn=<AddBackward0>)\n",
      "epoch: 3133 loss is tensor([-0.0146], grad_fn=<AddBackward0>)\n",
      "epoch: 3134 loss is tensor([-0.0936], grad_fn=<AddBackward0>)\n",
      "epoch: 3135 loss is tensor([-0.1353], grad_fn=<AddBackward0>)\n",
      "epoch: 3136 loss is tensor([-0.1038], grad_fn=<AddBackward0>)\n",
      "epoch: 3137 loss is tensor([-0.0265], grad_fn=<AddBackward0>)\n",
      "epoch: 3138 loss is tensor([-0.0093], grad_fn=<AddBackward0>)\n",
      "epoch: 3139 loss is tensor([-0.0874], grad_fn=<AddBackward0>)\n",
      "epoch: 3140 loss is tensor([-0.0332], grad_fn=<AddBackward0>)\n",
      "epoch: 3141 loss is tensor([-0.0082], grad_fn=<AddBackward0>)\n",
      "epoch: 3142 loss is tensor([0.0095], grad_fn=<AddBackward0>)\n",
      "epoch: 3143 loss is tensor([-0.0099], grad_fn=<AddBackward0>)\n",
      "epoch: 3144 loss is tensor([-0.0607], grad_fn=<AddBackward0>)\n",
      "epoch: 3145 loss is tensor([-0.0097], grad_fn=<AddBackward0>)\n",
      "epoch: 3146 loss is tensor([-0.0218], grad_fn=<AddBackward0>)\n",
      "epoch: 3147 loss is tensor([-0.0924], grad_fn=<AddBackward0>)\n",
      "epoch: 3148 loss is tensor([0.0096], grad_fn=<AddBackward0>)\n",
      "epoch: 3149 loss is tensor([-0.0679], grad_fn=<AddBackward0>)\n",
      "epoch: 3150 loss is tensor([-0.0238], grad_fn=<AddBackward0>)\n",
      "epoch: 3151 loss is tensor([-0.0362], grad_fn=<AddBackward0>)\n",
      "epoch: 3152 loss is tensor([-0.0113], grad_fn=<AddBackward0>)\n",
      "epoch: 3153 loss is tensor([-0.0247], grad_fn=<AddBackward0>)\n",
      "epoch: 3154 loss is tensor([-0.0770], grad_fn=<AddBackward0>)\n",
      "epoch: 3155 loss is tensor([-0.0539], grad_fn=<AddBackward0>)\n",
      "epoch: 3156 loss is tensor([0.0203], grad_fn=<AddBackward0>)\n",
      "epoch: 3157 loss is tensor([0.0078], grad_fn=<AddBackward0>)\n",
      "epoch: 3158 loss is tensor([0.0365], grad_fn=<AddBackward0>)\n",
      "epoch: 3159 loss is tensor([-0.0196], grad_fn=<AddBackward0>)\n",
      "epoch: 3160 loss is tensor([-0.0080], grad_fn=<AddBackward0>)\n",
      "epoch: 3161 loss is tensor([-0.0238], grad_fn=<AddBackward0>)\n",
      "epoch: 3162 loss is tensor([-0.0605], grad_fn=<AddBackward0>)\n",
      "epoch: 3163 loss is tensor([-0.0597], grad_fn=<AddBackward0>)\n",
      "epoch: 3164 loss is tensor([-0.0272], grad_fn=<AddBackward0>)\n",
      "epoch: 3165 loss is tensor([-0.0950], grad_fn=<AddBackward0>)\n",
      "epoch: 3166 loss is tensor([-0.1016], grad_fn=<AddBackward0>)\n",
      "epoch: 3167 loss is tensor([-0.0516], grad_fn=<AddBackward0>)\n",
      "epoch: 3168 loss is tensor([-0.0380], grad_fn=<AddBackward0>)\n",
      "epoch: 3169 loss is tensor([0.0029], grad_fn=<AddBackward0>)\n",
      "epoch: 3170 loss is tensor([-0.0716], grad_fn=<AddBackward0>)\n",
      "epoch: 3171 loss is tensor([-0.0651], grad_fn=<AddBackward0>)\n",
      "epoch: 3172 loss is tensor([-0.0680], grad_fn=<AddBackward0>)\n",
      "epoch: 3173 loss is tensor([-0.0538], grad_fn=<AddBackward0>)\n",
      "epoch: 3174 loss is tensor([-0.1131], grad_fn=<AddBackward0>)\n",
      "epoch: 3175 loss is tensor([-0.0699], grad_fn=<AddBackward0>)\n",
      "epoch: 3176 loss is tensor([-0.0749], grad_fn=<AddBackward0>)\n",
      "epoch: 3177 loss is tensor([-0.0462], grad_fn=<AddBackward0>)\n",
      "epoch: 3178 loss is tensor([-0.1057], grad_fn=<AddBackward0>)\n",
      "epoch: 3179 loss is tensor([-0.0466], grad_fn=<AddBackward0>)\n",
      "epoch: 3180 loss is tensor([-0.0402], grad_fn=<AddBackward0>)\n",
      "epoch: 3181 loss is tensor([-0.0383], grad_fn=<AddBackward0>)\n",
      "epoch: 3182 loss is tensor([-0.0277], grad_fn=<AddBackward0>)\n",
      "epoch: 3183 loss is tensor([-0.0422], grad_fn=<AddBackward0>)\n",
      "epoch: 3184 loss is tensor([-0.0216], grad_fn=<AddBackward0>)\n",
      "epoch: 3185 loss is tensor([0.0339], grad_fn=<AddBackward0>)\n",
      "epoch: 3186 loss is tensor([-0.0057], grad_fn=<AddBackward0>)\n",
      "epoch: 3187 loss is tensor([-0.0165], grad_fn=<AddBackward0>)\n",
      "epoch: 3188 loss is tensor([-0.0762], grad_fn=<AddBackward0>)\n",
      "epoch: 3189 loss is tensor([-0.0340], grad_fn=<AddBackward0>)\n",
      "epoch: 3190 loss is tensor([-0.0520], grad_fn=<AddBackward0>)\n",
      "epoch: 3191 loss is tensor([-0.0661], grad_fn=<AddBackward0>)\n",
      "epoch: 3192 loss is tensor([-0.0969], grad_fn=<AddBackward0>)\n",
      "epoch: 3193 loss is tensor([-0.0695], grad_fn=<AddBackward0>)\n",
      "epoch: 3194 loss is tensor([-0.0456], grad_fn=<AddBackward0>)\n",
      "epoch: 3195 loss is tensor([-0.0132], grad_fn=<AddBackward0>)\n",
      "epoch: 3196 loss is tensor([-0.0365], grad_fn=<AddBackward0>)\n",
      "epoch: 3197 loss is tensor([0.0115], grad_fn=<AddBackward0>)\n",
      "epoch: 3198 loss is tensor([-0.0182], grad_fn=<AddBackward0>)\n",
      "epoch: 3199 loss is tensor([-0.0181], grad_fn=<AddBackward0>)\n",
      "epoch: 3200 loss is tensor([-0.0060], grad_fn=<AddBackward0>)\n",
      "27\n"
=======
      "The number of epochs is: 2801\n",
      "The number of epochs is: 2802\n",
      "The number of epochs is: 2803\n",
      "The number of epochs is: 2804\n",
      "The number of epochs is: 2805\n",
      "The number of epochs is: 2806\n",
      "The number of epochs is: 2807\n",
      "The number of epochs is: 2808\n",
      "The number of epochs is: 2809\n",
      "The number of epochs is: 2810\n",
      "The number of epochs is: 2811\n",
      "The number of epochs is: 2812\n",
      "The number of epochs is: 2813\n",
      "The number of epochs is: 2814\n",
      "The number of epochs is: 2815\n",
      "The number of epochs is: 2816\n",
      "The number of epochs is: 2817\n",
      "The number of epochs is: 2818\n",
      "The number of epochs is: 2819\n",
      "The number of epochs is: 2820\n",
      "The number of epochs is: 2821\n",
      "The number of epochs is: 2822\n",
      "The number of epochs is: 2823\n",
      "The number of epochs is: 2824\n",
      "The number of epochs is: 2825\n",
      "The number of epochs is: 2826\n",
      "The number of epochs is: 2827\n",
      "The number of epochs is: 2828\n",
      "The number of epochs is: 2829\n",
      "The number of epochs is: 2830\n",
      "The number of epochs is: 2831\n",
      "The number of epochs is: 2832\n",
      "The number of epochs is: 2833\n",
      "The number of epochs is: 2834\n",
      "The number of epochs is: 2835\n",
      "The number of epochs is: 2836\n",
      "The number of epochs is: 2837\n",
      "The number of epochs is: 2838\n",
      "The number of epochs is: 2839\n",
      "The number of epochs is: 2840\n",
      "The number of epochs is: 2841\n",
      "The number of epochs is: 2842\n",
      "The number of epochs is: 2843\n",
      "The number of epochs is: 2844\n",
      "The number of epochs is: 2845\n",
      "The number of epochs is: 2846\n",
      "The number of epochs is: 2847\n",
      "The number of epochs is: 2848\n",
      "The number of epochs is: 2849\n",
      "The number of epochs is: 2850\n",
      "The number of epochs is: 2851\n",
      "The number of epochs is: 2852\n",
      "The number of epochs is: 2853\n",
      "The number of epochs is: 2854\n",
      "The number of epochs is: 2855\n",
      "The number of epochs is: 2856\n",
      "The number of epochs is: 2857\n",
      "The number of epochs is: 2858\n",
      "The number of epochs is: 2859\n",
      "The number of epochs is: 2860\n",
      "The number of epochs is: 2861\n",
      "The number of epochs is: 2862\n",
      "The number of epochs is: 2863\n",
      "The number of epochs is: 2864\n",
      "The number of epochs is: 2865\n",
      "The number of epochs is: 2866\n",
      "The number of epochs is: 2867\n",
      "The number of epochs is: 2868\n",
      "The number of epochs is: 2869\n",
      "The number of epochs is: 2870\n",
      "The number of epochs is: 2871\n",
      "The number of epochs is: 2872\n",
      "The number of epochs is: 2873\n",
      "The number of epochs is: 2874\n",
      "The number of epochs is: 2875\n",
      "The number of epochs is: 2876\n",
      "The number of epochs is: 2877\n",
      "The number of epochs is: 2878\n",
      "The number of epochs is: 2879\n",
      "The number of epochs is: 2880\n",
      "The number of epochs is: 2881\n",
      "The number of epochs is: 2882\n",
      "The number of epochs is: 2883\n",
      "The number of epochs is: 2884\n",
      "The number of epochs is: 2885\n",
      "The number of epochs is: 2886\n",
      "The number of epochs is: 2887\n",
      "The number of epochs is: 2888\n",
      "The number of epochs is: 2889\n",
      "The number of epochs is: 2890\n",
      "The number of epochs is: 2891\n",
      "The number of epochs is: 2892\n",
      "The number of epochs is: 2893\n",
      "The number of epochs is: 2894\n",
      "The number of epochs is: 2895\n",
      "The number of epochs is: 2896\n",
      "The number of epochs is: 2897\n",
      "The number of epochs is: 2898\n",
      "The number of epochs is: 2899\n",
      "The number of epochs is: 2900\n",
      "The number of epochs is: 2901\n",
      "The number of epochs is: 2902\n",
      "The number of epochs is: 2903\n",
      "The number of epochs is: 2904\n",
      "The number of epochs is: 2905\n",
      "The number of epochs is: 2906\n",
      "The number of epochs is: 2907\n",
      "The number of epochs is: 2908\n",
      "The number of epochs is: 2909\n",
      "The number of epochs is: 2910\n",
      "The number of epochs is: 2911\n",
      "The number of epochs is: 2912\n",
      "The number of epochs is: 2913\n",
      "The number of epochs is: 2914\n",
      "The number of epochs is: 2915\n",
      "The number of epochs is: 2916\n",
      "The number of epochs is: 2917\n",
      "The number of epochs is: 2918\n",
      "The number of epochs is: 2919\n",
      "The number of epochs is: 2920\n",
      "The number of epochs is: 2921\n",
      "The number of epochs is: 2922\n",
      "The number of epochs is: 2923\n",
      "The number of epochs is: 2924\n",
      "The number of epochs is: 2925\n",
      "The number of epochs is: 2926\n",
      "The number of epochs is: 2927\n",
      "The number of epochs is: 2928\n",
      "The number of epochs is: 2929\n",
      "The number of epochs is: 2930\n",
      "The number of epochs is: 2931\n",
      "The number of epochs is: 2932\n",
      "The number of epochs is: 2933\n",
      "The number of epochs is: 2934\n",
      "The number of epochs is: 2935\n",
      "The number of epochs is: 2936\n",
      "The number of epochs is: 2937\n",
      "The number of epochs is: 2938\n",
      "The number of epochs is: 2939\n",
      "The number of epochs is: 2940\n",
      "The number of epochs is: 2941\n",
      "The number of epochs is: 2942\n",
      "The number of epochs is: 2943\n",
      "The number of epochs is: 2944\n",
      "The number of epochs is: 2945\n",
      "The number of epochs is: 2946\n",
      "The number of epochs is: 2947\n",
      "The number of epochs is: 2948\n",
      "The number of epochs is: 2949\n",
      "The number of epochs is: 2950\n",
      "The number of epochs is: 2951\n",
      "The number of epochs is: 2952\n",
      "The number of epochs is: 2953\n",
      "The number of epochs is: 2954\n",
      "The number of epochs is: 2955\n",
      "The number of epochs is: 2956\n",
      "The number of epochs is: 2957\n",
      "The number of epochs is: 2958\n",
      "The number of epochs is: 2959\n",
      "The number of epochs is: 2960\n",
      "The number of epochs is: 2961\n",
      "The number of epochs is: 2962\n",
      "The number of epochs is: 2963\n",
      "The number of epochs is: 2964\n",
      "The number of epochs is: 2965\n",
      "The number of epochs is: 2966\n",
      "The number of epochs is: 2967\n",
      "The number of epochs is: 2968\n",
      "The number of epochs is: 2969\n",
      "The number of epochs is: 2970\n",
      "The number of epochs is: 2971\n",
      "The number of epochs is: 2972\n",
      "The number of epochs is: 2973\n",
      "The number of epochs is: 2974\n",
      "The number of epochs is: 2975\n",
      "The number of epochs is: 2976\n",
      "The number of epochs is: 2977\n",
      "The number of epochs is: 2978\n",
      "The number of epochs is: 2979\n",
      "The number of epochs is: 2980\n",
      "The number of epochs is: 2981\n",
      "The number of epochs is: 2982\n",
      "The number of epochs is: 2983\n",
      "The number of epochs is: 2984\n",
      "The number of epochs is: 2985\n",
      "The number of epochs is: 2986\n",
      "The number of epochs is: 2987\n",
      "The number of epochs is: 2988\n",
      "The number of epochs is: 2989\n",
      "The number of epochs is: 2990\n",
      "The number of epochs is: 2991\n",
      "The number of epochs is: 2992\n",
      "The number of epochs is: 2993\n",
      "The number of epochs is: 2994\n",
      "The number of epochs is: 2995\n",
      "The number of epochs is: 2996\n",
      "The number of epochs is: 2997\n",
      "The number of epochs is: 2998\n",
      "The number of epochs is: 2999\n",
      "The number of epochs is: 3000\n",
      "17\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3201 loss is tensor([-0.0530], grad_fn=<AddBackward0>)\n",
      "epoch: 3202 loss is tensor([-0.0880], grad_fn=<AddBackward0>)\n",
      "epoch: 3203 loss is tensor([-0.0612], grad_fn=<AddBackward0>)\n",
      "epoch: 3204 loss is tensor([-0.0652], grad_fn=<AddBackward0>)\n",
      "epoch: 3205 loss is tensor([-0.0669], grad_fn=<AddBackward0>)\n",
      "epoch: 3206 loss is tensor([-0.1012], grad_fn=<AddBackward0>)\n",
      "epoch: 3207 loss is tensor([-0.0433], grad_fn=<AddBackward0>)\n",
      "epoch: 3208 loss is tensor([-0.0681], grad_fn=<AddBackward0>)\n",
      "epoch: 3209 loss is tensor([0.0351], grad_fn=<AddBackward0>)\n",
      "epoch: 3210 loss is tensor([-0.0991], grad_fn=<AddBackward0>)\n",
      "epoch: 3211 loss is tensor([-0.0485], grad_fn=<AddBackward0>)\n",
      "epoch: 3212 loss is tensor([-0.0284], grad_fn=<AddBackward0>)\n",
      "epoch: 3213 loss is tensor([-0.0075], grad_fn=<AddBackward0>)\n",
      "epoch: 3214 loss is tensor([-0.0335], grad_fn=<AddBackward0>)\n",
      "epoch: 3215 loss is tensor([-0.0547], grad_fn=<AddBackward0>)\n",
      "epoch: 3216 loss is tensor([-0.0363], grad_fn=<AddBackward0>)\n",
      "epoch: 3217 loss is tensor([-0.0534], grad_fn=<AddBackward0>)\n",
      "epoch: 3218 loss is tensor([-0.0547], grad_fn=<AddBackward0>)\n",
      "epoch: 3219 loss is tensor([-0.0260], grad_fn=<AddBackward0>)\n",
      "epoch: 3220 loss is tensor([-0.0390], grad_fn=<AddBackward0>)\n",
      "epoch: 3221 loss is tensor([0.0239], grad_fn=<AddBackward0>)\n",
      "epoch: 3222 loss is tensor([0.0226], grad_fn=<AddBackward0>)\n",
      "epoch: 3223 loss is tensor([-0.0064], grad_fn=<AddBackward0>)\n",
      "epoch: 3224 loss is tensor([0.0021], grad_fn=<AddBackward0>)\n",
      "epoch: 3225 loss is tensor([-0.0326], grad_fn=<AddBackward0>)\n",
      "epoch: 3226 loss is tensor([-0.0190], grad_fn=<AddBackward0>)\n",
      "epoch: 3227 loss is tensor([-0.0414], grad_fn=<AddBackward0>)\n",
      "epoch: 3228 loss is tensor([-0.0242], grad_fn=<AddBackward0>)\n",
      "epoch: 3229 loss is tensor([-0.0479], grad_fn=<AddBackward0>)\n",
      "epoch: 3230 loss is tensor([-0.0106], grad_fn=<AddBackward0>)\n",
      "epoch: 3231 loss is tensor([-0.0676], grad_fn=<AddBackward0>)\n",
      "epoch: 3232 loss is tensor([-0.0451], grad_fn=<AddBackward0>)\n",
      "epoch: 3233 loss is tensor([-0.0532], grad_fn=<AddBackward0>)\n",
      "epoch: 3234 loss is tensor([-0.0849], grad_fn=<AddBackward0>)\n",
      "epoch: 3235 loss is tensor([-0.0433], grad_fn=<AddBackward0>)\n",
      "epoch: 3236 loss is tensor([-0.1399], grad_fn=<AddBackward0>)\n",
      "epoch: 3237 loss is tensor([-0.0786], grad_fn=<AddBackward0>)\n",
      "epoch: 3238 loss is tensor([-0.0904], grad_fn=<AddBackward0>)\n",
      "epoch: 3239 loss is tensor([-0.0105], grad_fn=<AddBackward0>)\n",
      "epoch: 3240 loss is tensor([-0.0541], grad_fn=<AddBackward0>)\n",
      "epoch: 3241 loss is tensor([-0.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 3242 loss is tensor([-0.0240], grad_fn=<AddBackward0>)\n",
      "epoch: 3243 loss is tensor([-0.0334], grad_fn=<AddBackward0>)\n",
      "epoch: 3244 loss is tensor([-0.0439], grad_fn=<AddBackward0>)\n",
      "epoch: 3245 loss is tensor([-0.0850], grad_fn=<AddBackward0>)\n",
      "epoch: 3246 loss is tensor([-0.0331], grad_fn=<AddBackward0>)\n",
      "epoch: 3247 loss is tensor([-0.0613], grad_fn=<AddBackward0>)\n",
      "epoch: 3248 loss is tensor([-0.0264], grad_fn=<AddBackward0>)\n",
      "epoch: 3249 loss is tensor([0.0026], grad_fn=<AddBackward0>)\n",
      "epoch: 3250 loss is tensor([-0.0268], grad_fn=<AddBackward0>)\n",
      "epoch: 3251 loss is tensor([0.0135], grad_fn=<AddBackward0>)\n",
      "epoch: 3252 loss is tensor([-0.0240], grad_fn=<AddBackward0>)\n",
      "epoch: 3253 loss is tensor([-0.0434], grad_fn=<AddBackward0>)\n",
      "epoch: 3254 loss is tensor([-0.0139], grad_fn=<AddBackward0>)\n",
      "epoch: 3255 loss is tensor([0.0216], grad_fn=<AddBackward0>)\n",
      "epoch: 3256 loss is tensor([0.0091], grad_fn=<AddBackward0>)\n",
      "epoch: 3257 loss is tensor([-0.1048], grad_fn=<AddBackward0>)\n",
      "epoch: 3258 loss is tensor([-0.0297], grad_fn=<AddBackward0>)\n",
      "epoch: 3259 loss is tensor([0.0010], grad_fn=<AddBackward0>)\n",
      "epoch: 3260 loss is tensor([-0.0506], grad_fn=<AddBackward0>)\n",
      "epoch: 3261 loss is tensor([-0.0644], grad_fn=<AddBackward0>)\n",
      "epoch: 3262 loss is tensor([-0.0276], grad_fn=<AddBackward0>)\n",
      "epoch: 3263 loss is tensor([-0.1000], grad_fn=<AddBackward0>)\n",
      "epoch: 3264 loss is tensor([-0.0385], grad_fn=<AddBackward0>)\n",
      "epoch: 3265 loss is tensor([-0.0117], grad_fn=<AddBackward0>)\n",
      "epoch: 3266 loss is tensor([0.0501], grad_fn=<AddBackward0>)\n",
      "epoch: 3267 loss is tensor([0.0078], grad_fn=<AddBackward0>)\n",
      "epoch: 3268 loss is tensor([-0.0005], grad_fn=<AddBackward0>)\n",
      "epoch: 3269 loss is tensor([0.0598], grad_fn=<AddBackward0>)\n",
      "epoch: 3270 loss is tensor([0.0322], grad_fn=<AddBackward0>)\n",
      "epoch: 3271 loss is tensor([-0.0172], grad_fn=<AddBackward0>)\n",
      "epoch: 3272 loss is tensor([0.0013], grad_fn=<AddBackward0>)\n",
      "epoch: 3273 loss is tensor([-0.0186], grad_fn=<AddBackward0>)\n",
      "epoch: 3274 loss is tensor([-0.0576], grad_fn=<AddBackward0>)\n",
      "epoch: 3275 loss is tensor([0.0028], grad_fn=<AddBackward0>)\n",
      "epoch: 3276 loss is tensor([-0.0465], grad_fn=<AddBackward0>)\n",
      "epoch: 3277 loss is tensor([-0.0009], grad_fn=<AddBackward0>)\n",
      "epoch: 3278 loss is tensor([-0.0639], grad_fn=<AddBackward0>)\n",
      "epoch: 3279 loss is tensor([-0.0358], grad_fn=<AddBackward0>)\n",
      "epoch: 3280 loss is tensor([0.0475], grad_fn=<AddBackward0>)\n",
      "epoch: 3281 loss is tensor([-0.0348], grad_fn=<AddBackward0>)\n",
      "epoch: 3282 loss is tensor([-0.0033], grad_fn=<AddBackward0>)\n",
      "epoch: 3283 loss is tensor([-0.0163], grad_fn=<AddBackward0>)\n",
      "epoch: 3284 loss is tensor([-0.0113], grad_fn=<AddBackward0>)\n",
      "epoch: 3285 loss is tensor([-0.0362], grad_fn=<AddBackward0>)\n",
      "epoch: 3286 loss is tensor([-0.0310], grad_fn=<AddBackward0>)\n",
      "epoch: 3287 loss is tensor([-0.0354], grad_fn=<AddBackward0>)\n",
      "epoch: 3288 loss is tensor([-0.0472], grad_fn=<AddBackward0>)\n",
      "epoch: 3289 loss is tensor([-0.0660], grad_fn=<AddBackward0>)\n",
      "epoch: 3290 loss is tensor([0.0123], grad_fn=<AddBackward0>)\n",
      "epoch: 3291 loss is tensor([-0.0883], grad_fn=<AddBackward0>)\n",
      "epoch: 3292 loss is tensor([-0.0520], grad_fn=<AddBackward0>)\n",
      "epoch: 3293 loss is tensor([-0.0279], grad_fn=<AddBackward0>)\n",
      "epoch: 3294 loss is tensor([-0.0793], grad_fn=<AddBackward0>)\n",
      "epoch: 3295 loss is tensor([0.0152], grad_fn=<AddBackward0>)\n",
      "epoch: 3296 loss is tensor([-0.0672], grad_fn=<AddBackward0>)\n",
      "epoch: 3297 loss is tensor([-0.0360], grad_fn=<AddBackward0>)\n",
      "epoch: 3298 loss is tensor([-0.0543], grad_fn=<AddBackward0>)\n",
      "epoch: 3299 loss is tensor([-0.0297], grad_fn=<AddBackward0>)\n",
      "epoch: 3300 loss is tensor([-0.0038], grad_fn=<AddBackward0>)\n",
      "61\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 3301 loss is tensor([-0.0649], grad_fn=<AddBackward0>)\n",
      "epoch: 3302 loss is tensor([-0.0691], grad_fn=<AddBackward0>)\n",
      "epoch: 3303 loss is tensor([-0.0429], grad_fn=<AddBackward0>)\n",
      "epoch: 3304 loss is tensor([-0.0325], grad_fn=<AddBackward0>)\n",
      "epoch: 3305 loss is tensor([-0.0129], grad_fn=<AddBackward0>)\n",
      "epoch: 3306 loss is tensor([-0.0066], grad_fn=<AddBackward0>)\n",
      "epoch: 3307 loss is tensor([-0.0588], grad_fn=<AddBackward0>)\n",
      "epoch: 3308 loss is tensor([-0.0757], grad_fn=<AddBackward0>)\n",
      "epoch: 3309 loss is tensor([-0.0066], grad_fn=<AddBackward0>)\n",
      "epoch: 3310 loss is tensor([-0.0253], grad_fn=<AddBackward0>)\n",
      "epoch: 3311 loss is tensor([0.0339], grad_fn=<AddBackward0>)\n",
      "epoch: 3312 loss is tensor([-0.0620], grad_fn=<AddBackward0>)\n",
      "epoch: 3313 loss is tensor([-0.0304], grad_fn=<AddBackward0>)\n",
      "epoch: 3314 loss is tensor([-0.0443], grad_fn=<AddBackward0>)\n",
      "epoch: 3315 loss is tensor([-0.0675], grad_fn=<AddBackward0>)\n",
      "epoch: 3316 loss is tensor([-0.0734], grad_fn=<AddBackward0>)\n",
      "epoch: 3317 loss is tensor([-0.0807], grad_fn=<AddBackward0>)\n",
      "epoch: 3318 loss is tensor([-0.0587], grad_fn=<AddBackward0>)\n",
      "epoch: 3319 loss is tensor([-0.1058], grad_fn=<AddBackward0>)\n",
      "epoch: 3320 loss is tensor([-0.0665], grad_fn=<AddBackward0>)\n",
      "epoch: 3321 loss is tensor([-0.0145], grad_fn=<AddBackward0>)\n",
      "epoch: 3322 loss is tensor([-0.0415], grad_fn=<AddBackward0>)\n",
      "epoch: 3323 loss is tensor([-0.0597], grad_fn=<AddBackward0>)\n",
      "epoch: 3324 loss is tensor([-0.0973], grad_fn=<AddBackward0>)\n",
      "epoch: 3325 loss is tensor([-0.0530], grad_fn=<AddBackward0>)\n",
      "epoch: 3326 loss is tensor([-0.0297], grad_fn=<AddBackward0>)\n",
      "epoch: 3327 loss is tensor([-0.0323], grad_fn=<AddBackward0>)\n",
      "epoch: 3328 loss is tensor([-0.0728], grad_fn=<AddBackward0>)\n",
      "epoch: 3329 loss is tensor([-0.0955], grad_fn=<AddBackward0>)\n",
      "epoch: 3330 loss is tensor([-0.0163], grad_fn=<AddBackward0>)\n",
      "epoch: 3331 loss is tensor([-0.0372], grad_fn=<AddBackward0>)\n",
      "epoch: 3332 loss is tensor([-0.0683], grad_fn=<AddBackward0>)\n",
      "epoch: 3333 loss is tensor([-0.0414], grad_fn=<AddBackward0>)\n",
      "epoch: 3334 loss is tensor([-0.0826], grad_fn=<AddBackward0>)\n",
      "epoch: 3335 loss is tensor([-0.0657], grad_fn=<AddBackward0>)\n",
      "epoch: 3336 loss is tensor([-0.1208], grad_fn=<AddBackward0>)\n",
      "epoch: 3337 loss is tensor([-0.0375], grad_fn=<AddBackward0>)\n",
      "epoch: 3338 loss is tensor([-0.0049], grad_fn=<AddBackward0>)\n",
      "epoch: 3339 loss is tensor([-0.0898], grad_fn=<AddBackward0>)\n",
      "epoch: 3340 loss is tensor([-0.0736], grad_fn=<AddBackward0>)\n",
      "epoch: 3341 loss is tensor([-0.1258], grad_fn=<AddBackward0>)\n",
      "epoch: 3342 loss is tensor([-0.0915], grad_fn=<AddBackward0>)\n",
      "epoch: 3343 loss is tensor([-0.0851], grad_fn=<AddBackward0>)\n",
      "epoch: 3344 loss is tensor([-0.0271], grad_fn=<AddBackward0>)\n",
      "epoch: 3345 loss is tensor([-0.0598], grad_fn=<AddBackward0>)\n",
      "epoch: 3346 loss is tensor([-0.0992], grad_fn=<AddBackward0>)\n",
      "epoch: 3347 loss is tensor([-0.0963], grad_fn=<AddBackward0>)\n",
      "epoch: 3348 loss is tensor([-0.0627], grad_fn=<AddBackward0>)\n",
      "epoch: 3349 loss is tensor([-0.0528], grad_fn=<AddBackward0>)\n",
      "epoch: 3350 loss is tensor([-0.0754], grad_fn=<AddBackward0>)\n",
      "epoch: 3351 loss is tensor([-0.0505], grad_fn=<AddBackward0>)\n",
      "epoch: 3352 loss is tensor([-0.0771], grad_fn=<AddBackward0>)\n",
      "epoch: 3353 loss is tensor([-0.1081], grad_fn=<AddBackward0>)\n",
      "epoch: 3354 loss is tensor([-0.0349], grad_fn=<AddBackward0>)\n",
      "epoch: 3355 loss is tensor([-0.0680], grad_fn=<AddBackward0>)\n",
      "epoch: 3356 loss is tensor([-0.0770], grad_fn=<AddBackward0>)\n",
      "epoch: 3357 loss is tensor([-0.0840], grad_fn=<AddBackward0>)\n",
      "epoch: 3358 loss is tensor([-0.0742], grad_fn=<AddBackward0>)\n",
      "epoch: 3359 loss is tensor([-0.0674], grad_fn=<AddBackward0>)\n",
      "epoch: 3360 loss is tensor([0.0216], grad_fn=<AddBackward0>)\n",
      "epoch: 3361 loss is tensor([-0.0027], grad_fn=<AddBackward0>)\n",
      "epoch: 3362 loss is tensor([-0.0702], grad_fn=<AddBackward0>)\n",
      "epoch: 3363 loss is tensor([-0.0845], grad_fn=<AddBackward0>)\n",
      "epoch: 3364 loss is tensor([-0.0332], grad_fn=<AddBackward0>)\n",
      "epoch: 3365 loss is tensor([-0.0608], grad_fn=<AddBackward0>)\n",
      "epoch: 3366 loss is tensor([-0.0715], grad_fn=<AddBackward0>)\n",
      "epoch: 3367 loss is tensor([-0.0877], grad_fn=<AddBackward0>)\n",
      "epoch: 3368 loss is tensor([-0.0830], grad_fn=<AddBackward0>)\n",
      "epoch: 3369 loss is tensor([-0.0555], grad_fn=<AddBackward0>)\n",
      "epoch: 3370 loss is tensor([-0.0996], grad_fn=<AddBackward0>)\n",
      "epoch: 3371 loss is tensor([-0.0763], grad_fn=<AddBackward0>)\n",
      "epoch: 3372 loss is tensor([-0.0849], grad_fn=<AddBackward0>)\n",
      "epoch: 3373 loss is tensor([-0.1243], grad_fn=<AddBackward0>)\n",
      "epoch: 3374 loss is tensor([-0.1234], grad_fn=<AddBackward0>)\n",
      "epoch: 3375 loss is tensor([-0.0385], grad_fn=<AddBackward0>)\n",
      "epoch: 3376 loss is tensor([-0.0407], grad_fn=<AddBackward0>)\n",
      "epoch: 3377 loss is tensor([-0.0573], grad_fn=<AddBackward0>)\n",
      "epoch: 3378 loss is tensor([-0.0505], grad_fn=<AddBackward0>)\n",
      "epoch: 3379 loss is tensor([-0.0095], grad_fn=<AddBackward0>)\n",
      "epoch: 3380 loss is tensor([0.0270], grad_fn=<AddBackward0>)\n",
      "epoch: 3381 loss is tensor([-0.0366], grad_fn=<AddBackward0>)\n",
      "epoch: 3382 loss is tensor([-0.0307], grad_fn=<AddBackward0>)\n",
      "epoch: 3383 loss is tensor([-0.0128], grad_fn=<AddBackward0>)\n",
      "epoch: 3384 loss is tensor([-0.0538], grad_fn=<AddBackward0>)\n",
      "epoch: 3385 loss is tensor([-0.0371], grad_fn=<AddBackward0>)\n",
      "epoch: 3386 loss is tensor([-0.0596], grad_fn=<AddBackward0>)\n",
      "epoch: 3387 loss is tensor([-0.0446], grad_fn=<AddBackward0>)\n",
      "epoch: 3388 loss is tensor([-0.0387], grad_fn=<AddBackward0>)\n",
      "epoch: 3389 loss is tensor([0.0363], grad_fn=<AddBackward0>)\n",
      "epoch: 3390 loss is tensor([-0.0648], grad_fn=<AddBackward0>)\n",
      "epoch: 3391 loss is tensor([-0.0263], grad_fn=<AddBackward0>)\n",
      "epoch: 3392 loss is tensor([0.0359], grad_fn=<AddBackward0>)\n",
      "epoch: 3393 loss is tensor([-0.0455], grad_fn=<AddBackward0>)\n",
      "epoch: 3394 loss is tensor([-0.0560], grad_fn=<AddBackward0>)\n",
      "epoch: 3395 loss is tensor([-0.0130], grad_fn=<AddBackward0>)\n",
      "epoch: 3396 loss is tensor([-0.0221], grad_fn=<AddBackward0>)\n",
      "epoch: 3397 loss is tensor([-0.0014], grad_fn=<AddBackward0>)\n",
      "epoch: 3398 loss is tensor([-0.0004], grad_fn=<AddBackward0>)\n",
      "epoch: 3399 loss is tensor([-0.0385], grad_fn=<AddBackward0>)\n",
      "epoch: 3400 loss is tensor([-0.0195], grad_fn=<AddBackward0>)\n",
      "46\n"
=======
      "The number of epochs is: 3001\n",
      "The number of epochs is: 3002\n",
      "The number of epochs is: 3003\n",
      "The number of epochs is: 3004\n",
      "The number of epochs is: 3005\n",
      "The number of epochs is: 3006\n",
      "The number of epochs is: 3007\n",
      "The number of epochs is: 3008\n",
      "The number of epochs is: 3009\n",
      "The number of epochs is: 3010\n",
      "The number of epochs is: 3011\n",
      "The number of epochs is: 3012\n",
      "The number of epochs is: 3013\n",
      "The number of epochs is: 3014\n",
      "The number of epochs is: 3015\n",
      "The number of epochs is: 3016\n",
      "The number of epochs is: 3017\n",
      "The number of epochs is: 3018\n",
      "The number of epochs is: 3019\n",
      "The number of epochs is: 3020\n",
      "The number of epochs is: 3021\n",
      "The number of epochs is: 3022\n",
      "The number of epochs is: 3023\n",
      "The number of epochs is: 3024\n",
      "The number of epochs is: 3025\n",
      "The number of epochs is: 3026\n",
      "The number of epochs is: 3027\n",
      "The number of epochs is: 3028\n",
      "The number of epochs is: 3029\n",
      "The number of epochs is: 3030\n",
      "The number of epochs is: 3031\n",
      "The number of epochs is: 3032\n",
      "The number of epochs is: 3033\n",
      "The number of epochs is: 3034\n",
      "The number of epochs is: 3035\n",
      "The number of epochs is: 3036\n",
      "The number of epochs is: 3037\n",
      "The number of epochs is: 3038\n",
      "The number of epochs is: 3039\n",
      "The number of epochs is: 3040\n",
      "The number of epochs is: 3041\n",
      "The number of epochs is: 3042\n",
      "The number of epochs is: 3043\n",
      "The number of epochs is: 3044\n",
      "The number of epochs is: 3045\n",
      "The number of epochs is: 3046\n",
      "The number of epochs is: 3047\n",
      "The number of epochs is: 3048\n",
      "The number of epochs is: 3049\n",
      "The number of epochs is: 3050\n",
      "The number of epochs is: 3051\n",
      "The number of epochs is: 3052\n",
      "The number of epochs is: 3053\n",
      "The number of epochs is: 3054\n",
      "The number of epochs is: 3055\n",
      "The number of epochs is: 3056\n",
      "The number of epochs is: 3057\n",
      "The number of epochs is: 3058\n",
      "The number of epochs is: 3059\n",
      "The number of epochs is: 3060\n",
      "The number of epochs is: 3061\n",
      "The number of epochs is: 3062\n",
      "The number of epochs is: 3063\n",
      "The number of epochs is: 3064\n",
      "The number of epochs is: 3065\n",
      "The number of epochs is: 3066\n",
      "The number of epochs is: 3067\n",
      "The number of epochs is: 3068\n",
      "The number of epochs is: 3069\n",
      "The number of epochs is: 3070\n",
      "The number of epochs is: 3071\n",
      "The number of epochs is: 3072\n",
      "The number of epochs is: 3073\n",
      "The number of epochs is: 3074\n",
      "The number of epochs is: 3075\n",
      "The number of epochs is: 3076\n",
      "The number of epochs is: 3077\n",
      "The number of epochs is: 3078\n",
      "The number of epochs is: 3079\n",
      "The number of epochs is: 3080\n",
      "The number of epochs is: 3081\n",
      "The number of epochs is: 3082\n",
      "The number of epochs is: 3083\n",
      "The number of epochs is: 3084\n",
      "The number of epochs is: 3085\n",
      "The number of epochs is: 3086\n",
      "The number of epochs is: 3087\n",
      "The number of epochs is: 3088\n",
      "The number of epochs is: 3089\n",
      "The number of epochs is: 3090\n",
      "The number of epochs is: 3091\n",
      "The number of epochs is: 3092\n",
      "The number of epochs is: 3093\n",
      "The number of epochs is: 3094\n",
      "The number of epochs is: 3095\n",
      "The number of epochs is: 3096\n",
      "The number of epochs is: 3097\n",
      "The number of epochs is: 3098\n",
      "The number of epochs is: 3099\n",
      "The number of epochs is: 3100\n",
      "The number of epochs is: 3101\n",
      "The number of epochs is: 3102\n",
      "The number of epochs is: 3103\n",
      "The number of epochs is: 3104\n",
      "The number of epochs is: 3105\n",
      "The number of epochs is: 3106\n",
      "The number of epochs is: 3107\n",
      "The number of epochs is: 3108\n",
      "The number of epochs is: 3109\n",
      "The number of epochs is: 3110\n",
      "The number of epochs is: 3111\n",
      "The number of epochs is: 3112\n",
      "The number of epochs is: 3113\n",
      "The number of epochs is: 3114\n",
      "The number of epochs is: 3115\n",
      "The number of epochs is: 3116\n",
      "The number of epochs is: 3117\n",
      "The number of epochs is: 3118\n",
      "The number of epochs is: 3119\n",
      "The number of epochs is: 3120\n",
      "The number of epochs is: 3121\n",
      "The number of epochs is: 3122\n",
      "The number of epochs is: 3123\n",
      "The number of epochs is: 3124\n",
      "The number of epochs is: 3125\n",
      "The number of epochs is: 3126\n",
      "The number of epochs is: 3127\n",
      "The number of epochs is: 3128\n",
      "The number of epochs is: 3129\n",
      "The number of epochs is: 3130\n",
      "The number of epochs is: 3131\n",
      "The number of epochs is: 3132\n",
      "The number of epochs is: 3133\n",
      "The number of epochs is: 3134\n",
      "The number of epochs is: 3135\n",
      "The number of epochs is: 3136\n",
      "The number of epochs is: 3137\n",
      "The number of epochs is: 3138\n",
      "The number of epochs is: 3139\n",
      "The number of epochs is: 3140\n",
      "The number of epochs is: 3141\n",
      "The number of epochs is: 3142\n",
      "The number of epochs is: 3143\n",
      "The number of epochs is: 3144\n",
      "The number of epochs is: 3145\n",
      "The number of epochs is: 3146\n",
      "The number of epochs is: 3147\n",
      "The number of epochs is: 3148\n",
      "The number of epochs is: 3149\n",
      "The number of epochs is: 3150\n",
      "The number of epochs is: 3151\n",
      "The number of epochs is: 3152\n",
      "The number of epochs is: 3153\n",
      "The number of epochs is: 3154\n",
      "The number of epochs is: 3155\n",
      "The number of epochs is: 3156\n",
      "The number of epochs is: 3157\n",
      "The number of epochs is: 3158\n",
      "The number of epochs is: 3159\n",
      "The number of epochs is: 3160\n",
      "The number of epochs is: 3161\n",
      "The number of epochs is: 3162\n",
      "The number of epochs is: 3163\n",
      "The number of epochs is: 3164\n",
      "The number of epochs is: 3165\n",
      "The number of epochs is: 3166\n",
      "The number of epochs is: 3167\n",
      "The number of epochs is: 3168\n",
      "The number of epochs is: 3169\n",
      "The number of epochs is: 3170\n",
      "The number of epochs is: 3171\n",
      "The number of epochs is: 3172\n",
      "The number of epochs is: 3173\n",
      "The number of epochs is: 3174\n",
      "The number of epochs is: 3175\n",
      "The number of epochs is: 3176\n",
      "The number of epochs is: 3177\n",
      "The number of epochs is: 3178\n",
      "The number of epochs is: 3179\n",
      "The number of epochs is: 3180\n",
      "The number of epochs is: 3181\n",
      "The number of epochs is: 3182\n",
      "The number of epochs is: 3183\n",
      "The number of epochs is: 3184\n",
      "The number of epochs is: 3185\n",
      "The number of epochs is: 3186\n",
      "The number of epochs is: 3187\n",
      "The number of epochs is: 3188\n",
      "The number of epochs is: 3189\n",
      "The number of epochs is: 3190\n",
      "The number of epochs is: 3191\n",
      "The number of epochs is: 3192\n",
      "The number of epochs is: 3193\n",
      "The number of epochs is: 3194\n",
      "The number of epochs is: 3195\n",
      "The number of epochs is: 3196\n",
      "The number of epochs is: 3197\n",
      "The number of epochs is: 3198\n",
      "The number of epochs is: 3199\n",
      "The number of epochs is: 3200\n",
      "24\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3401 loss is tensor([0.0107], grad_fn=<AddBackward0>)\n",
      "epoch: 3402 loss is tensor([-0.0382], grad_fn=<AddBackward0>)\n",
      "epoch: 3403 loss is tensor([-0.0590], grad_fn=<AddBackward0>)\n",
      "epoch: 3404 loss is tensor([-0.0438], grad_fn=<AddBackward0>)\n",
      "epoch: 3405 loss is tensor([-0.0118], grad_fn=<AddBackward0>)\n",
      "epoch: 3406 loss is tensor([-0.0697], grad_fn=<AddBackward0>)\n",
      "epoch: 3407 loss is tensor([-0.0537], grad_fn=<AddBackward0>)\n",
      "epoch: 3408 loss is tensor([-0.0763], grad_fn=<AddBackward0>)\n",
      "epoch: 3409 loss is tensor([-0.0477], grad_fn=<AddBackward0>)\n",
      "epoch: 3410 loss is tensor([-0.0917], grad_fn=<AddBackward0>)\n",
      "epoch: 3411 loss is tensor([-0.0641], grad_fn=<AddBackward0>)\n",
      "epoch: 3412 loss is tensor([-0.0905], grad_fn=<AddBackward0>)\n",
      "epoch: 3413 loss is tensor([-0.0326], grad_fn=<AddBackward0>)\n",
      "epoch: 3414 loss is tensor([-0.0701], grad_fn=<AddBackward0>)\n",
      "epoch: 3415 loss is tensor([-0.0451], grad_fn=<AddBackward0>)\n",
      "epoch: 3416 loss is tensor([-0.0388], grad_fn=<AddBackward0>)\n",
      "epoch: 3417 loss is tensor([-0.0329], grad_fn=<AddBackward0>)\n",
      "epoch: 3418 loss is tensor([-0.0590], grad_fn=<AddBackward0>)\n",
      "epoch: 3419 loss is tensor([-0.0648], grad_fn=<AddBackward0>)\n",
      "epoch: 3420 loss is tensor([-0.0710], grad_fn=<AddBackward0>)\n",
      "epoch: 3421 loss is tensor([-0.0348], grad_fn=<AddBackward0>)\n",
      "epoch: 3422 loss is tensor([-0.0786], grad_fn=<AddBackward0>)\n",
      "epoch: 3423 loss is tensor([-0.0756], grad_fn=<AddBackward0>)\n",
      "epoch: 3424 loss is tensor([-0.0045], grad_fn=<AddBackward0>)\n",
      "epoch: 3425 loss is tensor([0.0401], grad_fn=<AddBackward0>)\n",
      "epoch: 3426 loss is tensor([0.0228], grad_fn=<AddBackward0>)\n",
      "epoch: 3427 loss is tensor([0.0409], grad_fn=<AddBackward0>)\n",
      "epoch: 3428 loss is tensor([-0.0393], grad_fn=<AddBackward0>)\n",
      "epoch: 3429 loss is tensor([-0.0114], grad_fn=<AddBackward0>)\n",
      "epoch: 3430 loss is tensor([-0.0278], grad_fn=<AddBackward0>)\n",
      "epoch: 3431 loss is tensor([-0.0546], grad_fn=<AddBackward0>)\n",
      "epoch: 3432 loss is tensor([-0.0739], grad_fn=<AddBackward0>)\n",
      "epoch: 3433 loss is tensor([-0.0247], grad_fn=<AddBackward0>)\n",
      "epoch: 3434 loss is tensor([-0.0363], grad_fn=<AddBackward0>)\n",
      "epoch: 3435 loss is tensor([-0.0488], grad_fn=<AddBackward0>)\n",
      "epoch: 3436 loss is tensor([-0.0095], grad_fn=<AddBackward0>)\n",
      "epoch: 3437 loss is tensor([-0.0436], grad_fn=<AddBackward0>)\n",
      "epoch: 3438 loss is tensor([-0.0130], grad_fn=<AddBackward0>)\n",
      "epoch: 3439 loss is tensor([-0.0217], grad_fn=<AddBackward0>)\n",
      "epoch: 3440 loss is tensor([-0.0444], grad_fn=<AddBackward0>)\n",
      "epoch: 3441 loss is tensor([-0.0382], grad_fn=<AddBackward0>)\n",
      "epoch: 3442 loss is tensor([-0.0546], grad_fn=<AddBackward0>)\n",
      "epoch: 3443 loss is tensor([-0.0962], grad_fn=<AddBackward0>)\n",
      "epoch: 3444 loss is tensor([-0.0667], grad_fn=<AddBackward0>)\n",
      "epoch: 3445 loss is tensor([-0.0694], grad_fn=<AddBackward0>)\n",
      "epoch: 3446 loss is tensor([-0.0301], grad_fn=<AddBackward0>)\n",
      "epoch: 3447 loss is tensor([-0.0349], grad_fn=<AddBackward0>)\n",
      "epoch: 3448 loss is tensor([-0.0116], grad_fn=<AddBackward0>)\n",
      "epoch: 3449 loss is tensor([-0.0485], grad_fn=<AddBackward0>)\n",
      "epoch: 3450 loss is tensor([-0.0420], grad_fn=<AddBackward0>)\n",
      "epoch: 3451 loss is tensor([-0.0236], grad_fn=<AddBackward0>)\n",
      "epoch: 3452 loss is tensor([0.0190], grad_fn=<AddBackward0>)\n",
      "epoch: 3453 loss is tensor([-0.0155], grad_fn=<AddBackward0>)\n",
      "epoch: 3454 loss is tensor([-0.0262], grad_fn=<AddBackward0>)\n",
      "epoch: 3455 loss is tensor([-0.0588], grad_fn=<AddBackward0>)\n",
      "epoch: 3456 loss is tensor([-0.0386], grad_fn=<AddBackward0>)\n",
      "epoch: 3457 loss is tensor([-0.0300], grad_fn=<AddBackward0>)\n",
      "epoch: 3458 loss is tensor([-0.0536], grad_fn=<AddBackward0>)\n",
      "epoch: 3459 loss is tensor([0.0153], grad_fn=<AddBackward0>)\n",
      "epoch: 3460 loss is tensor([-0.0427], grad_fn=<AddBackward0>)\n",
      "epoch: 3461 loss is tensor([-0.0951], grad_fn=<AddBackward0>)\n",
      "epoch: 3462 loss is tensor([-0.0184], grad_fn=<AddBackward0>)\n",
      "epoch: 3463 loss is tensor([-0.0285], grad_fn=<AddBackward0>)\n",
      "epoch: 3464 loss is tensor([-0.0249], grad_fn=<AddBackward0>)\n",
      "epoch: 3465 loss is tensor([-0.0260], grad_fn=<AddBackward0>)\n",
      "epoch: 3466 loss is tensor([-0.0215], grad_fn=<AddBackward0>)\n",
      "epoch: 3467 loss is tensor([-0.0697], grad_fn=<AddBackward0>)\n",
      "epoch: 3468 loss is tensor([-0.0201], grad_fn=<AddBackward0>)\n",
      "epoch: 3469 loss is tensor([-0.0823], grad_fn=<AddBackward0>)\n",
      "epoch: 3470 loss is tensor([-0.0535], grad_fn=<AddBackward0>)\n",
      "epoch: 3471 loss is tensor([-0.0151], grad_fn=<AddBackward0>)\n",
      "epoch: 3472 loss is tensor([-0.0524], grad_fn=<AddBackward0>)\n",
      "epoch: 3473 loss is tensor([-0.0248], grad_fn=<AddBackward0>)\n",
      "epoch: 3474 loss is tensor([-0.0283], grad_fn=<AddBackward0>)\n",
      "epoch: 3475 loss is tensor([-0.0066], grad_fn=<AddBackward0>)\n",
      "epoch: 3476 loss is tensor([-0.0166], grad_fn=<AddBackward0>)\n",
      "epoch: 3477 loss is tensor([-0.0720], grad_fn=<AddBackward0>)\n",
      "epoch: 3478 loss is tensor([-0.0586], grad_fn=<AddBackward0>)\n",
      "epoch: 3479 loss is tensor([-0.0613], grad_fn=<AddBackward0>)\n",
      "epoch: 3480 loss is tensor([-0.0511], grad_fn=<AddBackward0>)\n",
      "epoch: 3481 loss is tensor([-0.0269], grad_fn=<AddBackward0>)\n",
      "epoch: 3482 loss is tensor([-0.0788], grad_fn=<AddBackward0>)\n",
      "epoch: 3483 loss is tensor([-0.0465], grad_fn=<AddBackward0>)\n",
      "epoch: 3484 loss is tensor([0.0293], grad_fn=<AddBackward0>)\n",
      "epoch: 3485 loss is tensor([0.0863], grad_fn=<AddBackward0>)\n",
      "epoch: 3486 loss is tensor([-0.0709], grad_fn=<AddBackward0>)\n",
      "epoch: 3487 loss is tensor([0.0622], grad_fn=<AddBackward0>)\n",
      "epoch: 3488 loss is tensor([-0.0232], grad_fn=<AddBackward0>)\n",
      "epoch: 3489 loss is tensor([0.0260], grad_fn=<AddBackward0>)\n",
      "epoch: 3490 loss is tensor([-0.0141], grad_fn=<AddBackward0>)\n",
      "epoch: 3491 loss is tensor([0.0425], grad_fn=<AddBackward0>)\n",
      "epoch: 3492 loss is tensor([0.0391], grad_fn=<AddBackward0>)\n",
      "epoch: 3493 loss is tensor([-0.0025], grad_fn=<AddBackward0>)\n",
      "epoch: 3494 loss is tensor([-0.0380], grad_fn=<AddBackward0>)\n",
      "epoch: 3495 loss is tensor([-0.0007], grad_fn=<AddBackward0>)\n",
      "epoch: 3496 loss is tensor([-0.0307], grad_fn=<AddBackward0>)\n",
      "epoch: 3497 loss is tensor([0.0087], grad_fn=<AddBackward0>)\n",
      "epoch: 3498 loss is tensor([-0.0452], grad_fn=<AddBackward0>)\n",
      "epoch: 3499 loss is tensor([0.0140], grad_fn=<AddBackward0>)\n",
      "epoch: 3500 loss is tensor([-0.0265], grad_fn=<AddBackward0>)\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3501 loss is tensor([-0.0323], grad_fn=<AddBackward0>)\n",
      "epoch: 3502 loss is tensor([0.0055], grad_fn=<AddBackward0>)\n",
      "epoch: 3503 loss is tensor([-0.0031], grad_fn=<AddBackward0>)\n",
      "epoch: 3504 loss is tensor([-0.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 3505 loss is tensor([-0.1184], grad_fn=<AddBackward0>)\n",
      "epoch: 3506 loss is tensor([-0.1118], grad_fn=<AddBackward0>)\n",
      "epoch: 3507 loss is tensor([-0.0894], grad_fn=<AddBackward0>)\n",
      "epoch: 3508 loss is tensor([-0.0669], grad_fn=<AddBackward0>)\n",
      "epoch: 3509 loss is tensor([0.0293], grad_fn=<AddBackward0>)\n",
      "epoch: 3510 loss is tensor([-0.0354], grad_fn=<AddBackward0>)\n",
      "epoch: 3511 loss is tensor([-0.0901], grad_fn=<AddBackward0>)\n",
      "epoch: 3512 loss is tensor([-0.0883], grad_fn=<AddBackward0>)\n",
      "epoch: 3513 loss is tensor([-0.0916], grad_fn=<AddBackward0>)\n",
      "epoch: 3514 loss is tensor([-0.1194], grad_fn=<AddBackward0>)\n",
      "epoch: 3515 loss is tensor([0.0005], grad_fn=<AddBackward0>)\n",
      "epoch: 3516 loss is tensor([-0.0596], grad_fn=<AddBackward0>)\n",
      "epoch: 3517 loss is tensor([-0.0100], grad_fn=<AddBackward0>)\n",
      "epoch: 3518 loss is tensor([-0.0508], grad_fn=<AddBackward0>)\n",
      "epoch: 3519 loss is tensor([-0.0742], grad_fn=<AddBackward0>)\n",
      "epoch: 3520 loss is tensor([-0.0805], grad_fn=<AddBackward0>)\n",
      "epoch: 3521 loss is tensor([-0.0563], grad_fn=<AddBackward0>)\n",
      "epoch: 3522 loss is tensor([-0.0522], grad_fn=<AddBackward0>)\n",
      "epoch: 3523 loss is tensor([-0.0521], grad_fn=<AddBackward0>)\n",
      "epoch: 3524 loss is tensor([-0.1049], grad_fn=<AddBackward0>)\n",
      "epoch: 3525 loss is tensor([-0.0835], grad_fn=<AddBackward0>)\n",
      "epoch: 3526 loss is tensor([-0.0123], grad_fn=<AddBackward0>)\n",
      "epoch: 3527 loss is tensor([-0.0140], grad_fn=<AddBackward0>)\n",
      "epoch: 3528 loss is tensor([-0.1134], grad_fn=<AddBackward0>)\n",
      "epoch: 3529 loss is tensor([-0.0804], grad_fn=<AddBackward0>)\n",
      "epoch: 3530 loss is tensor([-0.0591], grad_fn=<AddBackward0>)\n",
      "epoch: 3531 loss is tensor([-0.1008], grad_fn=<AddBackward0>)\n",
      "epoch: 3532 loss is tensor([-0.0497], grad_fn=<AddBackward0>)\n",
      "epoch: 3533 loss is tensor([-0.0833], grad_fn=<AddBackward0>)\n",
      "epoch: 3534 loss is tensor([-0.0505], grad_fn=<AddBackward0>)\n",
      "epoch: 3535 loss is tensor([-0.0373], grad_fn=<AddBackward0>)\n",
      "epoch: 3536 loss is tensor([-0.0514], grad_fn=<AddBackward0>)\n",
      "epoch: 3537 loss is tensor([-0.0242], grad_fn=<AddBackward0>)\n",
      "epoch: 3538 loss is tensor([-0.0590], grad_fn=<AddBackward0>)\n",
      "epoch: 3539 loss is tensor([-0.0193], grad_fn=<AddBackward0>)\n",
      "epoch: 3540 loss is tensor([-0.0279], grad_fn=<AddBackward0>)\n",
      "epoch: 3541 loss is tensor([-0.0189], grad_fn=<AddBackward0>)\n",
      "epoch: 3542 loss is tensor([-0.0258], grad_fn=<AddBackward0>)\n",
      "epoch: 3543 loss is tensor([-0.0212], grad_fn=<AddBackward0>)\n",
      "epoch: 3544 loss is tensor([0.0431], grad_fn=<AddBackward0>)\n",
      "epoch: 3545 loss is tensor([-0.0955], grad_fn=<AddBackward0>)\n",
      "epoch: 3546 loss is tensor([0.0040], grad_fn=<AddBackward0>)\n",
      "epoch: 3547 loss is tensor([-0.0364], grad_fn=<AddBackward0>)\n",
      "epoch: 3548 loss is tensor([-0.0023], grad_fn=<AddBackward0>)\n",
      "epoch: 3549 loss is tensor([-0.0761], grad_fn=<AddBackward0>)\n",
      "epoch: 3550 loss is tensor([-0.0719], grad_fn=<AddBackward0>)\n",
      "epoch: 3551 loss is tensor([-0.0604], grad_fn=<AddBackward0>)\n",
      "epoch: 3552 loss is tensor([-0.0083], grad_fn=<AddBackward0>)\n",
      "epoch: 3553 loss is tensor([-0.0495], grad_fn=<AddBackward0>)\n",
      "epoch: 3554 loss is tensor([-0.0203], grad_fn=<AddBackward0>)\n",
      "epoch: 3555 loss is tensor([-0.0532], grad_fn=<AddBackward0>)\n",
      "epoch: 3556 loss is tensor([-0.0697], grad_fn=<AddBackward0>)\n",
      "epoch: 3557 loss is tensor([-0.0370], grad_fn=<AddBackward0>)\n",
      "epoch: 3558 loss is tensor([-0.1154], grad_fn=<AddBackward0>)\n",
      "epoch: 3559 loss is tensor([-0.1105], grad_fn=<AddBackward0>)\n",
      "epoch: 3560 loss is tensor([-0.0823], grad_fn=<AddBackward0>)\n",
      "epoch: 3561 loss is tensor([0.0317], grad_fn=<AddBackward0>)\n",
      "epoch: 3562 loss is tensor([-0.0890], grad_fn=<AddBackward0>)\n",
      "epoch: 3563 loss is tensor([-0.1044], grad_fn=<AddBackward0>)\n",
      "epoch: 3564 loss is tensor([-0.1233], grad_fn=<AddBackward0>)\n",
      "epoch: 3565 loss is tensor([-0.0532], grad_fn=<AddBackward0>)\n",
      "epoch: 3566 loss is tensor([-0.1019], grad_fn=<AddBackward0>)\n",
      "epoch: 3567 loss is tensor([-0.0832], grad_fn=<AddBackward0>)\n",
      "epoch: 3568 loss is tensor([-0.0565], grad_fn=<AddBackward0>)\n",
      "epoch: 3569 loss is tensor([-0.0238], grad_fn=<AddBackward0>)\n",
      "epoch: 3570 loss is tensor([-0.0096], grad_fn=<AddBackward0>)\n",
      "epoch: 3571 loss is tensor([-0.0411], grad_fn=<AddBackward0>)\n",
      "epoch: 3572 loss is tensor([-0.0918], grad_fn=<AddBackward0>)\n",
      "epoch: 3573 loss is tensor([-0.0657], grad_fn=<AddBackward0>)\n",
      "epoch: 3574 loss is tensor([-0.0828], grad_fn=<AddBackward0>)\n",
      "epoch: 3575 loss is tensor([-0.0765], grad_fn=<AddBackward0>)\n",
      "epoch: 3576 loss is tensor([-0.0803], grad_fn=<AddBackward0>)\n",
      "epoch: 3577 loss is tensor([-0.1487], grad_fn=<AddBackward0>)\n",
      "epoch: 3578 loss is tensor([-0.0795], grad_fn=<AddBackward0>)\n",
      "epoch: 3579 loss is tensor([0.0304], grad_fn=<AddBackward0>)\n",
      "epoch: 3580 loss is tensor([-0.0539], grad_fn=<AddBackward0>)\n",
      "epoch: 3581 loss is tensor([0.0071], grad_fn=<AddBackward0>)\n",
      "epoch: 3582 loss is tensor([-0.0006], grad_fn=<AddBackward0>)\n",
      "epoch: 3583 loss is tensor([-0.0131], grad_fn=<AddBackward0>)\n",
      "epoch: 3584 loss is tensor([-0.0944], grad_fn=<AddBackward0>)\n",
      "epoch: 3585 loss is tensor([-0.0844], grad_fn=<AddBackward0>)\n",
      "epoch: 3586 loss is tensor([-0.0482], grad_fn=<AddBackward0>)\n",
      "epoch: 3587 loss is tensor([-0.0921], grad_fn=<AddBackward0>)\n",
      "epoch: 3588 loss is tensor([-0.0484], grad_fn=<AddBackward0>)\n",
      "epoch: 3589 loss is tensor([-0.1240], grad_fn=<AddBackward0>)\n",
      "epoch: 3590 loss is tensor([-0.0848], grad_fn=<AddBackward0>)\n",
      "epoch: 3591 loss is tensor([-0.0658], grad_fn=<AddBackward0>)\n",
      "epoch: 3592 loss is tensor([-0.0659], grad_fn=<AddBackward0>)\n",
      "epoch: 3593 loss is tensor([-0.0178], grad_fn=<AddBackward0>)\n",
      "epoch: 3594 loss is tensor([0.0157], grad_fn=<AddBackward0>)\n",
      "epoch: 3595 loss is tensor([-0.0885], grad_fn=<AddBackward0>)\n",
      "epoch: 3596 loss is tensor([-0.0476], grad_fn=<AddBackward0>)\n",
      "epoch: 3597 loss is tensor([-0.0489], grad_fn=<AddBackward0>)\n",
      "epoch: 3598 loss is tensor([-0.0410], grad_fn=<AddBackward0>)\n",
      "epoch: 3599 loss is tensor([-0.0849], grad_fn=<AddBackward0>)\n",
      "epoch: 3600 loss is tensor([-0.1246], grad_fn=<AddBackward0>)\n",
      "13\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 3601 loss is tensor([-0.0659], grad_fn=<AddBackward0>)\n",
      "epoch: 3602 loss is tensor([-0.1294], grad_fn=<AddBackward0>)\n",
      "epoch: 3603 loss is tensor([-0.1116], grad_fn=<AddBackward0>)\n",
      "epoch: 3604 loss is tensor([-0.0240], grad_fn=<AddBackward0>)\n",
      "epoch: 3605 loss is tensor([-0.0950], grad_fn=<AddBackward0>)\n",
      "epoch: 3606 loss is tensor([-0.1273], grad_fn=<AddBackward0>)\n",
      "epoch: 3607 loss is tensor([-0.0366], grad_fn=<AddBackward0>)\n",
      "epoch: 3608 loss is tensor([-0.0780], grad_fn=<AddBackward0>)\n",
      "epoch: 3609 loss is tensor([-0.0974], grad_fn=<AddBackward0>)\n",
      "epoch: 3610 loss is tensor([-0.1027], grad_fn=<AddBackward0>)\n",
      "epoch: 3611 loss is tensor([-0.0873], grad_fn=<AddBackward0>)\n",
      "epoch: 3612 loss is tensor([-0.1529], grad_fn=<AddBackward0>)\n",
      "epoch: 3613 loss is tensor([-0.0939], grad_fn=<AddBackward0>)\n",
      "epoch: 3614 loss is tensor([-0.0995], grad_fn=<AddBackward0>)\n",
      "epoch: 3615 loss is tensor([-0.0722], grad_fn=<AddBackward0>)\n",
      "epoch: 3616 loss is tensor([-0.1345], grad_fn=<AddBackward0>)\n",
      "epoch: 3617 loss is tensor([-0.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 3618 loss is tensor([-0.1116], grad_fn=<AddBackward0>)\n",
      "epoch: 3619 loss is tensor([-0.0388], grad_fn=<AddBackward0>)\n",
      "epoch: 3620 loss is tensor([-0.1145], grad_fn=<AddBackward0>)\n",
      "epoch: 3621 loss is tensor([-0.0699], grad_fn=<AddBackward0>)\n",
      "epoch: 3622 loss is tensor([-0.0652], grad_fn=<AddBackward0>)\n",
      "epoch: 3623 loss is tensor([-0.0783], grad_fn=<AddBackward0>)\n",
      "epoch: 3624 loss is tensor([-0.0529], grad_fn=<AddBackward0>)\n",
      "epoch: 3625 loss is tensor([-0.0530], grad_fn=<AddBackward0>)\n",
      "epoch: 3626 loss is tensor([-0.0350], grad_fn=<AddBackward0>)\n",
      "epoch: 3627 loss is tensor([-0.0452], grad_fn=<AddBackward0>)\n",
      "epoch: 3628 loss is tensor([-0.1021], grad_fn=<AddBackward0>)\n",
      "epoch: 3629 loss is tensor([-0.0290], grad_fn=<AddBackward0>)\n",
      "epoch: 3630 loss is tensor([-0.1064], grad_fn=<AddBackward0>)\n",
      "epoch: 3631 loss is tensor([-0.1070], grad_fn=<AddBackward0>)\n",
      "epoch: 3632 loss is tensor([-0.0441], grad_fn=<AddBackward0>)\n",
      "epoch: 3633 loss is tensor([-0.0392], grad_fn=<AddBackward0>)\n",
      "epoch: 3634 loss is tensor([-0.0633], grad_fn=<AddBackward0>)\n",
      "epoch: 3635 loss is tensor([-0.0472], grad_fn=<AddBackward0>)\n",
      "epoch: 3636 loss is tensor([-0.0209], grad_fn=<AddBackward0>)\n",
      "epoch: 3637 loss is tensor([-0.0795], grad_fn=<AddBackward0>)\n",
      "epoch: 3638 loss is tensor([-0.1140], grad_fn=<AddBackward0>)\n",
      "epoch: 3639 loss is tensor([-0.0610], grad_fn=<AddBackward0>)\n",
      "epoch: 3640 loss is tensor([-0.0358], grad_fn=<AddBackward0>)\n",
      "epoch: 3641 loss is tensor([-0.0640], grad_fn=<AddBackward0>)\n",
      "epoch: 3642 loss is tensor([-0.0624], grad_fn=<AddBackward0>)\n",
      "epoch: 3643 loss is tensor([-0.0969], grad_fn=<AddBackward0>)\n",
      "epoch: 3644 loss is tensor([-0.1142], grad_fn=<AddBackward0>)\n",
      "epoch: 3645 loss is tensor([-0.0538], grad_fn=<AddBackward0>)\n",
      "epoch: 3646 loss is tensor([-0.1161], grad_fn=<AddBackward0>)\n",
      "epoch: 3647 loss is tensor([-0.0887], grad_fn=<AddBackward0>)\n",
      "epoch: 3648 loss is tensor([-0.1131], grad_fn=<AddBackward0>)\n",
      "epoch: 3649 loss is tensor([-0.1050], grad_fn=<AddBackward0>)\n",
      "epoch: 3650 loss is tensor([-0.0700], grad_fn=<AddBackward0>)\n",
      "epoch: 3651 loss is tensor([-0.0875], grad_fn=<AddBackward0>)\n",
      "epoch: 3652 loss is tensor([-0.0824], grad_fn=<AddBackward0>)\n",
      "epoch: 3653 loss is tensor([-0.1091], grad_fn=<AddBackward0>)\n",
      "epoch: 3654 loss is tensor([-0.1239], grad_fn=<AddBackward0>)\n",
      "epoch: 3655 loss is tensor([-0.0513], grad_fn=<AddBackward0>)\n",
      "epoch: 3656 loss is tensor([-0.0866], grad_fn=<AddBackward0>)\n",
      "epoch: 3657 loss is tensor([-0.0724], grad_fn=<AddBackward0>)\n",
      "epoch: 3658 loss is tensor([-0.0435], grad_fn=<AddBackward0>)\n",
      "epoch: 3659 loss is tensor([-0.0403], grad_fn=<AddBackward0>)\n",
      "epoch: 3660 loss is tensor([-0.0603], grad_fn=<AddBackward0>)\n",
      "epoch: 3661 loss is tensor([-0.0946], grad_fn=<AddBackward0>)\n",
      "epoch: 3662 loss is tensor([-0.1372], grad_fn=<AddBackward0>)\n",
      "epoch: 3663 loss is tensor([-0.0689], grad_fn=<AddBackward0>)\n",
      "epoch: 3664 loss is tensor([-0.0683], grad_fn=<AddBackward0>)\n",
      "epoch: 3665 loss is tensor([-0.1113], grad_fn=<AddBackward0>)\n",
      "epoch: 3666 loss is tensor([-0.1202], grad_fn=<AddBackward0>)\n",
      "epoch: 3667 loss is tensor([-0.0684], grad_fn=<AddBackward0>)\n",
      "epoch: 3668 loss is tensor([-0.1177], grad_fn=<AddBackward0>)\n",
      "epoch: 3669 loss is tensor([-0.0916], grad_fn=<AddBackward0>)\n",
      "epoch: 3670 loss is tensor([-0.0977], grad_fn=<AddBackward0>)\n",
      "epoch: 3671 loss is tensor([-0.0882], grad_fn=<AddBackward0>)\n",
      "epoch: 3672 loss is tensor([-0.0674], grad_fn=<AddBackward0>)\n",
      "epoch: 3673 loss is tensor([-0.1170], grad_fn=<AddBackward0>)\n",
      "epoch: 3674 loss is tensor([-0.1013], grad_fn=<AddBackward0>)\n",
      "epoch: 3675 loss is tensor([-0.0669], grad_fn=<AddBackward0>)\n",
      "epoch: 3676 loss is tensor([-0.1207], grad_fn=<AddBackward0>)\n",
      "epoch: 3677 loss is tensor([-0.0871], grad_fn=<AddBackward0>)\n",
      "epoch: 3678 loss is tensor([-0.1040], grad_fn=<AddBackward0>)\n",
      "epoch: 3679 loss is tensor([-0.0950], grad_fn=<AddBackward0>)\n",
      "epoch: 3680 loss is tensor([-0.1344], grad_fn=<AddBackward0>)\n",
      "epoch: 3681 loss is tensor([-0.0905], grad_fn=<AddBackward0>)\n",
      "epoch: 3682 loss is tensor([-0.0738], grad_fn=<AddBackward0>)\n",
      "epoch: 3683 loss is tensor([-0.0917], grad_fn=<AddBackward0>)\n",
      "epoch: 3684 loss is tensor([-0.0468], grad_fn=<AddBackward0>)\n",
      "epoch: 3685 loss is tensor([-0.1176], grad_fn=<AddBackward0>)\n",
      "epoch: 3686 loss is tensor([-0.0692], grad_fn=<AddBackward0>)\n",
      "epoch: 3687 loss is tensor([-0.0010], grad_fn=<AddBackward0>)\n",
      "epoch: 3688 loss is tensor([-0.0435], grad_fn=<AddBackward0>)\n",
      "epoch: 3689 loss is tensor([-0.0536], grad_fn=<AddBackward0>)\n",
      "epoch: 3690 loss is tensor([-0.0594], grad_fn=<AddBackward0>)\n",
      "epoch: 3691 loss is tensor([-0.1084], grad_fn=<AddBackward0>)\n",
      "epoch: 3692 loss is tensor([-0.0907], grad_fn=<AddBackward0>)\n",
      "epoch: 3693 loss is tensor([-0.0526], grad_fn=<AddBackward0>)\n",
      "epoch: 3694 loss is tensor([-0.0898], grad_fn=<AddBackward0>)\n",
      "epoch: 3695 loss is tensor([-0.1243], grad_fn=<AddBackward0>)\n",
      "epoch: 3696 loss is tensor([-0.0831], grad_fn=<AddBackward0>)\n",
      "epoch: 3697 loss is tensor([-0.0931], grad_fn=<AddBackward0>)\n",
      "epoch: 3698 loss is tensor([-0.0872], grad_fn=<AddBackward0>)\n",
      "epoch: 3699 loss is tensor([-0.0297], grad_fn=<AddBackward0>)\n",
      "epoch: 3700 loss is tensor([-0.0313], grad_fn=<AddBackward0>)\n",
      "10\n"
=======
      "The number of epochs is: 3201\n",
      "The number of epochs is: 3202\n",
      "The number of epochs is: 3203\n",
      "The number of epochs is: 3204\n",
      "The number of epochs is: 3205\n",
      "The number of epochs is: 3206\n",
      "The number of epochs is: 3207\n",
      "The number of epochs is: 3208\n",
      "The number of epochs is: 3209\n",
      "The number of epochs is: 3210\n",
      "The number of epochs is: 3211\n",
      "The number of epochs is: 3212\n",
      "The number of epochs is: 3213\n",
      "The number of epochs is: 3214\n",
      "The number of epochs is: 3215\n",
      "The number of epochs is: 3216\n",
      "The number of epochs is: 3217\n",
      "The number of epochs is: 3218\n",
      "The number of epochs is: 3219\n",
      "The number of epochs is: 3220\n",
      "The number of epochs is: 3221\n",
      "The number of epochs is: 3222\n",
      "The number of epochs is: 3223\n",
      "The number of epochs is: 3224\n",
      "The number of epochs is: 3225\n",
      "The number of epochs is: 3226\n",
      "The number of epochs is: 3227\n",
      "The number of epochs is: 3228\n",
      "The number of epochs is: 3229\n",
      "The number of epochs is: 3230\n",
      "The number of epochs is: 3231\n",
      "The number of epochs is: 3232\n",
      "The number of epochs is: 3233\n",
      "The number of epochs is: 3234\n",
      "The number of epochs is: 3235\n",
      "The number of epochs is: 3236\n",
      "The number of epochs is: 3237\n",
      "The number of epochs is: 3238\n",
      "The number of epochs is: 3239\n",
      "The number of epochs is: 3240\n",
      "The number of epochs is: 3241\n",
      "The number of epochs is: 3242\n",
      "The number of epochs is: 3243\n",
      "The number of epochs is: 3244\n",
      "The number of epochs is: 3245\n",
      "The number of epochs is: 3246\n",
      "The number of epochs is: 3247\n",
      "The number of epochs is: 3248\n",
      "The number of epochs is: 3249\n",
      "The number of epochs is: 3250\n",
      "The number of epochs is: 3251\n",
      "The number of epochs is: 3252\n",
      "The number of epochs is: 3253\n",
      "The number of epochs is: 3254\n",
      "The number of epochs is: 3255\n",
      "The number of epochs is: 3256\n",
      "The number of epochs is: 3257\n",
      "The number of epochs is: 3258\n",
      "The number of epochs is: 3259\n",
      "The number of epochs is: 3260\n",
      "The number of epochs is: 3261\n",
      "The number of epochs is: 3262\n",
      "The number of epochs is: 3263\n",
      "The number of epochs is: 3264\n",
      "The number of epochs is: 3265\n",
      "The number of epochs is: 3266\n",
      "The number of epochs is: 3267\n",
      "The number of epochs is: 3268\n",
      "The number of epochs is: 3269\n",
      "The number of epochs is: 3270\n",
      "The number of epochs is: 3271\n",
      "The number of epochs is: 3272\n",
      "The number of epochs is: 3273\n",
      "The number of epochs is: 3274\n",
      "The number of epochs is: 3275\n",
      "The number of epochs is: 3276\n",
      "The number of epochs is: 3277\n",
      "The number of epochs is: 3278\n",
      "The number of epochs is: 3279\n",
      "The number of epochs is: 3280\n",
      "The number of epochs is: 3281\n",
      "The number of epochs is: 3282\n",
      "The number of epochs is: 3283\n",
      "The number of epochs is: 3284\n",
      "The number of epochs is: 3285\n",
      "The number of epochs is: 3286\n",
      "The number of epochs is: 3287\n",
      "The number of epochs is: 3288\n",
      "The number of epochs is: 3289\n",
      "The number of epochs is: 3290\n",
      "The number of epochs is: 3291\n",
      "The number of epochs is: 3292\n",
      "The number of epochs is: 3293\n",
      "The number of epochs is: 3294\n",
      "The number of epochs is: 3295\n",
      "The number of epochs is: 3296\n",
      "The number of epochs is: 3297\n",
      "The number of epochs is: 3298\n",
      "The number of epochs is: 3299\n",
      "The number of epochs is: 3300\n",
      "The number of epochs is: 3301\n",
      "The number of epochs is: 3302\n",
      "The number of epochs is: 3303\n",
      "The number of epochs is: 3304\n",
      "The number of epochs is: 3305\n",
      "The number of epochs is: 3306\n",
      "The number of epochs is: 3307\n",
      "The number of epochs is: 3308\n",
      "The number of epochs is: 3309\n",
      "The number of epochs is: 3310\n",
      "The number of epochs is: 3311\n",
      "The number of epochs is: 3312\n",
      "The number of epochs is: 3313\n",
      "The number of epochs is: 3314\n",
      "The number of epochs is: 3315\n",
      "The number of epochs is: 3316\n",
      "The number of epochs is: 3317\n",
      "The number of epochs is: 3318\n",
      "The number of epochs is: 3319\n",
      "The number of epochs is: 3320\n",
      "The number of epochs is: 3321\n",
      "The number of epochs is: 3322\n",
      "The number of epochs is: 3323\n",
      "The number of epochs is: 3324\n",
      "The number of epochs is: 3325\n",
      "The number of epochs is: 3326\n",
      "The number of epochs is: 3327\n",
      "The number of epochs is: 3328\n",
      "The number of epochs is: 3329\n",
      "The number of epochs is: 3330\n",
      "The number of epochs is: 3331\n",
      "The number of epochs is: 3332\n",
      "The number of epochs is: 3333\n",
      "The number of epochs is: 3334\n",
      "The number of epochs is: 3335\n",
      "The number of epochs is: 3336\n",
      "The number of epochs is: 3337\n",
      "The number of epochs is: 3338\n",
      "The number of epochs is: 3339\n",
      "The number of epochs is: 3340\n",
      "The number of epochs is: 3341\n",
      "The number of epochs is: 3342\n",
      "The number of epochs is: 3343\n",
      "The number of epochs is: 3344\n",
      "The number of epochs is: 3345\n",
      "The number of epochs is: 3346\n",
      "The number of epochs is: 3347\n",
      "The number of epochs is: 3348\n",
      "The number of epochs is: 3349\n",
      "The number of epochs is: 3350\n",
      "The number of epochs is: 3351\n",
      "The number of epochs is: 3352\n",
      "The number of epochs is: 3353\n",
      "The number of epochs is: 3354\n",
      "The number of epochs is: 3355\n",
      "The number of epochs is: 3356\n",
      "The number of epochs is: 3357\n",
      "The number of epochs is: 3358\n",
      "The number of epochs is: 3359\n",
      "The number of epochs is: 3360\n",
      "The number of epochs is: 3361\n",
      "The number of epochs is: 3362\n",
      "The number of epochs is: 3363\n",
      "The number of epochs is: 3364\n",
      "The number of epochs is: 3365\n",
      "The number of epochs is: 3366\n",
      "The number of epochs is: 3367\n",
      "The number of epochs is: 3368\n",
      "The number of epochs is: 3369\n",
      "The number of epochs is: 3370\n",
      "The number of epochs is: 3371\n",
      "The number of epochs is: 3372\n",
      "The number of epochs is: 3373\n",
      "The number of epochs is: 3374\n",
      "The number of epochs is: 3375\n",
      "The number of epochs is: 3376\n",
      "The number of epochs is: 3377\n",
      "The number of epochs is: 3378\n",
      "The number of epochs is: 3379\n",
      "The number of epochs is: 3380\n",
      "The number of epochs is: 3381\n",
      "The number of epochs is: 3382\n",
      "The number of epochs is: 3383\n",
      "The number of epochs is: 3384\n",
      "The number of epochs is: 3385\n",
      "The number of epochs is: 3386\n",
      "The number of epochs is: 3387\n",
      "The number of epochs is: 3388\n",
      "The number of epochs is: 3389\n",
      "The number of epochs is: 3390\n",
      "The number of epochs is: 3391\n",
      "The number of epochs is: 3392\n",
      "The number of epochs is: 3393\n",
      "The number of epochs is: 3394\n",
      "The number of epochs is: 3395\n",
      "The number of epochs is: 3396\n",
      "The number of epochs is: 3397\n",
      "The number of epochs is: 3398\n",
      "The number of epochs is: 3399\n",
      "The number of epochs is: 3400\n",
      "11\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 3701 loss is tensor([0.0058], grad_fn=<AddBackward0>)\n",
      "epoch: 3702 loss is tensor([-0.0887], grad_fn=<AddBackward0>)\n",
      "epoch: 3703 loss is tensor([-0.0617], grad_fn=<AddBackward0>)\n",
      "epoch: 3704 loss is tensor([-0.0551], grad_fn=<AddBackward0>)\n",
      "epoch: 3705 loss is tensor([-0.1155], grad_fn=<AddBackward0>)\n",
      "epoch: 3706 loss is tensor([-0.0300], grad_fn=<AddBackward0>)\n",
      "epoch: 3707 loss is tensor([-0.0788], grad_fn=<AddBackward0>)\n",
      "epoch: 3708 loss is tensor([-0.0657], grad_fn=<AddBackward0>)\n",
      "epoch: 3709 loss is tensor([-0.1340], grad_fn=<AddBackward0>)\n",
      "epoch: 3710 loss is tensor([-0.0710], grad_fn=<AddBackward0>)\n",
      "epoch: 3711 loss is tensor([-0.0830], grad_fn=<AddBackward0>)\n",
      "epoch: 3712 loss is tensor([-0.1020], grad_fn=<AddBackward0>)\n",
      "epoch: 3713 loss is tensor([-0.0354], grad_fn=<AddBackward0>)\n",
      "epoch: 3714 loss is tensor([0.0073], grad_fn=<AddBackward0>)\n",
      "epoch: 3715 loss is tensor([-0.0713], grad_fn=<AddBackward0>)\n",
      "epoch: 3716 loss is tensor([-0.0759], grad_fn=<AddBackward0>)\n",
      "epoch: 3717 loss is tensor([-0.1144], grad_fn=<AddBackward0>)\n",
      "epoch: 3718 loss is tensor([-0.1162], grad_fn=<AddBackward0>)\n",
      "epoch: 3719 loss is tensor([-0.0607], grad_fn=<AddBackward0>)\n",
      "epoch: 3720 loss is tensor([-0.0194], grad_fn=<AddBackward0>)\n",
      "epoch: 3721 loss is tensor([-0.0355], grad_fn=<AddBackward0>)\n",
      "epoch: 3722 loss is tensor([-0.0399], grad_fn=<AddBackward0>)\n",
      "epoch: 3723 loss is tensor([0.0425], grad_fn=<AddBackward0>)\n",
      "epoch: 3724 loss is tensor([-0.0333], grad_fn=<AddBackward0>)\n",
      "epoch: 3725 loss is tensor([-0.0397], grad_fn=<AddBackward0>)\n",
      "epoch: 3726 loss is tensor([-0.0772], grad_fn=<AddBackward0>)\n",
      "epoch: 3727 loss is tensor([-0.0661], grad_fn=<AddBackward0>)\n",
      "epoch: 3728 loss is tensor([-0.0895], grad_fn=<AddBackward0>)\n",
      "epoch: 3729 loss is tensor([-0.1049], grad_fn=<AddBackward0>)\n",
      "epoch: 3730 loss is tensor([-0.0823], grad_fn=<AddBackward0>)\n",
      "epoch: 3731 loss is tensor([-0.0983], grad_fn=<AddBackward0>)\n",
      "epoch: 3732 loss is tensor([-0.1292], grad_fn=<AddBackward0>)\n",
      "epoch: 3733 loss is tensor([-0.0383], grad_fn=<AddBackward0>)\n",
      "epoch: 3734 loss is tensor([-0.1359], grad_fn=<AddBackward0>)\n",
      "epoch: 3735 loss is tensor([-0.0923], grad_fn=<AddBackward0>)\n",
      "epoch: 3736 loss is tensor([-0.0573], grad_fn=<AddBackward0>)\n",
      "epoch: 3737 loss is tensor([0.0427], grad_fn=<AddBackward0>)\n",
      "epoch: 3738 loss is tensor([-0.0301], grad_fn=<AddBackward0>)\n",
      "epoch: 3739 loss is tensor([-0.1161], grad_fn=<AddBackward0>)\n",
      "epoch: 3740 loss is tensor([-0.0829], grad_fn=<AddBackward0>)\n",
      "epoch: 3741 loss is tensor([-0.0173], grad_fn=<AddBackward0>)\n",
      "epoch: 3742 loss is tensor([-0.1217], grad_fn=<AddBackward0>)\n",
      "epoch: 3743 loss is tensor([-0.0767], grad_fn=<AddBackward0>)\n",
      "epoch: 3744 loss is tensor([-0.1031], grad_fn=<AddBackward0>)\n",
      "epoch: 3745 loss is tensor([-0.0573], grad_fn=<AddBackward0>)\n",
      "epoch: 3746 loss is tensor([-0.1215], grad_fn=<AddBackward0>)\n",
      "epoch: 3747 loss is tensor([-0.0049], grad_fn=<AddBackward0>)\n",
      "epoch: 3748 loss is tensor([-0.0791], grad_fn=<AddBackward0>)\n",
      "epoch: 3749 loss is tensor([-0.0428], grad_fn=<AddBackward0>)\n",
      "epoch: 3750 loss is tensor([-0.0792], grad_fn=<AddBackward0>)\n",
      "epoch: 3751 loss is tensor([0.0423], grad_fn=<AddBackward0>)\n",
      "epoch: 3752 loss is tensor([-0.0746], grad_fn=<AddBackward0>)\n",
      "epoch: 3753 loss is tensor([-0.1028], grad_fn=<AddBackward0>)\n",
      "epoch: 3754 loss is tensor([-0.0575], grad_fn=<AddBackward0>)\n",
      "epoch: 3755 loss is tensor([-0.0486], grad_fn=<AddBackward0>)\n",
      "epoch: 3756 loss is tensor([-0.1102], grad_fn=<AddBackward0>)\n",
      "epoch: 3757 loss is tensor([-0.0471], grad_fn=<AddBackward0>)\n",
      "epoch: 3758 loss is tensor([-0.0842], grad_fn=<AddBackward0>)\n",
      "epoch: 3759 loss is tensor([-0.0915], grad_fn=<AddBackward0>)\n",
      "epoch: 3760 loss is tensor([-0.1063], grad_fn=<AddBackward0>)\n",
      "epoch: 3761 loss is tensor([-0.1080], grad_fn=<AddBackward0>)\n",
      "epoch: 3762 loss is tensor([-0.0007], grad_fn=<AddBackward0>)\n",
      "epoch: 3763 loss is tensor([-0.0074], grad_fn=<AddBackward0>)\n",
      "epoch: 3764 loss is tensor([-0.0973], grad_fn=<AddBackward0>)\n",
      "epoch: 3765 loss is tensor([0.0257], grad_fn=<AddBackward0>)\n",
      "epoch: 3766 loss is tensor([0.0017], grad_fn=<AddBackward0>)\n",
      "epoch: 3767 loss is tensor([-0.0368], grad_fn=<AddBackward0>)\n",
      "epoch: 3768 loss is tensor([-0.0453], grad_fn=<AddBackward0>)\n",
      "epoch: 3769 loss is tensor([-0.0359], grad_fn=<AddBackward0>)\n",
      "epoch: 3770 loss is tensor([-0.0617], grad_fn=<AddBackward0>)\n",
      "epoch: 3771 loss is tensor([-0.0175], grad_fn=<AddBackward0>)\n",
      "epoch: 3772 loss is tensor([-0.1124], grad_fn=<AddBackward0>)\n",
      "epoch: 3773 loss is tensor([-0.0623], grad_fn=<AddBackward0>)\n",
      "epoch: 3774 loss is tensor([-0.0664], grad_fn=<AddBackward0>)\n",
      "epoch: 3775 loss is tensor([-0.0472], grad_fn=<AddBackward0>)\n",
      "epoch: 3776 loss is tensor([-0.0991], grad_fn=<AddBackward0>)\n",
      "epoch: 3777 loss is tensor([-0.0358], grad_fn=<AddBackward0>)\n",
      "epoch: 3778 loss is tensor([-0.0805], grad_fn=<AddBackward0>)\n",
      "epoch: 3779 loss is tensor([-0.0883], grad_fn=<AddBackward0>)\n",
      "epoch: 3780 loss is tensor([-0.1166], grad_fn=<AddBackward0>)\n",
      "epoch: 3781 loss is tensor([-0.0238], grad_fn=<AddBackward0>)\n",
      "epoch: 3782 loss is tensor([-0.0759], grad_fn=<AddBackward0>)\n",
      "epoch: 3783 loss is tensor([-0.0952], grad_fn=<AddBackward0>)\n",
      "epoch: 3784 loss is tensor([-0.0588], grad_fn=<AddBackward0>)\n",
      "epoch: 3785 loss is tensor([-0.0383], grad_fn=<AddBackward0>)\n",
      "epoch: 3786 loss is tensor([-0.0089], grad_fn=<AddBackward0>)\n",
      "epoch: 3787 loss is tensor([0.0565], grad_fn=<AddBackward0>)\n",
      "epoch: 3788 loss is tensor([-0.0182], grad_fn=<AddBackward0>)\n",
      "epoch: 3789 loss is tensor([-0.0565], grad_fn=<AddBackward0>)\n",
      "epoch: 3790 loss is tensor([-0.0769], grad_fn=<AddBackward0>)\n",
      "epoch: 3791 loss is tensor([-0.0913], grad_fn=<AddBackward0>)\n",
      "epoch: 3792 loss is tensor([-0.0278], grad_fn=<AddBackward0>)\n",
      "epoch: 3793 loss is tensor([-0.0199], grad_fn=<AddBackward0>)\n",
      "epoch: 3794 loss is tensor([-0.0391], grad_fn=<AddBackward0>)\n",
      "epoch: 3795 loss is tensor([-0.0561], grad_fn=<AddBackward0>)\n",
      "epoch: 3796 loss is tensor([-0.1090], grad_fn=<AddBackward0>)\n",
      "epoch: 3797 loss is tensor([-0.0905], grad_fn=<AddBackward0>)\n",
      "epoch: 3798 loss is tensor([-0.0209], grad_fn=<AddBackward0>)\n",
      "epoch: 3799 loss is tensor([-0.0927], grad_fn=<AddBackward0>)\n",
      "epoch: 3800 loss is tensor([-0.0894], grad_fn=<AddBackward0>)\n",
      "28\n"
=======
      "The number of epochs is: 3401\n",
      "The number of epochs is: 3402\n",
      "The number of epochs is: 3403\n",
      "The number of epochs is: 3404\n",
      "The number of epochs is: 3405\n",
      "The number of epochs is: 3406\n",
      "The number of epochs is: 3407\n",
      "The number of epochs is: 3408\n",
      "The number of epochs is: 3409\n",
      "The number of epochs is: 3410\n",
      "The number of epochs is: 3411\n",
      "The number of epochs is: 3412\n",
      "The number of epochs is: 3413\n",
      "The number of epochs is: 3414\n",
      "The number of epochs is: 3415\n",
      "The number of epochs is: 3416\n",
      "The number of epochs is: 3417\n",
      "The number of epochs is: 3418\n",
      "The number of epochs is: 3419\n",
      "The number of epochs is: 3420\n",
      "The number of epochs is: 3421\n",
      "The number of epochs is: 3422\n",
      "The number of epochs is: 3423\n",
      "The number of epochs is: 3424\n",
      "The number of epochs is: 3425\n",
      "The number of epochs is: 3426\n",
      "The number of epochs is: 3427\n",
      "The number of epochs is: 3428\n",
      "The number of epochs is: 3429\n",
      "The number of epochs is: 3430\n",
      "The number of epochs is: 3431\n",
      "The number of epochs is: 3432\n",
      "The number of epochs is: 3433\n",
      "The number of epochs is: 3434\n",
      "The number of epochs is: 3435\n",
      "The number of epochs is: 3436\n",
      "The number of epochs is: 3437\n",
      "The number of epochs is: 3438\n",
      "The number of epochs is: 3439\n",
      "The number of epochs is: 3440\n",
      "The number of epochs is: 3441\n",
      "The number of epochs is: 3442\n",
      "The number of epochs is: 3443\n",
      "The number of epochs is: 3444\n",
      "The number of epochs is: 3445\n",
      "The number of epochs is: 3446\n",
      "The number of epochs is: 3447\n",
      "The number of epochs is: 3448\n",
      "The number of epochs is: 3449\n",
      "The number of epochs is: 3450\n",
      "The number of epochs is: 3451\n",
      "The number of epochs is: 3452\n",
      "The number of epochs is: 3453\n",
      "The number of epochs is: 3454\n",
      "The number of epochs is: 3455\n",
      "The number of epochs is: 3456\n",
      "The number of epochs is: 3457\n",
      "The number of epochs is: 3458\n",
      "The number of epochs is: 3459\n",
      "The number of epochs is: 3460\n",
      "The number of epochs is: 3461\n",
      "The number of epochs is: 3462\n",
      "The number of epochs is: 3463\n",
      "The number of epochs is: 3464\n",
      "The number of epochs is: 3465\n",
      "The number of epochs is: 3466\n",
      "The number of epochs is: 3467\n",
      "The number of epochs is: 3468\n",
      "The number of epochs is: 3469\n",
      "The number of epochs is: 3470\n",
      "The number of epochs is: 3471\n",
      "The number of epochs is: 3472\n",
      "The number of epochs is: 3473\n",
      "The number of epochs is: 3474\n",
      "The number of epochs is: 3475\n",
      "The number of epochs is: 3476\n",
      "The number of epochs is: 3477\n",
      "The number of epochs is: 3478\n",
      "The number of epochs is: 3479\n",
      "The number of epochs is: 3480\n",
      "The number of epochs is: 3481\n",
      "The number of epochs is: 3482\n",
      "The number of epochs is: 3483\n",
      "The number of epochs is: 3484\n",
      "The number of epochs is: 3485\n",
      "The number of epochs is: 3486\n",
      "The number of epochs is: 3487\n",
      "The number of epochs is: 3488\n",
      "The number of epochs is: 3489\n",
      "The number of epochs is: 3490\n",
      "The number of epochs is: 3491\n",
      "The number of epochs is: 3492\n",
      "The number of epochs is: 3493\n",
      "The number of epochs is: 3494\n",
      "The number of epochs is: 3495\n",
      "The number of epochs is: 3496\n",
      "The number of epochs is: 3497\n",
      "The number of epochs is: 3498\n",
      "The number of epochs is: 3499\n",
      "The number of epochs is: 3500\n",
      "The number of epochs is: 3501\n",
      "The number of epochs is: 3502\n",
      "The number of epochs is: 3503\n",
      "The number of epochs is: 3504\n",
      "The number of epochs is: 3505\n",
      "The number of epochs is: 3506\n",
      "The number of epochs is: 3507\n",
      "The number of epochs is: 3508\n",
      "The number of epochs is: 3509\n",
      "The number of epochs is: 3510\n",
      "The number of epochs is: 3511\n",
      "The number of epochs is: 3512\n",
      "The number of epochs is: 3513\n",
      "The number of epochs is: 3514\n",
      "The number of epochs is: 3515\n",
      "The number of epochs is: 3516\n",
      "The number of epochs is: 3517\n",
      "The number of epochs is: 3518\n",
      "The number of epochs is: 3519\n",
      "The number of epochs is: 3520\n",
      "The number of epochs is: 3521\n",
      "The number of epochs is: 3522\n",
      "The number of epochs is: 3523\n",
      "The number of epochs is: 3524\n",
      "The number of epochs is: 3525\n",
      "The number of epochs is: 3526\n",
      "The number of epochs is: 3527\n",
      "The number of epochs is: 3528\n",
      "The number of epochs is: 3529\n",
      "The number of epochs is: 3530\n",
      "The number of epochs is: 3531\n",
      "The number of epochs is: 3532\n",
      "The number of epochs is: 3533\n",
      "The number of epochs is: 3534\n",
      "The number of epochs is: 3535\n",
      "The number of epochs is: 3536\n",
      "The number of epochs is: 3537\n",
      "The number of epochs is: 3538\n",
      "The number of epochs is: 3539\n",
      "The number of epochs is: 3540\n",
      "The number of epochs is: 3541\n",
      "The number of epochs is: 3542\n",
      "The number of epochs is: 3543\n",
      "The number of epochs is: 3544\n",
      "The number of epochs is: 3545\n",
      "The number of epochs is: 3546\n",
      "The number of epochs is: 3547\n",
      "The number of epochs is: 3548\n",
      "The number of epochs is: 3549\n",
      "The number of epochs is: 3550\n",
      "The number of epochs is: 3551\n",
      "The number of epochs is: 3552\n",
      "The number of epochs is: 3553\n",
      "The number of epochs is: 3554\n",
      "The number of epochs is: 3555\n",
      "The number of epochs is: 3556\n",
      "The number of epochs is: 3557\n",
      "The number of epochs is: 3558\n",
      "The number of epochs is: 3559\n",
      "The number of epochs is: 3560\n",
      "The number of epochs is: 3561\n",
      "The number of epochs is: 3562\n",
      "The number of epochs is: 3563\n",
      "The number of epochs is: 3564\n",
      "The number of epochs is: 3565\n",
      "The number of epochs is: 3566\n",
      "The number of epochs is: 3567\n",
      "The number of epochs is: 3568\n",
      "The number of epochs is: 3569\n",
      "The number of epochs is: 3570\n",
      "The number of epochs is: 3571\n",
      "The number of epochs is: 3572\n",
      "The number of epochs is: 3573\n",
      "The number of epochs is: 3574\n",
      "The number of epochs is: 3575\n",
      "The number of epochs is: 3576\n",
      "The number of epochs is: 3577\n",
      "The number of epochs is: 3578\n",
      "The number of epochs is: 3579\n",
      "The number of epochs is: 3580\n",
      "The number of epochs is: 3581\n",
      "The number of epochs is: 3582\n",
      "The number of epochs is: 3583\n",
      "The number of epochs is: 3584\n",
      "The number of epochs is: 3585\n",
      "The number of epochs is: 3586\n",
      "The number of epochs is: 3587\n",
      "The number of epochs is: 3588\n",
      "The number of epochs is: 3589\n",
      "The number of epochs is: 3590\n",
      "The number of epochs is: 3591\n",
      "The number of epochs is: 3592\n",
      "The number of epochs is: 3593\n",
      "The number of epochs is: 3594\n",
      "The number of epochs is: 3595\n",
      "The number of epochs is: 3596\n",
      "The number of epochs is: 3597\n",
      "The number of epochs is: 3598\n",
      "The number of epochs is: 3599\n",
      "The number of epochs is: 3600\n",
      "53\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3801 loss is tensor([-0.0696], grad_fn=<AddBackward0>)\n",
      "epoch: 3802 loss is tensor([-0.0907], grad_fn=<AddBackward0>)\n",
      "epoch: 3803 loss is tensor([-0.1406], grad_fn=<AddBackward0>)\n",
      "epoch: 3804 loss is tensor([-0.1477], grad_fn=<AddBackward0>)\n",
      "epoch: 3805 loss is tensor([-0.1362], grad_fn=<AddBackward0>)\n",
      "epoch: 3806 loss is tensor([-0.0969], grad_fn=<AddBackward0>)\n",
      "epoch: 3807 loss is tensor([-0.0308], grad_fn=<AddBackward0>)\n",
      "epoch: 3808 loss is tensor([-0.0973], grad_fn=<AddBackward0>)\n",
      "epoch: 3809 loss is tensor([-0.1171], grad_fn=<AddBackward0>)\n",
      "epoch: 3810 loss is tensor([-0.0853], grad_fn=<AddBackward0>)\n",
      "epoch: 3811 loss is tensor([-0.0981], grad_fn=<AddBackward0>)\n",
      "epoch: 3812 loss is tensor([-0.1088], grad_fn=<AddBackward0>)\n",
      "epoch: 3813 loss is tensor([-0.1031], grad_fn=<AddBackward0>)\n",
      "epoch: 3814 loss is tensor([-0.0838], grad_fn=<AddBackward0>)\n",
      "epoch: 3815 loss is tensor([-0.1109], grad_fn=<AddBackward0>)\n",
      "epoch: 3816 loss is tensor([-0.0888], grad_fn=<AddBackward0>)\n",
      "epoch: 3817 loss is tensor([-0.1012], grad_fn=<AddBackward0>)\n",
      "epoch: 3818 loss is tensor([-0.0954], grad_fn=<AddBackward0>)\n",
      "epoch: 3819 loss is tensor([-0.0612], grad_fn=<AddBackward0>)\n",
      "epoch: 3820 loss is tensor([-0.0794], grad_fn=<AddBackward0>)\n",
      "epoch: 3821 loss is tensor([-0.0694], grad_fn=<AddBackward0>)\n",
      "epoch: 3822 loss is tensor([-0.1484], grad_fn=<AddBackward0>)\n",
      "epoch: 3823 loss is tensor([-0.0837], grad_fn=<AddBackward0>)\n",
      "epoch: 3824 loss is tensor([-0.1125], grad_fn=<AddBackward0>)\n",
      "epoch: 3825 loss is tensor([-0.0749], grad_fn=<AddBackward0>)\n",
      "epoch: 3826 loss is tensor([-0.1262], grad_fn=<AddBackward0>)\n",
      "epoch: 3827 loss is tensor([-0.0515], grad_fn=<AddBackward0>)\n",
      "epoch: 3828 loss is tensor([-0.0743], grad_fn=<AddBackward0>)\n",
      "epoch: 3829 loss is tensor([-0.1086], grad_fn=<AddBackward0>)\n",
      "epoch: 3830 loss is tensor([-0.1203], grad_fn=<AddBackward0>)\n",
      "epoch: 3831 loss is tensor([-0.0872], grad_fn=<AddBackward0>)\n",
      "epoch: 3832 loss is tensor([-0.1014], grad_fn=<AddBackward0>)\n",
      "epoch: 3833 loss is tensor([-0.0949], grad_fn=<AddBackward0>)\n",
      "epoch: 3834 loss is tensor([-0.1087], grad_fn=<AddBackward0>)\n",
      "epoch: 3835 loss is tensor([-0.1480], grad_fn=<AddBackward0>)\n",
      "epoch: 3836 loss is tensor([-0.1182], grad_fn=<AddBackward0>)\n",
      "epoch: 3837 loss is tensor([-0.1150], grad_fn=<AddBackward0>)\n",
      "epoch: 3838 loss is tensor([-0.1228], grad_fn=<AddBackward0>)\n",
      "epoch: 3839 loss is tensor([-0.0442], grad_fn=<AddBackward0>)\n",
      "epoch: 3840 loss is tensor([-0.1557], grad_fn=<AddBackward0>)\n",
      "epoch: 3841 loss is tensor([-0.1170], grad_fn=<AddBackward0>)\n",
      "epoch: 3842 loss is tensor([-0.1616], grad_fn=<AddBackward0>)\n",
      "epoch: 3843 loss is tensor([-0.1004], grad_fn=<AddBackward0>)\n",
      "epoch: 3844 loss is tensor([-0.1112], grad_fn=<AddBackward0>)\n",
      "epoch: 3845 loss is tensor([-0.1204], grad_fn=<AddBackward0>)\n",
      "epoch: 3846 loss is tensor([-0.0431], grad_fn=<AddBackward0>)\n",
      "epoch: 3847 loss is tensor([-0.1107], grad_fn=<AddBackward0>)\n",
      "epoch: 3848 loss is tensor([-0.1617], grad_fn=<AddBackward0>)\n",
      "epoch: 3849 loss is tensor([-0.1227], grad_fn=<AddBackward0>)\n",
      "epoch: 3850 loss is tensor([-0.1537], grad_fn=<AddBackward0>)\n",
      "epoch: 3851 loss is tensor([-0.0747], grad_fn=<AddBackward0>)\n",
      "epoch: 3852 loss is tensor([-0.1511], grad_fn=<AddBackward0>)\n",
      "epoch: 3853 loss is tensor([-0.1371], grad_fn=<AddBackward0>)\n",
      "epoch: 3854 loss is tensor([-0.1085], grad_fn=<AddBackward0>)\n",
      "epoch: 3855 loss is tensor([-0.0973], grad_fn=<AddBackward0>)\n",
      "epoch: 3856 loss is tensor([-0.1325], grad_fn=<AddBackward0>)\n",
      "epoch: 3857 loss is tensor([-0.1242], grad_fn=<AddBackward0>)\n",
      "epoch: 3858 loss is tensor([-0.0773], grad_fn=<AddBackward0>)\n",
      "epoch: 3859 loss is tensor([-0.1293], grad_fn=<AddBackward0>)\n",
      "epoch: 3860 loss is tensor([-0.1258], grad_fn=<AddBackward0>)\n",
      "epoch: 3861 loss is tensor([-0.1283], grad_fn=<AddBackward0>)\n",
      "epoch: 3862 loss is tensor([-0.1343], grad_fn=<AddBackward0>)\n",
      "epoch: 3863 loss is tensor([-0.1101], grad_fn=<AddBackward0>)\n",
      "epoch: 3864 loss is tensor([-0.0839], grad_fn=<AddBackward0>)\n",
      "epoch: 3865 loss is tensor([-0.1112], grad_fn=<AddBackward0>)\n",
      "epoch: 3866 loss is tensor([-0.1195], grad_fn=<AddBackward0>)\n",
      "epoch: 3867 loss is tensor([-0.0677], grad_fn=<AddBackward0>)\n",
      "epoch: 3868 loss is tensor([-0.1023], grad_fn=<AddBackward0>)\n",
      "epoch: 3869 loss is tensor([-0.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 3870 loss is tensor([-0.1207], grad_fn=<AddBackward0>)\n",
      "epoch: 3871 loss is tensor([-0.0920], grad_fn=<AddBackward0>)\n",
      "epoch: 3872 loss is tensor([-0.0727], grad_fn=<AddBackward0>)\n",
      "epoch: 3873 loss is tensor([-0.0911], grad_fn=<AddBackward0>)\n",
      "epoch: 3874 loss is tensor([-0.0969], grad_fn=<AddBackward0>)\n",
      "epoch: 3875 loss is tensor([-0.0894], grad_fn=<AddBackward0>)\n",
      "epoch: 3876 loss is tensor([-0.1126], grad_fn=<AddBackward0>)\n",
      "epoch: 3877 loss is tensor([-0.1318], grad_fn=<AddBackward0>)\n",
      "epoch: 3878 loss is tensor([-0.1385], grad_fn=<AddBackward0>)\n",
      "epoch: 3879 loss is tensor([-0.1623], grad_fn=<AddBackward0>)\n",
      "epoch: 3880 loss is tensor([-0.1050], grad_fn=<AddBackward0>)\n",
      "epoch: 3881 loss is tensor([-0.1436], grad_fn=<AddBackward0>)\n",
      "epoch: 3882 loss is tensor([-0.1658], grad_fn=<AddBackward0>)\n",
      "epoch: 3883 loss is tensor([-0.1204], grad_fn=<AddBackward0>)\n",
      "epoch: 3884 loss is tensor([-0.0784], grad_fn=<AddBackward0>)\n",
      "epoch: 3885 loss is tensor([-0.0956], grad_fn=<AddBackward0>)\n",
      "epoch: 3886 loss is tensor([-0.0588], grad_fn=<AddBackward0>)\n",
      "epoch: 3887 loss is tensor([-0.0611], grad_fn=<AddBackward0>)\n",
      "epoch: 3888 loss is tensor([-0.0912], grad_fn=<AddBackward0>)\n",
      "epoch: 3889 loss is tensor([-0.0935], grad_fn=<AddBackward0>)\n",
      "epoch: 3890 loss is tensor([-0.0955], grad_fn=<AddBackward0>)\n",
      "epoch: 3891 loss is tensor([-0.0653], grad_fn=<AddBackward0>)\n",
      "epoch: 3892 loss is tensor([-0.1002], grad_fn=<AddBackward0>)\n",
      "epoch: 3893 loss is tensor([-0.0600], grad_fn=<AddBackward0>)\n",
      "epoch: 3894 loss is tensor([-0.1041], grad_fn=<AddBackward0>)\n",
      "epoch: 3895 loss is tensor([-0.1317], grad_fn=<AddBackward0>)\n",
      "epoch: 3896 loss is tensor([-0.1015], grad_fn=<AddBackward0>)\n",
      "epoch: 3897 loss is tensor([-0.0919], grad_fn=<AddBackward0>)\n",
      "epoch: 3898 loss is tensor([-0.0885], grad_fn=<AddBackward0>)\n",
      "epoch: 3899 loss is tensor([-0.0367], grad_fn=<AddBackward0>)\n",
      "epoch: 3900 loss is tensor([0.0176], grad_fn=<AddBackward0>)\n",
      "23\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxVUlEQVR4nO3deXhb13nn8e/BDpJYuC8gKUqWtcsSGVqO4zhxvMRLHCtS0zTpTDpZJup0mqftNE+btJ7ETft0kqYzzXSatFPPTCfTrE0aybJjO7aVeElsJ7Esb5K1WJZli6S4ifsCEMuZPy4AAiApiSSAi+X9PA8eAPde3PtCEn88Ovece5XWGiGEEMXLYnYBQgghVkeCXAghipwEuRBCFDkJciGEKHIS5EIIUeRsZhy0rq5Od3R0mHFoIYQoWs8///yw1ro+c7kpQd7R0cHhw4fNOLQQQhQtpdSbiy2XrhUhhChyEuRCCFHkJMiFEKLISZALIUSRkyAXQogiJ0EuhBBFToJcCCGKnCnjyIUoJVprpkIRBidDDEwEGYo/TwUjxgZKoUi+RKFQ8QUqsSyx4GLbxPeSXB5/Mb9+fnny/YJtVMoxUvaFsSBtH2nHSq9p0W1QGXXEl6Z9j4w/i5SaUEtsg2Jri5fqSsdSfwVlLytBrpT6J+BOYFBrvS0b+xTCbFprJoIRhiaDDEyEGJwMMjgRSns9GF83G46aXW5Je9eGev75E7vMLqNgZatF/g3ga8A/Z2l/QuSM1prx2TCDk6F4MAcXtKYHJ42QDoZjCz5f4bDS6HVR73GyvdXPTR4njV4nDR4XDSnPHqctraWttUZr0KnvgcS9XTSa1Pu8GNvqlPXzn0msJ77N/PYLt9HMH1SnLsv4THKfi9S0VM0sWJ9Rs176ey16/OS+59cfeKGX7/7qLfrHgzT5XAv+PkSWglxr/ZRSqiMb+xJipbTWjM2EGUi2nI1ATg3nxPNcZGFAVzlt8SB20tnup8GTHs6NXicNXhdVzpX92CS6PFKWrOyLlpnaKiff/uVbHHyxl99+9xVml1OQ8tZHrpTaB+wDaG9vz9dhRQmIxTSjM3PzXRqTIQYzgnlwwgjsuejCgPa4bDR4nDR6XXSvqU62phu9LiOs48+VKwxokVtr6yrpbPez/0gv+961Lu1/OcKQt3+5Wut7gXsBuru75UahglhMc2F6bkF/c7IvejLEUDyoI7GF/2R8bnsyoK9ZW0m910ljvAWdDGmPC7fDasK3E9m0t6uVz993lFfPT7C1xWd2OQVHmiAi66IxzYWpULKfeWAi3hedEtiDEyGGpkJEFwlof4U9Gcjr6+uMYE5pOSda1C67BHS5uHN7M3/+wDEOHOmVIF+EBLlYlnA0Rv94kJ7RWXrHZukbm+X8eDCtq2N4KsQi+UxNpSPZlbGh0ZM8QdjodVKffHbitElAi3TVlQ7es7GB+17s43O3b8JmlSkwqbI1/PC7wA1AnVKqB7hHa/1/srFvkV9ToQh9Y7P0js7SEw/q3nho947OMjAZTBuBAFBb6aDBawTx5mZPslujPuUEYX2VE4dNfvjEyu3tauXRVwf4+elhbtjYYHY5BSVbo1Y+ko39iNzSWjM8NWeEc2pAp7wenw2nfcZmUTT7XQT8bq5bX0eg2k3A7yLgryBQ7abZ55IuDpEX79lUj89tZ/+RXgnyDNK1UkIS3R69Ga3ovvH596GMYXdVThsBv5sWv4uuNf5kQCfCut7jxGqRUQLCfE6blffvaOZfn+9hMhjG47KbXVLBkCAvItOhyKIt6UQLe2AiuKBvuq7KSaDazaZmDzdtbiDgdxOorqDF76LVX4HXbZPhXKJo7Ols5Vu/eIuHj/bzoe42s8spGBLkBUJrYyheb8pJxJ6MVvXYzNLdHtdeUUur3x1vTRtB3eJ3S7eHKCld7X46ais4cKRXgjyFBHmeLNbtkdlXndntUemwxoPZTdcaPy1+43VrPKyl20OUG6UUezpb+eqhU/SOzRLwu80uqSBIkGfJdHy0R09mUI9erNvDQcA/3+2RCOpAtVu6PYRYwp7OAF89dIr7Xujld9+z3uxyCoIE+WVI7fZItKJ7Ul73ji3e7dHkS+/2aEl2fbil20OIFWqvreDqjmr2H+nhP95whTR2kCAH0rs9+jJPJsaXZV4Fr8JhTbaed7b5kwGdWNbgcUm3hxA5srerlT/Z/wqv9I5zVavf7HJMVxZBPjMXSU5wSW1VJ173X6TbY2Ojhxs3NqS1pFur3fjcdmkJCGGSO7Y3c8/9x9h/pFeCnBIIcq01I9NzS05wuVS3x9vX1c63pqvdyX5q6fYQonD53HZu2dzI/S/1cff7NmMv8yn7RRXkz75+gcNnR+gbn0271sfFuj12tPlTRnoYQd3olW4PIYrdns4AD75ynidPDnHzlkazyzFVUQX5I8f6+cYzZ6mtdBConu/2SD2JKN0eQpSHd2+sp6bSwYEXeiXIzS5gOf7TLRv43O2bpNtDCIHdauGuHS1851dvMT4bxucu3yn7RdWx5HPbJcSFEEl7uwLMRWI89Mp5s0sxVVEFuRBCpNoe8HFFfSUHjvSaXYqpJMiFEEVLKcXerlZ+dXaEcyMzZpdjGglyIURR+0BnAIADL5Rvq1yCXAhR1AJ+N9euq2X/kR505u2ryoQEuRCi6O3pCnD2wgwvnBszuxRTSJALIYre7duacNkt7D/SY3YpppAgF0IUPY/Lznu3NPGjl88TikTNLifvJMiFECVhT1eAsZkwj58YMruUvMtKkCulblNKnVRKnVZKfS4b+xRCiOW4fn0ddVVODrxQft0rqw5ypZQV+DpwO7AF+IhSastq9yuEEMths1rYvbOFn54YZHR6zuxy8iobLfJdwGmt9Rmt9RzwPWB3FvYrhBDLsrcrQDiq+VGZTdnPRpAHgHMp73viy9IopfYppQ4rpQ4PDZVfH5YQIve2NHvZ2Ogpu9EreTvZqbW+V2vdrbXurq+vz9dhhRBlxJiyH+CFt8Z4Y3ja7HLyJhtB3gu0pbxvjS8TQoi8270zgFJwoIxa5dkI8ueAK5VSa5VSDuDDwP1Z2K8QQixbk8/FO9fXsf+FXmKZN+MtUasOcq11BPg08AhwHPi+1vrYavcrhBArtaczQM/oLIffHDW7lLzISh+51vohrfUGrfUVWuu/zMY+hRBipW7d2kSFw1o2Y8plZqcQouRUOm3cttWYsh8Ml/6UfQlyIURJ2tvVymQwwk+OD5pdSs5JkAshStK1V9TS6C2PKfsS5EKIkmS1KD7QGeCJk0NcmAqZXU5OSZALIUrW3s5WIjHNAy/1mV1KTkmQCyFK1sYmD1tbvOwv8ft5SpALIUrans4AL/eMc3pw0uxSckaCXAhR0u7a2YLVoth/pHRb5RLkQoiS1uBxcf2VddxXwlP2JciFECVvb1crfeNBfvHGBbNLyQkJciFEyXvvlkaqnDYOlGj3igS5EKLkuexW7tjexEOvnGd2rvSm7EuQCyHKwp7OVqbnojz6ar/ZpWSdBLkQoixcs7aGgN9dkqNXJMiFEGXBYlF8oLOFn702xOBk0OxyskqCXAhRNvZ0thLTcP+LpTVlX4JcCFE21jdUsaPVV3LdKxLkQoiysqczwKvnJzjRP2F2KVkjQS6EKCvv39GCzaJKaky5BLkQoqzUVjm5YWM9973YS7REpuxLkAshys7erlYGJkI88/qw2aVkxaqCXCn160qpY0qpmFKqO1tFCSFELt24qQGPy1YyJz1X2yI/CuwFnspCLUIIkRcuu5U7r2rhx0f7mQ5FzC5n1VYV5Frr41rrk9kqRggh8mVvV4DZcJRHjhX/lP289ZErpfYppQ4rpQ4PDQ3l67BCCLGo7jXVtNWUxpT9Swa5UuqQUuroIo/dyzmQ1vperXW31rq7vr5+5RULIUQWKKXY09nK068P0z9e3FP2LxnkWuubtdbbFnkczEeBQgiRK3s7A2gN971Y3K1yGX4ohChbHXWVdLX72X+kB62Ld0z5aocf7lFK9QDXAg8qpR7JTllCCJEfe7taOTUwxbG+4p2yv9pRKwe01q1aa6fWulFrfWu2ChNCiHy486pmHFYLB14o3u4V6VoRQpQ1f4WDGzc1cPDFPiLRmNnlrIgEuRCi7O3pCjA8FeJnp4tzyr4EuRCi7L1nYwP+CnvRjimXIBdClD2HzcL7r2rh0WP9TAbDZpezbBLkQgiBMWU/FInx8CvFN2VfglwIIYCdbX7W1lWy/4Ues0tZNglyIYTAmLK/tzPAL86M0DM6Y3Y5yyJBLoQQcR/oDABw8MU+kytZHglyIYSIa6upYFdHDT8ssin7EuRCCJFib1eAM0PTvNwzbnYpl02CXAghUty+vRmHzcL+I8Vz0lOCXAghUvjcdm7Z0sgDL59nLlIcU/YlyIUQIsPezgAj03M8eao47mYmQS6EEBnetaGe2koHB4pkTLkEuRBCZLBbLbx/RwuHjg8yPlP4U/YlyIUQYhG/1tXKXCTGg6+cN7uUS5IgF0KIRWwLeFnfUFUU3SsS5EIIsQilFHu7Ajx3dpQ3L0ybXc5FSZALIcQS9na24rRZ+OIDrxb0TE+b2QWIeaFIFIfVglLK7FJEEdFaE4rECIajBMMxZsNRguGo8TwXJRiJMjtnLNdaY1EKq0WhFFgtCotKPLjoOmP5/OvkOgtY1SLrLPH38XXW+PvU14l9pK4rJE0+F5+9bRN//qNX+f7hc/zG1e1ml7QoCfIC8l8ePM6pgSm+tHc7HXWVZpcjVikSjRGMxJidi8ZD1gjX2blocnkoYryfzQjhYDh9edrnw1FC8W1n40FdwI3FZVss5NN/sSy9TsV/caT9Ion/ArKmfM5iSf2lNb/O+CVDyucVKl7Tnz/wKu/aUE+zz232H9ECqwpypdRfA+8H5oDXgY9rrceyUFdZ2tTsZf+RXm7970/xBzdv4FPXr8Vmld6vbEq0XmeTLdX0sExdHozECCbXR1NCNpbyuWhaSzh1WTi6snR12iy4HVbcdiuu5MOC227F57Ynl7kdFlw2K27H/HbulG1dDmvKemNbq0URjWliOvGAmNZEYxqtWbguponq9HWLbXexdTGticXSt1tqndbG8ZZaF9PE69HxfRHf5tLrEseNJvcZIxjOXEf8OCl/JlrT4ndht1gYnw0XZJCr1fT7KKXeC/xUax1RSv0VgNb6s5f6XHd3tz58+PCKj5s3I2/AsQOw8zfB05SXQw5MBPnCwaM8cmyADY1VbGry4rJbcNriP4x2K05b/NluxWWzJJ8TP8zO5OuM7W2F220TjsZSugNiGSGbGqTzIRoKp7dkU5cZreBYWgAntl0Jm0XNh2MiKBcJT7fDijMenqnLnfHt3KkhnPZ549lps2CxFObfkTCfUup5rXV35vJVtci11o+mvP0F8MHV7K/gnPsl/OSLsOnOvAV5o9fFP360mx8fPc8/PPE6L/eMEQzHCEXiwbTK/0Y7bJZLhr7xi8G65C+Q1PfhqM7oDohltFQT3QGxtC6DRL9tYlkktrIvlQhPl82CK6UVW+GwUVNpSQ9KR3rrNnVdahAnQzVlO7v8z0gUsGz2kX8C+JelViql9gH7ANrbC/OEwQJDJ8Bih5q1eT/0bduauW1b84LlWmvCUU0wYgRgKDXk4y3O1PfJk2CR+XWhzHXxz02HIlyYMn5ZZO53OUHrsFqSYZ9omRotUgu1lQ5cfmt6K3UZXQTujF8khfo/DCHy6ZJBrpQ6BCzWHL1ba30wvs3dQAT49lL70VrfC9wLRtfKiqrNt6GTUHsFWO1mV5KklMJhUzhsFryu/NUVicZSfinECMXD32FTC7oIrNI1IEReXTLItdY3X2y9UupjwJ3ATbqQB1quxNBJaNpmdhUFwWa1YLNaqHTKQCchCs2qOv6UUrcBfwzcpbUurruVXko4CKNvQP0msysRQoiLWu0ZnK8BHuAxpdSLSqn/mYWaCsOF06BjULfB7EqEEOKiVjtqZX22Cik4wyeNZ2mRCyEKnIypWsrQSVAWqC3d31VCiNIgQb6UoZNQ3QF2l9mVCCHERUmQL2XopHSrCGECvcLJYeVMgnwx0YhxslNOdAqRVy8/fo7/+eknCM9FzS6lqEiQL2b0DYiFpUUuRJ7ZnTZiMc3MeMjsUoqKBPlihk4Yz/Ubza1DiDJT6XcAMD0+l58DHrsPghP5OVYOSZAvJhHk0rUiRF5V+pwATI/loUXe/wr84N/BS9/L/bFyTIJ8MUOnwNcGziqzKxGirCSCfCYfLfKXvmdcFG/br+X+WDkmQb6YoRPSrVJGvvnqN/nbI39rdhkCcFbasNgUMxM5bpFHI/Dy92HDrVBZm9tj5YEEeaZYDIZfgzoJ8nLxwuALPHHuCbPLEBhX96zwOpgey3GL/MzjMD0IOz6c2+PkiQR5pvG3IDIrLfIy4rA6CEVllEShqPQ5mc71qJUXvwPuarjy1tweJ08kyDMNyTVWyo3T6pQgLyBGkOewRT47BicehG0fBJsjd8fJIwnyTMkglxEr5cJhcTAXzdNwN3FJlT5HbseRv3oQoiHY8ZHcHSPPJMgzDZ2Eqkbjv12iLLhsLmmRF5AKn5PQTIRIOEezO1/6LtReCYGu3OzfBHK7l0xDJ2T8eJlxWKVFXkgqfEZ3x8z4HN46d3Z3PnIG3noWbvoC5OF+r7NTk/SfPkX/6VOcP32SyQvD7PnsPXjr6rN6HAnyVFrD8Cm46jfMrkTkkdPqJKqjRGIRbBb5kTBbpT8+KSgXQf7y9wEF2z+07I/GolEicyEic3OEQ8Zz4n1kbo5I2Hg9PTaaDO6x/vPGh5XCWVFBaHqacDCY3e+EBHm6yfMQmpARK2XGaTWCYy46V1ZBHo1EmJkYY2ZsjJnxMWanJrE7nDgrq3BVVeGqrMJZWYXD7UblofWaUBlvkV/O7E6tNZHwHOFgkEgoRDgUZC44SzhovA6H4svnQkRCISJPPUjE8U4iB3+cHsJpr+Pvw+nrYtHL7+qprK6hef0Gtr3nvTSv30Djuit5+l++ydEnDlHd0rLiP5ullM+/2ssh11gpSw6rERyhaIgKe4XJ1axONBJmZnyc6bFRZsbHmB4fTQb19Hj8Ob4uODV5WftUFguueLg7K+cDPnNZavgnXttd7nhdESNYg8H051BwPnSDs4RDIWYnpgnPnOHwA7/i1DO2+HYh4zPxbVI/q3VsGX9CPpRS2HofxeZwYHM4sDuc2BzO5HtXVVXa+4u9ti+yzlXloaq6ZsGR+8+8RkPHOiwW6zLqvTwS5KmGThnPMvSwrCRa5IV6wjMSDjMTD+TMMDbez4d1cHpq0X043G4qfH4qfNXUBtpo27KdCp+fSr+fCn81lT4/riovkbkQwakpQtNTBOOP0PQUwanU15OMDZwnOD1NaHoKHVs6SC1WK1rri26zgFKgbQyecTI1UoXd6cLudGJ3unB7vcZ7lyu+PL7O5cbucqYsc+FwGdslw/cnn8d2/IdY/+i1vF9+IxIOM3T2DXbccltO9i9BnmrohDFapTK7JyJEYUu0yPN5wjMyN5cM5NQwTgR1aliHZqYXr9tdYQSxr5q6tjVUbN9hhLOvOh7a8aD2+bE7c3OnK601c7Oz88E/NUVwejLtlwGAw+XG7nRiSwlhhys1iF1GGDuN1vE//+kztG6s5qaPbclOoeFZOHU/bLkrryEengtx9PHHOPzAfiJzIdq27sjJcSTIUw2fMqbm57E/UJgv1y3y2alJHrv371ICeoy52ZnFa6mspMJntJDr16yNh3F1WigngtrmMH8yi4qfxHNWVOCtb8jafit8TqYnsviL9eRDEBqHnfkZOx6amebFRx/iyEMHmRkfo2XjFm765O+wrvPqnBxvVUGulPoLYDcQAwaBj2mt+7JRmCmGTsCmO82uQuRZ6snOXLDZ7VzoOUelz09DxxVU+DNbzfOvbXZ7TmooNpU+B+NDs9nb4UvfA28AOq7P3j4XMTM+xvMPHeTFRx5kbnaGjp1v45oP/Dqtm7fl9LirbZH/tdb68wBKqd8DvgD8h1VXZYbpYZi5ICc6y1CiayUYzf6wMAC708XH/+YfcrLvUlXpc3L+9Hh2djZwDE4fgnf+IeTgRCPAxNAgzz2wn6M/fZRIJMyGa65j1+4P0rhufU6Ol2lVQa61Tr21RiVQvHdNTU7NlyAvN4V+srMcVfgcBKfDRMMxrPZVTkA/9Gfg9MC1v5uV2rTWzE5OMNLXw0hvD73Hj3LimacA2PKuG7n6rl+jpqU1K8e6XKvuI1dK/SXwW8A48J6LbLcP2AfQ3t6+2sNmX3LooYxYKTdmnOwUF5e8U9BECG/tKiYFvfEzeO1RQpt/H31uCNfGhcMClxKLRhkb6I8H9jlGz/cy0tvDSF9P2tBNu9PFjvfeQfede/DWZe88wXJcMsiVUoeApkVW3a21Pqi1vhu4Wyn1J8CngXsW24/W+l7gXoDu7u7Ca7mPvWncLcQbMLsSkWdOi7TIC03aNP2VBrnW8NgX0J4W3vr6E7i2DtP2D3+/YLPg9BSjfb3JwE60tMcG+olFI8ntKv3V1LS0suHt11HT0kZNoJWalla8dfUoi7mXrbpkkGutb77MfX0beIglgrzguWsgFoa5KeO/YaJs5Ppkp1i+ZIt8NVdBPHYA+o6gPvAPVEV76Pvudwk99VPGE90i8cCeGR9LfsRiteFvaqYm0Mb6XddS02KEdXVLAFdl4d76cbWjVq7UWr8Wf7sbOLH6kkySaIlPnId6CfJykjqzUxSGxPVWLvfeneFQkJnxceOSA+PjzIxeYOLhrzIS2sXIt55m9Hwv0Q0B+PrfAOCq8lATaGNd19VGWMdb176GJizW3JwQzaXV9pF/WSm1EWP44ZsU64gVAG/8+gcTvXIt8jIjJzsLj7vKjrIopseNmaZjA+cZ6+9jrP8840ODzEyMMZsS3OHQwhFHCi++Wj819Q107Hwb0QMH8WBl6//9v1R4fSZ8q9xZ7aiV4r/9dEIyyIt3GLxYGTnZaS6tNbMT44z2x8N6oJ+x/j7Ck6f4xQ/Gefo76ZOnUsfe+5uaqfD5qPBVU+H1GePxXTYqfvhhKgJXYvv4t5IT/EaiVga+9GWsA4MgQV6iPM3GswR52ZGuldzTsRhToyPxlvV863psoJ+xgT7mZucn/yhlwVNXj81RibOqlR03b8ff1Ex1YzO+xqZLX27gJ38B0X647Xtps7S973sfA1/5a8bvv5+Gz3wmV1/VFBLkCXYXVNQZXSuirNgsNmzKJi3yVYpFo0xeGIq3rNNb1+MD/UTC83++FqsNX0Mj/qZmApu24G9qNh6NLfgaGrDa7Dz49y8zeSHI1e/fdflFTJyHZ78O234NWjrTVtnq6qh65zsZv/8B6v/gD1BF2Be+FAnyVN5m45rkouw4rA5pkV+GSDjMxNDAgqAeGzjP+OBA2jW7bXZHPKBb6Nj5NqrjQe1vasJTW3/Jk4qVPgcDbyxzducTX4JYBG78/KKrvXfeydSTTxJ89VXc27cvb98FTII8lTcgLfIypLUmEotgVaXTQluNudkZxgcHjG6QRFDHu0Emh4fSrv/tcLvxN7ZQv2YdV15zXbxVbbSuq/w1qxpfXVXtYnYyTHA6jKvyMq5BM3gCXvgm7PptqFm76CaO9jYAoiMjK66rEEmQp/K2QM9zZlch8mw6PM1cbI4a1+XP+itm0UiEyQvDjA/2G4+BfsYHB+LvB5idnEjb3uXxUt3YTGDjZvzvvjEZ1P6mFtweb87uHtS2pYZf3n+GN18ZZuPbmy/9gZ98ERxV8K4/WnITi8cLQHTi8m6qUSwkyFN5W4wLZ4WDRp+5KAsjQaN1VuMujSBPXAvECOj+eOu6n4mhfsYGBpi8MJR2oweL1Yq3vgFfQxNXXnMFvoam+MPowzZrIkxDu4dKv5MzL11GkL/5rHGp2hs/D5W1S25m9RpzRKITWbogV4GQIE+VmBQ02Qc168ytReRNMsiLqEUeDgWZGBpkLK01HX890L9gXHWFz4+vsYmWDZvwN96At6ERf0MTvsYmqmpqc3L7sdVSFsXaHXWcePY8kbkoNscSNWoNj33eGHn29v940X1avEaLPDYpLfLSlTqWXIK8bFwIXgAKK8jDwSATw4NMDA2mPA8xPjTAxOAA02OjadvbnE78DU14Gxpp33oVvkajRe1raMJX34jdVZz/w1y3o56jT/Zy7vgIa3csceeu4/cbXaJ3/R04Ln7PVYvDgXK5pGulpCWn6ctY8nKS7xa51prg9FQypCeTYT3ExPAg40ODBDP6qS1WK57aOrx1Dazt7DYCutEIaX9jE26vL693us+Xlg1+HG4bZ14aXjzIo2E49EXjqqU7fvOy9mn1eIhl/PkWOwnyVMlJQTJypZyMzGY/yHUsxuTIBUb6ehjt62H0fB/jg/3J8E6dAANgczjx1tXjrW+gcd16vHUNeOsbks+V1dUF2f2Ra1abhTXbajn70jCxaAyLNWMUzJH/ByOvw0e+B9bLizOL1yst8pLmrAKnz5hUIMrGSHAEj92TnOG5HHPB2flLoPb1MtrXw8j5XkbP9xIJzY9Lt7vc+BuNVnTb1quM0G5oTAZ1Lkd/FLt1O+t57bkB+s+M03Jl9fyK0CQ88WVYcx1suPy701s9HjnZWfK8LdIiLzMjwZGLjliJxaJMDg/NB3VfL6PnjeepkQvzGyqFr76B6pZW2jZvoybQSnVzKzUtASqraySoV6h9aw0Wm+LMi8PpQf7M12B6yGiNL+PP1uLzEh2+cOkNi4gEeSZvi/SRl5mR4Ag1rhpCM9PxrpDetNb12Pm+tOnlzopKqlsCtG+9ippAG9UtAWqaA/ibWgrizvalxuGy0baphjdeGuK6D643fiFODsAzfwdbdkNr97L2Z/V4mXvjbG6KNYkEeSZvi3GzVlGSYtEo40MDye6Q0b5eAq8MUjVp4Wvf+I3kdspiwdfQSE1LK2uu6qSmJUBNs3GDgQqfX1rXebZ2Rx1vHr3Ahd5p6lqr4Mm/gmgIblr+fWysXg+xCTnZWdq8AZgaMM6GWy9jWrAoChPDQ+z/0j2M9fcRjczfvstV5SHqiBBb28gNO++Mt65b8Tc1YbXJ33+h6LiqDr5zkjdeGqLOdR6e/wZ0fwJqr1j2viweL9HJSbTWJfMLWYI8k7cF0DDZD/42s6sRWVLh9eFvamFd19XJsK5uCeCsqqLrW118avv72NX5QbPLFEuo9DlpWuvjzItDXD31P8Duhnd/dkX7sno9EI0Sm57BWlWZ5UrNIUGeKXUsuQR5ybA5HHzgj/7zguUjwRFiOlZQk4HE4tZ3N/Dz77/Gc6Nurr7z96BqiQlClzA/u3OiZILc3Fs/F6LUW76JkpccQ14i11kpZdvfHWBj7Sv8auojHJ74wIr3Yy3BC2dJkGfyyp2CykliVmeta+kLLYnCYHntYW60/Rkb10/xy4d6OfzQ2RXtJ3HhrFKa3SlBnsnlB3uF3GCiTCSCvNpZfYkthamiETj0Z1jqruDG37+dDbsa+eX9Z3j+x2eXvav5S9mWTpBLH3kmpWRSUBlJXjBLulYK24vfguFT8BvfwmK3c9PHtqA1/OK+Myil6Lp1zWXvyuorvSDPSotcKfUZpZRWStVlY3+mk0lBZWMkOIJFWfA5Suuu6iVlbhoe/y/Qdg1suhMAi0Vx88c2c2V3A88eeJ0jj7552buzeOJdKyXUR77qFrlSqg14L/DW6sspEN4AnP252VWIPBgJjuB3+rGW4QWpisazf2/M7fjQN9Om4lusFm7+uNEyf3b/6yil6Lyl/ZK7s8aDPFpCfeTZ6Fr5KvDHwMEs7KsweFuMPvJYFOQHvKSNzI7I0MNCNjUET/+t0RJvv2bBaovVwi2fMML8mR+eRinYefPFw1zZbFgqKkpqdueqglwptRvo1Vq/dKkZUkqpfcA+gPb2S//WNJW3xbgT9/QQeJrMrkbk0EhwREasFLKnvgLhmYtOxbdYLdzyyS2gNU//62mUUuy46eJzQCw+X3kNP1RKHVJKHV3ksRv4U+ALl3MgrfW9WuturXV3ff3KBvLnjUfGkpeLxAWzRAG68Doc/ifo+i2o33DRTa1WC7f8+62s66zn5z94jZcfP3fx7T2e8upa0VrfvNhypdR2YC2QaI23AkeUUru01v1ZrTLfUm/5FnibubWInLrUJWyFiX76F2B1wA2fu6zNrVYL7/33W3n0fx3jZ//yGkoptt/Quui2Fq+npE52rnjUitb6Fa11g9a6Q2vdAfQAXUUf4pAyTV/GkpeyUDTEVHhKWuSFqOd5OHYArv30sro3E2G+dkcdT33vFK880bP4dvELZ5UKmRC0mIpaoyUgXSslbTRo3MBYgrzAaA2PfQEq6uC631v2x602C7d+ahsdVxlhfvSphT/HVq+X2Hjp3CUoa0Eeb5kPZ2t/prJYjPt3yljykpacDCRBXjjmpuGFb8KbPze6VJyeFe3GarNw26e20bG9lie/c5JjP0sPc4u3tFrkMrNzKd6ABHmJy8VNl8UyaA0jZ6DnOeNx7lfGTV10FOo3wds+tqrdW+0Wbtu3nYfvfYUnvn0SgK3XG92mVo+H2NQUOhZDWYq/Y0KCfCneFuh93uwqRA7JBbPyLDRp/Ez1PAfn4uEd/2WKwwOBLrj+D6H1aljzjqzc2MVqt3D7vu08/I9GmGsN294VwOL1gNbEpqawxi9rW8wkyJdic0KwdPrQxELJPnIZtZJ9sRhceG2+pd1zGAZfBbSxvm4jbLrDCO3Wq40WeI4m31ntFm777W38+B+P8uR3TjI9HmKDx7gkQ3RiQoK8ZMVicPoQrL3e7EpEDo0ER3BanVTYKswupXjFYsbEufFzxmPwhBHevYfnG0IunxHWW+4ybpQc6Aa3P69l2uxWbv+d7Tz57ZMcfvAso2u8tClLyczulCBfTM9zxrUdNt9ldiUihy4EL1DjqimZ+zbmRDhojN4aPwdj52C8J/54a/51dG5+e2WBhi2wdU+8tb0LatcbAwhMZrVaeM9HN1FV4+K5H73B6PbfoWV4ApfZhWWBBPlijt9vDD+88r1mVyJyqOxndWoNs6Px1nRPPKjPpb+fHsz4kDJGdPlaoXknbH4/+Nrij1ao7gBnlQlf5vIopdh151pcwRF+/thGHn5wmt1Xhaj0O80ubVUkyDNpDccfgHU3gKv4+87E0kr+OivRCEz2LR3S4z0Qnk7/jM01H8obb0sPaX+bcfkKm8Oc75NFm97exMR/+0uOdf0u//qVw9zxH66ivn1lQx0LgQR5poGjMPYmXP8ZsysROTYSHOFK/5Vml7FyocmLh/RkH+hY+mcq6oxQrt8A628yXieDut2YDFcGXU1Wn5fa0ePctLmfn59bww++fJjOW9q5+n0d2BzFd8VTCfJMxx8w+vk23mF2JSKHtNbGJWwLdcRKLGZ0a4ylBHRmF0hwLP0zFpsx/8HXZpyozwxpbwAccmIXwFJZCUrhi13gw1/4EM/sP82RR97kjZeG+NCfXl10YS5Bnun4A9D+Dqgq8Cs0ilWZDk8zF5szr2slcRJx7K2MkI6/n+hNP4kI4PTNd3G0X5Me0r5WqGqU6+dfJmWxYPF4iE5M4qq0c+NHN3NldyODb04UXYiDBHm6C68bY11v+7LZlYgcS9502ZWDmy4nTiIuFdLj54whe2niJxH9bcbEmC2751vU/nhgu+R2dNlk9XiIpVzKtm1zDW2bC/R/aJcgQZ7q+APGc/y+gKJ0JYJ8WaNWEgE9PWwEcepjsj9laN4542YIqWzu+dZ00zbwtc+/97Ua3R5ZmMkoLp/F5yU6LuPIS8/xB6Cl0/jhEiUtecEsW1V8mN1QPKAHU16nhnX8fSyyyN6UcZLQ3xY/iXhzekj72qGipixOIhaTUrqUrQR5wkSfMRvtxs+bXYlYjVgs3mpOBPBgRigbr0fm+qECau69EaLRhfuxuY3zJJUN4I2Pma6sn39Upbx214BVfpSKjdXrYe7sm2aXkRXyry/hxIPGs8zmLAyxKIRnIRKcfw5OpLSYM1rNU/HnmQvG1fMyKYsx9K6yHirrGHG3QLiXmnd91uibToZ0fBtHZf6/s8gri7TIS9Dx+40L+Vzi3oBlKxZLCdVZY9RF2vPswuANzyyx3cU+G1+WOWJjMQ7PfPBWd0Db1QsDOdlqrk4b0THyyy/hef0BHO/+bO7+zERBs3q9cq2VkjIzAmefXtHdSHJKa+NC+7OjxuSPaAiiYYiEjKCLhhdZlvKIZL6Pbxudy/hcYj9zC5clwjUaWvn3sLnB7oo/xx82l/FcWT//OvFsd2d8xgX2CuMmA6lBbXevuCS5V6eweD3EZmbQ4TDKXtwnmiXIwQhJqx2O/LNxs+XN78/u/sNBY/LG7JgRysH48+zY0q8T2y16cm0ZLHbjujE2h/FsdRrfNXOZo2rx7WzORYI1I4xt8aC1uxYuszkL8iRf2V9nRWD1GJfgiE5NYavOwTDUPJIgB6heA/uehAP74F/+Lez4Tbj9y+njdqMR47KcySAeu/xQjsxe5ODKOI7bDy6/0QXgazWeXX5jubvaaI3aXPEQdsbDNh60Vkf6w5byugBDtBCMBEdY411jdhnCRFavcW2V2MQESJCXiIZN8MlD8NRfw8/+G7z+E6hqgNl4eM9d4qSIvdII3ETw1qybf58I6MS6tID2FcQlPsvNSHCEzobOVe9naGaIR84+woc3fRibRX6cioklfkOJ6ETxn/CUf3mpbA648W7YcCs8+RWjNduwdekQTgS0y1cSV4QrF9FYlLHQ2Iq7VkLREE+ce4KDpw/yTN8zRHWUzbWbeVvj27JbqMgpazLIi/9OYKsKcqXUnwGfAhLzjf9Ua/3QaosyXWs3/Jvvm12FyJHxuXFiOrasINdac3T4KAdfP8jDbzzMxNwEDRUNfHzbx7nrirtY61ubw4pFLlg88a6VEhiCmI0W+Ve11v81C/sRIi9G4jf8vZxRK4Mzg/zozI84ePogZ8bP4LQ6uan9JnZfsZtrmq/BKhepKlrWeJCXwlhy6VoRZSd5nRXn4kEeioZ4/K3Hue/1+3i271liOkZnQyf3XHsPt3bcisdRvDcgEPOSLXLpIwfg00qp3wIOA5/RWo8utpFSah+wD6C9vT0LhxViZYLRIAB/+OQf8o7md3Bd4Dre0fIO+qb7uP/0/Tx89mEm5yZpqmzik9s+yV1X3EWHr8PcokXWWSoqQCmiU8Uf5EprffENlDoENC2y6m7gF8AwoIG/AJq11p+41EG7u7v14cOHl1+tEFkQjUU59NYhnup5iqd7n05eQAvAZXVx85qb2b1+N7uadmFRMqKolJ28ehe+3btp+s93m13KZVFKPa+17s5cfskWudb65ss8wP8CfrSC2oTIK6vFyq0dt3Jrx63EdIyTIyd59vyzVDuruWXNLVQ5CvfmwSK7LJ4qOdmplGrWWp+Pv90DHF19SULkj0VZ2Fy7mc21m80uRZjAWuUhOjVldhmrtto+8q8opXZidK2cBX57tQUJIUS+WDweaZFrrT+arUKEECLfrB4P4cEBs8tYNTmTI4QoW0aLvPi7ViTIhRBly1oiJzslyIUQZctS5SE6OcmlhmEXOglyIUTZslZXQzRKdGzM7FJWRYJcCFG2nOuvACD02msmV7I6EuRCiLLl3GDco7fYg1wumiWEKFu2hgas1dUMfPmvGPvBv+LatAnX5k04N23GtWkjVp/v0jspABLkQoiypZSi9etfY+qJJwmeOM7U0z9n/L77kuvtgQDOzZtwbdqMa/MmXJs2YWtpQRXYLRQlyIUQZa2iq4uKrq7k+8jQEMETJwmeOE7o+AmCx48z9ZOfQnxki8Xrxfu+O2i+5x6zSl5AglwIIVLY6uupqq+n6vp3JpfFZmYInTpF8MQJgsdPYG9pMbHChSTIhRDiEiwVFbh37sS9c6fZpSxKRq0IIUSRkyAXQogiJ0EuhBBFToJcCCGKnAS5EEIUOQlyIYQochLkQghR5CTIhRCiyCkzLqiulBoC3lzlbuqA4SyUU4hK9buV6vcC+W7Fqti+2xqtdX3mQlOCPBuUUoe11t1m15ELpfrdSvV7gXy3YlUq3026VoQQoshJkAshRJEr5iC/1+wCcqhUv1upfi+Q71asSuK7FW0fuRBCCEMxt8iFEEIgQS6EEEWvqINcKfXrSqljSqmYUqrohxAppW5TSp1USp1WSn3O7HqyRSn1T0qpQaXUUbNryTalVJtS6nGl1Kvxf4u/b3ZN2aKUcimlfqWUein+3b5odk3ZpJSyKqVeUEr9yOxaVquogxw4CuwFnjK7kNVSSlmBrwO3A1uAjyiltphbVdZ8A7jN7CJyJAJ8Rmu9BXg78Lsl9PcWAm7UWu8AdgK3KaXebm5JWfX7wHGzi8iGog5yrfVxrfVJs+vIkl3Aaa31Ga31HPA9YLfJNWWF1vopYMTsOnJBa31ea30k/noSIxgC5laVHdowFX9rjz9KYnSEUqoVeB/wv82uJRuKOshLTAA4l/K+hxIJhHKhlOoAOoFfmlxK1sS7H14EBoHHtNal8t3+O/DHQMzkOrKi4INcKXVIKXV0kUdJtFZFaVBKVQE/BP5Aaz1hdj3ZorWOaq13Aq3ALqXUNpNLWjWl1J3AoNb6ebNryRab2QVcitb6ZrNryJNeoC3lfWt8mShwSik7Roh/W2u93+x6ckFrPaaUehzjXEexn7S+DrhLKXUH4AK8Sqlvaa3/rcl1rVjBt8jLyHPAlUqptUopB/Bh4H6TaxKXoJRSwP8Bjmut/8bserJJKVWvlPLHX7uBW4ATphaVBVrrP9Fat2qtOzB+zn5azCEORR7kSqk9Sqke4FrgQaXUI2bXtFJa6wjwaeARjBNm39daHzO3quxQSn0XeBbYqJTqUUp90uyasug64KPAjUqpF+OPO8wuKkuagceVUi9jNDQe01oX/VC9UiRT9IUQosgVdYtcCCGEBLkQQhQ9CXIhhChyEuRCCFHkJMiFEKLISZALIUSRkyAXQogi9/8BdJ5dylzdxq8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 3901 loss is tensor([-0.0696], grad_fn=<AddBackward0>)\n",
      "epoch: 3902 loss is tensor([-0.0493], grad_fn=<AddBackward0>)\n",
      "epoch: 3903 loss is tensor([-0.0407], grad_fn=<AddBackward0>)\n",
      "epoch: 3904 loss is tensor([-0.0595], grad_fn=<AddBackward0>)\n",
      "epoch: 3905 loss is tensor([-0.0884], grad_fn=<AddBackward0>)\n",
      "epoch: 3906 loss is tensor([-0.0578], grad_fn=<AddBackward0>)\n",
      "epoch: 3907 loss is tensor([-0.0333], grad_fn=<AddBackward0>)\n",
      "epoch: 3908 loss is tensor([-0.0798], grad_fn=<AddBackward0>)\n",
      "epoch: 3909 loss is tensor([-0.0620], grad_fn=<AddBackward0>)\n",
      "epoch: 3910 loss is tensor([-0.0396], grad_fn=<AddBackward0>)\n",
      "epoch: 3911 loss is tensor([-0.0431], grad_fn=<AddBackward0>)\n",
      "epoch: 3912 loss is tensor([-0.0531], grad_fn=<AddBackward0>)\n",
      "epoch: 3913 loss is tensor([-0.1063], grad_fn=<AddBackward0>)\n",
      "epoch: 3914 loss is tensor([-0.1085], grad_fn=<AddBackward0>)\n",
      "epoch: 3915 loss is tensor([-0.0874], grad_fn=<AddBackward0>)\n",
      "epoch: 3916 loss is tensor([-0.0464], grad_fn=<AddBackward0>)\n",
      "epoch: 3917 loss is tensor([-0.0809], grad_fn=<AddBackward0>)\n",
      "epoch: 3918 loss is tensor([-0.1226], grad_fn=<AddBackward0>)\n",
      "epoch: 3919 loss is tensor([-0.0796], grad_fn=<AddBackward0>)\n",
      "epoch: 3920 loss is tensor([-0.1036], grad_fn=<AddBackward0>)\n",
      "epoch: 3921 loss is tensor([-0.0806], grad_fn=<AddBackward0>)\n",
      "epoch: 3922 loss is tensor([-0.0651], grad_fn=<AddBackward0>)\n",
      "epoch: 3923 loss is tensor([-0.0754], grad_fn=<AddBackward0>)\n",
      "epoch: 3924 loss is tensor([-0.0620], grad_fn=<AddBackward0>)\n",
      "epoch: 3925 loss is tensor([-0.0671], grad_fn=<AddBackward0>)\n",
      "epoch: 3926 loss is tensor([-0.1186], grad_fn=<AddBackward0>)\n",
      "epoch: 3927 loss is tensor([-0.1310], grad_fn=<AddBackward0>)\n",
      "epoch: 3928 loss is tensor([-0.1268], grad_fn=<AddBackward0>)\n",
      "epoch: 3929 loss is tensor([-0.0863], grad_fn=<AddBackward0>)\n",
      "epoch: 3930 loss is tensor([-0.0580], grad_fn=<AddBackward0>)\n",
      "epoch: 3931 loss is tensor([-0.0448], grad_fn=<AddBackward0>)\n",
      "epoch: 3932 loss is tensor([-0.0554], grad_fn=<AddBackward0>)\n",
      "epoch: 3933 loss is tensor([-0.0545], grad_fn=<AddBackward0>)\n",
      "epoch: 3934 loss is tensor([-0.1571], grad_fn=<AddBackward0>)\n",
      "epoch: 3935 loss is tensor([-0.0984], grad_fn=<AddBackward0>)\n",
      "epoch: 3936 loss is tensor([-0.0865], grad_fn=<AddBackward0>)\n",
      "epoch: 3937 loss is tensor([-0.0841], grad_fn=<AddBackward0>)\n",
      "epoch: 3938 loss is tensor([0.0037], grad_fn=<AddBackward0>)\n",
      "epoch: 3939 loss is tensor([-0.0264], grad_fn=<AddBackward0>)\n",
      "epoch: 3940 loss is tensor([-0.0179], grad_fn=<AddBackward0>)\n",
      "epoch: 3941 loss is tensor([-0.0431], grad_fn=<AddBackward0>)\n",
      "epoch: 3942 loss is tensor([-0.0614], grad_fn=<AddBackward0>)\n",
      "epoch: 3943 loss is tensor([-0.0228], grad_fn=<AddBackward0>)\n",
      "epoch: 3944 loss is tensor([-0.0150], grad_fn=<AddBackward0>)\n",
      "epoch: 3945 loss is tensor([-0.0858], grad_fn=<AddBackward0>)\n",
      "epoch: 3946 loss is tensor([-0.0287], grad_fn=<AddBackward0>)\n",
      "epoch: 3947 loss is tensor([-0.1272], grad_fn=<AddBackward0>)\n",
      "epoch: 3948 loss is tensor([-0.1039], grad_fn=<AddBackward0>)\n",
      "epoch: 3949 loss is tensor([-0.0533], grad_fn=<AddBackward0>)\n",
      "epoch: 3950 loss is tensor([-0.1345], grad_fn=<AddBackward0>)\n",
      "epoch: 3951 loss is tensor([-0.1436], grad_fn=<AddBackward0>)\n",
      "epoch: 3952 loss is tensor([-0.0786], grad_fn=<AddBackward0>)\n",
      "epoch: 3953 loss is tensor([-0.1097], grad_fn=<AddBackward0>)\n",
      "epoch: 3954 loss is tensor([-0.1040], grad_fn=<AddBackward0>)\n",
      "epoch: 3955 loss is tensor([-0.1278], grad_fn=<AddBackward0>)\n",
      "epoch: 3956 loss is tensor([0.0461], grad_fn=<AddBackward0>)\n",
      "epoch: 3957 loss is tensor([-0.0424], grad_fn=<AddBackward0>)\n",
      "epoch: 3958 loss is tensor([-0.0694], grad_fn=<AddBackward0>)\n",
      "epoch: 3959 loss is tensor([0.0115], grad_fn=<AddBackward0>)\n",
      "epoch: 3960 loss is tensor([-0.0480], grad_fn=<AddBackward0>)\n",
      "epoch: 3961 loss is tensor([0.0093], grad_fn=<AddBackward0>)\n",
      "epoch: 3962 loss is tensor([-0.0599], grad_fn=<AddBackward0>)\n",
      "epoch: 3963 loss is tensor([-0.0497], grad_fn=<AddBackward0>)\n",
      "epoch: 3964 loss is tensor([-0.0771], grad_fn=<AddBackward0>)\n",
      "epoch: 3965 loss is tensor([-0.0790], grad_fn=<AddBackward0>)\n",
      "epoch: 3966 loss is tensor([-0.0566], grad_fn=<AddBackward0>)\n",
      "epoch: 3967 loss is tensor([-0.1049], grad_fn=<AddBackward0>)\n",
      "epoch: 3968 loss is tensor([-0.1157], grad_fn=<AddBackward0>)\n",
      "epoch: 3969 loss is tensor([-0.0533], grad_fn=<AddBackward0>)\n",
      "epoch: 3970 loss is tensor([-0.0424], grad_fn=<AddBackward0>)\n",
      "epoch: 3971 loss is tensor([-0.0341], grad_fn=<AddBackward0>)\n",
      "epoch: 3972 loss is tensor([-0.0741], grad_fn=<AddBackward0>)\n",
      "epoch: 3973 loss is tensor([-0.0672], grad_fn=<AddBackward0>)\n",
      "epoch: 3974 loss is tensor([-0.0312], grad_fn=<AddBackward0>)\n",
      "epoch: 3975 loss is tensor([-0.0526], grad_fn=<AddBackward0>)\n",
      "epoch: 3976 loss is tensor([-0.0192], grad_fn=<AddBackward0>)\n",
      "epoch: 3977 loss is tensor([-0.0715], grad_fn=<AddBackward0>)\n",
      "epoch: 3978 loss is tensor([-0.0901], grad_fn=<AddBackward0>)\n",
      "epoch: 3979 loss is tensor([-0.0528], grad_fn=<AddBackward0>)\n",
      "epoch: 3980 loss is tensor([-0.1089], grad_fn=<AddBackward0>)\n",
      "epoch: 3981 loss is tensor([-0.0347], grad_fn=<AddBackward0>)\n",
      "epoch: 3982 loss is tensor([-0.0397], grad_fn=<AddBackward0>)\n",
      "epoch: 3983 loss is tensor([-0.0766], grad_fn=<AddBackward0>)\n",
      "epoch: 3984 loss is tensor([-0.1140], grad_fn=<AddBackward0>)\n",
      "epoch: 3985 loss is tensor([-0.0265], grad_fn=<AddBackward0>)\n",
      "epoch: 3986 loss is tensor([-0.0422], grad_fn=<AddBackward0>)\n",
      "epoch: 3987 loss is tensor([-0.0678], grad_fn=<AddBackward0>)\n",
      "epoch: 3988 loss is tensor([-0.0379], grad_fn=<AddBackward0>)\n",
      "epoch: 3989 loss is tensor([-0.0723], grad_fn=<AddBackward0>)\n",
      "epoch: 3990 loss is tensor([-0.0680], grad_fn=<AddBackward0>)\n",
      "epoch: 3991 loss is tensor([-0.0851], grad_fn=<AddBackward0>)\n",
      "epoch: 3992 loss is tensor([-0.1062], grad_fn=<AddBackward0>)\n",
      "epoch: 3993 loss is tensor([-0.0686], grad_fn=<AddBackward0>)\n",
      "epoch: 3994 loss is tensor([-0.1157], grad_fn=<AddBackward0>)\n",
      "epoch: 3995 loss is tensor([-0.0617], grad_fn=<AddBackward0>)\n",
      "epoch: 3996 loss is tensor([-0.1045], grad_fn=<AddBackward0>)\n",
      "epoch: 3997 loss is tensor([-0.0888], grad_fn=<AddBackward0>)\n",
      "epoch: 3998 loss is tensor([-0.1144], grad_fn=<AddBackward0>)\n",
      "epoch: 3999 loss is tensor([-0.0674], grad_fn=<AddBackward0>)\n",
      "epoch: 4000 loss is tensor([-0.0311], grad_fn=<AddBackward0>)\n",
      "28\n"
=======
      "The number of epochs is: 3601\n",
      "The number of epochs is: 3602\n",
      "The number of epochs is: 3603\n",
      "The number of epochs is: 3604\n",
      "The number of epochs is: 3605\n",
      "The number of epochs is: 3606\n",
      "The number of epochs is: 3607\n",
      "The number of epochs is: 3608\n",
      "The number of epochs is: 3609\n",
      "The number of epochs is: 3610\n",
      "The number of epochs is: 3611\n",
      "The number of epochs is: 3612\n",
      "The number of epochs is: 3613\n",
      "The number of epochs is: 3614\n",
      "The number of epochs is: 3615\n",
      "The number of epochs is: 3616\n",
      "The number of epochs is: 3617\n",
      "The number of epochs is: 3618\n",
      "The number of epochs is: 3619\n",
      "The number of epochs is: 3620\n",
      "The number of epochs is: 3621\n",
      "The number of epochs is: 3622\n",
      "The number of epochs is: 3623\n",
      "The number of epochs is: 3624\n",
      "The number of epochs is: 3625\n",
      "The number of epochs is: 3626\n",
      "The number of epochs is: 3627\n",
      "The number of epochs is: 3628\n",
      "The number of epochs is: 3629\n",
      "The number of epochs is: 3630\n",
      "The number of epochs is: 3631\n",
      "The number of epochs is: 3632\n",
      "The number of epochs is: 3633\n",
      "The number of epochs is: 3634\n",
      "The number of epochs is: 3635\n",
      "The number of epochs is: 3636\n",
      "The number of epochs is: 3637\n",
      "The number of epochs is: 3638\n",
      "The number of epochs is: 3639\n",
      "The number of epochs is: 3640\n",
      "The number of epochs is: 3641\n",
      "The number of epochs is: 3642\n",
      "The number of epochs is: 3643\n",
      "The number of epochs is: 3644\n",
      "The number of epochs is: 3645\n",
      "The number of epochs is: 3646\n",
      "The number of epochs is: 3647\n",
      "The number of epochs is: 3648\n",
      "The number of epochs is: 3649\n",
      "The number of epochs is: 3650\n",
      "The number of epochs is: 3651\n",
      "The number of epochs is: 3652\n",
      "The number of epochs is: 3653\n",
      "The number of epochs is: 3654\n",
      "The number of epochs is: 3655\n",
      "The number of epochs is: 3656\n",
      "The number of epochs is: 3657\n",
      "The number of epochs is: 3658\n",
      "The number of epochs is: 3659\n",
      "The number of epochs is: 3660\n",
      "The number of epochs is: 3661\n",
      "The number of epochs is: 3662\n",
      "The number of epochs is: 3663\n",
      "The number of epochs is: 3664\n",
      "The number of epochs is: 3665\n",
      "The number of epochs is: 3666\n",
      "The number of epochs is: 3667\n",
      "The number of epochs is: 3668\n",
      "The number of epochs is: 3669\n",
      "The number of epochs is: 3670\n",
      "The number of epochs is: 3671\n",
      "The number of epochs is: 3672\n",
      "The number of epochs is: 3673\n",
      "The number of epochs is: 3674\n",
      "The number of epochs is: 3675\n",
      "The number of epochs is: 3676\n",
      "The number of epochs is: 3677\n",
      "The number of epochs is: 3678\n",
      "The number of epochs is: 3679\n",
      "The number of epochs is: 3680\n",
      "The number of epochs is: 3681\n",
      "The number of epochs is: 3682\n",
      "The number of epochs is: 3683\n",
      "The number of epochs is: 3684\n",
      "The number of epochs is: 3685\n",
      "The number of epochs is: 3686\n",
      "The number of epochs is: 3687\n",
      "The number of epochs is: 3688\n",
      "The number of epochs is: 3689\n",
      "The number of epochs is: 3690\n",
      "The number of epochs is: 3691\n",
      "The number of epochs is: 3692\n",
      "The number of epochs is: 3693\n",
      "The number of epochs is: 3694\n",
      "The number of epochs is: 3695\n",
      "The number of epochs is: 3696\n",
      "The number of epochs is: 3697\n",
      "The number of epochs is: 3698\n",
      "The number of epochs is: 3699\n",
      "The number of epochs is: 3700\n",
      "The number of epochs is: 3701\n",
      "The number of epochs is: 3702\n",
      "The number of epochs is: 3703\n",
      "The number of epochs is: 3704\n",
      "The number of epochs is: 3705\n",
      "The number of epochs is: 3706\n",
      "The number of epochs is: 3707\n",
      "The number of epochs is: 3708\n",
      "The number of epochs is: 3709\n",
      "The number of epochs is: 3710\n",
      "The number of epochs is: 3711\n",
      "The number of epochs is: 3712\n",
      "The number of epochs is: 3713\n",
      "The number of epochs is: 3714\n",
      "The number of epochs is: 3715\n",
      "The number of epochs is: 3716\n",
      "The number of epochs is: 3717\n",
      "The number of epochs is: 3718\n",
      "The number of epochs is: 3719\n",
      "The number of epochs is: 3720\n",
      "The number of epochs is: 3721\n",
      "The number of epochs is: 3722\n",
      "The number of epochs is: 3723\n",
      "The number of epochs is: 3724\n",
      "The number of epochs is: 3725\n",
      "The number of epochs is: 3726\n",
      "The number of epochs is: 3727\n",
      "The number of epochs is: 3728\n",
      "The number of epochs is: 3729\n",
      "The number of epochs is: 3730\n",
      "The number of epochs is: 3731\n",
      "The number of epochs is: 3732\n",
      "The number of epochs is: 3733\n",
      "The number of epochs is: 3734\n",
      "The number of epochs is: 3735\n",
      "The number of epochs is: 3736\n",
      "The number of epochs is: 3737\n",
      "The number of epochs is: 3738\n",
      "The number of epochs is: 3739\n",
      "The number of epochs is: 3740\n",
      "The number of epochs is: 3741\n",
      "The number of epochs is: 3742\n",
      "The number of epochs is: 3743\n",
      "The number of epochs is: 3744\n",
      "The number of epochs is: 3745\n",
      "The number of epochs is: 3746\n",
      "The number of epochs is: 3747\n",
      "The number of epochs is: 3748\n",
      "The number of epochs is: 3749\n",
      "The number of epochs is: 3750\n",
      "The number of epochs is: 3751\n",
      "The number of epochs is: 3752\n",
      "The number of epochs is: 3753\n",
      "The number of epochs is: 3754\n",
      "The number of epochs is: 3755\n",
      "The number of epochs is: 3756\n",
      "The number of epochs is: 3757\n",
      "The number of epochs is: 3758\n",
      "The number of epochs is: 3759\n",
      "The number of epochs is: 3760\n",
      "The number of epochs is: 3761\n",
      "The number of epochs is: 3762\n",
      "The number of epochs is: 3763\n",
      "The number of epochs is: 3764\n",
      "The number of epochs is: 3765\n",
      "The number of epochs is: 3766\n",
      "The number of epochs is: 3767\n",
      "The number of epochs is: 3768\n",
      "The number of epochs is: 3769\n",
      "The number of epochs is: 3770\n",
      "The number of epochs is: 3771\n",
      "The number of epochs is: 3772\n",
      "The number of epochs is: 3773\n",
      "The number of epochs is: 3774\n",
      "The number of epochs is: 3775\n",
      "The number of epochs is: 3776\n",
      "The number of epochs is: 3777\n",
      "The number of epochs is: 3778\n",
      "The number of epochs is: 3779\n",
      "The number of epochs is: 3780\n",
      "The number of epochs is: 3781\n",
      "The number of epochs is: 3782\n",
      "The number of epochs is: 3783\n",
      "The number of epochs is: 3784\n",
      "The number of epochs is: 3785\n",
      "The number of epochs is: 3786\n",
      "The number of epochs is: 3787\n",
      "The number of epochs is: 3788\n",
      "The number of epochs is: 3789\n",
      "The number of epochs is: 3790\n",
      "The number of epochs is: 3791\n",
      "The number of epochs is: 3792\n",
      "The number of epochs is: 3793\n",
      "The number of epochs is: 3794\n",
      "The number of epochs is: 3795\n",
      "The number of epochs is: 3796\n",
      "The number of epochs is: 3797\n",
      "The number of epochs is: 3798\n",
      "The number of epochs is: 3799\n",
      "The number of epochs is: 3800\n",
      "36\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4001 loss is tensor([-0.0988], grad_fn=<AddBackward0>)\n",
      "epoch: 4002 loss is tensor([-0.1444], grad_fn=<AddBackward0>)\n",
      "epoch: 4003 loss is tensor([-0.1023], grad_fn=<AddBackward0>)\n",
      "epoch: 4004 loss is tensor([-0.1815], grad_fn=<AddBackward0>)\n",
      "epoch: 4005 loss is tensor([-0.1554], grad_fn=<AddBackward0>)\n",
      "epoch: 4006 loss is tensor([-0.1065], grad_fn=<AddBackward0>)\n",
      "epoch: 4007 loss is tensor([-0.0909], grad_fn=<AddBackward0>)\n",
      "epoch: 4008 loss is tensor([-0.0739], grad_fn=<AddBackward0>)\n",
      "epoch: 4009 loss is tensor([-0.1545], grad_fn=<AddBackward0>)\n",
      "epoch: 4010 loss is tensor([-0.1306], grad_fn=<AddBackward0>)\n",
      "epoch: 4011 loss is tensor([-0.1040], grad_fn=<AddBackward0>)\n",
      "epoch: 4012 loss is tensor([-0.0972], grad_fn=<AddBackward0>)\n",
      "epoch: 4013 loss is tensor([-0.0868], grad_fn=<AddBackward0>)\n",
      "epoch: 4014 loss is tensor([-0.0642], grad_fn=<AddBackward0>)\n",
      "epoch: 4015 loss is tensor([-0.0465], grad_fn=<AddBackward0>)\n",
      "epoch: 4016 loss is tensor([-0.0497], grad_fn=<AddBackward0>)\n",
      "epoch: 4017 loss is tensor([-0.0904], grad_fn=<AddBackward0>)\n",
      "epoch: 4018 loss is tensor([-0.0867], grad_fn=<AddBackward0>)\n",
      "epoch: 4019 loss is tensor([-0.1395], grad_fn=<AddBackward0>)\n",
      "epoch: 4020 loss is tensor([-0.0478], grad_fn=<AddBackward0>)\n",
      "epoch: 4021 loss is tensor([-0.0679], grad_fn=<AddBackward0>)\n",
      "epoch: 4022 loss is tensor([-0.0641], grad_fn=<AddBackward0>)\n",
      "epoch: 4023 loss is tensor([-0.0668], grad_fn=<AddBackward0>)\n",
      "epoch: 4024 loss is tensor([-0.1473], grad_fn=<AddBackward0>)\n",
      "epoch: 4025 loss is tensor([-0.1243], grad_fn=<AddBackward0>)\n",
      "epoch: 4026 loss is tensor([-0.1154], grad_fn=<AddBackward0>)\n",
      "epoch: 4027 loss is tensor([-0.1100], grad_fn=<AddBackward0>)\n",
      "epoch: 4028 loss is tensor([-0.0541], grad_fn=<AddBackward0>)\n",
      "epoch: 4029 loss is tensor([-0.1389], grad_fn=<AddBackward0>)\n",
      "epoch: 4030 loss is tensor([-0.0827], grad_fn=<AddBackward0>)\n",
      "epoch: 4031 loss is tensor([-0.1107], grad_fn=<AddBackward0>)\n",
      "epoch: 4032 loss is tensor([-0.0592], grad_fn=<AddBackward0>)\n",
      "epoch: 4033 loss is tensor([-0.0437], grad_fn=<AddBackward0>)\n",
      "epoch: 4034 loss is tensor([-0.0723], grad_fn=<AddBackward0>)\n",
      "epoch: 4035 loss is tensor([-0.0715], grad_fn=<AddBackward0>)\n",
      "epoch: 4036 loss is tensor([-0.0648], grad_fn=<AddBackward0>)\n",
      "epoch: 4037 loss is tensor([-0.1100], grad_fn=<AddBackward0>)\n",
      "epoch: 4038 loss is tensor([-0.0110], grad_fn=<AddBackward0>)\n",
      "epoch: 4039 loss is tensor([-0.1224], grad_fn=<AddBackward0>)\n",
      "epoch: 4040 loss is tensor([-0.0384], grad_fn=<AddBackward0>)\n",
      "epoch: 4041 loss is tensor([-0.0334], grad_fn=<AddBackward0>)\n",
      "epoch: 4042 loss is tensor([-0.0712], grad_fn=<AddBackward0>)\n",
      "epoch: 4043 loss is tensor([-0.1534], grad_fn=<AddBackward0>)\n",
      "epoch: 4044 loss is tensor([-0.0503], grad_fn=<AddBackward0>)\n",
      "epoch: 4045 loss is tensor([-0.1195], grad_fn=<AddBackward0>)\n",
      "epoch: 4046 loss is tensor([-0.0790], grad_fn=<AddBackward0>)\n",
      "epoch: 4047 loss is tensor([-0.0355], grad_fn=<AddBackward0>)\n",
      "epoch: 4048 loss is tensor([-0.1125], grad_fn=<AddBackward0>)\n",
      "epoch: 4049 loss is tensor([-0.0870], grad_fn=<AddBackward0>)\n",
      "epoch: 4050 loss is tensor([-0.0839], grad_fn=<AddBackward0>)\n",
      "epoch: 4051 loss is tensor([-0.0716], grad_fn=<AddBackward0>)\n",
      "epoch: 4052 loss is tensor([-0.0872], grad_fn=<AddBackward0>)\n",
      "epoch: 4053 loss is tensor([-0.0376], grad_fn=<AddBackward0>)\n",
      "epoch: 4054 loss is tensor([-0.1113], grad_fn=<AddBackward0>)\n",
      "epoch: 4055 loss is tensor([-0.0460], grad_fn=<AddBackward0>)\n",
      "epoch: 4056 loss is tensor([-0.0714], grad_fn=<AddBackward0>)\n",
      "epoch: 4057 loss is tensor([-0.0606], grad_fn=<AddBackward0>)\n",
      "epoch: 4058 loss is tensor([-0.0952], grad_fn=<AddBackward0>)\n",
      "epoch: 4059 loss is tensor([-0.0936], grad_fn=<AddBackward0>)\n",
      "epoch: 4060 loss is tensor([-0.0800], grad_fn=<AddBackward0>)\n",
      "epoch: 4061 loss is tensor([-0.1520], grad_fn=<AddBackward0>)\n",
      "epoch: 4062 loss is tensor([-0.1296], grad_fn=<AddBackward0>)\n",
      "epoch: 4063 loss is tensor([-0.0592], grad_fn=<AddBackward0>)\n",
      "epoch: 4064 loss is tensor([-0.1043], grad_fn=<AddBackward0>)\n",
      "epoch: 4065 loss is tensor([-0.0921], grad_fn=<AddBackward0>)\n",
      "epoch: 4066 loss is tensor([-0.1703], grad_fn=<AddBackward0>)\n",
      "epoch: 4067 loss is tensor([-0.1443], grad_fn=<AddBackward0>)\n",
      "epoch: 4068 loss is tensor([-0.0745], grad_fn=<AddBackward0>)\n",
      "epoch: 4069 loss is tensor([-0.0517], grad_fn=<AddBackward0>)\n",
      "epoch: 4070 loss is tensor([-0.1249], grad_fn=<AddBackward0>)\n",
      "epoch: 4071 loss is tensor([-0.1374], grad_fn=<AddBackward0>)\n",
      "epoch: 4072 loss is tensor([-0.1582], grad_fn=<AddBackward0>)\n",
      "epoch: 4073 loss is tensor([-0.0859], grad_fn=<AddBackward0>)\n",
      "epoch: 4074 loss is tensor([-0.1146], grad_fn=<AddBackward0>)\n",
      "epoch: 4075 loss is tensor([-0.0776], grad_fn=<AddBackward0>)\n",
      "epoch: 4076 loss is tensor([-0.1109], grad_fn=<AddBackward0>)\n",
      "epoch: 4077 loss is tensor([-0.1228], grad_fn=<AddBackward0>)\n",
      "epoch: 4078 loss is tensor([-0.1528], grad_fn=<AddBackward0>)\n",
      "epoch: 4079 loss is tensor([-0.1308], grad_fn=<AddBackward0>)\n",
      "epoch: 4080 loss is tensor([-0.1630], grad_fn=<AddBackward0>)\n",
      "epoch: 4081 loss is tensor([-0.0750], grad_fn=<AddBackward0>)\n",
      "epoch: 4082 loss is tensor([-0.0315], grad_fn=<AddBackward0>)\n",
      "epoch: 4083 loss is tensor([-0.1209], grad_fn=<AddBackward0>)\n",
      "epoch: 4084 loss is tensor([-0.0934], grad_fn=<AddBackward0>)\n",
      "epoch: 4085 loss is tensor([-0.0963], grad_fn=<AddBackward0>)\n",
      "epoch: 4086 loss is tensor([-0.1080], grad_fn=<AddBackward0>)\n",
      "epoch: 4087 loss is tensor([-0.0732], grad_fn=<AddBackward0>)\n",
      "epoch: 4088 loss is tensor([-0.1118], grad_fn=<AddBackward0>)\n",
      "epoch: 4089 loss is tensor([-0.1111], grad_fn=<AddBackward0>)\n",
      "epoch: 4090 loss is tensor([-0.1122], grad_fn=<AddBackward0>)\n",
      "epoch: 4091 loss is tensor([-0.0914], grad_fn=<AddBackward0>)\n",
      "epoch: 4092 loss is tensor([-0.1249], grad_fn=<AddBackward0>)\n",
      "epoch: 4093 loss is tensor([-0.0964], grad_fn=<AddBackward0>)\n",
      "epoch: 4094 loss is tensor([-0.0153], grad_fn=<AddBackward0>)\n",
      "epoch: 4095 loss is tensor([-0.1426], grad_fn=<AddBackward0>)\n",
      "epoch: 4096 loss is tensor([-0.0796], grad_fn=<AddBackward0>)\n",
      "epoch: 4097 loss is tensor([-0.1011], grad_fn=<AddBackward0>)\n",
      "epoch: 4098 loss is tensor([-0.0706], grad_fn=<AddBackward0>)\n",
      "epoch: 4099 loss is tensor([-0.1299], grad_fn=<AddBackward0>)\n",
      "epoch: 4100 loss is tensor([-0.0900], grad_fn=<AddBackward0>)\n",
      "30\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 4101 loss is tensor([-0.0396], grad_fn=<AddBackward0>)\n",
      "epoch: 4102 loss is tensor([-0.0755], grad_fn=<AddBackward0>)\n",
      "epoch: 4103 loss is tensor([-0.1095], grad_fn=<AddBackward0>)\n",
      "epoch: 4104 loss is tensor([-0.0757], grad_fn=<AddBackward0>)\n",
      "epoch: 4105 loss is tensor([-0.1276], grad_fn=<AddBackward0>)\n",
      "epoch: 4106 loss is tensor([-0.0835], grad_fn=<AddBackward0>)\n",
      "epoch: 4107 loss is tensor([-0.1158], grad_fn=<AddBackward0>)\n",
      "epoch: 4108 loss is tensor([-0.1159], grad_fn=<AddBackward0>)\n",
      "epoch: 4109 loss is tensor([-0.1080], grad_fn=<AddBackward0>)\n",
      "epoch: 4110 loss is tensor([-0.1196], grad_fn=<AddBackward0>)\n",
      "epoch: 4111 loss is tensor([-0.0902], grad_fn=<AddBackward0>)\n",
      "epoch: 4112 loss is tensor([-0.1210], grad_fn=<AddBackward0>)\n",
      "epoch: 4113 loss is tensor([-0.1275], grad_fn=<AddBackward0>)\n",
      "epoch: 4114 loss is tensor([-0.1089], grad_fn=<AddBackward0>)\n",
      "epoch: 4115 loss is tensor([-0.0963], grad_fn=<AddBackward0>)\n",
      "epoch: 4116 loss is tensor([-0.0997], grad_fn=<AddBackward0>)\n",
      "epoch: 4117 loss is tensor([-0.0982], grad_fn=<AddBackward0>)\n",
      "epoch: 4118 loss is tensor([-0.0868], grad_fn=<AddBackward0>)\n",
      "epoch: 4119 loss is tensor([-0.1028], grad_fn=<AddBackward0>)\n",
      "epoch: 4120 loss is tensor([-0.0942], grad_fn=<AddBackward0>)\n",
      "epoch: 4121 loss is tensor([-0.0699], grad_fn=<AddBackward0>)\n",
      "epoch: 4122 loss is tensor([-0.0812], grad_fn=<AddBackward0>)\n",
      "epoch: 4123 loss is tensor([-0.1882], grad_fn=<AddBackward0>)\n",
      "epoch: 4124 loss is tensor([-0.1554], grad_fn=<AddBackward0>)\n",
      "epoch: 4125 loss is tensor([-0.1354], grad_fn=<AddBackward0>)\n",
      "epoch: 4126 loss is tensor([-0.1442], grad_fn=<AddBackward0>)\n",
      "epoch: 4127 loss is tensor([-0.1451], grad_fn=<AddBackward0>)\n",
      "epoch: 4128 loss is tensor([-0.1679], grad_fn=<AddBackward0>)\n",
      "epoch: 4129 loss is tensor([-0.0818], grad_fn=<AddBackward0>)\n",
      "epoch: 4130 loss is tensor([-0.1321], grad_fn=<AddBackward0>)\n",
      "epoch: 4131 loss is tensor([-0.0826], grad_fn=<AddBackward0>)\n",
      "epoch: 4132 loss is tensor([-0.0839], grad_fn=<AddBackward0>)\n",
      "epoch: 4133 loss is tensor([-0.0675], grad_fn=<AddBackward0>)\n",
      "epoch: 4134 loss is tensor([-0.1522], grad_fn=<AddBackward0>)\n",
      "epoch: 4135 loss is tensor([-0.0555], grad_fn=<AddBackward0>)\n",
      "epoch: 4136 loss is tensor([-0.1466], grad_fn=<AddBackward0>)\n",
      "epoch: 4137 loss is tensor([-0.1343], grad_fn=<AddBackward0>)\n",
      "epoch: 4138 loss is tensor([-0.1060], grad_fn=<AddBackward0>)\n",
      "epoch: 4139 loss is tensor([-0.1488], grad_fn=<AddBackward0>)\n",
      "epoch: 4140 loss is tensor([-0.0778], grad_fn=<AddBackward0>)\n",
      "epoch: 4141 loss is tensor([-0.1929], grad_fn=<AddBackward0>)\n",
      "epoch: 4142 loss is tensor([-0.0801], grad_fn=<AddBackward0>)\n",
      "epoch: 4143 loss is tensor([-0.1267], grad_fn=<AddBackward0>)\n",
      "epoch: 4144 loss is tensor([-0.1398], grad_fn=<AddBackward0>)\n",
      "epoch: 4145 loss is tensor([-0.1312], grad_fn=<AddBackward0>)\n",
      "epoch: 4146 loss is tensor([-0.1074], grad_fn=<AddBackward0>)\n",
      "epoch: 4147 loss is tensor([-0.1478], grad_fn=<AddBackward0>)\n",
      "epoch: 4148 loss is tensor([-0.0981], grad_fn=<AddBackward0>)\n",
      "epoch: 4149 loss is tensor([-0.1231], grad_fn=<AddBackward0>)\n",
      "epoch: 4150 loss is tensor([-0.0835], grad_fn=<AddBackward0>)\n",
      "epoch: 4151 loss is tensor([-0.1936], grad_fn=<AddBackward0>)\n",
      "epoch: 4152 loss is tensor([-0.1230], grad_fn=<AddBackward0>)\n",
      "epoch: 4153 loss is tensor([-0.1154], grad_fn=<AddBackward0>)\n",
      "epoch: 4154 loss is tensor([-0.1569], grad_fn=<AddBackward0>)\n",
      "epoch: 4155 loss is tensor([-0.1092], grad_fn=<AddBackward0>)\n",
      "epoch: 4156 loss is tensor([-0.1469], grad_fn=<AddBackward0>)\n",
      "epoch: 4157 loss is tensor([-0.1952], grad_fn=<AddBackward0>)\n",
      "epoch: 4158 loss is tensor([-0.1493], grad_fn=<AddBackward0>)\n",
      "epoch: 4159 loss is tensor([-0.1474], grad_fn=<AddBackward0>)\n",
      "epoch: 4160 loss is tensor([-0.1975], grad_fn=<AddBackward0>)\n",
      "epoch: 4161 loss is tensor([-0.1251], grad_fn=<AddBackward0>)\n",
      "epoch: 4162 loss is tensor([-0.1447], grad_fn=<AddBackward0>)\n",
      "epoch: 4163 loss is tensor([-0.0926], grad_fn=<AddBackward0>)\n",
      "epoch: 4164 loss is tensor([-0.1229], grad_fn=<AddBackward0>)\n",
      "epoch: 4165 loss is tensor([-0.1504], grad_fn=<AddBackward0>)\n",
      "epoch: 4166 loss is tensor([-0.0999], grad_fn=<AddBackward0>)\n",
      "epoch: 4167 loss is tensor([-0.0697], grad_fn=<AddBackward0>)\n",
      "epoch: 4168 loss is tensor([-0.1344], grad_fn=<AddBackward0>)\n",
      "epoch: 4169 loss is tensor([-0.1373], grad_fn=<AddBackward0>)\n",
      "epoch: 4170 loss is tensor([-0.1602], grad_fn=<AddBackward0>)\n",
      "epoch: 4171 loss is tensor([-0.1631], grad_fn=<AddBackward0>)\n",
      "epoch: 4172 loss is tensor([-0.1415], grad_fn=<AddBackward0>)\n",
      "epoch: 4173 loss is tensor([-0.1272], grad_fn=<AddBackward0>)\n",
      "epoch: 4174 loss is tensor([-0.1359], grad_fn=<AddBackward0>)\n",
      "epoch: 4175 loss is tensor([-0.1216], grad_fn=<AddBackward0>)\n",
      "epoch: 4176 loss is tensor([-0.1902], grad_fn=<AddBackward0>)\n",
      "epoch: 4177 loss is tensor([-0.1202], grad_fn=<AddBackward0>)\n",
      "epoch: 4178 loss is tensor([-0.0933], grad_fn=<AddBackward0>)\n",
      "epoch: 4179 loss is tensor([-0.1454], grad_fn=<AddBackward0>)\n",
      "epoch: 4180 loss is tensor([-0.1219], grad_fn=<AddBackward0>)\n",
      "epoch: 4181 loss is tensor([-0.1376], grad_fn=<AddBackward0>)\n",
      "epoch: 4182 loss is tensor([-0.1527], grad_fn=<AddBackward0>)\n",
      "epoch: 4183 loss is tensor([-0.1254], grad_fn=<AddBackward0>)\n",
      "epoch: 4184 loss is tensor([-0.0940], grad_fn=<AddBackward0>)\n",
      "epoch: 4185 loss is tensor([-0.1092], grad_fn=<AddBackward0>)\n",
      "epoch: 4186 loss is tensor([-0.1556], grad_fn=<AddBackward0>)\n",
      "epoch: 4187 loss is tensor([-0.1023], grad_fn=<AddBackward0>)\n",
      "epoch: 4188 loss is tensor([-0.1496], grad_fn=<AddBackward0>)\n",
      "epoch: 4189 loss is tensor([-0.1761], grad_fn=<AddBackward0>)\n",
      "epoch: 4190 loss is tensor([-0.1181], grad_fn=<AddBackward0>)\n",
      "epoch: 4191 loss is tensor([-0.1580], grad_fn=<AddBackward0>)\n",
      "epoch: 4192 loss is tensor([-0.1408], grad_fn=<AddBackward0>)\n",
      "epoch: 4193 loss is tensor([-0.1358], grad_fn=<AddBackward0>)\n",
      "epoch: 4194 loss is tensor([-0.2076], grad_fn=<AddBackward0>)\n",
      "epoch: 4195 loss is tensor([-0.2063], grad_fn=<AddBackward0>)\n",
      "epoch: 4196 loss is tensor([-0.1661], grad_fn=<AddBackward0>)\n",
      "epoch: 4197 loss is tensor([-0.1490], grad_fn=<AddBackward0>)\n",
      "epoch: 4198 loss is tensor([-0.1296], grad_fn=<AddBackward0>)\n",
      "epoch: 4199 loss is tensor([-0.1390], grad_fn=<AddBackward0>)\n",
      "epoch: 4200 loss is tensor([-0.1188], grad_fn=<AddBackward0>)\n",
      "20\n"
=======
      "The number of epochs is: 3801\n",
      "The number of epochs is: 3802\n",
      "The number of epochs is: 3803\n",
      "The number of epochs is: 3804\n",
      "The number of epochs is: 3805\n",
      "The number of epochs is: 3806\n",
      "The number of epochs is: 3807\n",
      "The number of epochs is: 3808\n",
      "The number of epochs is: 3809\n",
      "The number of epochs is: 3810\n",
      "The number of epochs is: 3811\n",
      "The number of epochs is: 3812\n",
      "The number of epochs is: 3813\n",
      "The number of epochs is: 3814\n",
      "The number of epochs is: 3815\n",
      "The number of epochs is: 3816\n",
      "The number of epochs is: 3817\n",
      "The number of epochs is: 3818\n",
      "The number of epochs is: 3819\n",
      "The number of epochs is: 3820\n",
      "The number of epochs is: 3821\n",
      "The number of epochs is: 3822\n",
      "The number of epochs is: 3823\n",
      "The number of epochs is: 3824\n",
      "The number of epochs is: 3825\n",
      "The number of epochs is: 3826\n",
      "The number of epochs is: 3827\n",
      "The number of epochs is: 3828\n",
      "The number of epochs is: 3829\n",
      "The number of epochs is: 3830\n",
      "The number of epochs is: 3831\n",
      "The number of epochs is: 3832\n",
      "The number of epochs is: 3833\n",
      "The number of epochs is: 3834\n",
      "The number of epochs is: 3835\n",
      "The number of epochs is: 3836\n",
      "The number of epochs is: 3837\n",
      "The number of epochs is: 3838\n",
      "The number of epochs is: 3839\n",
      "The number of epochs is: 3840\n",
      "The number of epochs is: 3841\n",
      "The number of epochs is: 3842\n",
      "The number of epochs is: 3843\n",
      "The number of epochs is: 3844\n",
      "The number of epochs is: 3845\n",
      "The number of epochs is: 3846\n",
      "The number of epochs is: 3847\n",
      "The number of epochs is: 3848\n",
      "The number of epochs is: 3849\n",
      "The number of epochs is: 3850\n",
      "The number of epochs is: 3851\n",
      "The number of epochs is: 3852\n",
      "The number of epochs is: 3853\n",
      "The number of epochs is: 3854\n",
      "The number of epochs is: 3855\n",
      "The number of epochs is: 3856\n",
      "The number of epochs is: 3857\n",
      "The number of epochs is: 3858\n",
      "The number of epochs is: 3859\n",
      "The number of epochs is: 3860\n",
      "The number of epochs is: 3861\n",
      "The number of epochs is: 3862\n",
      "The number of epochs is: 3863\n",
      "The number of epochs is: 3864\n",
      "The number of epochs is: 3865\n",
      "The number of epochs is: 3866\n",
      "The number of epochs is: 3867\n",
      "The number of epochs is: 3868\n",
      "The number of epochs is: 3869\n",
      "The number of epochs is: 3870\n",
      "The number of epochs is: 3871\n",
      "The number of epochs is: 3872\n",
      "The number of epochs is: 3873\n",
      "The number of epochs is: 3874\n",
      "The number of epochs is: 3875\n",
      "The number of epochs is: 3876\n",
      "The number of epochs is: 3877\n",
      "The number of epochs is: 3878\n",
      "The number of epochs is: 3879\n",
      "The number of epochs is: 3880\n",
      "The number of epochs is: 3881\n",
      "The number of epochs is: 3882\n",
      "The number of epochs is: 3883\n",
      "The number of epochs is: 3884\n",
      "The number of epochs is: 3885\n",
      "The number of epochs is: 3886\n",
      "The number of epochs is: 3887\n",
      "The number of epochs is: 3888\n",
      "The number of epochs is: 3889\n",
      "The number of epochs is: 3890\n",
      "The number of epochs is: 3891\n",
      "The number of epochs is: 3892\n",
      "The number of epochs is: 3893\n",
      "The number of epochs is: 3894\n",
      "The number of epochs is: 3895\n",
      "The number of epochs is: 3896\n",
      "The number of epochs is: 3897\n",
      "The number of epochs is: 3898\n",
      "The number of epochs is: 3899\n",
      "The number of epochs is: 3900\n",
      "The number of epochs is: 3901\n",
      "The number of epochs is: 3902\n",
      "The number of epochs is: 3903\n",
      "The number of epochs is: 3904\n",
      "The number of epochs is: 3905\n",
      "The number of epochs is: 3906\n",
      "The number of epochs is: 3907\n",
      "The number of epochs is: 3908\n",
      "The number of epochs is: 3909\n",
      "The number of epochs is: 3910\n",
      "The number of epochs is: 3911\n",
      "The number of epochs is: 3912\n",
      "The number of epochs is: 3913\n",
      "The number of epochs is: 3914\n",
      "The number of epochs is: 3915\n",
      "The number of epochs is: 3916\n",
      "The number of epochs is: 3917\n",
      "The number of epochs is: 3918\n",
      "The number of epochs is: 3919\n",
      "The number of epochs is: 3920\n",
      "The number of epochs is: 3921\n",
      "The number of epochs is: 3922\n",
      "The number of epochs is: 3923\n",
      "The number of epochs is: 3924\n",
      "The number of epochs is: 3925\n",
      "The number of epochs is: 3926\n",
      "The number of epochs is: 3927\n",
      "The number of epochs is: 3928\n",
      "The number of epochs is: 3929\n",
      "The number of epochs is: 3930\n",
      "The number of epochs is: 3931\n",
      "The number of epochs is: 3932\n",
      "The number of epochs is: 3933\n",
      "The number of epochs is: 3934\n",
      "The number of epochs is: 3935\n",
      "The number of epochs is: 3936\n",
      "The number of epochs is: 3937\n",
      "The number of epochs is: 3938\n",
      "The number of epochs is: 3939\n",
      "The number of epochs is: 3940\n",
      "The number of epochs is: 3941\n",
      "The number of epochs is: 3942\n",
      "The number of epochs is: 3943\n",
      "The number of epochs is: 3944\n",
      "The number of epochs is: 3945\n",
      "The number of epochs is: 3946\n",
      "The number of epochs is: 3947\n",
      "The number of epochs is: 3948\n",
      "The number of epochs is: 3949\n",
      "The number of epochs is: 3950\n",
      "The number of epochs is: 3951\n",
      "The number of epochs is: 3952\n",
      "The number of epochs is: 3953\n",
      "The number of epochs is: 3954\n",
      "The number of epochs is: 3955\n",
      "The number of epochs is: 3956\n",
      "The number of epochs is: 3957\n",
      "The number of epochs is: 3958\n",
      "The number of epochs is: 3959\n",
      "The number of epochs is: 3960\n",
      "The number of epochs is: 3961\n",
      "The number of epochs is: 3962\n",
      "The number of epochs is: 3963\n",
      "The number of epochs is: 3964\n",
      "The number of epochs is: 3965\n",
      "The number of epochs is: 3966\n",
      "The number of epochs is: 3967\n",
      "The number of epochs is: 3968\n",
      "The number of epochs is: 3969\n",
      "The number of epochs is: 3970\n",
      "The number of epochs is: 3971\n",
      "The number of epochs is: 3972\n",
      "The number of epochs is: 3973\n",
      "The number of epochs is: 3974\n",
      "The number of epochs is: 3975\n",
      "The number of epochs is: 3976\n",
      "The number of epochs is: 3977\n",
      "The number of epochs is: 3978\n",
      "The number of epochs is: 3979\n",
      "The number of epochs is: 3980\n",
      "The number of epochs is: 3981\n",
      "The number of epochs is: 3982\n",
      "The number of epochs is: 3983\n",
      "The number of epochs is: 3984\n",
      "The number of epochs is: 3985\n",
      "The number of epochs is: 3986\n",
      "The number of epochs is: 3987\n",
      "The number of epochs is: 3988\n",
      "The number of epochs is: 3989\n",
      "The number of epochs is: 3990\n",
      "The number of epochs is: 3991\n",
      "The number of epochs is: 3992\n",
      "The number of epochs is: 3993\n",
      "The number of epochs is: 3994\n",
      "The number of epochs is: 3995\n",
      "The number of epochs is: 3996\n",
      "The number of epochs is: 3997\n",
      "The number of epochs is: 3998\n",
      "The number of epochs is: 3999\n",
      "The number of epochs is: 4000\n",
      "31\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4201 loss is tensor([-0.1484], grad_fn=<AddBackward0>)\n",
      "epoch: 4202 loss is tensor([-0.1615], grad_fn=<AddBackward0>)\n",
      "epoch: 4203 loss is tensor([-0.1333], grad_fn=<AddBackward0>)\n",
      "epoch: 4204 loss is tensor([-0.1142], grad_fn=<AddBackward0>)\n",
      "epoch: 4205 loss is tensor([-0.1281], grad_fn=<AddBackward0>)\n",
      "epoch: 4206 loss is tensor([-0.1261], grad_fn=<AddBackward0>)\n",
      "epoch: 4207 loss is tensor([-0.1050], grad_fn=<AddBackward0>)\n",
      "epoch: 4208 loss is tensor([-0.1595], grad_fn=<AddBackward0>)\n",
      "epoch: 4209 loss is tensor([-0.1342], grad_fn=<AddBackward0>)\n",
      "epoch: 4210 loss is tensor([-0.1169], grad_fn=<AddBackward0>)\n",
      "epoch: 4211 loss is tensor([-0.1789], grad_fn=<AddBackward0>)\n",
      "epoch: 4212 loss is tensor([-0.0849], grad_fn=<AddBackward0>)\n",
      "epoch: 4213 loss is tensor([-0.1045], grad_fn=<AddBackward0>)\n",
      "epoch: 4214 loss is tensor([-0.1468], grad_fn=<AddBackward0>)\n",
      "epoch: 4215 loss is tensor([-0.1071], grad_fn=<AddBackward0>)\n",
      "epoch: 4216 loss is tensor([-0.1384], grad_fn=<AddBackward0>)\n",
      "epoch: 4217 loss is tensor([-0.1738], grad_fn=<AddBackward0>)\n",
      "epoch: 4218 loss is tensor([-0.1316], grad_fn=<AddBackward0>)\n",
      "epoch: 4219 loss is tensor([-0.1886], grad_fn=<AddBackward0>)\n",
      "epoch: 4220 loss is tensor([-0.1565], grad_fn=<AddBackward0>)\n",
      "epoch: 4221 loss is tensor([-0.1035], grad_fn=<AddBackward0>)\n",
      "epoch: 4222 loss is tensor([-0.0572], grad_fn=<AddBackward0>)\n",
      "epoch: 4223 loss is tensor([-0.0834], grad_fn=<AddBackward0>)\n",
      "epoch: 4224 loss is tensor([-0.0838], grad_fn=<AddBackward0>)\n",
      "epoch: 4225 loss is tensor([-0.1820], grad_fn=<AddBackward0>)\n",
      "epoch: 4226 loss is tensor([-0.0842], grad_fn=<AddBackward0>)\n",
      "epoch: 4227 loss is tensor([-0.1540], grad_fn=<AddBackward0>)\n",
      "epoch: 4228 loss is tensor([-0.1393], grad_fn=<AddBackward0>)\n",
      "epoch: 4229 loss is tensor([-0.1348], grad_fn=<AddBackward0>)\n",
      "epoch: 4230 loss is tensor([-0.1387], grad_fn=<AddBackward0>)\n",
      "epoch: 4231 loss is tensor([-0.1041], grad_fn=<AddBackward0>)\n",
      "epoch: 4232 loss is tensor([-0.0847], grad_fn=<AddBackward0>)\n",
      "epoch: 4233 loss is tensor([-0.0968], grad_fn=<AddBackward0>)\n",
      "epoch: 4234 loss is tensor([-0.0427], grad_fn=<AddBackward0>)\n",
      "epoch: 4235 loss is tensor([-0.0634], grad_fn=<AddBackward0>)\n",
      "epoch: 4236 loss is tensor([-0.0983], grad_fn=<AddBackward0>)\n",
      "epoch: 4237 loss is tensor([-0.1434], grad_fn=<AddBackward0>)\n",
      "epoch: 4238 loss is tensor([-0.1033], grad_fn=<AddBackward0>)\n",
      "epoch: 4239 loss is tensor([-0.0914], grad_fn=<AddBackward0>)\n",
      "epoch: 4240 loss is tensor([-0.1063], grad_fn=<AddBackward0>)\n",
      "epoch: 4241 loss is tensor([-0.0903], grad_fn=<AddBackward0>)\n",
      "epoch: 4242 loss is tensor([-0.0555], grad_fn=<AddBackward0>)\n",
      "epoch: 4243 loss is tensor([-0.0630], grad_fn=<AddBackward0>)\n",
      "epoch: 4244 loss is tensor([-0.0576], grad_fn=<AddBackward0>)\n",
      "epoch: 4245 loss is tensor([-0.1025], grad_fn=<AddBackward0>)\n",
      "epoch: 4246 loss is tensor([-0.0737], grad_fn=<AddBackward0>)\n",
      "epoch: 4247 loss is tensor([-0.0828], grad_fn=<AddBackward0>)\n",
      "epoch: 4248 loss is tensor([-0.1155], grad_fn=<AddBackward0>)\n",
      "epoch: 4249 loss is tensor([-0.0899], grad_fn=<AddBackward0>)\n",
      "epoch: 4250 loss is tensor([-0.1088], grad_fn=<AddBackward0>)\n",
      "epoch: 4251 loss is tensor([-0.1530], grad_fn=<AddBackward0>)\n",
      "epoch: 4252 loss is tensor([-0.1249], grad_fn=<AddBackward0>)\n",
      "epoch: 4253 loss is tensor([-0.0905], grad_fn=<AddBackward0>)\n",
      "epoch: 4254 loss is tensor([-0.1239], grad_fn=<AddBackward0>)\n",
      "epoch: 4255 loss is tensor([-0.1550], grad_fn=<AddBackward0>)\n",
      "epoch: 4256 loss is tensor([-0.1543], grad_fn=<AddBackward0>)\n",
      "epoch: 4257 loss is tensor([-0.1675], grad_fn=<AddBackward0>)\n",
      "epoch: 4258 loss is tensor([-0.1456], grad_fn=<AddBackward0>)\n",
      "epoch: 4259 loss is tensor([-0.1130], grad_fn=<AddBackward0>)\n",
      "epoch: 4260 loss is tensor([-0.1057], grad_fn=<AddBackward0>)\n",
      "epoch: 4261 loss is tensor([-0.1593], grad_fn=<AddBackward0>)\n",
      "epoch: 4262 loss is tensor([-0.1440], grad_fn=<AddBackward0>)\n",
      "epoch: 4263 loss is tensor([-0.1528], grad_fn=<AddBackward0>)\n",
      "epoch: 4264 loss is tensor([-0.1197], grad_fn=<AddBackward0>)\n",
      "epoch: 4265 loss is tensor([-0.2023], grad_fn=<AddBackward0>)\n",
      "epoch: 4266 loss is tensor([-0.1149], grad_fn=<AddBackward0>)\n",
      "epoch: 4267 loss is tensor([-0.1221], grad_fn=<AddBackward0>)\n",
      "epoch: 4268 loss is tensor([-0.1178], grad_fn=<AddBackward0>)\n",
      "epoch: 4269 loss is tensor([-0.1861], grad_fn=<AddBackward0>)\n",
      "epoch: 4270 loss is tensor([-0.1428], grad_fn=<AddBackward0>)\n",
      "epoch: 4271 loss is tensor([-0.1508], grad_fn=<AddBackward0>)\n",
      "epoch: 4272 loss is tensor([-0.1922], grad_fn=<AddBackward0>)\n",
      "epoch: 4273 loss is tensor([-0.0764], grad_fn=<AddBackward0>)\n",
      "epoch: 4274 loss is tensor([-0.0946], grad_fn=<AddBackward0>)\n",
      "epoch: 4275 loss is tensor([-0.0848], grad_fn=<AddBackward0>)\n",
      "epoch: 4276 loss is tensor([-0.1434], grad_fn=<AddBackward0>)\n",
      "epoch: 4277 loss is tensor([-0.0970], grad_fn=<AddBackward0>)\n",
      "epoch: 4278 loss is tensor([-0.1066], grad_fn=<AddBackward0>)\n",
      "epoch: 4279 loss is tensor([-0.0736], grad_fn=<AddBackward0>)\n",
      "epoch: 4280 loss is tensor([-0.1016], grad_fn=<AddBackward0>)\n",
      "epoch: 4281 loss is tensor([-0.0688], grad_fn=<AddBackward0>)\n",
      "epoch: 4282 loss is tensor([-0.1385], grad_fn=<AddBackward0>)\n",
      "epoch: 4283 loss is tensor([-0.1222], grad_fn=<AddBackward0>)\n",
      "epoch: 4284 loss is tensor([-0.1331], grad_fn=<AddBackward0>)\n",
      "epoch: 4285 loss is tensor([-0.1547], grad_fn=<AddBackward0>)\n",
      "epoch: 4286 loss is tensor([-0.1416], grad_fn=<AddBackward0>)\n",
      "epoch: 4287 loss is tensor([-0.0401], grad_fn=<AddBackward0>)\n",
      "epoch: 4288 loss is tensor([-0.1179], grad_fn=<AddBackward0>)\n",
      "epoch: 4289 loss is tensor([-0.1055], grad_fn=<AddBackward0>)\n",
      "epoch: 4290 loss is tensor([-0.0190], grad_fn=<AddBackward0>)\n",
      "epoch: 4291 loss is tensor([-0.1068], grad_fn=<AddBackward0>)\n",
      "epoch: 4292 loss is tensor([-0.0571], grad_fn=<AddBackward0>)\n",
      "epoch: 4293 loss is tensor([-0.0906], grad_fn=<AddBackward0>)\n",
      "epoch: 4294 loss is tensor([-0.1255], grad_fn=<AddBackward0>)\n",
      "epoch: 4295 loss is tensor([-0.1159], grad_fn=<AddBackward0>)\n",
      "epoch: 4296 loss is tensor([-0.1385], grad_fn=<AddBackward0>)\n",
      "epoch: 4297 loss is tensor([-0.0649], grad_fn=<AddBackward0>)\n",
      "epoch: 4298 loss is tensor([-0.1035], grad_fn=<AddBackward0>)\n",
      "epoch: 4299 loss is tensor([-0.0668], grad_fn=<AddBackward0>)\n",
      "epoch: 4300 loss is tensor([-0.1031], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 4301 loss is tensor([-0.1031], grad_fn=<AddBackward0>)\n",
      "epoch: 4302 loss is tensor([-0.0699], grad_fn=<AddBackward0>)\n",
      "epoch: 4303 loss is tensor([-0.0668], grad_fn=<AddBackward0>)\n",
      "epoch: 4304 loss is tensor([-0.0827], grad_fn=<AddBackward0>)\n",
      "epoch: 4305 loss is tensor([-0.1166], grad_fn=<AddBackward0>)\n",
      "epoch: 4306 loss is tensor([-0.0954], grad_fn=<AddBackward0>)\n",
      "epoch: 4307 loss is tensor([-0.1584], grad_fn=<AddBackward0>)\n",
      "epoch: 4308 loss is tensor([-0.1275], grad_fn=<AddBackward0>)\n",
      "epoch: 4309 loss is tensor([-0.0829], grad_fn=<AddBackward0>)\n",
      "epoch: 4310 loss is tensor([-0.0972], grad_fn=<AddBackward0>)\n",
      "epoch: 4311 loss is tensor([-0.1718], grad_fn=<AddBackward0>)\n",
      "epoch: 4312 loss is tensor([-0.0698], grad_fn=<AddBackward0>)\n",
      "epoch: 4313 loss is tensor([-0.1349], grad_fn=<AddBackward0>)\n",
      "epoch: 4314 loss is tensor([-0.0958], grad_fn=<AddBackward0>)\n",
      "epoch: 4315 loss is tensor([-0.0928], grad_fn=<AddBackward0>)\n",
      "epoch: 4316 loss is tensor([-0.1624], grad_fn=<AddBackward0>)\n",
      "epoch: 4317 loss is tensor([-0.1432], grad_fn=<AddBackward0>)\n",
      "epoch: 4318 loss is tensor([-0.1656], grad_fn=<AddBackward0>)\n",
      "epoch: 4319 loss is tensor([-0.1243], grad_fn=<AddBackward0>)\n",
      "epoch: 4320 loss is tensor([-0.1351], grad_fn=<AddBackward0>)\n",
      "epoch: 4321 loss is tensor([-0.1430], grad_fn=<AddBackward0>)\n",
      "epoch: 4322 loss is tensor([-0.1144], grad_fn=<AddBackward0>)\n",
      "epoch: 4323 loss is tensor([-0.2002], grad_fn=<AddBackward0>)\n",
      "epoch: 4324 loss is tensor([-0.1610], grad_fn=<AddBackward0>)\n",
      "epoch: 4325 loss is tensor([-0.1725], grad_fn=<AddBackward0>)\n",
      "epoch: 4326 loss is tensor([-0.0726], grad_fn=<AddBackward0>)\n",
      "epoch: 4327 loss is tensor([-0.1834], grad_fn=<AddBackward0>)\n",
      "epoch: 4328 loss is tensor([-0.0907], grad_fn=<AddBackward0>)\n",
      "epoch: 4329 loss is tensor([-0.1114], grad_fn=<AddBackward0>)\n",
      "epoch: 4330 loss is tensor([-0.1743], grad_fn=<AddBackward0>)\n",
      "epoch: 4331 loss is tensor([-0.0993], grad_fn=<AddBackward0>)\n",
      "epoch: 4332 loss is tensor([-0.0717], grad_fn=<AddBackward0>)\n",
      "epoch: 4333 loss is tensor([-0.0253], grad_fn=<AddBackward0>)\n",
      "epoch: 4334 loss is tensor([-0.0913], grad_fn=<AddBackward0>)\n",
      "epoch: 4335 loss is tensor([-0.0976], grad_fn=<AddBackward0>)\n",
      "epoch: 4336 loss is tensor([-0.0526], grad_fn=<AddBackward0>)\n",
      "epoch: 4337 loss is tensor([-0.0898], grad_fn=<AddBackward0>)\n",
      "epoch: 4338 loss is tensor([-0.0432], grad_fn=<AddBackward0>)\n",
      "epoch: 4339 loss is tensor([-0.0827], grad_fn=<AddBackward0>)\n",
      "epoch: 4340 loss is tensor([-0.0428], grad_fn=<AddBackward0>)\n",
      "epoch: 4341 loss is tensor([-0.0875], grad_fn=<AddBackward0>)\n",
      "epoch: 4342 loss is tensor([-0.1269], grad_fn=<AddBackward0>)\n",
      "epoch: 4343 loss is tensor([-0.0988], grad_fn=<AddBackward0>)\n",
      "epoch: 4344 loss is tensor([-0.0953], grad_fn=<AddBackward0>)\n",
      "epoch: 4345 loss is tensor([-0.0935], grad_fn=<AddBackward0>)\n",
      "epoch: 4346 loss is tensor([-0.1491], grad_fn=<AddBackward0>)\n",
      "epoch: 4347 loss is tensor([-0.1002], grad_fn=<AddBackward0>)\n",
      "epoch: 4348 loss is tensor([-0.1531], grad_fn=<AddBackward0>)\n",
      "epoch: 4349 loss is tensor([-0.1214], grad_fn=<AddBackward0>)\n",
      "epoch: 4350 loss is tensor([-0.1955], grad_fn=<AddBackward0>)\n",
      "epoch: 4351 loss is tensor([-0.1630], grad_fn=<AddBackward0>)\n",
      "epoch: 4352 loss is tensor([-0.1465], grad_fn=<AddBackward0>)\n",
      "epoch: 4353 loss is tensor([-0.1838], grad_fn=<AddBackward0>)\n",
      "epoch: 4354 loss is tensor([-0.1095], grad_fn=<AddBackward0>)\n",
      "epoch: 4355 loss is tensor([-0.1101], grad_fn=<AddBackward0>)\n",
      "epoch: 4356 loss is tensor([-0.1302], grad_fn=<AddBackward0>)\n",
      "epoch: 4357 loss is tensor([-0.1197], grad_fn=<AddBackward0>)\n",
      "epoch: 4358 loss is tensor([-0.1556], grad_fn=<AddBackward0>)\n",
      "epoch: 4359 loss is tensor([-0.1858], grad_fn=<AddBackward0>)\n",
      "epoch: 4360 loss is tensor([-0.1358], grad_fn=<AddBackward0>)\n",
      "epoch: 4361 loss is tensor([-0.1517], grad_fn=<AddBackward0>)\n",
      "epoch: 4362 loss is tensor([-0.1151], grad_fn=<AddBackward0>)\n",
      "epoch: 4363 loss is tensor([-0.1000], grad_fn=<AddBackward0>)\n",
      "epoch: 4364 loss is tensor([-0.1624], grad_fn=<AddBackward0>)\n",
      "epoch: 4365 loss is tensor([-0.1448], grad_fn=<AddBackward0>)\n",
      "epoch: 4366 loss is tensor([-0.1166], grad_fn=<AddBackward0>)\n",
      "epoch: 4367 loss is tensor([-0.1235], grad_fn=<AddBackward0>)\n",
      "epoch: 4368 loss is tensor([-0.1418], grad_fn=<AddBackward0>)\n",
      "epoch: 4369 loss is tensor([-0.0928], grad_fn=<AddBackward0>)\n",
      "epoch: 4370 loss is tensor([-0.1470], grad_fn=<AddBackward0>)\n",
      "epoch: 4371 loss is tensor([-0.0810], grad_fn=<AddBackward0>)\n",
      "epoch: 4372 loss is tensor([-0.1498], grad_fn=<AddBackward0>)\n",
      "epoch: 4373 loss is tensor([-0.0898], grad_fn=<AddBackward0>)\n",
      "epoch: 4374 loss is tensor([-0.0601], grad_fn=<AddBackward0>)\n",
      "epoch: 4375 loss is tensor([-0.1081], grad_fn=<AddBackward0>)\n",
      "epoch: 4376 loss is tensor([-0.0881], grad_fn=<AddBackward0>)\n",
      "epoch: 4377 loss is tensor([-0.1117], grad_fn=<AddBackward0>)\n",
      "epoch: 4378 loss is tensor([-0.0436], grad_fn=<AddBackward0>)\n",
      "epoch: 4379 loss is tensor([-0.0495], grad_fn=<AddBackward0>)\n",
      "epoch: 4380 loss is tensor([-0.0424], grad_fn=<AddBackward0>)\n",
      "epoch: 4381 loss is tensor([-0.1011], grad_fn=<AddBackward0>)\n",
      "epoch: 4382 loss is tensor([-0.0346], grad_fn=<AddBackward0>)\n",
      "epoch: 4383 loss is tensor([-0.0807], grad_fn=<AddBackward0>)\n",
      "epoch: 4384 loss is tensor([-0.0375], grad_fn=<AddBackward0>)\n",
      "epoch: 4385 loss is tensor([-0.1143], grad_fn=<AddBackward0>)\n",
      "epoch: 4386 loss is tensor([-0.0648], grad_fn=<AddBackward0>)\n",
      "epoch: 4387 loss is tensor([-0.0411], grad_fn=<AddBackward0>)\n",
      "epoch: 4388 loss is tensor([-0.0523], grad_fn=<AddBackward0>)\n",
      "epoch: 4389 loss is tensor([-0.1185], grad_fn=<AddBackward0>)\n",
      "epoch: 4390 loss is tensor([-0.0789], grad_fn=<AddBackward0>)\n",
      "epoch: 4391 loss is tensor([-0.1034], grad_fn=<AddBackward0>)\n",
      "epoch: 4392 loss is tensor([-0.1021], grad_fn=<AddBackward0>)\n",
      "epoch: 4393 loss is tensor([-0.0968], grad_fn=<AddBackward0>)\n",
      "epoch: 4394 loss is tensor([-0.0358], grad_fn=<AddBackward0>)\n",
      "epoch: 4395 loss is tensor([-0.0881], grad_fn=<AddBackward0>)\n",
      "epoch: 4396 loss is tensor([-0.1374], grad_fn=<AddBackward0>)\n",
      "epoch: 4397 loss is tensor([-0.0855], grad_fn=<AddBackward0>)\n",
      "epoch: 4398 loss is tensor([-0.0649], grad_fn=<AddBackward0>)\n",
      "epoch: 4399 loss is tensor([-0.1140], grad_fn=<AddBackward0>)\n",
      "epoch: 4400 loss is tensor([-0.0797], grad_fn=<AddBackward0>)\n",
      "11\n"
=======
      "The number of epochs is: 4001\n",
      "The number of epochs is: 4002\n",
      "The number of epochs is: 4003\n",
      "The number of epochs is: 4004\n",
      "The number of epochs is: 4005\n",
      "The number of epochs is: 4006\n",
      "The number of epochs is: 4007\n",
      "The number of epochs is: 4008\n",
      "The number of epochs is: 4009\n",
      "The number of epochs is: 4010\n",
      "The number of epochs is: 4011\n",
      "The number of epochs is: 4012\n",
      "The number of epochs is: 4013\n",
      "The number of epochs is: 4014\n",
      "The number of epochs is: 4015\n",
      "The number of epochs is: 4016\n",
      "The number of epochs is: 4017\n",
      "The number of epochs is: 4018\n",
      "The number of epochs is: 4019\n",
      "The number of epochs is: 4020\n",
      "The number of epochs is: 4021\n",
      "The number of epochs is: 4022\n",
      "The number of epochs is: 4023\n",
      "The number of epochs is: 4024\n",
      "The number of epochs is: 4025\n",
      "The number of epochs is: 4026\n",
      "The number of epochs is: 4027\n",
      "The number of epochs is: 4028\n",
      "The number of epochs is: 4029\n",
      "The number of epochs is: 4030\n",
      "The number of epochs is: 4031\n",
      "The number of epochs is: 4032\n",
      "The number of epochs is: 4033\n",
      "The number of epochs is: 4034\n",
      "The number of epochs is: 4035\n",
      "The number of epochs is: 4036\n",
      "The number of epochs is: 4037\n",
      "The number of epochs is: 4038\n",
      "The number of epochs is: 4039\n",
      "The number of epochs is: 4040\n",
      "The number of epochs is: 4041\n",
      "The number of epochs is: 4042\n",
      "The number of epochs is: 4043\n",
      "The number of epochs is: 4044\n",
      "The number of epochs is: 4045\n",
      "The number of epochs is: 4046\n",
      "The number of epochs is: 4047\n",
      "The number of epochs is: 4048\n",
      "The number of epochs is: 4049\n",
      "The number of epochs is: 4050\n",
      "The number of epochs is: 4051\n",
      "The number of epochs is: 4052\n",
      "The number of epochs is: 4053\n",
      "The number of epochs is: 4054\n",
      "The number of epochs is: 4055\n",
      "The number of epochs is: 4056\n",
      "The number of epochs is: 4057\n",
      "The number of epochs is: 4058\n",
      "The number of epochs is: 4059\n",
      "The number of epochs is: 4060\n",
      "The number of epochs is: 4061\n",
      "The number of epochs is: 4062\n",
      "The number of epochs is: 4063\n",
      "The number of epochs is: 4064\n",
      "The number of epochs is: 4065\n",
      "The number of epochs is: 4066\n",
      "The number of epochs is: 4067\n",
      "The number of epochs is: 4068\n",
      "The number of epochs is: 4069\n",
      "The number of epochs is: 4070\n",
      "The number of epochs is: 4071\n",
      "The number of epochs is: 4072\n",
      "The number of epochs is: 4073\n",
      "The number of epochs is: 4074\n",
      "The number of epochs is: 4075\n",
      "The number of epochs is: 4076\n",
      "The number of epochs is: 4077\n",
      "The number of epochs is: 4078\n",
      "The number of epochs is: 4079\n",
      "The number of epochs is: 4080\n",
      "The number of epochs is: 4081\n",
      "The number of epochs is: 4082\n",
      "The number of epochs is: 4083\n",
      "The number of epochs is: 4084\n",
      "The number of epochs is: 4085\n",
      "The number of epochs is: 4086\n",
      "The number of epochs is: 4087\n",
      "The number of epochs is: 4088\n",
      "The number of epochs is: 4089\n",
      "The number of epochs is: 4090\n",
      "The number of epochs is: 4091\n",
      "The number of epochs is: 4092\n",
      "The number of epochs is: 4093\n",
      "The number of epochs is: 4094\n",
      "The number of epochs is: 4095\n",
      "The number of epochs is: 4096\n",
      "The number of epochs is: 4097\n",
      "The number of epochs is: 4098\n",
      "The number of epochs is: 4099\n",
      "The number of epochs is: 4100\n",
      "The number of epochs is: 4101\n",
      "The number of epochs is: 4102\n",
      "The number of epochs is: 4103\n",
      "The number of epochs is: 4104\n",
      "The number of epochs is: 4105\n",
      "The number of epochs is: 4106\n",
      "The number of epochs is: 4107\n",
      "The number of epochs is: 4108\n",
      "The number of epochs is: 4109\n",
      "The number of epochs is: 4110\n",
      "The number of epochs is: 4111\n",
      "The number of epochs is: 4112\n",
      "The number of epochs is: 4113\n",
      "The number of epochs is: 4114\n",
      "The number of epochs is: 4115\n",
      "The number of epochs is: 4116\n",
      "The number of epochs is: 4117\n",
      "The number of epochs is: 4118\n",
      "The number of epochs is: 4119\n",
      "The number of epochs is: 4120\n",
      "The number of epochs is: 4121\n",
      "The number of epochs is: 4122\n",
      "The number of epochs is: 4123\n",
      "The number of epochs is: 4124\n",
      "The number of epochs is: 4125\n",
      "The number of epochs is: 4126\n",
      "The number of epochs is: 4127\n",
      "The number of epochs is: 4128\n",
      "The number of epochs is: 4129\n",
      "The number of epochs is: 4130\n",
      "The number of epochs is: 4131\n",
      "The number of epochs is: 4132\n",
      "The number of epochs is: 4133\n",
      "The number of epochs is: 4134\n",
      "The number of epochs is: 4135\n",
      "The number of epochs is: 4136\n",
      "The number of epochs is: 4137\n",
      "The number of epochs is: 4138\n",
      "The number of epochs is: 4139\n",
      "The number of epochs is: 4140\n",
      "The number of epochs is: 4141\n",
      "The number of epochs is: 4142\n",
      "The number of epochs is: 4143\n",
      "The number of epochs is: 4144\n",
      "The number of epochs is: 4145\n",
      "The number of epochs is: 4146\n",
      "The number of epochs is: 4147\n",
      "The number of epochs is: 4148\n",
      "The number of epochs is: 4149\n",
      "The number of epochs is: 4150\n",
      "The number of epochs is: 4151\n",
      "The number of epochs is: 4152\n",
      "The number of epochs is: 4153\n",
      "The number of epochs is: 4154\n",
      "The number of epochs is: 4155\n",
      "The number of epochs is: 4156\n",
      "The number of epochs is: 4157\n",
      "The number of epochs is: 4158\n",
      "The number of epochs is: 4159\n",
      "The number of epochs is: 4160\n",
      "The number of epochs is: 4161\n",
      "The number of epochs is: 4162\n",
      "The number of epochs is: 4163\n",
      "The number of epochs is: 4164\n",
      "The number of epochs is: 4165\n",
      "The number of epochs is: 4166\n",
      "The number of epochs is: 4167\n",
      "The number of epochs is: 4168\n",
      "The number of epochs is: 4169\n",
      "The number of epochs is: 4170\n",
      "The number of epochs is: 4171\n",
      "The number of epochs is: 4172\n",
      "The number of epochs is: 4173\n",
      "The number of epochs is: 4174\n",
      "The number of epochs is: 4175\n",
      "The number of epochs is: 4176\n",
      "The number of epochs is: 4177\n",
      "The number of epochs is: 4178\n",
      "The number of epochs is: 4179\n",
      "The number of epochs is: 4180\n",
      "The number of epochs is: 4181\n",
      "The number of epochs is: 4182\n",
      "The number of epochs is: 4183\n",
      "The number of epochs is: 4184\n",
      "The number of epochs is: 4185\n",
      "The number of epochs is: 4186\n",
      "The number of epochs is: 4187\n",
      "The number of epochs is: 4188\n",
      "The number of epochs is: 4189\n",
      "The number of epochs is: 4190\n",
      "The number of epochs is: 4191\n",
      "The number of epochs is: 4192\n",
      "The number of epochs is: 4193\n",
      "The number of epochs is: 4194\n",
      "The number of epochs is: 4195\n",
      "The number of epochs is: 4196\n",
      "The number of epochs is: 4197\n",
      "The number of epochs is: 4198\n",
      "The number of epochs is: 4199\n",
      "The number of epochs is: 4200\n",
      "44\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4401 loss is tensor([-0.1642], grad_fn=<AddBackward0>)\n",
      "epoch: 4402 loss is tensor([-0.1342], grad_fn=<AddBackward0>)\n",
      "epoch: 4403 loss is tensor([-0.0792], grad_fn=<AddBackward0>)\n",
      "epoch: 4404 loss is tensor([-0.1663], grad_fn=<AddBackward0>)\n",
      "epoch: 4405 loss is tensor([-0.1092], grad_fn=<AddBackward0>)\n",
      "epoch: 4406 loss is tensor([-0.1341], grad_fn=<AddBackward0>)\n",
      "epoch: 4407 loss is tensor([-0.0958], grad_fn=<AddBackward0>)\n",
      "epoch: 4408 loss is tensor([-0.1448], grad_fn=<AddBackward0>)\n",
      "epoch: 4409 loss is tensor([-0.0895], grad_fn=<AddBackward0>)\n",
      "epoch: 4410 loss is tensor([-0.1730], grad_fn=<AddBackward0>)\n",
      "epoch: 4411 loss is tensor([-0.0603], grad_fn=<AddBackward0>)\n",
      "epoch: 4412 loss is tensor([-0.0969], grad_fn=<AddBackward0>)\n",
      "epoch: 4413 loss is tensor([-0.0949], grad_fn=<AddBackward0>)\n",
      "epoch: 4414 loss is tensor([-0.1541], grad_fn=<AddBackward0>)\n",
      "epoch: 4415 loss is tensor([-0.1249], grad_fn=<AddBackward0>)\n",
      "epoch: 4416 loss is tensor([-0.1717], grad_fn=<AddBackward0>)\n",
      "epoch: 4417 loss is tensor([-0.0955], grad_fn=<AddBackward0>)\n",
      "epoch: 4418 loss is tensor([-0.0977], grad_fn=<AddBackward0>)\n",
      "epoch: 4419 loss is tensor([-0.1177], grad_fn=<AddBackward0>)\n",
      "epoch: 4420 loss is tensor([-0.0877], grad_fn=<AddBackward0>)\n",
      "epoch: 4421 loss is tensor([-0.1483], grad_fn=<AddBackward0>)\n",
      "epoch: 4422 loss is tensor([-0.1106], grad_fn=<AddBackward0>)\n",
      "epoch: 4423 loss is tensor([-0.1132], grad_fn=<AddBackward0>)\n",
      "epoch: 4424 loss is tensor([-0.1454], grad_fn=<AddBackward0>)\n",
      "epoch: 4425 loss is tensor([-0.1317], grad_fn=<AddBackward0>)\n",
      "epoch: 4426 loss is tensor([-0.1079], grad_fn=<AddBackward0>)\n",
      "epoch: 4427 loss is tensor([-0.0885], grad_fn=<AddBackward0>)\n",
      "epoch: 4428 loss is tensor([-0.0955], grad_fn=<AddBackward0>)\n",
      "epoch: 4429 loss is tensor([-0.0831], grad_fn=<AddBackward0>)\n",
      "epoch: 4430 loss is tensor([-0.1116], grad_fn=<AddBackward0>)\n",
      "epoch: 4431 loss is tensor([-0.1608], grad_fn=<AddBackward0>)\n",
      "epoch: 4432 loss is tensor([-0.1746], grad_fn=<AddBackward0>)\n",
      "epoch: 4433 loss is tensor([-0.1084], grad_fn=<AddBackward0>)\n",
      "epoch: 4434 loss is tensor([-0.1659], grad_fn=<AddBackward0>)\n",
      "epoch: 4435 loss is tensor([-0.1482], grad_fn=<AddBackward0>)\n",
      "epoch: 4436 loss is tensor([-0.1670], grad_fn=<AddBackward0>)\n",
      "epoch: 4437 loss is tensor([-0.1082], grad_fn=<AddBackward0>)\n",
      "epoch: 4438 loss is tensor([-0.1326], grad_fn=<AddBackward0>)\n",
      "epoch: 4439 loss is tensor([-0.1478], grad_fn=<AddBackward0>)\n",
      "epoch: 4440 loss is tensor([-0.1901], grad_fn=<AddBackward0>)\n",
      "epoch: 4441 loss is tensor([-0.1802], grad_fn=<AddBackward0>)\n",
      "epoch: 4442 loss is tensor([-0.1188], grad_fn=<AddBackward0>)\n",
      "epoch: 4443 loss is tensor([-0.1639], grad_fn=<AddBackward0>)\n",
      "epoch: 4444 loss is tensor([-0.1181], grad_fn=<AddBackward0>)\n",
      "epoch: 4445 loss is tensor([-0.0975], grad_fn=<AddBackward0>)\n",
      "epoch: 4446 loss is tensor([-0.1352], grad_fn=<AddBackward0>)\n",
      "epoch: 4447 loss is tensor([-0.1505], grad_fn=<AddBackward0>)\n",
      "epoch: 4448 loss is tensor([-0.1761], grad_fn=<AddBackward0>)\n",
      "epoch: 4449 loss is tensor([-0.1511], grad_fn=<AddBackward0>)\n",
      "epoch: 4450 loss is tensor([-0.1344], grad_fn=<AddBackward0>)\n",
      "epoch: 4451 loss is tensor([-0.1270], grad_fn=<AddBackward0>)\n",
      "epoch: 4452 loss is tensor([-0.2126], grad_fn=<AddBackward0>)\n",
      "epoch: 4453 loss is tensor([-0.1298], grad_fn=<AddBackward0>)\n",
      "epoch: 4454 loss is tensor([-0.1649], grad_fn=<AddBackward0>)\n",
      "epoch: 4455 loss is tensor([-0.1380], grad_fn=<AddBackward0>)\n",
      "epoch: 4456 loss is tensor([-0.1655], grad_fn=<AddBackward0>)\n",
      "epoch: 4457 loss is tensor([-0.1235], grad_fn=<AddBackward0>)\n",
      "epoch: 4458 loss is tensor([-0.1584], grad_fn=<AddBackward0>)\n",
      "epoch: 4459 loss is tensor([-0.1933], grad_fn=<AddBackward0>)\n",
      "epoch: 4460 loss is tensor([-0.1752], grad_fn=<AddBackward0>)\n",
      "epoch: 4461 loss is tensor([-0.1762], grad_fn=<AddBackward0>)\n",
      "epoch: 4462 loss is tensor([-0.1388], grad_fn=<AddBackward0>)\n",
      "epoch: 4463 loss is tensor([-0.2042], grad_fn=<AddBackward0>)\n",
      "epoch: 4464 loss is tensor([-0.1332], grad_fn=<AddBackward0>)\n",
      "epoch: 4465 loss is tensor([-0.1784], grad_fn=<AddBackward0>)\n",
      "epoch: 4466 loss is tensor([-0.1227], grad_fn=<AddBackward0>)\n",
      "epoch: 4467 loss is tensor([-0.1315], grad_fn=<AddBackward0>)\n",
      "epoch: 4468 loss is tensor([-0.1088], grad_fn=<AddBackward0>)\n",
      "epoch: 4469 loss is tensor([-0.1033], grad_fn=<AddBackward0>)\n",
      "epoch: 4470 loss is tensor([-0.1354], grad_fn=<AddBackward0>)\n",
      "epoch: 4471 loss is tensor([-0.1314], grad_fn=<AddBackward0>)\n",
      "epoch: 4472 loss is tensor([-0.1181], grad_fn=<AddBackward0>)\n",
      "epoch: 4473 loss is tensor([-0.0999], grad_fn=<AddBackward0>)\n",
      "epoch: 4474 loss is tensor([-0.1113], grad_fn=<AddBackward0>)\n",
      "epoch: 4475 loss is tensor([-0.1201], grad_fn=<AddBackward0>)\n",
      "epoch: 4476 loss is tensor([-0.1530], grad_fn=<AddBackward0>)\n",
      "epoch: 4477 loss is tensor([-0.0999], grad_fn=<AddBackward0>)\n",
      "epoch: 4478 loss is tensor([-0.0718], grad_fn=<AddBackward0>)\n",
      "epoch: 4479 loss is tensor([-0.1469], grad_fn=<AddBackward0>)\n",
      "epoch: 4480 loss is tensor([-0.1181], grad_fn=<AddBackward0>)\n",
      "epoch: 4481 loss is tensor([-0.0422], grad_fn=<AddBackward0>)\n",
      "epoch: 4482 loss is tensor([-0.1261], grad_fn=<AddBackward0>)\n",
      "epoch: 4483 loss is tensor([-0.0830], grad_fn=<AddBackward0>)\n",
      "epoch: 4484 loss is tensor([-0.1703], grad_fn=<AddBackward0>)\n",
      "epoch: 4485 loss is tensor([-0.1481], grad_fn=<AddBackward0>)\n",
      "epoch: 4486 loss is tensor([-0.1034], grad_fn=<AddBackward0>)\n",
      "epoch: 4487 loss is tensor([-0.1838], grad_fn=<AddBackward0>)\n",
      "epoch: 4488 loss is tensor([-0.1255], grad_fn=<AddBackward0>)\n",
      "epoch: 4489 loss is tensor([-0.2013], grad_fn=<AddBackward0>)\n",
      "epoch: 4490 loss is tensor([-0.1206], grad_fn=<AddBackward0>)\n",
      "epoch: 4491 loss is tensor([-0.1768], grad_fn=<AddBackward0>)\n",
      "epoch: 4492 loss is tensor([-0.1767], grad_fn=<AddBackward0>)\n",
      "epoch: 4493 loss is tensor([-0.1530], grad_fn=<AddBackward0>)\n",
      "epoch: 4494 loss is tensor([-0.1492], grad_fn=<AddBackward0>)\n",
      "epoch: 4495 loss is tensor([-0.1056], grad_fn=<AddBackward0>)\n",
      "epoch: 4496 loss is tensor([-0.1097], grad_fn=<AddBackward0>)\n",
      "epoch: 4497 loss is tensor([-0.1413], grad_fn=<AddBackward0>)\n",
      "epoch: 4498 loss is tensor([-0.1244], grad_fn=<AddBackward0>)\n",
      "epoch: 4499 loss is tensor([-0.1086], grad_fn=<AddBackward0>)\n",
      "epoch: 4500 loss is tensor([-0.1251], grad_fn=<AddBackward0>)\n",
      "39\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4501 loss is tensor([-0.1108], grad_fn=<AddBackward0>)\n",
      "epoch: 4502 loss is tensor([-0.1554], grad_fn=<AddBackward0>)\n",
      "epoch: 4503 loss is tensor([-0.1421], grad_fn=<AddBackward0>)\n",
      "epoch: 4504 loss is tensor([-0.1099], grad_fn=<AddBackward0>)\n",
      "epoch: 4505 loss is tensor([-0.1435], grad_fn=<AddBackward0>)\n",
      "epoch: 4506 loss is tensor([-0.1462], grad_fn=<AddBackward0>)\n",
      "epoch: 4507 loss is tensor([-0.1288], grad_fn=<AddBackward0>)\n",
      "epoch: 4508 loss is tensor([-0.1262], grad_fn=<AddBackward0>)\n",
      "epoch: 4509 loss is tensor([-0.1232], grad_fn=<AddBackward0>)\n",
      "epoch: 4510 loss is tensor([-0.1728], grad_fn=<AddBackward0>)\n",
      "epoch: 4511 loss is tensor([-0.1288], grad_fn=<AddBackward0>)\n",
      "epoch: 4512 loss is tensor([-0.1784], grad_fn=<AddBackward0>)\n",
      "epoch: 4513 loss is tensor([-0.0951], grad_fn=<AddBackward0>)\n",
      "epoch: 4514 loss is tensor([-0.1857], grad_fn=<AddBackward0>)\n",
      "epoch: 4515 loss is tensor([-0.1580], grad_fn=<AddBackward0>)\n",
      "epoch: 4516 loss is tensor([-0.1732], grad_fn=<AddBackward0>)\n",
      "epoch: 4517 loss is tensor([-0.1458], grad_fn=<AddBackward0>)\n",
      "epoch: 4518 loss is tensor([-0.1087], grad_fn=<AddBackward0>)\n",
      "epoch: 4519 loss is tensor([-0.1423], grad_fn=<AddBackward0>)\n",
      "epoch: 4520 loss is tensor([-0.1729], grad_fn=<AddBackward0>)\n",
      "epoch: 4521 loss is tensor([-0.1087], grad_fn=<AddBackward0>)\n",
      "epoch: 4522 loss is tensor([-0.1268], grad_fn=<AddBackward0>)\n",
      "epoch: 4523 loss is tensor([-0.1534], grad_fn=<AddBackward0>)\n",
      "epoch: 4524 loss is tensor([-0.1300], grad_fn=<AddBackward0>)\n",
      "epoch: 4525 loss is tensor([-0.1030], grad_fn=<AddBackward0>)\n",
      "epoch: 4526 loss is tensor([-0.0975], grad_fn=<AddBackward0>)\n",
      "epoch: 4527 loss is tensor([-0.1404], grad_fn=<AddBackward0>)\n",
      "epoch: 4528 loss is tensor([-0.1306], grad_fn=<AddBackward0>)\n",
      "epoch: 4529 loss is tensor([-0.1217], grad_fn=<AddBackward0>)\n",
      "epoch: 4530 loss is tensor([-0.1595], grad_fn=<AddBackward0>)\n",
      "epoch: 4531 loss is tensor([-0.1481], grad_fn=<AddBackward0>)\n",
      "epoch: 4532 loss is tensor([-0.2271], grad_fn=<AddBackward0>)\n",
      "epoch: 4533 loss is tensor([-0.1222], grad_fn=<AddBackward0>)\n",
      "epoch: 4534 loss is tensor([-0.1814], grad_fn=<AddBackward0>)\n",
      "epoch: 4535 loss is tensor([-0.2166], grad_fn=<AddBackward0>)\n",
      "epoch: 4536 loss is tensor([-0.1715], grad_fn=<AddBackward0>)\n",
      "epoch: 4537 loss is tensor([-0.1575], grad_fn=<AddBackward0>)\n",
      "epoch: 4538 loss is tensor([-0.1558], grad_fn=<AddBackward0>)\n",
      "epoch: 4539 loss is tensor([-0.1533], grad_fn=<AddBackward0>)\n",
      "epoch: 4540 loss is tensor([-0.1337], grad_fn=<AddBackward0>)\n",
      "epoch: 4541 loss is tensor([-0.2145], grad_fn=<AddBackward0>)\n",
      "epoch: 4542 loss is tensor([-0.1700], grad_fn=<AddBackward0>)\n",
      "epoch: 4543 loss is tensor([-0.0949], grad_fn=<AddBackward0>)\n",
      "epoch: 4544 loss is tensor([-0.1633], grad_fn=<AddBackward0>)\n",
      "epoch: 4545 loss is tensor([-0.1040], grad_fn=<AddBackward0>)\n",
      "epoch: 4546 loss is tensor([-0.0812], grad_fn=<AddBackward0>)\n",
      "epoch: 4547 loss is tensor([-0.1422], grad_fn=<AddBackward0>)\n",
      "epoch: 4548 loss is tensor([-0.1228], grad_fn=<AddBackward0>)\n",
      "epoch: 4549 loss is tensor([-0.0597], grad_fn=<AddBackward0>)\n",
      "epoch: 4550 loss is tensor([-0.1031], grad_fn=<AddBackward0>)\n",
      "epoch: 4551 loss is tensor([-0.0749], grad_fn=<AddBackward0>)\n",
      "epoch: 4552 loss is tensor([-0.1257], grad_fn=<AddBackward0>)\n",
      "epoch: 4553 loss is tensor([-0.0751], grad_fn=<AddBackward0>)\n",
      "epoch: 4554 loss is tensor([-0.0707], grad_fn=<AddBackward0>)\n",
      "epoch: 4555 loss is tensor([-0.1079], grad_fn=<AddBackward0>)\n",
      "epoch: 4556 loss is tensor([-0.1413], grad_fn=<AddBackward0>)\n",
      "epoch: 4557 loss is tensor([-0.0074], grad_fn=<AddBackward0>)\n",
      "epoch: 4558 loss is tensor([-0.0895], grad_fn=<AddBackward0>)\n",
      "epoch: 4559 loss is tensor([-0.1419], grad_fn=<AddBackward0>)\n",
      "epoch: 4560 loss is tensor([-0.1490], grad_fn=<AddBackward0>)\n",
      "epoch: 4561 loss is tensor([-0.1054], grad_fn=<AddBackward0>)\n",
      "epoch: 4562 loss is tensor([-0.1000], grad_fn=<AddBackward0>)\n",
      "epoch: 4563 loss is tensor([-0.1826], grad_fn=<AddBackward0>)\n",
      "epoch: 4564 loss is tensor([-0.1678], grad_fn=<AddBackward0>)\n",
      "epoch: 4565 loss is tensor([-0.1302], grad_fn=<AddBackward0>)\n",
      "epoch: 4566 loss is tensor([-0.0375], grad_fn=<AddBackward0>)\n",
      "epoch: 4567 loss is tensor([-0.0833], grad_fn=<AddBackward0>)\n",
      "epoch: 4568 loss is tensor([-0.1278], grad_fn=<AddBackward0>)\n",
      "epoch: 4569 loss is tensor([-0.1541], grad_fn=<AddBackward0>)\n",
      "epoch: 4570 loss is tensor([-0.1552], grad_fn=<AddBackward0>)\n",
      "epoch: 4571 loss is tensor([-0.1544], grad_fn=<AddBackward0>)\n",
      "epoch: 4572 loss is tensor([-0.1479], grad_fn=<AddBackward0>)\n",
      "epoch: 4573 loss is tensor([-0.1022], grad_fn=<AddBackward0>)\n",
      "epoch: 4574 loss is tensor([-0.0811], grad_fn=<AddBackward0>)\n",
      "epoch: 4575 loss is tensor([-0.0906], grad_fn=<AddBackward0>)\n",
      "epoch: 4576 loss is tensor([-0.1227], grad_fn=<AddBackward0>)\n",
      "epoch: 4577 loss is tensor([-0.0832], grad_fn=<AddBackward0>)\n",
      "epoch: 4578 loss is tensor([-0.0734], grad_fn=<AddBackward0>)\n",
      "epoch: 4579 loss is tensor([-0.0961], grad_fn=<AddBackward0>)\n",
      "epoch: 4580 loss is tensor([-0.0801], grad_fn=<AddBackward0>)\n",
      "epoch: 4581 loss is tensor([-0.0714], grad_fn=<AddBackward0>)\n",
      "epoch: 4582 loss is tensor([-0.0705], grad_fn=<AddBackward0>)\n",
      "epoch: 4583 loss is tensor([-0.0972], grad_fn=<AddBackward0>)\n",
      "epoch: 4584 loss is tensor([-0.0364], grad_fn=<AddBackward0>)\n",
      "epoch: 4585 loss is tensor([-0.1197], grad_fn=<AddBackward0>)\n",
      "epoch: 4586 loss is tensor([-0.1651], grad_fn=<AddBackward0>)\n",
      "epoch: 4587 loss is tensor([-0.0739], grad_fn=<AddBackward0>)\n",
      "epoch: 4588 loss is tensor([-0.0958], grad_fn=<AddBackward0>)\n",
      "epoch: 4589 loss is tensor([-0.1079], grad_fn=<AddBackward0>)\n",
      "epoch: 4590 loss is tensor([-0.0919], grad_fn=<AddBackward0>)\n",
      "epoch: 4591 loss is tensor([-0.1602], grad_fn=<AddBackward0>)\n",
      "epoch: 4592 loss is tensor([-0.0682], grad_fn=<AddBackward0>)\n",
      "epoch: 4593 loss is tensor([-0.0319], grad_fn=<AddBackward0>)\n",
      "epoch: 4594 loss is tensor([-0.0495], grad_fn=<AddBackward0>)\n",
      "epoch: 4595 loss is tensor([-0.0737], grad_fn=<AddBackward0>)\n",
      "epoch: 4596 loss is tensor([-0.1409], grad_fn=<AddBackward0>)\n",
      "epoch: 4597 loss is tensor([-0.0996], grad_fn=<AddBackward0>)\n",
      "epoch: 4598 loss is tensor([-0.0746], grad_fn=<AddBackward0>)\n",
      "epoch: 4599 loss is tensor([-0.0688], grad_fn=<AddBackward0>)\n",
      "epoch: 4600 loss is tensor([-0.1182], grad_fn=<AddBackward0>)\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4601 loss is tensor([-0.0571], grad_fn=<AddBackward0>)\n",
      "epoch: 4602 loss is tensor([-0.1197], grad_fn=<AddBackward0>)\n",
      "epoch: 4603 loss is tensor([-0.1200], grad_fn=<AddBackward0>)\n",
      "epoch: 4604 loss is tensor([-0.1503], grad_fn=<AddBackward0>)\n",
      "epoch: 4605 loss is tensor([-0.1413], grad_fn=<AddBackward0>)\n",
      "epoch: 4606 loss is tensor([-0.1783], grad_fn=<AddBackward0>)\n",
      "epoch: 4607 loss is tensor([-0.1268], grad_fn=<AddBackward0>)\n",
      "epoch: 4608 loss is tensor([-0.1437], grad_fn=<AddBackward0>)\n",
      "epoch: 4609 loss is tensor([-0.1002], grad_fn=<AddBackward0>)\n",
      "epoch: 4610 loss is tensor([-0.0149], grad_fn=<AddBackward0>)\n",
      "epoch: 4611 loss is tensor([-0.1189], grad_fn=<AddBackward0>)\n",
      "epoch: 4612 loss is tensor([-0.0891], grad_fn=<AddBackward0>)\n",
      "epoch: 4613 loss is tensor([-0.0527], grad_fn=<AddBackward0>)\n",
      "epoch: 4614 loss is tensor([-0.0262], grad_fn=<AddBackward0>)\n",
      "epoch: 4615 loss is tensor([-0.0533], grad_fn=<AddBackward0>)\n",
      "epoch: 4616 loss is tensor([-0.0759], grad_fn=<AddBackward0>)\n",
      "epoch: 4617 loss is tensor([-0.0440], grad_fn=<AddBackward0>)\n",
      "epoch: 4618 loss is tensor([-0.1487], grad_fn=<AddBackward0>)\n",
      "epoch: 4619 loss is tensor([-0.1192], grad_fn=<AddBackward0>)\n",
      "epoch: 4620 loss is tensor([-0.0532], grad_fn=<AddBackward0>)\n",
      "epoch: 4621 loss is tensor([-0.1010], grad_fn=<AddBackward0>)\n",
      "epoch: 4622 loss is tensor([-0.0721], grad_fn=<AddBackward0>)\n",
      "epoch: 4623 loss is tensor([-0.0907], grad_fn=<AddBackward0>)\n",
      "epoch: 4624 loss is tensor([-0.1234], grad_fn=<AddBackward0>)\n",
      "epoch: 4625 loss is tensor([-0.1605], grad_fn=<AddBackward0>)\n",
      "epoch: 4626 loss is tensor([-0.1174], grad_fn=<AddBackward0>)\n",
      "epoch: 4627 loss is tensor([-0.0070], grad_fn=<AddBackward0>)\n",
      "epoch: 4628 loss is tensor([-0.1023], grad_fn=<AddBackward0>)\n",
      "epoch: 4629 loss is tensor([-0.0647], grad_fn=<AddBackward0>)\n",
      "epoch: 4630 loss is tensor([-0.1500], grad_fn=<AddBackward0>)\n",
      "epoch: 4631 loss is tensor([-0.0408], grad_fn=<AddBackward0>)\n",
      "epoch: 4632 loss is tensor([-0.0593], grad_fn=<AddBackward0>)\n",
      "epoch: 4633 loss is tensor([-0.0691], grad_fn=<AddBackward0>)\n",
      "epoch: 4634 loss is tensor([-0.1212], grad_fn=<AddBackward0>)\n",
      "epoch: 4635 loss is tensor([-0.1042], grad_fn=<AddBackward0>)\n",
      "epoch: 4636 loss is tensor([-0.1121], grad_fn=<AddBackward0>)\n",
      "epoch: 4637 loss is tensor([-0.0730], grad_fn=<AddBackward0>)\n",
      "epoch: 4638 loss is tensor([-0.1466], grad_fn=<AddBackward0>)\n",
      "epoch: 4639 loss is tensor([-0.1491], grad_fn=<AddBackward0>)\n",
      "epoch: 4640 loss is tensor([-0.1196], grad_fn=<AddBackward0>)\n",
      "epoch: 4641 loss is tensor([-0.1278], grad_fn=<AddBackward0>)\n",
      "epoch: 4642 loss is tensor([-0.0865], grad_fn=<AddBackward0>)\n",
      "epoch: 4643 loss is tensor([-0.0782], grad_fn=<AddBackward0>)\n",
      "epoch: 4644 loss is tensor([-0.0756], grad_fn=<AddBackward0>)\n",
      "epoch: 4645 loss is tensor([-0.1506], grad_fn=<AddBackward0>)\n",
      "epoch: 4646 loss is tensor([-0.1720], grad_fn=<AddBackward0>)\n",
      "epoch: 4647 loss is tensor([-0.1581], grad_fn=<AddBackward0>)\n",
      "epoch: 4648 loss is tensor([-0.1606], grad_fn=<AddBackward0>)\n",
      "epoch: 4649 loss is tensor([-0.1534], grad_fn=<AddBackward0>)\n",
      "epoch: 4650 loss is tensor([-0.1327], grad_fn=<AddBackward0>)\n",
      "epoch: 4651 loss is tensor([-0.1244], grad_fn=<AddBackward0>)\n",
      "epoch: 4652 loss is tensor([-0.1237], grad_fn=<AddBackward0>)\n",
      "epoch: 4653 loss is tensor([-0.1928], grad_fn=<AddBackward0>)\n",
      "epoch: 4654 loss is tensor([-0.1832], grad_fn=<AddBackward0>)\n",
      "epoch: 4655 loss is tensor([-0.2099], grad_fn=<AddBackward0>)\n",
      "epoch: 4656 loss is tensor([-0.1556], grad_fn=<AddBackward0>)\n",
      "epoch: 4657 loss is tensor([-0.1041], grad_fn=<AddBackward0>)\n",
      "epoch: 4658 loss is tensor([-0.1524], grad_fn=<AddBackward0>)\n",
      "epoch: 4659 loss is tensor([-0.1370], grad_fn=<AddBackward0>)\n",
      "epoch: 4660 loss is tensor([-0.1599], grad_fn=<AddBackward0>)\n",
      "epoch: 4661 loss is tensor([-0.1629], grad_fn=<AddBackward0>)\n",
      "epoch: 4662 loss is tensor([-0.1498], grad_fn=<AddBackward0>)\n",
      "epoch: 4663 loss is tensor([-0.1674], grad_fn=<AddBackward0>)\n",
      "epoch: 4664 loss is tensor([-0.0946], grad_fn=<AddBackward0>)\n",
      "epoch: 4665 loss is tensor([-0.1247], grad_fn=<AddBackward0>)\n",
      "epoch: 4666 loss is tensor([-0.1387], grad_fn=<AddBackward0>)\n",
      "epoch: 4667 loss is tensor([-0.1475], grad_fn=<AddBackward0>)\n",
      "epoch: 4668 loss is tensor([-0.1405], grad_fn=<AddBackward0>)\n",
      "epoch: 4669 loss is tensor([-0.1628], grad_fn=<AddBackward0>)\n",
      "epoch: 4670 loss is tensor([-0.1241], grad_fn=<AddBackward0>)\n",
      "epoch: 4671 loss is tensor([-0.1028], grad_fn=<AddBackward0>)\n",
      "epoch: 4672 loss is tensor([-0.1927], grad_fn=<AddBackward0>)\n",
      "epoch: 4673 loss is tensor([-0.1110], grad_fn=<AddBackward0>)\n",
      "epoch: 4674 loss is tensor([-0.1696], grad_fn=<AddBackward0>)\n",
      "epoch: 4675 loss is tensor([-0.1080], grad_fn=<AddBackward0>)\n",
      "epoch: 4676 loss is tensor([-0.1963], grad_fn=<AddBackward0>)\n",
      "epoch: 4677 loss is tensor([-0.1471], grad_fn=<AddBackward0>)\n",
      "epoch: 4678 loss is tensor([-0.1313], grad_fn=<AddBackward0>)\n",
      "epoch: 4679 loss is tensor([-0.1748], grad_fn=<AddBackward0>)\n",
      "epoch: 4680 loss is tensor([-0.1698], grad_fn=<AddBackward0>)\n",
      "epoch: 4681 loss is tensor([-0.2448], grad_fn=<AddBackward0>)\n",
      "epoch: 4682 loss is tensor([-0.1468], grad_fn=<AddBackward0>)\n",
      "epoch: 4683 loss is tensor([-0.1828], grad_fn=<AddBackward0>)\n",
      "epoch: 4684 loss is tensor([-0.1359], grad_fn=<AddBackward0>)\n",
      "epoch: 4685 loss is tensor([-0.1965], grad_fn=<AddBackward0>)\n",
      "epoch: 4686 loss is tensor([-0.1729], grad_fn=<AddBackward0>)\n",
      "epoch: 4687 loss is tensor([-0.1084], grad_fn=<AddBackward0>)\n",
      "epoch: 4688 loss is tensor([-0.1879], grad_fn=<AddBackward0>)\n",
      "epoch: 4689 loss is tensor([-0.1029], grad_fn=<AddBackward0>)\n",
      "epoch: 4690 loss is tensor([-0.1516], grad_fn=<AddBackward0>)\n",
      "epoch: 4691 loss is tensor([-0.1399], grad_fn=<AddBackward0>)\n",
      "epoch: 4692 loss is tensor([-0.2318], grad_fn=<AddBackward0>)\n",
      "epoch: 4693 loss is tensor([-0.1670], grad_fn=<AddBackward0>)\n",
      "epoch: 4694 loss is tensor([-0.1828], grad_fn=<AddBackward0>)\n",
      "epoch: 4695 loss is tensor([-0.1993], grad_fn=<AddBackward0>)\n",
      "epoch: 4696 loss is tensor([-0.1935], grad_fn=<AddBackward0>)\n",
      "epoch: 4697 loss is tensor([-0.0935], grad_fn=<AddBackward0>)\n",
      "epoch: 4698 loss is tensor([-0.1027], grad_fn=<AddBackward0>)\n",
      "epoch: 4699 loss is tensor([-0.1045], grad_fn=<AddBackward0>)\n",
      "epoch: 4700 loss is tensor([-0.1531], grad_fn=<AddBackward0>)\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4701 loss is tensor([-0.1401], grad_fn=<AddBackward0>)\n",
      "epoch: 4702 loss is tensor([-0.1860], grad_fn=<AddBackward0>)\n",
      "epoch: 4703 loss is tensor([-0.2036], grad_fn=<AddBackward0>)\n",
      "epoch: 4704 loss is tensor([-0.1765], grad_fn=<AddBackward0>)\n",
      "epoch: 4705 loss is tensor([-0.1924], grad_fn=<AddBackward0>)\n",
      "epoch: 4706 loss is tensor([-0.1437], grad_fn=<AddBackward0>)\n",
      "epoch: 4707 loss is tensor([-0.0964], grad_fn=<AddBackward0>)\n",
      "epoch: 4708 loss is tensor([-0.1492], grad_fn=<AddBackward0>)\n",
      "epoch: 4709 loss is tensor([-0.1132], grad_fn=<AddBackward0>)\n",
      "epoch: 4710 loss is tensor([-0.1247], grad_fn=<AddBackward0>)\n",
      "epoch: 4711 loss is tensor([-0.1183], grad_fn=<AddBackward0>)\n",
      "epoch: 4712 loss is tensor([-0.1361], grad_fn=<AddBackward0>)\n",
      "epoch: 4713 loss is tensor([-0.1522], grad_fn=<AddBackward0>)\n",
      "epoch: 4714 loss is tensor([-0.1118], grad_fn=<AddBackward0>)\n",
      "epoch: 4715 loss is tensor([-0.1521], grad_fn=<AddBackward0>)\n",
      "epoch: 4716 loss is tensor([-0.1554], grad_fn=<AddBackward0>)\n",
      "epoch: 4717 loss is tensor([-0.1434], grad_fn=<AddBackward0>)\n",
      "epoch: 4718 loss is tensor([-0.1519], grad_fn=<AddBackward0>)\n",
      "epoch: 4719 loss is tensor([-0.1221], grad_fn=<AddBackward0>)\n",
      "epoch: 4720 loss is tensor([-0.1668], grad_fn=<AddBackward0>)\n",
      "epoch: 4721 loss is tensor([-0.1123], grad_fn=<AddBackward0>)\n",
      "epoch: 4722 loss is tensor([-0.1699], grad_fn=<AddBackward0>)\n",
      "epoch: 4723 loss is tensor([-0.1602], grad_fn=<AddBackward0>)\n",
      "epoch: 4724 loss is tensor([-0.1927], grad_fn=<AddBackward0>)\n",
      "epoch: 4725 loss is tensor([-0.1192], grad_fn=<AddBackward0>)\n",
      "epoch: 4726 loss is tensor([-0.1077], grad_fn=<AddBackward0>)\n",
      "epoch: 4727 loss is tensor([-0.1383], grad_fn=<AddBackward0>)\n",
      "epoch: 4728 loss is tensor([-0.1097], grad_fn=<AddBackward0>)\n",
      "epoch: 4729 loss is tensor([-0.1234], grad_fn=<AddBackward0>)\n",
      "epoch: 4730 loss is tensor([-0.1688], grad_fn=<AddBackward0>)\n",
      "epoch: 4731 loss is tensor([-0.1752], grad_fn=<AddBackward0>)\n",
      "epoch: 4732 loss is tensor([-0.2026], grad_fn=<AddBackward0>)\n",
      "epoch: 4733 loss is tensor([-0.2142], grad_fn=<AddBackward0>)\n",
      "epoch: 4734 loss is tensor([-0.1903], grad_fn=<AddBackward0>)\n",
      "epoch: 4735 loss is tensor([-0.1371], grad_fn=<AddBackward0>)\n",
      "epoch: 4736 loss is tensor([-0.1861], grad_fn=<AddBackward0>)\n",
      "epoch: 4737 loss is tensor([-0.1593], grad_fn=<AddBackward0>)\n",
      "epoch: 4738 loss is tensor([-0.1406], grad_fn=<AddBackward0>)\n",
      "epoch: 4739 loss is tensor([-0.1046], grad_fn=<AddBackward0>)\n",
      "epoch: 4740 loss is tensor([-0.1878], grad_fn=<AddBackward0>)\n",
      "epoch: 4741 loss is tensor([-0.1530], grad_fn=<AddBackward0>)\n",
      "epoch: 4742 loss is tensor([-0.1175], grad_fn=<AddBackward0>)\n",
      "epoch: 4743 loss is tensor([-0.1416], grad_fn=<AddBackward0>)\n",
      "epoch: 4744 loss is tensor([-0.1498], grad_fn=<AddBackward0>)\n",
      "epoch: 4745 loss is tensor([-0.1309], grad_fn=<AddBackward0>)\n",
      "epoch: 4746 loss is tensor([-0.1753], grad_fn=<AddBackward0>)\n",
      "epoch: 4747 loss is tensor([-0.1630], grad_fn=<AddBackward0>)\n",
      "epoch: 4748 loss is tensor([-0.1601], grad_fn=<AddBackward0>)\n",
      "epoch: 4749 loss is tensor([-0.1949], grad_fn=<AddBackward0>)\n",
      "epoch: 4750 loss is tensor([-0.1917], grad_fn=<AddBackward0>)\n",
      "epoch: 4751 loss is tensor([-0.1688], grad_fn=<AddBackward0>)\n",
      "epoch: 4752 loss is tensor([-0.1173], grad_fn=<AddBackward0>)\n",
      "epoch: 4753 loss is tensor([-0.1808], grad_fn=<AddBackward0>)\n",
      "epoch: 4754 loss is tensor([-0.1711], grad_fn=<AddBackward0>)\n",
      "epoch: 4755 loss is tensor([-0.1940], grad_fn=<AddBackward0>)\n",
      "epoch: 4756 loss is tensor([-0.1163], grad_fn=<AddBackward0>)\n",
      "epoch: 4757 loss is tensor([-0.1705], grad_fn=<AddBackward0>)\n",
      "epoch: 4758 loss is tensor([-0.1804], grad_fn=<AddBackward0>)\n",
      "epoch: 4759 loss is tensor([-0.1758], grad_fn=<AddBackward0>)\n",
      "epoch: 4760 loss is tensor([-0.1764], grad_fn=<AddBackward0>)\n",
      "epoch: 4761 loss is tensor([-0.1918], grad_fn=<AddBackward0>)\n",
      "epoch: 4762 loss is tensor([-0.1088], grad_fn=<AddBackward0>)\n",
      "epoch: 4763 loss is tensor([-0.2118], grad_fn=<AddBackward0>)\n",
      "epoch: 4764 loss is tensor([-0.1078], grad_fn=<AddBackward0>)\n",
      "epoch: 4765 loss is tensor([-0.1892], grad_fn=<AddBackward0>)\n",
      "epoch: 4766 loss is tensor([-0.1755], grad_fn=<AddBackward0>)\n",
      "epoch: 4767 loss is tensor([-0.1946], grad_fn=<AddBackward0>)\n",
      "epoch: 4768 loss is tensor([-0.1833], grad_fn=<AddBackward0>)\n",
      "epoch: 4769 loss is tensor([-0.2156], grad_fn=<AddBackward0>)\n",
      "epoch: 4770 loss is tensor([-0.1605], grad_fn=<AddBackward0>)\n",
      "epoch: 4771 loss is tensor([-0.2147], grad_fn=<AddBackward0>)\n",
      "epoch: 4772 loss is tensor([-0.1772], grad_fn=<AddBackward0>)\n",
      "epoch: 4773 loss is tensor([-0.1524], grad_fn=<AddBackward0>)\n",
      "epoch: 4774 loss is tensor([-0.1830], grad_fn=<AddBackward0>)\n",
      "epoch: 4775 loss is tensor([-0.1710], grad_fn=<AddBackward0>)\n",
      "epoch: 4776 loss is tensor([-0.1603], grad_fn=<AddBackward0>)\n",
      "epoch: 4777 loss is tensor([-0.1386], grad_fn=<AddBackward0>)\n",
      "epoch: 4778 loss is tensor([-0.1629], grad_fn=<AddBackward0>)\n",
      "epoch: 4779 loss is tensor([-0.1823], grad_fn=<AddBackward0>)\n",
      "epoch: 4780 loss is tensor([-0.0541], grad_fn=<AddBackward0>)\n",
      "epoch: 4781 loss is tensor([-0.1013], grad_fn=<AddBackward0>)\n",
      "epoch: 4782 loss is tensor([-0.1728], grad_fn=<AddBackward0>)\n",
      "epoch: 4783 loss is tensor([-0.1040], grad_fn=<AddBackward0>)\n",
      "epoch: 4784 loss is tensor([-0.1875], grad_fn=<AddBackward0>)\n",
      "epoch: 4785 loss is tensor([-0.1777], grad_fn=<AddBackward0>)\n",
      "epoch: 4786 loss is tensor([-0.1526], grad_fn=<AddBackward0>)\n",
      "epoch: 4787 loss is tensor([-0.1775], grad_fn=<AddBackward0>)\n",
      "epoch: 4788 loss is tensor([-0.1843], grad_fn=<AddBackward0>)\n",
      "epoch: 4789 loss is tensor([-0.1134], grad_fn=<AddBackward0>)\n",
      "epoch: 4790 loss is tensor([-0.1874], grad_fn=<AddBackward0>)\n",
      "epoch: 4791 loss is tensor([-0.1424], grad_fn=<AddBackward0>)\n",
      "epoch: 4792 loss is tensor([-0.1993], grad_fn=<AddBackward0>)\n",
      "epoch: 4793 loss is tensor([-0.1659], grad_fn=<AddBackward0>)\n",
      "epoch: 4794 loss is tensor([-0.1161], grad_fn=<AddBackward0>)\n",
      "epoch: 4795 loss is tensor([-0.1914], grad_fn=<AddBackward0>)\n",
      "epoch: 4796 loss is tensor([-0.1312], grad_fn=<AddBackward0>)\n",
      "epoch: 4797 loss is tensor([-0.1415], grad_fn=<AddBackward0>)\n",
      "epoch: 4798 loss is tensor([-0.1162], grad_fn=<AddBackward0>)\n",
      "epoch: 4799 loss is tensor([-0.1354], grad_fn=<AddBackward0>)\n",
      "epoch: 4800 loss is tensor([-0.1809], grad_fn=<AddBackward0>)\n",
      "62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4801 loss is tensor([-0.0825], grad_fn=<AddBackward0>)\n",
      "epoch: 4802 loss is tensor([-0.1350], grad_fn=<AddBackward0>)\n",
      "epoch: 4803 loss is tensor([-0.1449], grad_fn=<AddBackward0>)\n",
      "epoch: 4804 loss is tensor([-0.1600], grad_fn=<AddBackward0>)\n",
      "epoch: 4805 loss is tensor([-0.1552], grad_fn=<AddBackward0>)\n",
      "epoch: 4806 loss is tensor([-0.0747], grad_fn=<AddBackward0>)\n",
      "epoch: 4807 loss is tensor([-0.1443], grad_fn=<AddBackward0>)\n",
      "epoch: 4808 loss is tensor([-0.1272], grad_fn=<AddBackward0>)\n",
      "epoch: 4809 loss is tensor([-0.1846], grad_fn=<AddBackward0>)\n",
      "epoch: 4810 loss is tensor([-0.1381], grad_fn=<AddBackward0>)\n",
      "epoch: 4811 loss is tensor([-0.1825], grad_fn=<AddBackward0>)\n",
      "epoch: 4812 loss is tensor([-0.2264], grad_fn=<AddBackward0>)\n",
      "epoch: 4813 loss is tensor([-0.1738], grad_fn=<AddBackward0>)\n",
      "epoch: 4814 loss is tensor([-0.1785], grad_fn=<AddBackward0>)\n",
      "epoch: 4815 loss is tensor([-0.1873], grad_fn=<AddBackward0>)\n",
      "epoch: 4816 loss is tensor([-0.1610], grad_fn=<AddBackward0>)\n",
      "epoch: 4817 loss is tensor([-0.1670], grad_fn=<AddBackward0>)\n",
      "epoch: 4818 loss is tensor([-0.1961], grad_fn=<AddBackward0>)\n",
      "epoch: 4819 loss is tensor([-0.1472], grad_fn=<AddBackward0>)\n",
      "epoch: 4820 loss is tensor([-0.1896], grad_fn=<AddBackward0>)\n",
      "epoch: 4821 loss is tensor([-0.1391], grad_fn=<AddBackward0>)\n",
      "epoch: 4822 loss is tensor([-0.2095], grad_fn=<AddBackward0>)\n",
      "epoch: 4823 loss is tensor([-0.2013], grad_fn=<AddBackward0>)\n",
      "epoch: 4824 loss is tensor([-0.1862], grad_fn=<AddBackward0>)\n",
      "epoch: 4825 loss is tensor([-0.1534], grad_fn=<AddBackward0>)\n",
      "epoch: 4826 loss is tensor([-0.1781], grad_fn=<AddBackward0>)\n",
      "epoch: 4827 loss is tensor([-0.1250], grad_fn=<AddBackward0>)\n",
      "epoch: 4828 loss is tensor([-0.2157], grad_fn=<AddBackward0>)\n",
      "epoch: 4829 loss is tensor([-0.2049], grad_fn=<AddBackward0>)\n",
      "epoch: 4830 loss is tensor([-0.1425], grad_fn=<AddBackward0>)\n",
      "epoch: 4831 loss is tensor([-0.1733], grad_fn=<AddBackward0>)\n",
      "epoch: 4832 loss is tensor([-0.1574], grad_fn=<AddBackward0>)\n",
      "epoch: 4833 loss is tensor([-0.2149], grad_fn=<AddBackward0>)\n",
      "epoch: 4834 loss is tensor([-0.1629], grad_fn=<AddBackward0>)\n",
      "epoch: 4835 loss is tensor([-0.2523], grad_fn=<AddBackward0>)\n",
      "epoch: 4836 loss is tensor([-0.1620], grad_fn=<AddBackward0>)\n",
      "epoch: 4837 loss is tensor([-0.2005], grad_fn=<AddBackward0>)\n",
      "epoch: 4838 loss is tensor([-0.1818], grad_fn=<AddBackward0>)\n",
      "epoch: 4839 loss is tensor([-0.1992], grad_fn=<AddBackward0>)\n",
      "epoch: 4840 loss is tensor([-0.1991], grad_fn=<AddBackward0>)\n",
      "epoch: 4841 loss is tensor([-0.1331], grad_fn=<AddBackward0>)\n",
      "epoch: 4842 loss is tensor([-0.2182], grad_fn=<AddBackward0>)\n",
      "epoch: 4843 loss is tensor([-0.1695], grad_fn=<AddBackward0>)\n",
      "epoch: 4844 loss is tensor([-0.1594], grad_fn=<AddBackward0>)\n",
      "epoch: 4845 loss is tensor([-0.2071], grad_fn=<AddBackward0>)\n",
      "epoch: 4846 loss is tensor([-0.1355], grad_fn=<AddBackward0>)\n",
      "epoch: 4847 loss is tensor([-0.1615], grad_fn=<AddBackward0>)\n",
      "epoch: 4848 loss is tensor([-0.0973], grad_fn=<AddBackward0>)\n",
      "epoch: 4849 loss is tensor([-0.0881], grad_fn=<AddBackward0>)\n",
      "epoch: 4850 loss is tensor([-0.1138], grad_fn=<AddBackward0>)\n",
      "epoch: 4851 loss is tensor([-0.1163], grad_fn=<AddBackward0>)\n",
      "epoch: 4852 loss is tensor([-0.1292], grad_fn=<AddBackward0>)\n",
      "epoch: 4853 loss is tensor([-0.1773], grad_fn=<AddBackward0>)\n",
      "epoch: 4854 loss is tensor([-0.1887], grad_fn=<AddBackward0>)\n",
      "epoch: 4855 loss is tensor([-0.1730], grad_fn=<AddBackward0>)\n",
      "epoch: 4856 loss is tensor([-0.1329], grad_fn=<AddBackward0>)\n",
      "epoch: 4857 loss is tensor([-0.1864], grad_fn=<AddBackward0>)\n",
      "epoch: 4858 loss is tensor([-0.1595], grad_fn=<AddBackward0>)\n",
      "epoch: 4859 loss is tensor([-0.2047], grad_fn=<AddBackward0>)\n",
      "epoch: 4860 loss is tensor([-0.1717], grad_fn=<AddBackward0>)\n",
      "epoch: 4861 loss is tensor([-0.1618], grad_fn=<AddBackward0>)\n",
      "epoch: 4862 loss is tensor([-0.1370], grad_fn=<AddBackward0>)\n",
      "epoch: 4863 loss is tensor([-0.1134], grad_fn=<AddBackward0>)\n",
      "epoch: 4864 loss is tensor([-0.1551], grad_fn=<AddBackward0>)\n",
      "epoch: 4865 loss is tensor([-0.1487], grad_fn=<AddBackward0>)\n",
      "epoch: 4866 loss is tensor([-0.1827], grad_fn=<AddBackward0>)\n",
      "epoch: 4867 loss is tensor([-0.1061], grad_fn=<AddBackward0>)\n",
      "epoch: 4868 loss is tensor([-0.0920], grad_fn=<AddBackward0>)\n",
      "epoch: 4869 loss is tensor([-0.1792], grad_fn=<AddBackward0>)\n",
      "epoch: 4870 loss is tensor([-0.1537], grad_fn=<AddBackward0>)\n",
      "epoch: 4871 loss is tensor([-0.1368], grad_fn=<AddBackward0>)\n",
      "epoch: 4872 loss is tensor([-0.1203], grad_fn=<AddBackward0>)\n",
      "epoch: 4873 loss is tensor([-0.1829], grad_fn=<AddBackward0>)\n",
      "epoch: 4874 loss is tensor([-0.1293], grad_fn=<AddBackward0>)\n",
      "epoch: 4875 loss is tensor([-0.1251], grad_fn=<AddBackward0>)\n",
      "epoch: 4876 loss is tensor([-0.1518], grad_fn=<AddBackward0>)\n",
      "epoch: 4877 loss is tensor([-0.1639], grad_fn=<AddBackward0>)\n",
      "epoch: 4878 loss is tensor([-0.1385], grad_fn=<AddBackward0>)\n",
      "epoch: 4879 loss is tensor([-0.0985], grad_fn=<AddBackward0>)\n",
      "epoch: 4880 loss is tensor([-0.1349], grad_fn=<AddBackward0>)\n",
      "epoch: 4881 loss is tensor([-0.1598], grad_fn=<AddBackward0>)\n",
      "epoch: 4882 loss is tensor([-0.1896], grad_fn=<AddBackward0>)\n",
      "epoch: 4883 loss is tensor([-0.1511], grad_fn=<AddBackward0>)\n",
      "epoch: 4884 loss is tensor([-0.1781], grad_fn=<AddBackward0>)\n",
      "epoch: 4885 loss is tensor([-0.1168], grad_fn=<AddBackward0>)\n",
      "epoch: 4886 loss is tensor([-0.1373], grad_fn=<AddBackward0>)\n",
      "epoch: 4887 loss is tensor([-0.1944], grad_fn=<AddBackward0>)\n",
      "epoch: 4888 loss is tensor([-0.1682], grad_fn=<AddBackward0>)\n",
      "epoch: 4889 loss is tensor([-0.1243], grad_fn=<AddBackward0>)\n",
      "epoch: 4890 loss is tensor([-0.1798], grad_fn=<AddBackward0>)\n",
      "epoch: 4891 loss is tensor([-0.2089], grad_fn=<AddBackward0>)\n",
      "epoch: 4892 loss is tensor([-0.2180], grad_fn=<AddBackward0>)\n",
      "epoch: 4893 loss is tensor([-0.1752], grad_fn=<AddBackward0>)\n",
      "epoch: 4894 loss is tensor([-0.1410], grad_fn=<AddBackward0>)\n",
      "epoch: 4895 loss is tensor([-0.1692], grad_fn=<AddBackward0>)\n",
      "epoch: 4896 loss is tensor([-0.1709], grad_fn=<AddBackward0>)\n",
      "epoch: 4897 loss is tensor([-0.1514], grad_fn=<AddBackward0>)\n",
      "epoch: 4898 loss is tensor([-0.1531], grad_fn=<AddBackward0>)\n",
      "epoch: 4899 loss is tensor([-0.1958], grad_fn=<AddBackward0>)\n",
      "epoch: 4900 loss is tensor([-0.1212], grad_fn=<AddBackward0>)\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4901 loss is tensor([-0.1925], grad_fn=<AddBackward0>)\n",
      "epoch: 4902 loss is tensor([-0.0680], grad_fn=<AddBackward0>)\n",
      "epoch: 4903 loss is tensor([-0.1194], grad_fn=<AddBackward0>)\n",
      "epoch: 4904 loss is tensor([-0.0915], grad_fn=<AddBackward0>)\n",
      "epoch: 4905 loss is tensor([-0.1396], grad_fn=<AddBackward0>)\n",
      "epoch: 4906 loss is tensor([-0.0888], grad_fn=<AddBackward0>)\n",
      "epoch: 4907 loss is tensor([-0.1639], grad_fn=<AddBackward0>)\n",
      "epoch: 4908 loss is tensor([-0.0802], grad_fn=<AddBackward0>)\n",
      "epoch: 4909 loss is tensor([-0.1267], grad_fn=<AddBackward0>)\n",
      "epoch: 4910 loss is tensor([-0.0676], grad_fn=<AddBackward0>)\n",
      "epoch: 4911 loss is tensor([-0.1094], grad_fn=<AddBackward0>)\n",
      "epoch: 4912 loss is tensor([-0.1415], grad_fn=<AddBackward0>)\n",
      "epoch: 4913 loss is tensor([-0.0562], grad_fn=<AddBackward0>)\n",
      "epoch: 4914 loss is tensor([-0.2115], grad_fn=<AddBackward0>)\n",
      "epoch: 4915 loss is tensor([-0.1611], grad_fn=<AddBackward0>)\n",
      "epoch: 4916 loss is tensor([-0.1909], grad_fn=<AddBackward0>)\n",
      "epoch: 4917 loss is tensor([-0.1886], grad_fn=<AddBackward0>)\n",
      "epoch: 4918 loss is tensor([-0.1437], grad_fn=<AddBackward0>)\n",
      "epoch: 4919 loss is tensor([-0.1702], grad_fn=<AddBackward0>)\n",
      "epoch: 4920 loss is tensor([-0.1565], grad_fn=<AddBackward0>)\n",
      "epoch: 4921 loss is tensor([-0.1563], grad_fn=<AddBackward0>)\n",
      "epoch: 4922 loss is tensor([-0.1356], grad_fn=<AddBackward0>)\n",
      "epoch: 4923 loss is tensor([-0.1653], grad_fn=<AddBackward0>)\n",
      "epoch: 4924 loss is tensor([-0.1816], grad_fn=<AddBackward0>)\n",
      "epoch: 4925 loss is tensor([-0.1480], grad_fn=<AddBackward0>)\n",
      "epoch: 4926 loss is tensor([-0.1709], grad_fn=<AddBackward0>)\n",
      "epoch: 4927 loss is tensor([-0.1777], grad_fn=<AddBackward0>)\n",
      "epoch: 4928 loss is tensor([-0.1069], grad_fn=<AddBackward0>)\n",
      "epoch: 4929 loss is tensor([-0.1300], grad_fn=<AddBackward0>)\n",
      "epoch: 4930 loss is tensor([-0.1127], grad_fn=<AddBackward0>)\n",
      "epoch: 4931 loss is tensor([-0.1480], grad_fn=<AddBackward0>)\n",
      "epoch: 4932 loss is tensor([-0.1934], grad_fn=<AddBackward0>)\n",
      "epoch: 4933 loss is tensor([-0.1808], grad_fn=<AddBackward0>)\n",
      "epoch: 4934 loss is tensor([-0.1771], grad_fn=<AddBackward0>)\n",
      "epoch: 4935 loss is tensor([-0.1219], grad_fn=<AddBackward0>)\n",
      "epoch: 4936 loss is tensor([-0.1871], grad_fn=<AddBackward0>)\n",
      "epoch: 4937 loss is tensor([-0.1634], grad_fn=<AddBackward0>)\n",
      "epoch: 4938 loss is tensor([-0.1718], grad_fn=<AddBackward0>)\n",
      "epoch: 4939 loss is tensor([-0.1837], grad_fn=<AddBackward0>)\n",
      "epoch: 4940 loss is tensor([-0.2238], grad_fn=<AddBackward0>)\n",
      "epoch: 4941 loss is tensor([-0.1870], grad_fn=<AddBackward0>)\n",
      "epoch: 4942 loss is tensor([-0.1646], grad_fn=<AddBackward0>)\n",
      "epoch: 4943 loss is tensor([-0.2123], grad_fn=<AddBackward0>)\n",
      "epoch: 4944 loss is tensor([-0.2240], grad_fn=<AddBackward0>)\n",
      "epoch: 4945 loss is tensor([-0.2040], grad_fn=<AddBackward0>)\n",
      "epoch: 4946 loss is tensor([-0.1186], grad_fn=<AddBackward0>)\n",
      "epoch: 4947 loss is tensor([-0.1421], grad_fn=<AddBackward0>)\n",
      "epoch: 4948 loss is tensor([-0.1936], grad_fn=<AddBackward0>)\n",
      "epoch: 4949 loss is tensor([-0.1827], grad_fn=<AddBackward0>)\n",
      "epoch: 4950 loss is tensor([-0.0867], grad_fn=<AddBackward0>)\n",
      "epoch: 4951 loss is tensor([-0.0187], grad_fn=<AddBackward0>)\n",
      "epoch: 4952 loss is tensor([-0.0691], grad_fn=<AddBackward0>)\n",
      "epoch: 4953 loss is tensor([-0.0699], grad_fn=<AddBackward0>)\n",
      "epoch: 4954 loss is tensor([-0.1042], grad_fn=<AddBackward0>)\n",
      "epoch: 4955 loss is tensor([-0.1560], grad_fn=<AddBackward0>)\n",
      "epoch: 4956 loss is tensor([-0.0512], grad_fn=<AddBackward0>)\n",
      "epoch: 4957 loss is tensor([-0.1517], grad_fn=<AddBackward0>)\n",
      "epoch: 4958 loss is tensor([-0.1589], grad_fn=<AddBackward0>)\n",
      "epoch: 4959 loss is tensor([-0.1951], grad_fn=<AddBackward0>)\n",
      "epoch: 4960 loss is tensor([-0.1355], grad_fn=<AddBackward0>)\n",
      "epoch: 4961 loss is tensor([-0.1203], grad_fn=<AddBackward0>)\n",
      "epoch: 4962 loss is tensor([-0.1670], grad_fn=<AddBackward0>)\n",
      "epoch: 4963 loss is tensor([-0.0885], grad_fn=<AddBackward0>)\n",
      "epoch: 4964 loss is tensor([-0.2139], grad_fn=<AddBackward0>)\n",
      "epoch: 4965 loss is tensor([-0.1473], grad_fn=<AddBackward0>)\n",
      "epoch: 4966 loss is tensor([-0.1571], grad_fn=<AddBackward0>)\n",
      "epoch: 4967 loss is tensor([-0.1247], grad_fn=<AddBackward0>)\n",
      "epoch: 4968 loss is tensor([-0.1520], grad_fn=<AddBackward0>)\n",
      "epoch: 4969 loss is tensor([-0.1753], grad_fn=<AddBackward0>)\n",
      "epoch: 4970 loss is tensor([-0.1592], grad_fn=<AddBackward0>)\n",
      "epoch: 4971 loss is tensor([-0.1870], grad_fn=<AddBackward0>)\n",
      "epoch: 4972 loss is tensor([-0.1402], grad_fn=<AddBackward0>)\n",
      "epoch: 4973 loss is tensor([-0.1396], grad_fn=<AddBackward0>)\n",
      "epoch: 4974 loss is tensor([-0.1532], grad_fn=<AddBackward0>)\n",
      "epoch: 4975 loss is tensor([-0.1967], grad_fn=<AddBackward0>)\n",
      "epoch: 4976 loss is tensor([-0.2023], grad_fn=<AddBackward0>)\n",
      "epoch: 4977 loss is tensor([-0.1978], grad_fn=<AddBackward0>)\n",
      "epoch: 4978 loss is tensor([-0.1816], grad_fn=<AddBackward0>)\n",
      "epoch: 4979 loss is tensor([-0.1204], grad_fn=<AddBackward0>)\n",
      "epoch: 4980 loss is tensor([-0.1657], grad_fn=<AddBackward0>)\n",
      "epoch: 4981 loss is tensor([-0.1742], grad_fn=<AddBackward0>)\n",
      "epoch: 4982 loss is tensor([-0.2243], grad_fn=<AddBackward0>)\n",
      "epoch: 4983 loss is tensor([-0.1658], grad_fn=<AddBackward0>)\n",
      "epoch: 4984 loss is tensor([-0.1841], grad_fn=<AddBackward0>)\n",
      "epoch: 4985 loss is tensor([-0.2301], grad_fn=<AddBackward0>)\n",
      "epoch: 4986 loss is tensor([-0.1610], grad_fn=<AddBackward0>)\n",
      "epoch: 4987 loss is tensor([-0.1751], grad_fn=<AddBackward0>)\n",
      "epoch: 4988 loss is tensor([-0.1946], grad_fn=<AddBackward0>)\n",
      "epoch: 4989 loss is tensor([-0.1456], grad_fn=<AddBackward0>)\n",
      "epoch: 4990 loss is tensor([-0.2541], grad_fn=<AddBackward0>)\n",
      "epoch: 4991 loss is tensor([-0.2020], grad_fn=<AddBackward0>)\n",
      "epoch: 4992 loss is tensor([-0.2567], grad_fn=<AddBackward0>)\n",
      "epoch: 4993 loss is tensor([-0.2012], grad_fn=<AddBackward0>)\n",
      "epoch: 4994 loss is tensor([-0.1978], grad_fn=<AddBackward0>)\n",
      "epoch: 4995 loss is tensor([-0.2064], grad_fn=<AddBackward0>)\n",
      "epoch: 4996 loss is tensor([-0.2130], grad_fn=<AddBackward0>)\n",
      "epoch: 4997 loss is tensor([-0.2475], grad_fn=<AddBackward0>)\n",
      "epoch: 4998 loss is tensor([-0.2018], grad_fn=<AddBackward0>)\n",
      "epoch: 4999 loss is tensor([-0.1598], grad_fn=<AddBackward0>)\n",
      "epoch: 5000 loss is tensor([-0.1554], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 5001 loss is tensor([-0.1781], grad_fn=<AddBackward0>)\n",
      "epoch: 5002 loss is tensor([-0.2104], grad_fn=<AddBackward0>)\n",
      "epoch: 5003 loss is tensor([-0.1630], grad_fn=<AddBackward0>)\n",
      "epoch: 5004 loss is tensor([-0.2461], grad_fn=<AddBackward0>)\n",
      "epoch: 5005 loss is tensor([-0.1624], grad_fn=<AddBackward0>)\n",
      "epoch: 5006 loss is tensor([-0.1930], grad_fn=<AddBackward0>)\n",
      "epoch: 5007 loss is tensor([-0.1887], grad_fn=<AddBackward0>)\n",
      "epoch: 5008 loss is tensor([-0.2127], grad_fn=<AddBackward0>)\n",
      "epoch: 5009 loss is tensor([-0.2015], grad_fn=<AddBackward0>)\n",
      "epoch: 5010 loss is tensor([-0.1579], grad_fn=<AddBackward0>)\n",
      "epoch: 5011 loss is tensor([-0.1373], grad_fn=<AddBackward0>)\n",
      "epoch: 5012 loss is tensor([-0.1847], grad_fn=<AddBackward0>)\n",
      "epoch: 5013 loss is tensor([-0.2173], grad_fn=<AddBackward0>)\n",
      "epoch: 5014 loss is tensor([-0.1279], grad_fn=<AddBackward0>)\n",
      "epoch: 5015 loss is tensor([-0.1739], grad_fn=<AddBackward0>)\n",
      "epoch: 5016 loss is tensor([-0.2041], grad_fn=<AddBackward0>)\n",
      "epoch: 5017 loss is tensor([-0.1339], grad_fn=<AddBackward0>)\n",
      "epoch: 5018 loss is tensor([-0.1919], grad_fn=<AddBackward0>)\n",
      "epoch: 5019 loss is tensor([-0.1565], grad_fn=<AddBackward0>)\n",
      "epoch: 5020 loss is tensor([-0.2224], grad_fn=<AddBackward0>)\n",
      "epoch: 5021 loss is tensor([-0.1829], grad_fn=<AddBackward0>)\n",
      "epoch: 5022 loss is tensor([-0.2052], grad_fn=<AddBackward0>)\n",
      "epoch: 5023 loss is tensor([-0.1393], grad_fn=<AddBackward0>)\n",
      "epoch: 5024 loss is tensor([-0.1424], grad_fn=<AddBackward0>)\n",
      "epoch: 5025 loss is tensor([-0.1638], grad_fn=<AddBackward0>)\n",
      "epoch: 5026 loss is tensor([-0.0819], grad_fn=<AddBackward0>)\n",
      "epoch: 5027 loss is tensor([-0.1575], grad_fn=<AddBackward0>)\n",
      "epoch: 5028 loss is tensor([-0.1857], grad_fn=<AddBackward0>)\n",
      "epoch: 5029 loss is tensor([-0.1511], grad_fn=<AddBackward0>)\n",
      "epoch: 5030 loss is tensor([-0.1647], grad_fn=<AddBackward0>)\n",
      "epoch: 5031 loss is tensor([-0.0726], grad_fn=<AddBackward0>)\n",
      "epoch: 5032 loss is tensor([-0.1709], grad_fn=<AddBackward0>)\n",
      "epoch: 5033 loss is tensor([-0.2191], grad_fn=<AddBackward0>)\n",
      "epoch: 5034 loss is tensor([-0.1774], grad_fn=<AddBackward0>)\n",
      "epoch: 5035 loss is tensor([-0.1275], grad_fn=<AddBackward0>)\n",
      "epoch: 5036 loss is tensor([-0.1582], grad_fn=<AddBackward0>)\n",
      "epoch: 5037 loss is tensor([-0.1344], grad_fn=<AddBackward0>)\n",
      "epoch: 5038 loss is tensor([-0.2455], grad_fn=<AddBackward0>)\n",
      "epoch: 5039 loss is tensor([-0.1372], grad_fn=<AddBackward0>)\n",
      "epoch: 5040 loss is tensor([-0.1675], grad_fn=<AddBackward0>)\n",
      "epoch: 5041 loss is tensor([-0.2036], grad_fn=<AddBackward0>)\n",
      "epoch: 5042 loss is tensor([-0.2346], grad_fn=<AddBackward0>)\n",
      "epoch: 5043 loss is tensor([-0.1887], grad_fn=<AddBackward0>)\n",
      "epoch: 5044 loss is tensor([-0.1822], grad_fn=<AddBackward0>)\n",
      "epoch: 5045 loss is tensor([-0.1801], grad_fn=<AddBackward0>)\n",
      "epoch: 5046 loss is tensor([-0.1308], grad_fn=<AddBackward0>)\n",
      "epoch: 5047 loss is tensor([-0.1607], grad_fn=<AddBackward0>)\n",
      "epoch: 5048 loss is tensor([-0.1414], grad_fn=<AddBackward0>)\n",
      "epoch: 5049 loss is tensor([-0.1663], grad_fn=<AddBackward0>)\n",
      "epoch: 5050 loss is tensor([-0.2394], grad_fn=<AddBackward0>)\n",
      "epoch: 5051 loss is tensor([-0.1533], grad_fn=<AddBackward0>)\n",
      "epoch: 5052 loss is tensor([-0.1934], grad_fn=<AddBackward0>)\n",
      "epoch: 5053 loss is tensor([-0.1706], grad_fn=<AddBackward0>)\n",
      "epoch: 5054 loss is tensor([-0.1220], grad_fn=<AddBackward0>)\n",
      "epoch: 5055 loss is tensor([-0.1811], grad_fn=<AddBackward0>)\n",
      "epoch: 5056 loss is tensor([-0.1832], grad_fn=<AddBackward0>)\n",
      "epoch: 5057 loss is tensor([-0.1305], grad_fn=<AddBackward0>)\n",
      "epoch: 5058 loss is tensor([-0.2369], grad_fn=<AddBackward0>)\n",
      "epoch: 5059 loss is tensor([-0.1312], grad_fn=<AddBackward0>)\n",
      "epoch: 5060 loss is tensor([-0.1508], grad_fn=<AddBackward0>)\n",
      "epoch: 5061 loss is tensor([-0.1522], grad_fn=<AddBackward0>)\n",
      "epoch: 5062 loss is tensor([-0.1534], grad_fn=<AddBackward0>)\n",
      "epoch: 5063 loss is tensor([-0.2014], grad_fn=<AddBackward0>)\n",
      "epoch: 5064 loss is tensor([-0.1188], grad_fn=<AddBackward0>)\n",
      "epoch: 5065 loss is tensor([-0.0828], grad_fn=<AddBackward0>)\n",
      "epoch: 5066 loss is tensor([-0.1782], grad_fn=<AddBackward0>)\n",
      "epoch: 5067 loss is tensor([-0.2220], grad_fn=<AddBackward0>)\n",
      "epoch: 5068 loss is tensor([-0.1535], grad_fn=<AddBackward0>)\n",
      "epoch: 5069 loss is tensor([-0.1698], grad_fn=<AddBackward0>)\n",
      "epoch: 5070 loss is tensor([-0.1591], grad_fn=<AddBackward0>)\n",
      "epoch: 5071 loss is tensor([-0.1714], grad_fn=<AddBackward0>)\n",
      "epoch: 5072 loss is tensor([-0.1401], grad_fn=<AddBackward0>)\n",
      "epoch: 5073 loss is tensor([-0.1657], grad_fn=<AddBackward0>)\n",
      "epoch: 5074 loss is tensor([-0.1806], grad_fn=<AddBackward0>)\n",
      "epoch: 5075 loss is tensor([-0.1745], grad_fn=<AddBackward0>)\n",
      "epoch: 5076 loss is tensor([-0.2105], grad_fn=<AddBackward0>)\n",
      "epoch: 5077 loss is tensor([-0.1446], grad_fn=<AddBackward0>)\n",
      "epoch: 5078 loss is tensor([-0.1832], grad_fn=<AddBackward0>)\n",
      "epoch: 5079 loss is tensor([-0.1597], grad_fn=<AddBackward0>)\n",
      "epoch: 5080 loss is tensor([-0.1985], grad_fn=<AddBackward0>)\n",
      "epoch: 5081 loss is tensor([-0.2100], grad_fn=<AddBackward0>)\n",
      "epoch: 5082 loss is tensor([-0.2261], grad_fn=<AddBackward0>)\n",
      "epoch: 5083 loss is tensor([-0.2097], grad_fn=<AddBackward0>)\n",
      "epoch: 5084 loss is tensor([-0.2248], grad_fn=<AddBackward0>)\n",
      "epoch: 5085 loss is tensor([-0.1987], grad_fn=<AddBackward0>)\n",
      "epoch: 5086 loss is tensor([-0.1937], grad_fn=<AddBackward0>)\n",
      "epoch: 5087 loss is tensor([-0.1944], grad_fn=<AddBackward0>)\n",
      "epoch: 5088 loss is tensor([-0.2076], grad_fn=<AddBackward0>)\n",
      "epoch: 5089 loss is tensor([-0.1843], grad_fn=<AddBackward0>)\n",
      "epoch: 5090 loss is tensor([-0.1957], grad_fn=<AddBackward0>)\n",
      "epoch: 5091 loss is tensor([-0.1874], grad_fn=<AddBackward0>)\n",
      "epoch: 5092 loss is tensor([-0.2227], grad_fn=<AddBackward0>)\n",
      "epoch: 5093 loss is tensor([-0.1924], grad_fn=<AddBackward0>)\n",
      "epoch: 5094 loss is tensor([-0.1753], grad_fn=<AddBackward0>)\n",
      "epoch: 5095 loss is tensor([-0.1972], grad_fn=<AddBackward0>)\n",
      "epoch: 5096 loss is tensor([-0.2071], grad_fn=<AddBackward0>)\n",
      "epoch: 5097 loss is tensor([-0.2210], grad_fn=<AddBackward0>)\n",
      "epoch: 5098 loss is tensor([-0.1616], grad_fn=<AddBackward0>)\n",
      "epoch: 5099 loss is tensor([-0.2220], grad_fn=<AddBackward0>)\n",
      "epoch: 5100 loss is tensor([-0.1913], grad_fn=<AddBackward0>)\n",
      "31\n"
=======
      "The number of epochs is: 4201\n",
      "The number of epochs is: 4202\n",
      "The number of epochs is: 4203\n",
      "The number of epochs is: 4204\n",
      "The number of epochs is: 4205\n",
      "The number of epochs is: 4206\n",
      "The number of epochs is: 4207\n",
      "The number of epochs is: 4208\n",
      "The number of epochs is: 4209\n",
      "The number of epochs is: 4210\n",
      "The number of epochs is: 4211\n",
      "The number of epochs is: 4212\n",
      "The number of epochs is: 4213\n",
      "The number of epochs is: 4214\n",
      "The number of epochs is: 4215\n",
      "The number of epochs is: 4216\n",
      "The number of epochs is: 4217\n",
      "The number of epochs is: 4218\n",
      "The number of epochs is: 4219\n",
      "The number of epochs is: 4220\n",
      "The number of epochs is: 4221\n",
      "The number of epochs is: 4222\n",
      "The number of epochs is: 4223\n",
      "The number of epochs is: 4224\n",
      "The number of epochs is: 4225\n",
      "The number of epochs is: 4226\n",
      "The number of epochs is: 4227\n",
      "The number of epochs is: 4228\n",
      "The number of epochs is: 4229\n",
      "The number of epochs is: 4230\n",
      "The number of epochs is: 4231\n",
      "The number of epochs is: 4232\n",
      "The number of epochs is: 4233\n",
      "The number of epochs is: 4234\n",
      "The number of epochs is: 4235\n",
      "The number of epochs is: 4236\n",
      "The number of epochs is: 4237\n",
      "The number of epochs is: 4238\n",
      "The number of epochs is: 4239\n",
      "The number of epochs is: 4240\n",
      "The number of epochs is: 4241\n",
      "The number of epochs is: 4242\n",
      "The number of epochs is: 4243\n",
      "The number of epochs is: 4244\n",
      "The number of epochs is: 4245\n",
      "The number of epochs is: 4246\n",
      "The number of epochs is: 4247\n",
      "The number of epochs is: 4248\n",
      "The number of epochs is: 4249\n",
      "The number of epochs is: 4250\n",
      "The number of epochs is: 4251\n",
      "The number of epochs is: 4252\n",
      "The number of epochs is: 4253\n",
      "The number of epochs is: 4254\n",
      "The number of epochs is: 4255\n",
      "The number of epochs is: 4256\n",
      "The number of epochs is: 4257\n",
      "The number of epochs is: 4258\n",
      "The number of epochs is: 4259\n",
      "The number of epochs is: 4260\n",
      "The number of epochs is: 4261\n",
      "The number of epochs is: 4262\n",
      "The number of epochs is: 4263\n",
      "The number of epochs is: 4264\n",
      "The number of epochs is: 4265\n",
      "The number of epochs is: 4266\n",
      "The number of epochs is: 4267\n",
      "The number of epochs is: 4268\n",
      "The number of epochs is: 4269\n",
      "The number of epochs is: 4270\n",
      "The number of epochs is: 4271\n",
      "The number of epochs is: 4272\n",
      "The number of epochs is: 4273\n",
      "The number of epochs is: 4274\n",
      "The number of epochs is: 4275\n",
      "The number of epochs is: 4276\n",
      "The number of epochs is: 4277\n",
      "The number of epochs is: 4278\n",
      "The number of epochs is: 4279\n",
      "The number of epochs is: 4280\n",
      "The number of epochs is: 4281\n",
      "The number of epochs is: 4282\n",
      "The number of epochs is: 4283\n",
      "The number of epochs is: 4284\n",
      "The number of epochs is: 4285\n",
      "The number of epochs is: 4286\n",
      "The number of epochs is: 4287\n",
      "The number of epochs is: 4288\n",
      "The number of epochs is: 4289\n",
      "The number of epochs is: 4290\n",
      "The number of epochs is: 4291\n",
      "The number of epochs is: 4292\n",
      "The number of epochs is: 4293\n",
      "The number of epochs is: 4294\n",
      "The number of epochs is: 4295\n",
      "The number of epochs is: 4296\n",
      "The number of epochs is: 4297\n",
      "The number of epochs is: 4298\n",
      "The number of epochs is: 4299\n",
      "The number of epochs is: 4300\n",
      "The number of epochs is: 4301\n",
      "The number of epochs is: 4302\n",
      "The number of epochs is: 4303\n",
      "The number of epochs is: 4304\n",
      "The number of epochs is: 4305\n",
      "The number of epochs is: 4306\n",
      "The number of epochs is: 4307\n",
      "The number of epochs is: 4308\n",
      "The number of epochs is: 4309\n",
      "The number of epochs is: 4310\n",
      "The number of epochs is: 4311\n",
      "The number of epochs is: 4312\n",
      "The number of epochs is: 4313\n",
      "The number of epochs is: 4314\n",
      "The number of epochs is: 4315\n",
      "The number of epochs is: 4316\n",
      "The number of epochs is: 4317\n",
      "The number of epochs is: 4318\n",
      "The number of epochs is: 4319\n",
      "The number of epochs is: 4320\n",
      "The number of epochs is: 4321\n",
      "The number of epochs is: 4322\n",
      "The number of epochs is: 4323\n",
      "The number of epochs is: 4324\n",
      "The number of epochs is: 4325\n",
      "The number of epochs is: 4326\n",
      "The number of epochs is: 4327\n",
      "The number of epochs is: 4328\n",
      "The number of epochs is: 4329\n",
      "The number of epochs is: 4330\n",
      "The number of epochs is: 4331\n",
      "The number of epochs is: 4332\n",
      "The number of epochs is: 4333\n",
      "The number of epochs is: 4334\n",
      "The number of epochs is: 4335\n",
      "The number of epochs is: 4336\n",
      "The number of epochs is: 4337\n",
      "The number of epochs is: 4338\n",
      "The number of epochs is: 4339\n",
      "The number of epochs is: 4340\n",
      "The number of epochs is: 4341\n",
      "The number of epochs is: 4342\n",
      "The number of epochs is: 4343\n",
      "The number of epochs is: 4344\n",
      "The number of epochs is: 4345\n",
      "The number of epochs is: 4346\n",
      "The number of epochs is: 4347\n",
      "The number of epochs is: 4348\n",
      "The number of epochs is: 4349\n",
      "The number of epochs is: 4350\n",
      "The number of epochs is: 4351\n",
      "The number of epochs is: 4352\n",
      "The number of epochs is: 4353\n",
      "The number of epochs is: 4354\n",
      "The number of epochs is: 4355\n",
      "The number of epochs is: 4356\n",
      "The number of epochs is: 4357\n",
      "The number of epochs is: 4358\n",
      "The number of epochs is: 4359\n",
      "The number of epochs is: 4360\n",
      "The number of epochs is: 4361\n",
      "The number of epochs is: 4362\n",
      "The number of epochs is: 4363\n",
      "The number of epochs is: 4364\n",
      "The number of epochs is: 4365\n",
      "The number of epochs is: 4366\n",
      "The number of epochs is: 4367\n",
      "The number of epochs is: 4368\n",
      "The number of epochs is: 4369\n",
      "The number of epochs is: 4370\n",
      "The number of epochs is: 4371\n",
      "The number of epochs is: 4372\n",
      "The number of epochs is: 4373\n",
      "The number of epochs is: 4374\n",
      "The number of epochs is: 4375\n",
      "The number of epochs is: 4376\n",
      "The number of epochs is: 4377\n",
      "The number of epochs is: 4378\n",
      "The number of epochs is: 4379\n",
      "The number of epochs is: 4380\n",
      "The number of epochs is: 4381\n",
      "The number of epochs is: 4382\n",
      "The number of epochs is: 4383\n",
      "The number of epochs is: 4384\n",
      "The number of epochs is: 4385\n",
      "The number of epochs is: 4386\n",
      "The number of epochs is: 4387\n",
      "The number of epochs is: 4388\n",
      "The number of epochs is: 4389\n",
      "The number of epochs is: 4390\n",
      "The number of epochs is: 4391\n",
      "The number of epochs is: 4392\n",
      "The number of epochs is: 4393\n",
      "The number of epochs is: 4394\n",
      "The number of epochs is: 4395\n",
      "The number of epochs is: 4396\n",
      "The number of epochs is: 4397\n",
      "The number of epochs is: 4398\n",
      "The number of epochs is: 4399\n",
      "The number of epochs is: 4400\n",
      "19\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5101 loss is tensor([-0.1479], grad_fn=<AddBackward0>)\n",
      "epoch: 5102 loss is tensor([-0.1919], grad_fn=<AddBackward0>)\n",
      "epoch: 5103 loss is tensor([-0.2249], grad_fn=<AddBackward0>)\n",
      "epoch: 5104 loss is tensor([-0.1860], grad_fn=<AddBackward0>)\n",
      "epoch: 5105 loss is tensor([-0.1394], grad_fn=<AddBackward0>)\n",
      "epoch: 5106 loss is tensor([-0.1064], grad_fn=<AddBackward0>)\n",
      "epoch: 5107 loss is tensor([-0.1633], grad_fn=<AddBackward0>)\n",
      "epoch: 5108 loss is tensor([-0.1773], grad_fn=<AddBackward0>)\n",
      "epoch: 5109 loss is tensor([-0.1899], grad_fn=<AddBackward0>)\n",
      "epoch: 5110 loss is tensor([-0.2202], grad_fn=<AddBackward0>)\n",
      "epoch: 5111 loss is tensor([-0.1742], grad_fn=<AddBackward0>)\n",
      "epoch: 5112 loss is tensor([-0.1207], grad_fn=<AddBackward0>)\n",
      "epoch: 5113 loss is tensor([-0.1567], grad_fn=<AddBackward0>)\n",
      "epoch: 5114 loss is tensor([-0.2166], grad_fn=<AddBackward0>)\n",
      "epoch: 5115 loss is tensor([-0.1526], grad_fn=<AddBackward0>)\n",
      "epoch: 5116 loss is tensor([-0.1905], grad_fn=<AddBackward0>)\n",
      "epoch: 5117 loss is tensor([-0.2088], grad_fn=<AddBackward0>)\n",
      "epoch: 5118 loss is tensor([-0.1834], grad_fn=<AddBackward0>)\n",
      "epoch: 5119 loss is tensor([-0.1943], grad_fn=<AddBackward0>)\n",
      "epoch: 5120 loss is tensor([-0.1685], grad_fn=<AddBackward0>)\n",
      "epoch: 5121 loss is tensor([-0.2474], grad_fn=<AddBackward0>)\n",
      "epoch: 5122 loss is tensor([-0.1448], grad_fn=<AddBackward0>)\n",
      "epoch: 5123 loss is tensor([-0.2012], grad_fn=<AddBackward0>)\n",
      "epoch: 5124 loss is tensor([-0.2267], grad_fn=<AddBackward0>)\n",
      "epoch: 5125 loss is tensor([-0.1941], grad_fn=<AddBackward0>)\n",
      "epoch: 5126 loss is tensor([-0.2343], grad_fn=<AddBackward0>)\n",
      "epoch: 5127 loss is tensor([-0.1969], grad_fn=<AddBackward0>)\n",
      "epoch: 5128 loss is tensor([-0.2481], grad_fn=<AddBackward0>)\n",
      "epoch: 5129 loss is tensor([-0.2200], grad_fn=<AddBackward0>)\n",
      "epoch: 5130 loss is tensor([-0.1881], grad_fn=<AddBackward0>)\n",
      "epoch: 5131 loss is tensor([-0.2361], grad_fn=<AddBackward0>)\n",
      "epoch: 5132 loss is tensor([-0.1932], grad_fn=<AddBackward0>)\n",
      "epoch: 5133 loss is tensor([-0.1616], grad_fn=<AddBackward0>)\n",
      "epoch: 5134 loss is tensor([-0.2116], grad_fn=<AddBackward0>)\n",
      "epoch: 5135 loss is tensor([-0.1622], grad_fn=<AddBackward0>)\n",
      "epoch: 5136 loss is tensor([-0.2030], grad_fn=<AddBackward0>)\n",
      "epoch: 5137 loss is tensor([-0.1973], grad_fn=<AddBackward0>)\n",
      "epoch: 5138 loss is tensor([-0.1858], grad_fn=<AddBackward0>)\n",
      "epoch: 5139 loss is tensor([-0.2101], grad_fn=<AddBackward0>)\n",
      "epoch: 5140 loss is tensor([-0.2340], grad_fn=<AddBackward0>)\n",
      "epoch: 5141 loss is tensor([-0.1901], grad_fn=<AddBackward0>)\n",
      "epoch: 5142 loss is tensor([-0.1625], grad_fn=<AddBackward0>)\n",
      "epoch: 5143 loss is tensor([-0.2019], grad_fn=<AddBackward0>)\n",
      "epoch: 5144 loss is tensor([-0.1899], grad_fn=<AddBackward0>)\n",
      "epoch: 5145 loss is tensor([-0.2192], grad_fn=<AddBackward0>)\n",
      "epoch: 5146 loss is tensor([-0.2192], grad_fn=<AddBackward0>)\n",
      "epoch: 5147 loss is tensor([-0.1553], grad_fn=<AddBackward0>)\n",
      "epoch: 5148 loss is tensor([-0.2016], grad_fn=<AddBackward0>)\n",
      "epoch: 5149 loss is tensor([-0.2312], grad_fn=<AddBackward0>)\n",
      "epoch: 5150 loss is tensor([-0.1383], grad_fn=<AddBackward0>)\n",
      "epoch: 5151 loss is tensor([-0.1808], grad_fn=<AddBackward0>)\n",
      "epoch: 5152 loss is tensor([-0.2234], grad_fn=<AddBackward0>)\n",
      "epoch: 5153 loss is tensor([-0.2568], grad_fn=<AddBackward0>)\n",
      "epoch: 5154 loss is tensor([-0.1794], grad_fn=<AddBackward0>)\n",
      "epoch: 5155 loss is tensor([-0.1806], grad_fn=<AddBackward0>)\n",
      "epoch: 5156 loss is tensor([-0.1800], grad_fn=<AddBackward0>)\n",
      "epoch: 5157 loss is tensor([-0.1484], grad_fn=<AddBackward0>)\n",
      "epoch: 5158 loss is tensor([-0.1535], grad_fn=<AddBackward0>)\n",
      "epoch: 5159 loss is tensor([-0.1856], grad_fn=<AddBackward0>)\n",
      "epoch: 5160 loss is tensor([-0.2230], grad_fn=<AddBackward0>)\n",
      "epoch: 5161 loss is tensor([-0.1785], grad_fn=<AddBackward0>)\n",
      "epoch: 5162 loss is tensor([-0.1821], grad_fn=<AddBackward0>)\n",
      "epoch: 5163 loss is tensor([-0.2035], grad_fn=<AddBackward0>)\n",
      "epoch: 5164 loss is tensor([-0.1421], grad_fn=<AddBackward0>)\n",
      "epoch: 5165 loss is tensor([-0.1996], grad_fn=<AddBackward0>)\n",
      "epoch: 5166 loss is tensor([-0.1505], grad_fn=<AddBackward0>)\n",
      "epoch: 5167 loss is tensor([-0.2387], grad_fn=<AddBackward0>)\n",
      "epoch: 5168 loss is tensor([-0.2395], grad_fn=<AddBackward0>)\n",
      "epoch: 5169 loss is tensor([-0.2610], grad_fn=<AddBackward0>)\n",
      "epoch: 5170 loss is tensor([-0.1136], grad_fn=<AddBackward0>)\n",
      "epoch: 5171 loss is tensor([-0.2079], grad_fn=<AddBackward0>)\n",
      "epoch: 5172 loss is tensor([-0.1432], grad_fn=<AddBackward0>)\n",
      "epoch: 5173 loss is tensor([-0.0855], grad_fn=<AddBackward0>)\n",
      "epoch: 5174 loss is tensor([-0.1392], grad_fn=<AddBackward0>)\n",
      "epoch: 5175 loss is tensor([-0.2043], grad_fn=<AddBackward0>)\n",
      "epoch: 5176 loss is tensor([-0.1308], grad_fn=<AddBackward0>)\n",
      "epoch: 5177 loss is tensor([-0.1015], grad_fn=<AddBackward0>)\n",
      "epoch: 5178 loss is tensor([-0.1226], grad_fn=<AddBackward0>)\n",
      "epoch: 5179 loss is tensor([-0.1717], grad_fn=<AddBackward0>)\n",
      "epoch: 5180 loss is tensor([-0.1084], grad_fn=<AddBackward0>)\n",
      "epoch: 5181 loss is tensor([-0.0813], grad_fn=<AddBackward0>)\n",
      "epoch: 5182 loss is tensor([-0.1808], grad_fn=<AddBackward0>)\n",
      "epoch: 5183 loss is tensor([-0.1833], grad_fn=<AddBackward0>)\n",
      "epoch: 5184 loss is tensor([-0.0936], grad_fn=<AddBackward0>)\n",
      "epoch: 5185 loss is tensor([-0.1013], grad_fn=<AddBackward0>)\n",
      "epoch: 5186 loss is tensor([-0.1201], grad_fn=<AddBackward0>)\n",
      "epoch: 5187 loss is tensor([-0.1109], grad_fn=<AddBackward0>)\n",
      "epoch: 5188 loss is tensor([-0.1366], grad_fn=<AddBackward0>)\n",
      "epoch: 5189 loss is tensor([-0.1217], grad_fn=<AddBackward0>)\n",
      "epoch: 5190 loss is tensor([-0.1873], grad_fn=<AddBackward0>)\n",
      "epoch: 5191 loss is tensor([-0.1840], grad_fn=<AddBackward0>)\n",
      "epoch: 5192 loss is tensor([-0.1674], grad_fn=<AddBackward0>)\n",
      "epoch: 5193 loss is tensor([-0.1414], grad_fn=<AddBackward0>)\n",
      "epoch: 5194 loss is tensor([-0.2065], grad_fn=<AddBackward0>)\n",
      "epoch: 5195 loss is tensor([-0.1632], grad_fn=<AddBackward0>)\n",
      "epoch: 5196 loss is tensor([-0.1527], grad_fn=<AddBackward0>)\n",
      "epoch: 5197 loss is tensor([-0.1973], grad_fn=<AddBackward0>)\n",
      "epoch: 5198 loss is tensor([-0.1237], grad_fn=<AddBackward0>)\n",
      "epoch: 5199 loss is tensor([-0.1306], grad_fn=<AddBackward0>)\n",
      "epoch: 5200 loss is tensor([-0.1235], grad_fn=<AddBackward0>)\n",
      "50\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD4CAYAAADxeG0DAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAbzElEQVR4nO3de3Bc53nf8e+DveC6JAACBGjeqYqUZN0FS3FdJ5XNuHbtseJkkspuXLv9g22mzqXNjMeOZpJmOp5JHafuH+04ZW1l0qlq1bWtKOO4vmji2Kk7lk3KEknLpCxRF5IRsCAJYBcAsdenf+xiAVK8gbtnD86e32cGs9hd4JxnpdFPL57zvuc1d0dERKKrK+wCRESkOQpyEZGIU5CLiEScglxEJOIU5CIiEZcM46QjIyO+a9euME4tIhJZhw8fPuvuo5e+HkqQ79q1i0OHDoVxahGRyDKzVy/3ulorIiIRpyAXEYk4BbmISMQpyEVEIq4lQW5m7zazE2b2opl9ohXHFBGR69N0kJtZAvgvwHuA24APmtltzR5XRESuTytG5PcDL7r7SXcvAo8DD7XguCIich1aEeRbgVOrnp+uv3YRMztgZofM7ND09HQLTnt1S6UK/+Ebx3nu1Gzg5xIRCVPbLna6+0F3n3D3idHRNyxMarlsrsDn/uYlXpjKB34uEZEwtSLIzwDbVz3fVn8tVNn8EgCbN/SEXImISLBaEeQ/Am42s91mlgYeBv6yBcdtylSuAMDmTHfIlYiIBKvpe624e9nMPgZ8E0gAj7r7T5qurEmNEbmCXEQ6XEtumuXuXwe+3opjtUo2XyCVMIb60mGXIiISqI5d2ZnNFRgd6Kary8IuRUQkUJ0b5PklRnWhU0RioHODPFdQf1xEYqFzgzy/xNgGBbmIdL6ODPJCucLMYonNGbVWRKTzdWSQT+c1h1xE4qMjgzy7HORqrYhIDHRmkDdWdaq1IiKdryODfLpxnxWNyEWk83VkkE/lCnQZbOpXkItI5+vIIM/mlxgZ6CahVZ0iEgMdGuQFxrSqU0RiojODXKs6RSRGOjPI80u60CkisdFxQV6uVDm3UGRUUw9FJCY6LsjPzhdxR/dZEZHY6LggX9kZSCNyEYmHjgty7dUpInHTVJCb2a+a2U/MrGpmE60qqhlZreoUkZhpdkR+DPhl4HstqKUlsrkCZjAyoCAXkXhoavNld/8pgNn6WUGZzRfY1J8mlei4rpGIyGV1XNplc0uaeigisXLNEbmZPQWMX+atR9z9yes9kZkdAA4A7Nix47oLXKtsXqs6RSRerhnk7r6/FSdy94PAQYCJiQlvxTEvJ5tf4tYtmaAOLyKy7nRUa6VSdc7OFzWHXERipdnphx8ws9PAW4G/MrNvtqasG3NuoUCl6pp6KCKx0uyslSeAJ1pUS9OyWgwkIjHUUa2V6camy2qtiEh8dFSQr9xnRSNyEYmPjgry5fusjCrIRSRGOirIs/klBvtSdCcTYZciItI2nRXkuQJjmnooIjHTWUGeL2jqoYjETmcFeW5J/XERiZ2OCXJ3Z3q+oFWdIhI7HRPkM4slShXXXp0iEjsdE+Taq1NE4qpjgryxV6dG5CISMx0T5NmcVnWKSDx1TpAv32dFrRURiZmOCfLpfIFMT5LetFZ1iki8dEyQT+WW1FYRkVjqmCDP5guM6fa1IhJDHRTkGpGLSDx1RJC7O9lcQRtKiEgsdUSQ5y6UKZSrGpGLSCw1u/nyH5vZcTM7YmZPmNlgi+pak8aqTo3IRSSGmh2Rfxu43d3vBF4APtl8SWu3ModcI3IRiZ+mgtzdv+Xu5frTHwDbmi9p7bRXp4jEWSt75P8C+D9XetPMDpjZITM7ND093cLTrr7PilorIhI/yWv9gJk9BYxf5q1H3P3J+s88ApSBx650HHc/CBwEmJiY8Buq9gqyuQL96QQD3df8OCIiHeeayefu+6/2vpl9FHgf8E53b2lAX69sfkmjcRGJraaGsGb2buDjwC+4+2JrSlq7bL6gLd5EJLaa7ZH/ZyADfNvMnjWzP21BTWuW1X1WRCTGmhqRu/vfa1UhzdB9VkQkziK/snO+UGaxWNGIXERiK/JB3tgZSFu8iUhMRT7IG3PItTOQiMRU5IN8eVXnmEbkIhJTkQ/y6fp9VkY1IheRmIp8kGfzBbqTXWzo0apOEYmnyAf5VG6JzRu6MbOwSxERCUXkgzybKzCmtoqIxFj0gzy/pKmHIhJrHRDkBU09FJFYi3SQXyhWyC+VdcMsEYm1SAf5yhxyjchFJL4iHuTaq1NEJNpB3tjiTUEuIvEV6SCfWr5hli52ikiMRTrIs/kCqYQx1JcKuxQRkdBEPMiX2Jzp0apOEYm1aAd5Tnt1iog0FeRm9u/N7Eh9v85vmdmbWlXY9aiNyBXkIhJvzY7I/9jd73T3u4GvAb/ffEnXT3t1iog0GeTunlv1tB/w5sq5foVyhdnFkkbkIhJ7Td/E28w+BfwzYA548Co/dwA4ALBjx45mT6s55CIiddcckZvZU2Z27DJfDwG4+yPuvh14DPjYlY7j7gfdfcLdJ0ZHR5sufGVVp1orIhJv1xyRu/v+6zzWY8DXgT9oqqLrNF2/z4pG5CISd83OWrl51dOHgOPNlXP9NCIXEalptkf+R2a2D6gCrwL/qvmSrs9UbolElzHcn27XKUVE1qWmgtzdf6VVhaxVNldgZCBNokurOkUk3iK7snN6XjsDiYhAhIM8mytoDrmICFEO8nxBM1ZERIhokJcrVc4tFBgdUJCLiEQyyM8tFHGHUd1nRUQkmkHeWJ6vHrmISESDfHlVp4JcRCSaQT69vKpTrRURkWgG+fLyfF3sFBGJbJAvMdSXIp2MZPkiIi0VySSsLQZSW0VEBKIa5HltuiwisiySQT6d1/J8EZFlkQtyd2c6X2BUy/NFRIAIBvnsYolipaoeuYhIXeSCfHpeqzpFRFaLXJBreb6IyMWa3eqt7RrL87WqU0RWcXdKFadQrlAoVymWq6seKxd9v/xeoVSl4k5vKkFPKkFvOkFvqv6Vvvh5d7KLrnW6I1lLgtzMfhf4DDDq7mdbccwraazq1IhcZN0oV6oUK7VgXH5cDs/Lh+nFgdp4v3TJcS55rVCpUiitCuJyleKq8wStd1XY96S6VoI+naQ31dV4vyeVYGLnMO+9c0vgNUELgtzMtgPvAl5rvpxry+YK9KUTDHRH7o8JkZarVr0eeG8caRaXQ68RrFWKlcobwnZ1KF4xYK8awFUqVW/6syS6jO5kF93JLtLJLrqTifrjymuD6RTdme43vp/qojvRRXcqQTpRf77qOFc7ZqLLWCpVuVCscKFUWXksVViqf79Yf1xa/f6qx9yFElNzK7+3UCjzZ99/hdHMW7l/93AL/k1fXSvS8LPAx4EnW3Csa8rml9Qfl0grlCvMLJSYWSwys1Dk/PJj/bXzC0XyS6WLA/jSoK6/Vqy0ZhS6EmyJi8M0laA70cVAd5Lu/lVhuMawXP1aT6qLdCJx0e+nE10kE5G7ZHdFi8Uy7/rs9/jkV4/wV7/1dnpSiUDP11SQm9lDwBl3f87s6r0jMzsAHADYsWPHDZ8zm9fyfFk/SpVqPZBLnF8oNoJ4ZqHIzOJKMK9+faFYueLxMj1JhvvTZHqSjXDM9CQvE5AXj0bTiZXQvTRgL/s7q95PJYxr/fcra9OXTvKpD9zBRx79IX/63Zf4nf17Az3fNYPczJ4Cxi/z1iPA71Frq1yTux8EDgJMTEzc8N9h0/kCt71pw43+usgVlStVZi+U6qPjWvjOLJYaAdwYOS+W6kFdJL9UvuLxBrqTDPWnGO5LM9SX5qbRAYb60gz3pxjqT9de708z3J9msC/FUF+aVAeNSuPuF/aO8tY9m/j281PhB7m777/c62Z2B7AbWB6NbwOeMbP73X2ypVWuouX5cj0qVWfuwqr2RWNUXLpk1LwS1nMXSlc8Xl86wVBfmqH+WuDu2tRXD+X0Sij3pS4K5u5ksH9Oy/p389gATzxzBncP9K+eG26tuPtRYPPyczN7BZgIctbKYrHMfKGs1krMVKtOfqnM+VUBvDw6nqmPjs9f0m+evVDCr/B3XzrZxab+dCOItw71MdyXYvDSYO5P1Z73pQPvcUpn2jHcR75QZnaxxFB/OrDzRGrqhxYDRZ+7ky+UG6Pk2cVLesuNsC5xfrHIbH3EfKVZEelEV2OUPNyf5tbxDSvtjProePXIeagvRW8qoZ6wtMWO4T4AXju/GI0gd/ddrTrWlWgO+fri7iwUK40WxUXti/roePaSYJ5ZKFK+Qignu6wxGh7sS3Hz5oFLesm1wF4dzP1phbKsXzs39QPw6vlF7to+GNh5ojUib6zqVJAH4UKxsmoq3OpwvqR9sdxbXihdcfpbl1HvKdeCeNdIH/f2DzZCeLBvJZiXQznTnVQoS0fZPtwLwGvnFgI9T7SCvNFaUY98rZZv/3tiKs+JydrX63NL9fZGLaSXSpcPZTMY7F2ZabF9uI+7tg0yuLqFsaqVMdxXmz63Xpczi7RLXzrJjuE+jp3JBXqeaAV5vkAqYQz1pcIuZV3LL5V4YSrPicl5TkzmOD6Z54WpPDOLK7MyRga62T7cy5aNPdz2pg2resmpS0bNaTb2pkgolEVuyL07Bvn+S+cCnbkSqSBPdhlVh0K5qlkE1FYInpxe4MRkvhHWJybznJm90PiZ/nSCveMZ3n37OHvHMuwbz7BvLMOmAbWnRNrhvp1D/MWzf8fpmQtsr1/8bLVIBfntWzdSqTo/fT3HPTuGwi6nbapV59TMYi2sJ/Mcn6o9vnx2oXHhMJUwbhod4L6dQ3zogR3sq4f21sFetThEQrScVc+8NqMgB7hz20YAjp6Z68ggd3fOzhfrI+xcY4T9wtQ8F0ory7q3D/eyb2wD73rzGPvGN7BvLMPukX7SSa0KFFlvbhnP0JdO8OPXZnno7q2BnCNSQb5lYw8jA90cOT0XdilNmy+UG0Hd+JrKc36h2PiZkYE0e8cyPHz/9sYIe+9Yhn7d+VEkMpKJLu7aNsjhV2eCO0dgRw6AmXHnto0cOT3b3IEWzkK1ApmxltR1NcVylZNn598Q2KdnVvrYfekEe8cy/OKtY+wbz3DLeIa94xlG1McW6Qj37hzkv373JBeKFXrTrb++F6kgB7hj60b+5kSWhUL5xkemj/9TSKTgo19rWV3VqnNm9gLHJ/OcmMxxYqo2Y+Tk9EofO9ll7Bnt554dQzz8lu3sG9/ALepji3S8+3YOUa46R07P8sCeTS0/fuSC/M5tG6k6PP96jrfsusEbtqd6oLh4wzWcnS/ULjquGmH/bCp/0e1Jtw31sm8sw/76KHvfeIY9IwPqY4vE0D3ba9f0Dr82oyCH2ogc4MjpuRsP8mQvLJ675o8trO5jNy485jk7v9LHHu5Ps28sw69ObG8E9s2bB8j0aK67iNQM9afZM9rPM6/OBnL8yAX55g09jG/o4WgzffJUD5SWGk9LlSovn11YaYtMznNiKsep8yt97N5Ugr1jA7zjls2NmSL7xjOMDKS1rFxEruneHUP89fFsIAuDIhfkAHds28j/e+kcXzl8mvt3D1/33Ex35/TMBRILkJnP88gXf8wLU3lemp6nVKn1sRNdxp6Rfu7cNsiv3bcyyt4+1Kc+tojcsPt2DvHlw6d59dwiu0b6W3rsSAb5L929lR++fJ7f/d/PAbB1sJcHdg/zwJ5hHti9iZ2b+phZLNXmYtfbIscn8/xsap75QplPJXP8o8Qih1+dYd94hgdv2dwYYe8Z7deGACLSctuHagPO1+eWFOQA771zC++5fZwTU3mePnmOp18+z3dfmOarPz4D1KbzLa668DjYl2LfWIZfuXcr+8Y38M5Xd7LpxR/w/U+8I6yPICIxs3z31rEA7t4aySAH6Ooybt2ygVu3bOCjb9uNu/Nidp6nXz7PC1N5dgz3Ne4rMprpvrgnlR+8qEcuIhK0yVwtc8Y3tv7urZEN8kuZGTePZbh5LHPtH071glegUoZEx/wjEJF1LJsrkOlJ0pdufebEc1Jzsv5/xPKFq/+ciEiLTM4tMb4hmL0UmgpyM/t3ZnbGzJ6tf/3jVhUWqFRt1w61V0SkXSZzS4wFFOStGON/1t0/04LjtI9G5CLSZlO5JW66aSSQY8e7taIRuYi0QaXqZPMFxjcGcyO8VgT5x8zsiJk9ambRuEl4SiNyEWmfc/MFKlUPr0duZk+Z2bHLfD0EfA64CbgbeB34k6sc54CZHTKzQ9PT062q/8Yk1SMXkfZZnnoYWo/c3fdfz4HM7L8BV7wvrLsfBA4CTExM+PUWGIjGiFxBLiLBm5wLbg45ND9rZcuqpx8AjjVXTpssj8gV5CLSBlP5AkBgrZVmZ6182szuBhx4BfiXzRbUFssj8pJ65CISvKm5JRJdxqaAdv1qKsjd/cOtKqStkmqtiEj7TOaW2JzpJhHQHVRjPv1QI3IRCd5UbonNAbVVIK5BnlKPXETap7Y8P7jN1OMZ5GqtiEgbTeaCu88KxD3INY9cRAK2WCyTXyozFtDUQ4hrkHd1QaJbKztFJHCNOeQakQcg2aMRuYgEbioX7BxyiHOQp3o0IheRwE0tL89XayUAGpGLSBs0tnjTiDwAqV6NyEUkcJNzS2S6k/R3B7etZHyDPNkD5ULYVYhIh5u7UGKwPxXoOeIb5Im05pGLSOCKlSrpRLBRG88gd4fzJ2FwR9iViEiHK5WrpBTkAchPwuJZGL8z7EpEpMMVK1W6kwry1puq3zZ9/I5w6xCRjleqaEQejMkjtcexN4dbh4h0vFLZFeSBmDwKgzuhZ2PYlYhIhytUqqTUWgnA5FG1VUSkLUplzVppveICnHtJFzpFpC2KlSrpZDA7Ay2LX5BPPQ+4RuQi0haRuNhpZr9pZsfN7Cdm9ulWFBWo5Qud47eHW4eIxEI7WitNLf43sweBh4C73L1gZptbU1aAJo/WLnJu3B52JSISA8UIXOz8DeCP3L0A4O7Z5ksK2OTRWn/cgu1ZiYgAFCNwsXMv8HYze9rMvmtmb7nSD5rZATM7ZGaHpqenmzztDapWIPu8+uMi0ja1i50ht1bM7Clg/DJvPVL//WHg54C3AF8ysz3u7pf+sLsfBA4CTExMvOH9tjh/EkqLMKb+uIi0R6nipBLBdgCuGeTuvv9K75nZbwBfrQf3D82sCowAIQ25r6FxoVMjchEJXqXqVKrrf2XnXwAPApjZXiANnG3ymMGZPApdKRi9JexKRCQGchdKAPSng9tUApqctQI8CjxqZseAIvCRy7VV1o3Jo7UQT6bDrkREYuDomTkAbnvThkDP01SQu3sR+PUW1RK8yWNw0zvCrkJEYuK5U7MA3LEt2Ps6xWdl53wW5ie1EEhE2ua507PsGe1nQ4+2emuNyaO1R13oFJE2cHeePTXH3dsGAz9X/IJcUw9FpA1en1vi7HyBu7YPBn6u+AT51LHasvy+4bArEZEYWO6PK8hbafKoRuMi0jbPnZ4jlTBu3ZIJ/FzxCPLSBTj7gvrjItI2z52a5dYtG+hOJgI/VzyCPPs8eFVBLiJtUa06R8/McVcbLnRCXIJ88ljtUUEuIm1w8uw884VyW/rjEJsgPwrpTG3DZRGRgD17qrai8+7t7dngPT5BPn47dMXj44pIuP72Z9MMdCfZMzLQlvN1frJVq7Wph2qriEgbPHtqlief/Ts+9MAOurras4FN5wf5zMtQnFeQi0jgKlXn9588xuZMN7/1zpvbdt7OD/Kp+oVOzSEXkYB96dApjpye45H33spAd7C3rl2t84N88ihYAjbfGnYlItLBZheLfPobx7l/9zDvv+tNbT13PIJ8ZC+kesOuREQ62Ge+dYLcUpk/fP+bsTZv7h6PIFd/XEQCdOzMHI89/Rof/rmd3Lol2E0kLqd9TZwwLJ6H3Bndg1xEAnXT6AD/Zv9ePvL3d4Vy/s4Oct2DXETaoDedaOsslUs1FeRm9r+AffWng8Csu9/dZE2t07gHuYJcRDpXs3t2/pPl783sT4C5pitqpcmjkNkCA6NhVyIiEpiWtFasdon214D1tbOxLnSKSAy0atbK24Epd//ZlX7AzA6Y2SEzOzQ9Pd2i015FuQBnT2ghkIh0vGuOyM3sKWD8Mm894u5P1r//IPDFqx3H3Q8CBwEmJiZ8jXWu3fRxqJY1IheRjnfNIHf3/Vd738ySwC8D97WqqJZozFi5M9w6REQC1orWyn7guLufbsGxWmfyKKT6YXh32JWIiASqFUH+MNdoq4Ri8hiM3QZdwe+XJyISpqZnrbj7R1tQR+tVCkB773cgIhKGzr3Xyp4H4cyh2jJ9EZEO1rlBfvO7wKtw8jthVyIiEqjODfKt98Le99Q2XRYR6WCde9OsrgR86PGwqxARCVznjshFRGJCQS4iEnEKchGRiFOQi4hEnIJcRCTiFOQiIhGnIBcRiTgFuYhIxJl78Hs8vOGkZtPAq20/8cVGgLMh19AsfYb1IeqfIer1Q3w+w053f8MmxKEE+XpgZofcfSLsOpqhz7A+RP0zRL1+0GdQa0VEJOIU5CIiERfnID8YdgEtoM+wPkT9M0S9foj5Z4htj1xEpFPEeUQuItIRFOQiIhEXyyA3s3eb2Qkze9HMPhF2PWtlZo+aWdbMjoVdy40ws+1m9h0ze97MfmJmvx12TWtlZj1m9kMze67+Gf4w7JpulJklzOzHZva1sGu5EWb2ipkdNbNnzexQ2PWslZkNmtmXzey4mf3UzN665mPErUduZgngBeAXgdPAj4APuvvzoRa2Bmb288A88N/d/faw61krM9sCbHH3Z8wsAxwGfili/w4M6Hf3eTNLAf8X+G13/0HIpa2Zmf1bYALY4O7vC7uetTKzV4AJd4/kgiAz+3Pgb93982aWBvrcfXYtx4jjiPx+4EV3P+nuReBx4KGQa1oTd/8ecD7sOm6Uu7/u7s/Uv88DPwW2hlvV2njNfP1pqv4VuVGRmW0D3gt8Puxa4sjMNgI/D3wBwN2Law1xiGeQbwVOrXp+moiFSCcxs13APcDTIZeyZvWWxLNAFvi2u0fuMwD/Cfg4UA25jmY48C0zO2xmB8IuZo12A9PAn9XbW583s/61HiSOQS7rhJkNAF8Bfsfdc2HXs1buXnH3u4FtwP1mFqk2l5m9D8i6++Gwa2nSP3D3e4H3AP+63nqMiiRwL/A5d78HWADWfN0ujkF+Bti+6vm2+mvSRvW+8leAx9z9q2HX04z6n8LfAd4dcilr9Tbg/fUe8+PAO8zsf4Rb0tq5+5n6YxZ4glr7NCpOA6dX/TX3ZWrBviZxDPIfATeb2e76hYWHgb8MuaZYqV8o/ALwU3f/j2HXcyPMbNTMBuvf91K7eH481KLWyN0/6e7b3H0Xtf8O/trdfz3kstbEzPrrF8yptyTeBURmNpe7TwKnzGxf/aV3Amu+6J9saVUR4O5lM/sY8E0gATzq7j8Juaw1MbMvAv8QGDGz08AfuPsXwq1qTd4GfBg4Wu8xA/yeu389vJLWbAvw5/VZUF3Al9w9ktP3Im4MeKI2NiAJ/E93/0a4Ja3ZbwKP1QeWJ4F/vtYDxG76oYhIp4lja0VEpKMoyEVEIk5BLiIScQpyEZGIU5CLiEScglxEJOIU5CIiEff/Aa1TfitWYg0rAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 5201 loss is tensor([-0.1820], grad_fn=<AddBackward0>)\n",
      "epoch: 5202 loss is tensor([-0.1465], grad_fn=<AddBackward0>)\n",
      "epoch: 5203 loss is tensor([-0.1085], grad_fn=<AddBackward0>)\n",
      "epoch: 5204 loss is tensor([-0.1531], grad_fn=<AddBackward0>)\n",
      "epoch: 5205 loss is tensor([-0.1751], grad_fn=<AddBackward0>)\n",
      "epoch: 5206 loss is tensor([-0.1447], grad_fn=<AddBackward0>)\n",
      "epoch: 5207 loss is tensor([-0.1825], grad_fn=<AddBackward0>)\n",
      "epoch: 5208 loss is tensor([-0.1404], grad_fn=<AddBackward0>)\n",
      "epoch: 5209 loss is tensor([-0.1230], grad_fn=<AddBackward0>)\n",
      "epoch: 5210 loss is tensor([-0.1234], grad_fn=<AddBackward0>)\n",
      "epoch: 5211 loss is tensor([-0.1909], grad_fn=<AddBackward0>)\n",
      "epoch: 5212 loss is tensor([-0.1799], grad_fn=<AddBackward0>)\n",
      "epoch: 5213 loss is tensor([-0.1895], grad_fn=<AddBackward0>)\n",
      "epoch: 5214 loss is tensor([-0.1286], grad_fn=<AddBackward0>)\n",
      "epoch: 5215 loss is tensor([-0.1521], grad_fn=<AddBackward0>)\n",
      "epoch: 5216 loss is tensor([-0.1860], grad_fn=<AddBackward0>)\n",
      "epoch: 5217 loss is tensor([-0.0976], grad_fn=<AddBackward0>)\n",
      "epoch: 5218 loss is tensor([-0.1616], grad_fn=<AddBackward0>)\n",
      "epoch: 5219 loss is tensor([-0.1828], grad_fn=<AddBackward0>)\n",
      "epoch: 5220 loss is tensor([-0.1012], grad_fn=<AddBackward0>)\n",
      "epoch: 5221 loss is tensor([-0.1312], grad_fn=<AddBackward0>)\n",
      "epoch: 5222 loss is tensor([-0.1486], grad_fn=<AddBackward0>)\n",
      "epoch: 5223 loss is tensor([-0.1584], grad_fn=<AddBackward0>)\n",
      "epoch: 5224 loss is tensor([-0.1687], grad_fn=<AddBackward0>)\n",
      "epoch: 5225 loss is tensor([-0.1777], grad_fn=<AddBackward0>)\n",
      "epoch: 5226 loss is tensor([-0.1892], grad_fn=<AddBackward0>)\n",
      "epoch: 5227 loss is tensor([-0.1352], grad_fn=<AddBackward0>)\n",
      "epoch: 5228 loss is tensor([-0.2231], grad_fn=<AddBackward0>)\n",
      "epoch: 5229 loss is tensor([-0.1774], grad_fn=<AddBackward0>)\n",
      "epoch: 5230 loss is tensor([-0.1544], grad_fn=<AddBackward0>)\n",
      "epoch: 5231 loss is tensor([-0.2256], grad_fn=<AddBackward0>)\n",
      "epoch: 5232 loss is tensor([-0.1751], grad_fn=<AddBackward0>)\n",
      "epoch: 5233 loss is tensor([-0.1902], grad_fn=<AddBackward0>)\n",
      "epoch: 5234 loss is tensor([-0.1923], grad_fn=<AddBackward0>)\n",
      "epoch: 5235 loss is tensor([-0.1645], grad_fn=<AddBackward0>)\n",
      "epoch: 5236 loss is tensor([-0.2038], grad_fn=<AddBackward0>)\n",
      "epoch: 5237 loss is tensor([-0.1272], grad_fn=<AddBackward0>)\n",
      "epoch: 5238 loss is tensor([-0.1786], grad_fn=<AddBackward0>)\n",
      "epoch: 5239 loss is tensor([-0.0863], grad_fn=<AddBackward0>)\n",
      "epoch: 5240 loss is tensor([-0.0643], grad_fn=<AddBackward0>)\n",
      "epoch: 5241 loss is tensor([-0.1218], grad_fn=<AddBackward0>)\n",
      "epoch: 5242 loss is tensor([-0.1496], grad_fn=<AddBackward0>)\n",
      "epoch: 5243 loss is tensor([-0.0832], grad_fn=<AddBackward0>)\n",
      "epoch: 5244 loss is tensor([-0.1415], grad_fn=<AddBackward0>)\n",
      "epoch: 5245 loss is tensor([-0.1430], grad_fn=<AddBackward0>)\n",
      "epoch: 5246 loss is tensor([-0.1439], grad_fn=<AddBackward0>)\n",
      "epoch: 5247 loss is tensor([-0.1228], grad_fn=<AddBackward0>)\n",
      "epoch: 5248 loss is tensor([-0.1856], grad_fn=<AddBackward0>)\n",
      "epoch: 5249 loss is tensor([-0.1500], grad_fn=<AddBackward0>)\n",
      "epoch: 5250 loss is tensor([-0.1621], grad_fn=<AddBackward0>)\n",
      "epoch: 5251 loss is tensor([-0.2154], grad_fn=<AddBackward0>)\n",
      "epoch: 5252 loss is tensor([-0.1955], grad_fn=<AddBackward0>)\n",
      "epoch: 5253 loss is tensor([-0.1699], grad_fn=<AddBackward0>)\n",
      "epoch: 5254 loss is tensor([-0.1728], grad_fn=<AddBackward0>)\n",
      "epoch: 5255 loss is tensor([-0.1183], grad_fn=<AddBackward0>)\n",
      "epoch: 5256 loss is tensor([-0.1633], grad_fn=<AddBackward0>)\n",
      "epoch: 5257 loss is tensor([-0.1771], grad_fn=<AddBackward0>)\n",
      "epoch: 5258 loss is tensor([-0.1361], grad_fn=<AddBackward0>)\n",
      "epoch: 5259 loss is tensor([-0.1628], grad_fn=<AddBackward0>)\n",
      "epoch: 5260 loss is tensor([-0.2211], grad_fn=<AddBackward0>)\n",
      "epoch: 5261 loss is tensor([-0.1747], grad_fn=<AddBackward0>)\n",
      "epoch: 5262 loss is tensor([-0.1317], grad_fn=<AddBackward0>)\n",
      "epoch: 5263 loss is tensor([-0.1642], grad_fn=<AddBackward0>)\n",
      "epoch: 5264 loss is tensor([-0.1614], grad_fn=<AddBackward0>)\n",
      "epoch: 5265 loss is tensor([-0.1864], grad_fn=<AddBackward0>)\n",
      "epoch: 5266 loss is tensor([-0.1581], grad_fn=<AddBackward0>)\n",
      "epoch: 5267 loss is tensor([-0.1269], grad_fn=<AddBackward0>)\n",
      "epoch: 5268 loss is tensor([-0.1559], grad_fn=<AddBackward0>)\n",
      "epoch: 5269 loss is tensor([-0.0868], grad_fn=<AddBackward0>)\n",
      "epoch: 5270 loss is tensor([-0.1395], grad_fn=<AddBackward0>)\n",
      "epoch: 5271 loss is tensor([-0.1702], grad_fn=<AddBackward0>)\n",
      "epoch: 5272 loss is tensor([-0.1838], grad_fn=<AddBackward0>)\n",
      "epoch: 5273 loss is tensor([-0.2167], grad_fn=<AddBackward0>)\n",
      "epoch: 5274 loss is tensor([-0.1993], grad_fn=<AddBackward0>)\n",
      "epoch: 5275 loss is tensor([-0.1933], grad_fn=<AddBackward0>)\n",
      "epoch: 5276 loss is tensor([-0.1955], grad_fn=<AddBackward0>)\n",
      "epoch: 5277 loss is tensor([-0.1807], grad_fn=<AddBackward0>)\n",
      "epoch: 5278 loss is tensor([-0.1843], grad_fn=<AddBackward0>)\n",
      "epoch: 5279 loss is tensor([-0.1834], grad_fn=<AddBackward0>)\n",
      "epoch: 5280 loss is tensor([-0.2301], grad_fn=<AddBackward0>)\n",
      "epoch: 5281 loss is tensor([-0.2326], grad_fn=<AddBackward0>)\n",
      "epoch: 5282 loss is tensor([-0.2341], grad_fn=<AddBackward0>)\n",
      "epoch: 5283 loss is tensor([-0.2040], grad_fn=<AddBackward0>)\n",
      "epoch: 5284 loss is tensor([-0.1698], grad_fn=<AddBackward0>)\n",
      "epoch: 5285 loss is tensor([-0.2004], grad_fn=<AddBackward0>)\n",
      "epoch: 5286 loss is tensor([-0.1383], grad_fn=<AddBackward0>)\n",
      "epoch: 5287 loss is tensor([-0.1750], grad_fn=<AddBackward0>)\n",
      "epoch: 5288 loss is tensor([-0.1122], grad_fn=<AddBackward0>)\n",
      "epoch: 5289 loss is tensor([-0.2273], grad_fn=<AddBackward0>)\n",
      "epoch: 5290 loss is tensor([-0.2211], grad_fn=<AddBackward0>)\n",
      "epoch: 5291 loss is tensor([-0.1577], grad_fn=<AddBackward0>)\n",
      "epoch: 5292 loss is tensor([-0.1306], grad_fn=<AddBackward0>)\n",
      "epoch: 5293 loss is tensor([-0.1477], grad_fn=<AddBackward0>)\n",
      "epoch: 5294 loss is tensor([-0.1529], grad_fn=<AddBackward0>)\n",
      "epoch: 5295 loss is tensor([-0.2087], grad_fn=<AddBackward0>)\n",
      "epoch: 5296 loss is tensor([-0.2220], grad_fn=<AddBackward0>)\n",
      "epoch: 5297 loss is tensor([-0.1677], grad_fn=<AddBackward0>)\n",
      "epoch: 5298 loss is tensor([-0.1614], grad_fn=<AddBackward0>)\n",
      "epoch: 5299 loss is tensor([-0.1970], grad_fn=<AddBackward0>)\n",
      "epoch: 5300 loss is tensor([-0.2140], grad_fn=<AddBackward0>)\n",
      "43\n"
=======
      "The number of epochs is: 4401\n",
      "The number of epochs is: 4402\n",
      "The number of epochs is: 4403\n",
      "The number of epochs is: 4404\n",
      "The number of epochs is: 4405\n",
      "The number of epochs is: 4406\n",
      "The number of epochs is: 4407\n",
      "The number of epochs is: 4408\n",
      "The number of epochs is: 4409\n",
      "The number of epochs is: 4410\n",
      "The number of epochs is: 4411\n",
      "The number of epochs is: 4412\n",
      "The number of epochs is: 4413\n",
      "The number of epochs is: 4414\n",
      "The number of epochs is: 4415\n",
      "The number of epochs is: 4416\n",
      "The number of epochs is: 4417\n",
      "The number of epochs is: 4418\n",
      "The number of epochs is: 4419\n",
      "The number of epochs is: 4420\n",
      "The number of epochs is: 4421\n",
      "The number of epochs is: 4422\n",
      "The number of epochs is: 4423\n",
      "The number of epochs is: 4424\n",
      "The number of epochs is: 4425\n",
      "The number of epochs is: 4426\n",
      "The number of epochs is: 4427\n",
      "The number of epochs is: 4428\n",
      "The number of epochs is: 4429\n",
      "The number of epochs is: 4430\n",
      "The number of epochs is: 4431\n",
      "The number of epochs is: 4432\n",
      "The number of epochs is: 4433\n",
      "The number of epochs is: 4434\n",
      "The number of epochs is: 4435\n",
      "The number of epochs is: 4436\n",
      "The number of epochs is: 4437\n",
      "The number of epochs is: 4438\n",
      "The number of epochs is: 4439\n",
      "The number of epochs is: 4440\n",
      "The number of epochs is: 4441\n",
      "The number of epochs is: 4442\n",
      "The number of epochs is: 4443\n",
      "The number of epochs is: 4444\n",
      "The number of epochs is: 4445\n",
      "The number of epochs is: 4446\n",
      "The number of epochs is: 4447\n",
      "The number of epochs is: 4448\n",
      "The number of epochs is: 4449\n",
      "The number of epochs is: 4450\n",
      "The number of epochs is: 4451\n",
      "The number of epochs is: 4452\n",
      "The number of epochs is: 4453\n",
      "The number of epochs is: 4454\n",
      "The number of epochs is: 4455\n",
      "The number of epochs is: 4456\n",
      "The number of epochs is: 4457\n",
      "The number of epochs is: 4458\n",
      "The number of epochs is: 4459\n",
      "The number of epochs is: 4460\n",
      "The number of epochs is: 4461\n",
      "The number of epochs is: 4462\n",
      "The number of epochs is: 4463\n",
      "The number of epochs is: 4464\n",
      "The number of epochs is: 4465\n",
      "The number of epochs is: 4466\n",
      "The number of epochs is: 4467\n",
      "The number of epochs is: 4468\n",
      "The number of epochs is: 4469\n",
      "The number of epochs is: 4470\n",
      "The number of epochs is: 4471\n",
      "The number of epochs is: 4472\n",
      "The number of epochs is: 4473\n",
      "The number of epochs is: 4474\n",
      "The number of epochs is: 4475\n",
      "The number of epochs is: 4476\n",
      "The number of epochs is: 4477\n",
      "The number of epochs is: 4478\n",
      "The number of epochs is: 4479\n",
      "The number of epochs is: 4480\n",
      "The number of epochs is: 4481\n",
      "The number of epochs is: 4482\n",
      "The number of epochs is: 4483\n",
      "The number of epochs is: 4484\n",
      "The number of epochs is: 4485\n",
      "The number of epochs is: 4486\n",
      "The number of epochs is: 4487\n",
      "The number of epochs is: 4488\n",
      "The number of epochs is: 4489\n",
      "The number of epochs is: 4490\n",
      "The number of epochs is: 4491\n",
      "The number of epochs is: 4492\n",
      "The number of epochs is: 4493\n",
      "The number of epochs is: 4494\n",
      "The number of epochs is: 4495\n",
      "The number of epochs is: 4496\n",
      "The number of epochs is: 4497\n",
      "The number of epochs is: 4498\n",
      "The number of epochs is: 4499\n",
      "The number of epochs is: 4500\n",
      "The number of epochs is: 4501\n",
      "The number of epochs is: 4502\n",
      "The number of epochs is: 4503\n",
      "The number of epochs is: 4504\n",
      "The number of epochs is: 4505\n",
      "The number of epochs is: 4506\n",
      "The number of epochs is: 4507\n",
      "The number of epochs is: 4508\n",
      "The number of epochs is: 4509\n",
      "The number of epochs is: 4510\n",
      "The number of epochs is: 4511\n",
      "The number of epochs is: 4512\n",
      "The number of epochs is: 4513\n",
      "The number of epochs is: 4514\n",
      "The number of epochs is: 4515\n",
      "The number of epochs is: 4516\n",
      "The number of epochs is: 4517\n",
      "The number of epochs is: 4518\n",
      "The number of epochs is: 4519\n",
      "The number of epochs is: 4520\n",
      "The number of epochs is: 4521\n",
      "The number of epochs is: 4522\n",
      "The number of epochs is: 4523\n",
      "The number of epochs is: 4524\n",
      "The number of epochs is: 4525\n",
      "The number of epochs is: 4526\n",
      "The number of epochs is: 4527\n",
      "The number of epochs is: 4528\n",
      "The number of epochs is: 4529\n",
      "The number of epochs is: 4530\n",
      "The number of epochs is: 4531\n",
      "The number of epochs is: 4532\n",
      "The number of epochs is: 4533\n",
      "The number of epochs is: 4534\n",
      "The number of epochs is: 4535\n",
      "The number of epochs is: 4536\n",
      "The number of epochs is: 4537\n",
      "The number of epochs is: 4538\n",
      "The number of epochs is: 4539\n",
      "The number of epochs is: 4540\n",
      "The number of epochs is: 4541\n",
      "The number of epochs is: 4542\n",
      "The number of epochs is: 4543\n",
      "The number of epochs is: 4544\n",
      "The number of epochs is: 4545\n",
      "The number of epochs is: 4546\n",
      "The number of epochs is: 4547\n",
      "The number of epochs is: 4548\n",
      "The number of epochs is: 4549\n",
      "The number of epochs is: 4550\n",
      "The number of epochs is: 4551\n",
      "The number of epochs is: 4552\n",
      "The number of epochs is: 4553\n",
      "The number of epochs is: 4554\n",
      "The number of epochs is: 4555\n",
      "The number of epochs is: 4556\n",
      "The number of epochs is: 4557\n",
      "The number of epochs is: 4558\n",
      "The number of epochs is: 4559\n",
      "The number of epochs is: 4560\n",
      "The number of epochs is: 4561\n",
      "The number of epochs is: 4562\n",
      "The number of epochs is: 4563\n",
      "The number of epochs is: 4564\n",
      "The number of epochs is: 4565\n",
      "The number of epochs is: 4566\n",
      "The number of epochs is: 4567\n",
      "The number of epochs is: 4568\n",
      "The number of epochs is: 4569\n",
      "The number of epochs is: 4570\n",
      "The number of epochs is: 4571\n",
      "The number of epochs is: 4572\n",
      "The number of epochs is: 4573\n",
      "The number of epochs is: 4574\n",
      "The number of epochs is: 4575\n",
      "The number of epochs is: 4576\n",
      "The number of epochs is: 4577\n",
      "The number of epochs is: 4578\n",
      "The number of epochs is: 4579\n",
      "The number of epochs is: 4580\n",
      "The number of epochs is: 4581\n",
      "The number of epochs is: 4582\n",
      "The number of epochs is: 4583\n",
      "The number of epochs is: 4584\n",
      "The number of epochs is: 4585\n",
      "The number of epochs is: 4586\n",
      "The number of epochs is: 4587\n",
      "The number of epochs is: 4588\n",
      "The number of epochs is: 4589\n",
      "The number of epochs is: 4590\n",
      "The number of epochs is: 4591\n",
      "The number of epochs is: 4592\n",
      "The number of epochs is: 4593\n",
      "The number of epochs is: 4594\n",
      "The number of epochs is: 4595\n",
      "The number of epochs is: 4596\n",
      "The number of epochs is: 4597\n",
      "The number of epochs is: 4598\n",
      "The number of epochs is: 4599\n",
      "The number of epochs is: 4600\n",
      "29\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5301 loss is tensor([-0.2187], grad_fn=<AddBackward0>)\n",
      "epoch: 5302 loss is tensor([-0.2427], grad_fn=<AddBackward0>)\n",
      "epoch: 5303 loss is tensor([-0.1667], grad_fn=<AddBackward0>)\n",
      "epoch: 5304 loss is tensor([-0.1958], grad_fn=<AddBackward0>)\n",
      "epoch: 5305 loss is tensor([-0.2277], grad_fn=<AddBackward0>)\n",
      "epoch: 5306 loss is tensor([-0.1615], grad_fn=<AddBackward0>)\n",
      "epoch: 5307 loss is tensor([-0.1898], grad_fn=<AddBackward0>)\n",
      "epoch: 5308 loss is tensor([-0.1019], grad_fn=<AddBackward0>)\n",
      "epoch: 5309 loss is tensor([-0.1902], grad_fn=<AddBackward0>)\n",
      "epoch: 5310 loss is tensor([-0.2090], grad_fn=<AddBackward0>)\n",
      "epoch: 5311 loss is tensor([-0.1303], grad_fn=<AddBackward0>)\n",
      "epoch: 5312 loss is tensor([-0.2301], grad_fn=<AddBackward0>)\n",
      "epoch: 5313 loss is tensor([-0.2228], grad_fn=<AddBackward0>)\n",
      "epoch: 5314 loss is tensor([-0.1892], grad_fn=<AddBackward0>)\n",
      "epoch: 5315 loss is tensor([-0.1706], grad_fn=<AddBackward0>)\n",
      "epoch: 5316 loss is tensor([-0.1551], grad_fn=<AddBackward0>)\n",
      "epoch: 5317 loss is tensor([-0.2061], grad_fn=<AddBackward0>)\n",
      "epoch: 5318 loss is tensor([-0.2049], grad_fn=<AddBackward0>)\n",
      "epoch: 5319 loss is tensor([-0.1646], grad_fn=<AddBackward0>)\n",
      "epoch: 5320 loss is tensor([-0.1654], grad_fn=<AddBackward0>)\n",
      "epoch: 5321 loss is tensor([-0.1981], grad_fn=<AddBackward0>)\n",
      "epoch: 5322 loss is tensor([-0.1994], grad_fn=<AddBackward0>)\n",
      "epoch: 5323 loss is tensor([-0.2236], grad_fn=<AddBackward0>)\n",
      "epoch: 5324 loss is tensor([-0.1664], grad_fn=<AddBackward0>)\n",
      "epoch: 5325 loss is tensor([-0.2090], grad_fn=<AddBackward0>)\n",
      "epoch: 5326 loss is tensor([-0.1937], grad_fn=<AddBackward0>)\n",
      "epoch: 5327 loss is tensor([-0.1850], grad_fn=<AddBackward0>)\n",
      "epoch: 5328 loss is tensor([-0.1252], grad_fn=<AddBackward0>)\n",
      "epoch: 5329 loss is tensor([-0.2392], grad_fn=<AddBackward0>)\n",
      "epoch: 5330 loss is tensor([-0.1991], grad_fn=<AddBackward0>)\n",
      "epoch: 5331 loss is tensor([-0.2301], grad_fn=<AddBackward0>)\n",
      "epoch: 5332 loss is tensor([-0.1678], grad_fn=<AddBackward0>)\n",
      "epoch: 5333 loss is tensor([-0.1993], grad_fn=<AddBackward0>)\n",
      "epoch: 5334 loss is tensor([-0.2358], grad_fn=<AddBackward0>)\n",
      "epoch: 5335 loss is tensor([-0.2426], grad_fn=<AddBackward0>)\n",
      "epoch: 5336 loss is tensor([-0.2454], grad_fn=<AddBackward0>)\n",
      "epoch: 5337 loss is tensor([-0.1976], grad_fn=<AddBackward0>)\n",
      "epoch: 5338 loss is tensor([-0.2322], grad_fn=<AddBackward0>)\n",
      "epoch: 5339 loss is tensor([-0.1650], grad_fn=<AddBackward0>)\n",
      "epoch: 5340 loss is tensor([-0.2056], grad_fn=<AddBackward0>)\n",
      "epoch: 5341 loss is tensor([-0.2592], grad_fn=<AddBackward0>)\n",
      "epoch: 5342 loss is tensor([-0.2066], grad_fn=<AddBackward0>)\n",
      "epoch: 5343 loss is tensor([-0.1879], grad_fn=<AddBackward0>)\n",
      "epoch: 5344 loss is tensor([-0.2394], grad_fn=<AddBackward0>)\n",
      "epoch: 5345 loss is tensor([-0.1756], grad_fn=<AddBackward0>)\n",
      "epoch: 5346 loss is tensor([-0.1569], grad_fn=<AddBackward0>)\n",
      "epoch: 5347 loss is tensor([-0.1617], grad_fn=<AddBackward0>)\n",
      "epoch: 5348 loss is tensor([-0.1687], grad_fn=<AddBackward0>)\n",
      "epoch: 5349 loss is tensor([-0.2204], grad_fn=<AddBackward0>)\n",
      "epoch: 5350 loss is tensor([-0.1365], grad_fn=<AddBackward0>)\n",
      "epoch: 5351 loss is tensor([-0.1833], grad_fn=<AddBackward0>)\n",
      "epoch: 5352 loss is tensor([-0.1545], grad_fn=<AddBackward0>)\n",
      "epoch: 5353 loss is tensor([-0.2364], grad_fn=<AddBackward0>)\n",
      "epoch: 5354 loss is tensor([-0.1696], grad_fn=<AddBackward0>)\n",
      "epoch: 5355 loss is tensor([-0.1553], grad_fn=<AddBackward0>)\n",
      "epoch: 5356 loss is tensor([-0.1549], grad_fn=<AddBackward0>)\n",
      "epoch: 5357 loss is tensor([-0.1249], grad_fn=<AddBackward0>)\n",
      "epoch: 5358 loss is tensor([-0.2152], grad_fn=<AddBackward0>)\n",
      "epoch: 5359 loss is tensor([-0.1490], grad_fn=<AddBackward0>)\n",
      "epoch: 5360 loss is tensor([-0.1842], grad_fn=<AddBackward0>)\n",
      "epoch: 5361 loss is tensor([-0.1642], grad_fn=<AddBackward0>)\n",
      "epoch: 5362 loss is tensor([-0.1368], grad_fn=<AddBackward0>)\n",
      "epoch: 5363 loss is tensor([-0.2363], grad_fn=<AddBackward0>)\n",
      "epoch: 5364 loss is tensor([-0.1765], grad_fn=<AddBackward0>)\n",
      "epoch: 5365 loss is tensor([-0.2008], grad_fn=<AddBackward0>)\n",
      "epoch: 5366 loss is tensor([-0.1762], grad_fn=<AddBackward0>)\n",
      "epoch: 5367 loss is tensor([-0.1542], grad_fn=<AddBackward0>)\n",
      "epoch: 5368 loss is tensor([-0.1685], grad_fn=<AddBackward0>)\n",
      "epoch: 5369 loss is tensor([-0.1796], grad_fn=<AddBackward0>)\n",
      "epoch: 5370 loss is tensor([-0.1905], grad_fn=<AddBackward0>)\n",
      "epoch: 5371 loss is tensor([-0.2085], grad_fn=<AddBackward0>)\n",
      "epoch: 5372 loss is tensor([-0.2521], grad_fn=<AddBackward0>)\n",
      "epoch: 5373 loss is tensor([-0.2139], grad_fn=<AddBackward0>)\n",
      "epoch: 5374 loss is tensor([-0.2313], grad_fn=<AddBackward0>)\n",
      "epoch: 5375 loss is tensor([-0.2103], grad_fn=<AddBackward0>)\n",
      "epoch: 5376 loss is tensor([-0.1859], grad_fn=<AddBackward0>)\n",
      "epoch: 5377 loss is tensor([-0.2061], grad_fn=<AddBackward0>)\n",
      "epoch: 5378 loss is tensor([-0.2094], grad_fn=<AddBackward0>)\n",
      "epoch: 5379 loss is tensor([-0.1818], grad_fn=<AddBackward0>)\n",
      "epoch: 5380 loss is tensor([-0.1997], grad_fn=<AddBackward0>)\n",
      "epoch: 5381 loss is tensor([-0.1013], grad_fn=<AddBackward0>)\n",
      "epoch: 5382 loss is tensor([-0.2385], grad_fn=<AddBackward0>)\n",
      "epoch: 5383 loss is tensor([-0.2691], grad_fn=<AddBackward0>)\n",
      "epoch: 5384 loss is tensor([-0.2612], grad_fn=<AddBackward0>)\n",
      "epoch: 5385 loss is tensor([-0.1916], grad_fn=<AddBackward0>)\n",
      "epoch: 5386 loss is tensor([-0.1863], grad_fn=<AddBackward0>)\n",
      "epoch: 5387 loss is tensor([-0.2911], grad_fn=<AddBackward0>)\n",
      "epoch: 5388 loss is tensor([-0.1920], grad_fn=<AddBackward0>)\n",
      "epoch: 5389 loss is tensor([-0.2226], grad_fn=<AddBackward0>)\n",
      "epoch: 5390 loss is tensor([-0.1874], grad_fn=<AddBackward0>)\n",
      "epoch: 5391 loss is tensor([-0.2263], grad_fn=<AddBackward0>)\n",
      "epoch: 5392 loss is tensor([-0.2085], grad_fn=<AddBackward0>)\n",
      "epoch: 5393 loss is tensor([-0.2372], grad_fn=<AddBackward0>)\n",
      "epoch: 5394 loss is tensor([-0.1923], grad_fn=<AddBackward0>)\n",
      "epoch: 5395 loss is tensor([-0.1547], grad_fn=<AddBackward0>)\n",
      "epoch: 5396 loss is tensor([-0.2045], grad_fn=<AddBackward0>)\n",
      "epoch: 5397 loss is tensor([-0.2489], grad_fn=<AddBackward0>)\n",
      "epoch: 5398 loss is tensor([-0.1970], grad_fn=<AddBackward0>)\n",
      "epoch: 5399 loss is tensor([-0.1743], grad_fn=<AddBackward0>)\n",
      "epoch: 5400 loss is tensor([-0.1780], grad_fn=<AddBackward0>)\n",
      "10\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 5401 loss is tensor([-0.1411], grad_fn=<AddBackward0>)\n",
      "epoch: 5402 loss is tensor([-0.2144], grad_fn=<AddBackward0>)\n",
      "epoch: 5403 loss is tensor([-0.1684], grad_fn=<AddBackward0>)\n",
      "epoch: 5404 loss is tensor([-0.1891], grad_fn=<AddBackward0>)\n",
      "epoch: 5405 loss is tensor([-0.1485], grad_fn=<AddBackward0>)\n",
      "epoch: 5406 loss is tensor([-0.1509], grad_fn=<AddBackward0>)\n",
      "epoch: 5407 loss is tensor([-0.1526], grad_fn=<AddBackward0>)\n",
      "epoch: 5408 loss is tensor([-0.1256], grad_fn=<AddBackward0>)\n",
      "epoch: 5409 loss is tensor([-0.1688], grad_fn=<AddBackward0>)\n",
      "epoch: 5410 loss is tensor([-0.1159], grad_fn=<AddBackward0>)\n",
      "epoch: 5411 loss is tensor([-0.1499], grad_fn=<AddBackward0>)\n",
      "epoch: 5412 loss is tensor([-0.1869], grad_fn=<AddBackward0>)\n",
      "epoch: 5413 loss is tensor([-0.1189], grad_fn=<AddBackward0>)\n",
      "epoch: 5414 loss is tensor([-0.1937], grad_fn=<AddBackward0>)\n",
      "epoch: 5415 loss is tensor([-0.1557], grad_fn=<AddBackward0>)\n",
      "epoch: 5416 loss is tensor([-0.1964], grad_fn=<AddBackward0>)\n",
      "epoch: 5417 loss is tensor([-0.2040], grad_fn=<AddBackward0>)\n",
      "epoch: 5418 loss is tensor([-0.1465], grad_fn=<AddBackward0>)\n",
      "epoch: 5419 loss is tensor([-0.2079], grad_fn=<AddBackward0>)\n",
      "epoch: 5420 loss is tensor([-0.1906], grad_fn=<AddBackward0>)\n",
      "epoch: 5421 loss is tensor([-0.2002], grad_fn=<AddBackward0>)\n",
      "epoch: 5422 loss is tensor([-0.2134], grad_fn=<AddBackward0>)\n",
      "epoch: 5423 loss is tensor([-0.2580], grad_fn=<AddBackward0>)\n",
      "epoch: 5424 loss is tensor([-0.2202], grad_fn=<AddBackward0>)\n",
      "epoch: 5425 loss is tensor([-0.2429], grad_fn=<AddBackward0>)\n",
      "epoch: 5426 loss is tensor([-0.1459], grad_fn=<AddBackward0>)\n",
      "epoch: 5427 loss is tensor([-0.2261], grad_fn=<AddBackward0>)\n",
      "epoch: 5428 loss is tensor([-0.1979], grad_fn=<AddBackward0>)\n",
      "epoch: 5429 loss is tensor([-0.2420], grad_fn=<AddBackward0>)\n",
      "epoch: 5430 loss is tensor([-0.2111], grad_fn=<AddBackward0>)\n",
      "epoch: 5431 loss is tensor([-0.1726], grad_fn=<AddBackward0>)\n",
      "epoch: 5432 loss is tensor([-0.1235], grad_fn=<AddBackward0>)\n",
      "epoch: 5433 loss is tensor([-0.2233], grad_fn=<AddBackward0>)\n",
      "epoch: 5434 loss is tensor([-0.1273], grad_fn=<AddBackward0>)\n",
      "epoch: 5435 loss is tensor([-0.1004], grad_fn=<AddBackward0>)\n",
      "epoch: 5436 loss is tensor([-0.1537], grad_fn=<AddBackward0>)\n",
      "epoch: 5437 loss is tensor([-0.1546], grad_fn=<AddBackward0>)\n",
      "epoch: 5438 loss is tensor([-0.1754], grad_fn=<AddBackward0>)\n",
      "epoch: 5439 loss is tensor([-0.1466], grad_fn=<AddBackward0>)\n",
      "epoch: 5440 loss is tensor([-0.1570], grad_fn=<AddBackward0>)\n",
      "epoch: 5441 loss is tensor([-0.1468], grad_fn=<AddBackward0>)\n",
      "epoch: 5442 loss is tensor([-0.1620], grad_fn=<AddBackward0>)\n",
      "epoch: 5443 loss is tensor([-0.2096], grad_fn=<AddBackward0>)\n",
      "epoch: 5444 loss is tensor([-0.1691], grad_fn=<AddBackward0>)\n",
      "epoch: 5445 loss is tensor([-0.2032], grad_fn=<AddBackward0>)\n",
      "epoch: 5446 loss is tensor([-0.1940], grad_fn=<AddBackward0>)\n",
      "epoch: 5447 loss is tensor([-0.1980], grad_fn=<AddBackward0>)\n",
      "epoch: 5448 loss is tensor([-0.1740], grad_fn=<AddBackward0>)\n",
      "epoch: 5449 loss is tensor([-0.1447], grad_fn=<AddBackward0>)\n",
      "epoch: 5450 loss is tensor([-0.2267], grad_fn=<AddBackward0>)\n",
      "epoch: 5451 loss is tensor([-0.2021], grad_fn=<AddBackward0>)\n",
      "epoch: 5452 loss is tensor([-0.1790], grad_fn=<AddBackward0>)\n",
      "epoch: 5453 loss is tensor([-0.2396], grad_fn=<AddBackward0>)\n",
      "epoch: 5454 loss is tensor([-0.1735], grad_fn=<AddBackward0>)\n",
      "epoch: 5455 loss is tensor([-0.1852], grad_fn=<AddBackward0>)\n",
      "epoch: 5456 loss is tensor([-0.1168], grad_fn=<AddBackward0>)\n",
      "epoch: 5457 loss is tensor([-0.1942], grad_fn=<AddBackward0>)\n",
      "epoch: 5458 loss is tensor([-0.1821], grad_fn=<AddBackward0>)\n",
      "epoch: 5459 loss is tensor([-0.1615], grad_fn=<AddBackward0>)\n",
      "epoch: 5460 loss is tensor([-0.1607], grad_fn=<AddBackward0>)\n",
      "epoch: 5461 loss is tensor([-0.2201], grad_fn=<AddBackward0>)\n",
      "epoch: 5462 loss is tensor([-0.1836], grad_fn=<AddBackward0>)\n",
      "epoch: 5463 loss is tensor([-0.2625], grad_fn=<AddBackward0>)\n",
      "epoch: 5464 loss is tensor([-0.2434], grad_fn=<AddBackward0>)\n",
      "epoch: 5465 loss is tensor([-0.2243], grad_fn=<AddBackward0>)\n",
      "epoch: 5466 loss is tensor([-0.1686], grad_fn=<AddBackward0>)\n",
      "epoch: 5467 loss is tensor([-0.1898], grad_fn=<AddBackward0>)\n",
      "epoch: 5468 loss is tensor([-0.2168], grad_fn=<AddBackward0>)\n",
      "epoch: 5469 loss is tensor([-0.1969], grad_fn=<AddBackward0>)\n",
      "epoch: 5470 loss is tensor([-0.2205], grad_fn=<AddBackward0>)\n",
      "epoch: 5471 loss is tensor([-0.2391], grad_fn=<AddBackward0>)\n",
      "epoch: 5472 loss is tensor([-0.1601], grad_fn=<AddBackward0>)\n",
      "epoch: 5473 loss is tensor([-0.2186], grad_fn=<AddBackward0>)\n",
      "epoch: 5474 loss is tensor([-0.1923], grad_fn=<AddBackward0>)\n",
      "epoch: 5475 loss is tensor([-0.2408], grad_fn=<AddBackward0>)\n",
      "epoch: 5476 loss is tensor([-0.2086], grad_fn=<AddBackward0>)\n",
      "epoch: 5477 loss is tensor([-0.2340], grad_fn=<AddBackward0>)\n",
      "epoch: 5478 loss is tensor([-0.1830], grad_fn=<AddBackward0>)\n",
      "epoch: 5479 loss is tensor([-0.2219], grad_fn=<AddBackward0>)\n",
      "epoch: 5480 loss is tensor([-0.1614], grad_fn=<AddBackward0>)\n",
      "epoch: 5481 loss is tensor([-0.1930], grad_fn=<AddBackward0>)\n",
      "epoch: 5482 loss is tensor([-0.2378], grad_fn=<AddBackward0>)\n",
      "epoch: 5483 loss is tensor([-0.2330], grad_fn=<AddBackward0>)\n",
      "epoch: 5484 loss is tensor([-0.2180], grad_fn=<AddBackward0>)\n",
      "epoch: 5485 loss is tensor([-0.2264], grad_fn=<AddBackward0>)\n",
      "epoch: 5486 loss is tensor([-0.2167], grad_fn=<AddBackward0>)\n",
      "epoch: 5487 loss is tensor([-0.1908], grad_fn=<AddBackward0>)\n",
      "epoch: 5488 loss is tensor([-0.1726], grad_fn=<AddBackward0>)\n",
      "epoch: 5489 loss is tensor([-0.1910], grad_fn=<AddBackward0>)\n",
      "epoch: 5490 loss is tensor([-0.1905], grad_fn=<AddBackward0>)\n",
      "epoch: 5491 loss is tensor([-0.1913], grad_fn=<AddBackward0>)\n",
      "epoch: 5492 loss is tensor([-0.2209], grad_fn=<AddBackward0>)\n",
      "epoch: 5493 loss is tensor([-0.2303], grad_fn=<AddBackward0>)\n",
      "epoch: 5494 loss is tensor([-0.2247], grad_fn=<AddBackward0>)\n",
      "epoch: 5495 loss is tensor([-0.2664], grad_fn=<AddBackward0>)\n",
      "epoch: 5496 loss is tensor([-0.2053], grad_fn=<AddBackward0>)\n",
      "epoch: 5497 loss is tensor([-0.2528], grad_fn=<AddBackward0>)\n",
      "epoch: 5498 loss is tensor([-0.1917], grad_fn=<AddBackward0>)\n",
      "epoch: 5499 loss is tensor([-0.2455], grad_fn=<AddBackward0>)\n",
      "epoch: 5500 loss is tensor([-0.1347], grad_fn=<AddBackward0>)\n",
      "53\n"
=======
      "The number of epochs is: 4601\n",
      "The number of epochs is: 4602\n",
      "The number of epochs is: 4603\n",
      "The number of epochs is: 4604\n",
      "The number of epochs is: 4605\n",
      "The number of epochs is: 4606\n",
      "The number of epochs is: 4607\n",
      "The number of epochs is: 4608\n",
      "The number of epochs is: 4609\n",
      "The number of epochs is: 4610\n",
      "The number of epochs is: 4611\n",
      "The number of epochs is: 4612\n",
      "The number of epochs is: 4613\n",
      "The number of epochs is: 4614\n",
      "The number of epochs is: 4615\n",
      "The number of epochs is: 4616\n",
      "The number of epochs is: 4617\n",
      "The number of epochs is: 4618\n",
      "The number of epochs is: 4619\n",
      "The number of epochs is: 4620\n",
      "The number of epochs is: 4621\n",
      "The number of epochs is: 4622\n",
      "The number of epochs is: 4623\n",
      "The number of epochs is: 4624\n",
      "The number of epochs is: 4625\n",
      "The number of epochs is: 4626\n",
      "The number of epochs is: 4627\n",
      "The number of epochs is: 4628\n",
      "The number of epochs is: 4629\n",
      "The number of epochs is: 4630\n",
      "The number of epochs is: 4631\n",
      "The number of epochs is: 4632\n",
      "The number of epochs is: 4633\n",
      "The number of epochs is: 4634\n",
      "The number of epochs is: 4635\n",
      "The number of epochs is: 4636\n",
      "The number of epochs is: 4637\n",
      "The number of epochs is: 4638\n",
      "The number of epochs is: 4639\n",
      "The number of epochs is: 4640\n",
      "The number of epochs is: 4641\n",
      "The number of epochs is: 4642\n",
      "The number of epochs is: 4643\n",
      "The number of epochs is: 4644\n",
      "The number of epochs is: 4645\n",
      "The number of epochs is: 4646\n",
      "The number of epochs is: 4647\n",
      "The number of epochs is: 4648\n",
      "The number of epochs is: 4649\n",
      "The number of epochs is: 4650\n",
      "The number of epochs is: 4651\n",
      "The number of epochs is: 4652\n",
      "The number of epochs is: 4653\n",
      "The number of epochs is: 4654\n",
      "The number of epochs is: 4655\n",
      "The number of epochs is: 4656\n",
      "The number of epochs is: 4657\n",
      "The number of epochs is: 4658\n",
      "The number of epochs is: 4659\n",
      "The number of epochs is: 4660\n",
      "The number of epochs is: 4661\n",
      "The number of epochs is: 4662\n",
      "The number of epochs is: 4663\n",
      "The number of epochs is: 4664\n",
      "The number of epochs is: 4665\n",
      "The number of epochs is: 4666\n",
      "The number of epochs is: 4667\n",
      "The number of epochs is: 4668\n",
      "The number of epochs is: 4669\n",
      "The number of epochs is: 4670\n",
      "The number of epochs is: 4671\n",
      "The number of epochs is: 4672\n",
      "The number of epochs is: 4673\n",
      "The number of epochs is: 4674\n",
      "The number of epochs is: 4675\n",
      "The number of epochs is: 4676\n",
      "The number of epochs is: 4677\n",
      "The number of epochs is: 4678\n",
      "The number of epochs is: 4679\n",
      "The number of epochs is: 4680\n",
      "The number of epochs is: 4681\n",
      "The number of epochs is: 4682\n",
      "The number of epochs is: 4683\n",
      "The number of epochs is: 4684\n",
      "The number of epochs is: 4685\n",
      "The number of epochs is: 4686\n",
      "The number of epochs is: 4687\n",
      "The number of epochs is: 4688\n",
      "The number of epochs is: 4689\n",
      "The number of epochs is: 4690\n",
      "The number of epochs is: 4691\n",
      "The number of epochs is: 4692\n",
      "The number of epochs is: 4693\n",
      "The number of epochs is: 4694\n",
      "The number of epochs is: 4695\n",
      "The number of epochs is: 4696\n",
      "The number of epochs is: 4697\n",
      "The number of epochs is: 4698\n",
      "The number of epochs is: 4699\n",
      "The number of epochs is: 4700\n",
      "The number of epochs is: 4701\n",
      "The number of epochs is: 4702\n",
      "The number of epochs is: 4703\n",
      "The number of epochs is: 4704\n",
      "The number of epochs is: 4705\n",
      "The number of epochs is: 4706\n",
      "The number of epochs is: 4707\n",
      "The number of epochs is: 4708\n",
      "The number of epochs is: 4709\n",
      "The number of epochs is: 4710\n",
      "The number of epochs is: 4711\n",
      "The number of epochs is: 4712\n",
      "The number of epochs is: 4713\n",
      "The number of epochs is: 4714\n",
      "The number of epochs is: 4715\n",
      "The number of epochs is: 4716\n",
      "The number of epochs is: 4717\n",
      "The number of epochs is: 4718\n",
      "The number of epochs is: 4719\n",
      "The number of epochs is: 4720\n",
      "The number of epochs is: 4721\n",
      "The number of epochs is: 4722\n",
      "The number of epochs is: 4723\n",
      "The number of epochs is: 4724\n",
      "The number of epochs is: 4725\n",
      "The number of epochs is: 4726\n",
      "The number of epochs is: 4727\n",
      "The number of epochs is: 4728\n",
      "The number of epochs is: 4729\n",
      "The number of epochs is: 4730\n",
      "The number of epochs is: 4731\n",
      "The number of epochs is: 4732\n",
      "The number of epochs is: 4733\n",
      "The number of epochs is: 4734\n",
      "The number of epochs is: 4735\n",
      "The number of epochs is: 4736\n",
      "The number of epochs is: 4737\n",
      "The number of epochs is: 4738\n",
      "The number of epochs is: 4739\n",
      "The number of epochs is: 4740\n",
      "The number of epochs is: 4741\n",
      "The number of epochs is: 4742\n",
      "The number of epochs is: 4743\n",
      "The number of epochs is: 4744\n",
      "The number of epochs is: 4745\n",
      "The number of epochs is: 4746\n",
      "The number of epochs is: 4747\n",
      "The number of epochs is: 4748\n",
      "The number of epochs is: 4749\n",
      "The number of epochs is: 4750\n",
      "The number of epochs is: 4751\n",
      "The number of epochs is: 4752\n",
      "The number of epochs is: 4753\n",
      "The number of epochs is: 4754\n",
      "The number of epochs is: 4755\n",
      "The number of epochs is: 4756\n",
      "The number of epochs is: 4757\n",
      "The number of epochs is: 4758\n",
      "The number of epochs is: 4759\n",
      "The number of epochs is: 4760\n",
      "The number of epochs is: 4761\n",
      "The number of epochs is: 4762\n",
      "The number of epochs is: 4763\n",
      "The number of epochs is: 4764\n",
      "The number of epochs is: 4765\n",
      "The number of epochs is: 4766\n",
      "The number of epochs is: 4767\n",
      "The number of epochs is: 4768\n",
      "The number of epochs is: 4769\n",
      "The number of epochs is: 4770\n",
      "The number of epochs is: 4771\n",
      "The number of epochs is: 4772\n",
      "The number of epochs is: 4773\n",
      "The number of epochs is: 4774\n",
      "The number of epochs is: 4775\n",
      "The number of epochs is: 4776\n",
      "The number of epochs is: 4777\n",
      "The number of epochs is: 4778\n",
      "The number of epochs is: 4779\n",
      "The number of epochs is: 4780\n",
      "The number of epochs is: 4781\n",
      "The number of epochs is: 4782\n",
      "The number of epochs is: 4783\n",
      "The number of epochs is: 4784\n",
      "The number of epochs is: 4785\n",
      "The number of epochs is: 4786\n",
      "The number of epochs is: 4787\n",
      "The number of epochs is: 4788\n",
      "The number of epochs is: 4789\n",
      "The number of epochs is: 4790\n",
      "The number of epochs is: 4791\n",
      "The number of epochs is: 4792\n",
      "The number of epochs is: 4793\n",
      "The number of epochs is: 4794\n",
      "The number of epochs is: 4795\n",
      "The number of epochs is: 4796\n",
      "The number of epochs is: 4797\n",
      "The number of epochs is: 4798\n",
      "The number of epochs is: 4799\n",
      "The number of epochs is: 4800\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 5501 loss is tensor([-0.2538], grad_fn=<AddBackward0>)\n",
      "epoch: 5502 loss is tensor([-0.2116], grad_fn=<AddBackward0>)\n",
      "epoch: 5503 loss is tensor([-0.1950], grad_fn=<AddBackward0>)\n",
      "epoch: 5504 loss is tensor([-0.2387], grad_fn=<AddBackward0>)\n",
      "epoch: 5505 loss is tensor([-0.1983], grad_fn=<AddBackward0>)\n",
      "epoch: 5506 loss is tensor([-0.1355], grad_fn=<AddBackward0>)\n",
      "epoch: 5507 loss is tensor([-0.1752], grad_fn=<AddBackward0>)\n",
      "epoch: 5508 loss is tensor([-0.2026], grad_fn=<AddBackward0>)\n",
      "epoch: 5509 loss is tensor([-0.1686], grad_fn=<AddBackward0>)\n",
      "epoch: 5510 loss is tensor([-0.1807], grad_fn=<AddBackward0>)\n",
      "epoch: 5511 loss is tensor([-0.1661], grad_fn=<AddBackward0>)\n",
      "epoch: 5512 loss is tensor([-0.1522], grad_fn=<AddBackward0>)\n",
      "epoch: 5513 loss is tensor([-0.0781], grad_fn=<AddBackward0>)\n",
      "epoch: 5514 loss is tensor([-0.1168], grad_fn=<AddBackward0>)\n",
      "epoch: 5515 loss is tensor([-0.1652], grad_fn=<AddBackward0>)\n",
      "epoch: 5516 loss is tensor([-0.1591], grad_fn=<AddBackward0>)\n",
      "epoch: 5517 loss is tensor([-0.2207], grad_fn=<AddBackward0>)\n",
      "epoch: 5518 loss is tensor([-0.1606], grad_fn=<AddBackward0>)\n",
      "epoch: 5519 loss is tensor([-0.2006], grad_fn=<AddBackward0>)\n",
      "epoch: 5520 loss is tensor([-0.1961], grad_fn=<AddBackward0>)\n",
      "epoch: 5521 loss is tensor([-0.1882], grad_fn=<AddBackward0>)\n",
      "epoch: 5522 loss is tensor([-0.2231], grad_fn=<AddBackward0>)\n",
      "epoch: 5523 loss is tensor([-0.2035], grad_fn=<AddBackward0>)\n",
      "epoch: 5524 loss is tensor([-0.1015], grad_fn=<AddBackward0>)\n",
      "epoch: 5525 loss is tensor([-0.2128], grad_fn=<AddBackward0>)\n",
      "epoch: 5526 loss is tensor([-0.1494], grad_fn=<AddBackward0>)\n",
      "epoch: 5527 loss is tensor([-0.1801], grad_fn=<AddBackward0>)\n",
      "epoch: 5528 loss is tensor([-0.2406], grad_fn=<AddBackward0>)\n",
      "epoch: 5529 loss is tensor([-0.1303], grad_fn=<AddBackward0>)\n",
      "epoch: 5530 loss is tensor([-0.2121], grad_fn=<AddBackward0>)\n",
      "epoch: 5531 loss is tensor([-0.2025], grad_fn=<AddBackward0>)\n",
      "epoch: 5532 loss is tensor([-0.1489], grad_fn=<AddBackward0>)\n",
      "epoch: 5533 loss is tensor([-0.2291], grad_fn=<AddBackward0>)\n",
      "epoch: 5534 loss is tensor([-0.2507], grad_fn=<AddBackward0>)\n",
      "epoch: 5535 loss is tensor([-0.1475], grad_fn=<AddBackward0>)\n",
      "epoch: 5536 loss is tensor([-0.1761], grad_fn=<AddBackward0>)\n",
      "epoch: 5537 loss is tensor([-0.0706], grad_fn=<AddBackward0>)\n",
      "epoch: 5538 loss is tensor([-0.1735], grad_fn=<AddBackward0>)\n",
      "epoch: 5539 loss is tensor([-0.2074], grad_fn=<AddBackward0>)\n",
      "epoch: 5540 loss is tensor([-0.1521], grad_fn=<AddBackward0>)\n",
      "epoch: 5541 loss is tensor([-0.1815], grad_fn=<AddBackward0>)\n",
      "epoch: 5542 loss is tensor([-0.1965], grad_fn=<AddBackward0>)\n",
      "epoch: 5543 loss is tensor([-0.1530], grad_fn=<AddBackward0>)\n",
      "epoch: 5544 loss is tensor([-0.1951], grad_fn=<AddBackward0>)\n",
      "epoch: 5545 loss is tensor([-0.1897], grad_fn=<AddBackward0>)\n",
      "epoch: 5546 loss is tensor([-0.1938], grad_fn=<AddBackward0>)\n",
      "epoch: 5547 loss is tensor([-0.1708], grad_fn=<AddBackward0>)\n",
      "epoch: 5548 loss is tensor([-0.1697], grad_fn=<AddBackward0>)\n",
      "epoch: 5549 loss is tensor([-0.2349], grad_fn=<AddBackward0>)\n",
      "epoch: 5550 loss is tensor([-0.1915], grad_fn=<AddBackward0>)\n",
      "epoch: 5551 loss is tensor([-0.2200], grad_fn=<AddBackward0>)\n",
      "epoch: 5552 loss is tensor([-0.2266], grad_fn=<AddBackward0>)\n",
      "epoch: 5553 loss is tensor([-0.1861], grad_fn=<AddBackward0>)\n",
      "epoch: 5554 loss is tensor([-0.1788], grad_fn=<AddBackward0>)\n",
      "epoch: 5555 loss is tensor([-0.1868], grad_fn=<AddBackward0>)\n",
      "epoch: 5556 loss is tensor([-0.2080], grad_fn=<AddBackward0>)\n",
      "epoch: 5557 loss is tensor([-0.1916], grad_fn=<AddBackward0>)\n",
      "epoch: 5558 loss is tensor([-0.1668], grad_fn=<AddBackward0>)\n",
      "epoch: 5559 loss is tensor([-0.1478], grad_fn=<AddBackward0>)\n",
      "epoch: 5560 loss is tensor([-0.1484], grad_fn=<AddBackward0>)\n",
      "epoch: 5561 loss is tensor([-0.1459], grad_fn=<AddBackward0>)\n",
      "epoch: 5562 loss is tensor([-0.1655], grad_fn=<AddBackward0>)\n",
      "epoch: 5563 loss is tensor([-0.1696], grad_fn=<AddBackward0>)\n",
      "epoch: 5564 loss is tensor([-0.2062], grad_fn=<AddBackward0>)\n",
      "epoch: 5565 loss is tensor([-0.2382], grad_fn=<AddBackward0>)\n",
      "epoch: 5566 loss is tensor([-0.1299], grad_fn=<AddBackward0>)\n",
      "epoch: 5567 loss is tensor([-0.2244], grad_fn=<AddBackward0>)\n",
      "epoch: 5568 loss is tensor([-0.1619], grad_fn=<AddBackward0>)\n",
      "epoch: 5569 loss is tensor([-0.1902], grad_fn=<AddBackward0>)\n",
      "epoch: 5570 loss is tensor([-0.2076], grad_fn=<AddBackward0>)\n",
      "epoch: 5571 loss is tensor([-0.1726], grad_fn=<AddBackward0>)\n",
      "epoch: 5572 loss is tensor([-0.2008], grad_fn=<AddBackward0>)\n",
      "epoch: 5573 loss is tensor([-0.2370], grad_fn=<AddBackward0>)\n",
      "epoch: 5574 loss is tensor([-0.1915], grad_fn=<AddBackward0>)\n",
      "epoch: 5575 loss is tensor([-0.2156], grad_fn=<AddBackward0>)\n",
      "epoch: 5576 loss is tensor([-0.1747], grad_fn=<AddBackward0>)\n",
      "epoch: 5577 loss is tensor([-0.1506], grad_fn=<AddBackward0>)\n",
      "epoch: 5578 loss is tensor([-0.2155], grad_fn=<AddBackward0>)\n",
      "epoch: 5579 loss is tensor([-0.1912], grad_fn=<AddBackward0>)\n",
      "epoch: 5580 loss is tensor([-0.2009], grad_fn=<AddBackward0>)\n",
      "epoch: 5581 loss is tensor([-0.1748], grad_fn=<AddBackward0>)\n",
      "epoch: 5582 loss is tensor([-0.1990], grad_fn=<AddBackward0>)\n",
      "epoch: 5583 loss is tensor([-0.2358], grad_fn=<AddBackward0>)\n",
      "epoch: 5584 loss is tensor([-0.1363], grad_fn=<AddBackward0>)\n",
      "epoch: 5585 loss is tensor([-0.2079], grad_fn=<AddBackward0>)\n",
      "epoch: 5586 loss is tensor([-0.2244], grad_fn=<AddBackward0>)\n",
      "epoch: 5587 loss is tensor([-0.2297], grad_fn=<AddBackward0>)\n",
      "epoch: 5588 loss is tensor([-0.2235], grad_fn=<AddBackward0>)\n",
      "epoch: 5589 loss is tensor([-0.2137], grad_fn=<AddBackward0>)\n",
      "epoch: 5590 loss is tensor([-0.1977], grad_fn=<AddBackward0>)\n",
      "epoch: 5591 loss is tensor([-0.1990], grad_fn=<AddBackward0>)\n",
      "epoch: 5592 loss is tensor([-0.2012], grad_fn=<AddBackward0>)\n",
      "epoch: 5593 loss is tensor([-0.2441], grad_fn=<AddBackward0>)\n",
      "epoch: 5594 loss is tensor([-0.2602], grad_fn=<AddBackward0>)\n",
      "epoch: 5595 loss is tensor([-0.2371], grad_fn=<AddBackward0>)\n",
      "epoch: 5596 loss is tensor([-0.1769], grad_fn=<AddBackward0>)\n",
      "epoch: 5597 loss is tensor([-0.2305], grad_fn=<AddBackward0>)\n",
      "epoch: 5598 loss is tensor([-0.2522], grad_fn=<AddBackward0>)\n",
      "epoch: 5599 loss is tensor([-0.2163], grad_fn=<AddBackward0>)\n",
      "epoch: 5600 loss is tensor([-0.1794], grad_fn=<AddBackward0>)\n",
      "50\n"
=======
      "51\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 5601 loss is tensor([-0.3030], grad_fn=<AddBackward0>)\n",
      "epoch: 5602 loss is tensor([-0.2124], grad_fn=<AddBackward0>)\n",
      "epoch: 5603 loss is tensor([-0.2281], grad_fn=<AddBackward0>)\n",
      "epoch: 5604 loss is tensor([-0.2388], grad_fn=<AddBackward0>)\n",
      "epoch: 5605 loss is tensor([-0.1817], grad_fn=<AddBackward0>)\n",
      "epoch: 5606 loss is tensor([-0.2181], grad_fn=<AddBackward0>)\n",
      "epoch: 5607 loss is tensor([-0.2165], grad_fn=<AddBackward0>)\n",
      "epoch: 5608 loss is tensor([-0.2562], grad_fn=<AddBackward0>)\n",
      "epoch: 5609 loss is tensor([-0.2298], grad_fn=<AddBackward0>)\n",
      "epoch: 5610 loss is tensor([-0.1406], grad_fn=<AddBackward0>)\n",
      "epoch: 5611 loss is tensor([-0.1810], grad_fn=<AddBackward0>)\n",
      "epoch: 5612 loss is tensor([-0.2021], grad_fn=<AddBackward0>)\n",
      "epoch: 5613 loss is tensor([-0.2003], grad_fn=<AddBackward0>)\n",
      "epoch: 5614 loss is tensor([-0.1944], grad_fn=<AddBackward0>)\n",
      "epoch: 5615 loss is tensor([-0.1962], grad_fn=<AddBackward0>)\n",
      "epoch: 5616 loss is tensor([-0.1723], grad_fn=<AddBackward0>)\n",
      "epoch: 5617 loss is tensor([-0.1842], grad_fn=<AddBackward0>)\n",
      "epoch: 5618 loss is tensor([-0.2129], grad_fn=<AddBackward0>)\n",
      "epoch: 5619 loss is tensor([-0.1746], grad_fn=<AddBackward0>)\n",
      "epoch: 5620 loss is tensor([-0.1579], grad_fn=<AddBackward0>)\n",
      "epoch: 5621 loss is tensor([-0.1957], grad_fn=<AddBackward0>)\n",
      "epoch: 5622 loss is tensor([-0.2251], grad_fn=<AddBackward0>)\n",
      "epoch: 5623 loss is tensor([-0.2108], grad_fn=<AddBackward0>)\n",
      "epoch: 5624 loss is tensor([-0.2074], grad_fn=<AddBackward0>)\n",
      "epoch: 5625 loss is tensor([-0.1894], grad_fn=<AddBackward0>)\n",
      "epoch: 5626 loss is tensor([-0.2580], grad_fn=<AddBackward0>)\n",
      "epoch: 5627 loss is tensor([-0.1736], grad_fn=<AddBackward0>)\n",
      "epoch: 5628 loss is tensor([-0.2147], grad_fn=<AddBackward0>)\n",
      "epoch: 5629 loss is tensor([-0.2241], grad_fn=<AddBackward0>)\n",
      "epoch: 5630 loss is tensor([-0.2378], grad_fn=<AddBackward0>)\n",
      "epoch: 5631 loss is tensor([-0.1524], grad_fn=<AddBackward0>)\n",
      "epoch: 5632 loss is tensor([-0.2395], grad_fn=<AddBackward0>)\n",
      "epoch: 5633 loss is tensor([-0.2056], grad_fn=<AddBackward0>)\n",
      "epoch: 5634 loss is tensor([-0.2178], grad_fn=<AddBackward0>)\n",
      "epoch: 5635 loss is tensor([-0.2568], grad_fn=<AddBackward0>)\n",
      "epoch: 5636 loss is tensor([-0.2067], grad_fn=<AddBackward0>)\n",
      "epoch: 5637 loss is tensor([-0.1777], grad_fn=<AddBackward0>)\n",
      "epoch: 5638 loss is tensor([-0.1834], grad_fn=<AddBackward0>)\n",
      "epoch: 5639 loss is tensor([-0.2524], grad_fn=<AddBackward0>)\n",
      "epoch: 5640 loss is tensor([-0.1770], grad_fn=<AddBackward0>)\n",
      "epoch: 5641 loss is tensor([-0.2547], grad_fn=<AddBackward0>)\n",
      "epoch: 5642 loss is tensor([-0.2567], grad_fn=<AddBackward0>)\n",
      "epoch: 5643 loss is tensor([-0.2451], grad_fn=<AddBackward0>)\n",
      "epoch: 5644 loss is tensor([-0.2011], grad_fn=<AddBackward0>)\n",
      "epoch: 5645 loss is tensor([-0.2675], grad_fn=<AddBackward0>)\n",
      "epoch: 5646 loss is tensor([-0.2564], grad_fn=<AddBackward0>)\n",
      "epoch: 5647 loss is tensor([-0.3082], grad_fn=<AddBackward0>)\n",
      "epoch: 5648 loss is tensor([-0.2133], grad_fn=<AddBackward0>)\n",
      "epoch: 5649 loss is tensor([-0.2402], grad_fn=<AddBackward0>)\n",
      "epoch: 5650 loss is tensor([-0.2367], grad_fn=<AddBackward0>)\n",
      "epoch: 5651 loss is tensor([-0.2430], grad_fn=<AddBackward0>)\n",
      "epoch: 5652 loss is tensor([-0.1382], grad_fn=<AddBackward0>)\n",
      "epoch: 5653 loss is tensor([-0.2133], grad_fn=<AddBackward0>)\n",
      "epoch: 5654 loss is tensor([-0.2177], grad_fn=<AddBackward0>)\n",
      "epoch: 5655 loss is tensor([-0.1984], grad_fn=<AddBackward0>)\n",
      "epoch: 5656 loss is tensor([-0.2525], grad_fn=<AddBackward0>)\n",
      "epoch: 5657 loss is tensor([-0.2600], grad_fn=<AddBackward0>)\n",
      "epoch: 5658 loss is tensor([-0.2168], grad_fn=<AddBackward0>)\n",
      "epoch: 5659 loss is tensor([-0.2596], grad_fn=<AddBackward0>)\n",
      "epoch: 5660 loss is tensor([-0.1906], grad_fn=<AddBackward0>)\n",
      "epoch: 5661 loss is tensor([-0.2156], grad_fn=<AddBackward0>)\n",
      "epoch: 5662 loss is tensor([-0.1828], grad_fn=<AddBackward0>)\n",
      "epoch: 5663 loss is tensor([-0.2108], grad_fn=<AddBackward0>)\n",
      "epoch: 5664 loss is tensor([-0.1904], grad_fn=<AddBackward0>)\n",
      "epoch: 5665 loss is tensor([-0.2005], grad_fn=<AddBackward0>)\n",
      "epoch: 5666 loss is tensor([-0.1618], grad_fn=<AddBackward0>)\n",
      "epoch: 5667 loss is tensor([-0.2235], grad_fn=<AddBackward0>)\n",
      "epoch: 5668 loss is tensor([-0.1992], grad_fn=<AddBackward0>)\n",
      "epoch: 5669 loss is tensor([-0.2420], grad_fn=<AddBackward0>)\n",
      "epoch: 5670 loss is tensor([-0.2279], grad_fn=<AddBackward0>)\n",
      "epoch: 5671 loss is tensor([-0.2276], grad_fn=<AddBackward0>)\n",
      "epoch: 5672 loss is tensor([-0.2004], grad_fn=<AddBackward0>)\n",
      "epoch: 5673 loss is tensor([-0.2013], grad_fn=<AddBackward0>)\n",
      "epoch: 5674 loss is tensor([-0.2035], grad_fn=<AddBackward0>)\n",
      "epoch: 5675 loss is tensor([-0.1684], grad_fn=<AddBackward0>)\n",
      "epoch: 5676 loss is tensor([-0.2203], grad_fn=<AddBackward0>)\n",
      "epoch: 5677 loss is tensor([-0.2146], grad_fn=<AddBackward0>)\n",
      "epoch: 5678 loss is tensor([-0.2468], grad_fn=<AddBackward0>)\n",
      "epoch: 5679 loss is tensor([-0.2012], grad_fn=<AddBackward0>)\n",
      "epoch: 5680 loss is tensor([-0.2126], grad_fn=<AddBackward0>)\n",
      "epoch: 5681 loss is tensor([-0.2303], grad_fn=<AddBackward0>)\n",
      "epoch: 5682 loss is tensor([-0.2261], grad_fn=<AddBackward0>)\n",
      "epoch: 5683 loss is tensor([-0.1757], grad_fn=<AddBackward0>)\n",
      "epoch: 5684 loss is tensor([-0.1771], grad_fn=<AddBackward0>)\n",
      "epoch: 5685 loss is tensor([-0.2191], grad_fn=<AddBackward0>)\n",
      "epoch: 5686 loss is tensor([-0.1559], grad_fn=<AddBackward0>)\n",
      "epoch: 5687 loss is tensor([-0.2044], grad_fn=<AddBackward0>)\n",
      "epoch: 5688 loss is tensor([-0.2440], grad_fn=<AddBackward0>)\n",
      "epoch: 5689 loss is tensor([-0.1755], grad_fn=<AddBackward0>)\n",
      "epoch: 5690 loss is tensor([-0.1821], grad_fn=<AddBackward0>)\n",
      "epoch: 5691 loss is tensor([-0.2226], grad_fn=<AddBackward0>)\n",
      "epoch: 5692 loss is tensor([-0.1898], grad_fn=<AddBackward0>)\n",
      "epoch: 5693 loss is tensor([-0.1785], grad_fn=<AddBackward0>)\n",
      "epoch: 5694 loss is tensor([-0.1853], grad_fn=<AddBackward0>)\n",
      "epoch: 5695 loss is tensor([-0.2104], grad_fn=<AddBackward0>)\n",
      "epoch: 5696 loss is tensor([-0.2127], grad_fn=<AddBackward0>)\n",
      "epoch: 5697 loss is tensor([-0.1649], grad_fn=<AddBackward0>)\n",
      "epoch: 5698 loss is tensor([-0.2118], grad_fn=<AddBackward0>)\n",
      "epoch: 5699 loss is tensor([-0.1995], grad_fn=<AddBackward0>)\n",
      "epoch: 5700 loss is tensor([-0.2413], grad_fn=<AddBackward0>)\n",
      "29\n"
=======
      "The number of epochs is: 4801\n",
      "The number of epochs is: 4802\n",
      "The number of epochs is: 4803\n",
      "The number of epochs is: 4804\n",
      "The number of epochs is: 4805\n",
      "The number of epochs is: 4806\n",
      "The number of epochs is: 4807\n",
      "The number of epochs is: 4808\n",
      "The number of epochs is: 4809\n",
      "The number of epochs is: 4810\n",
      "The number of epochs is: 4811\n",
      "The number of epochs is: 4812\n",
      "The number of epochs is: 4813\n",
      "The number of epochs is: 4814\n",
      "The number of epochs is: 4815\n",
      "The number of epochs is: 4816\n",
      "The number of epochs is: 4817\n",
      "The number of epochs is: 4818\n",
      "The number of epochs is: 4819\n",
      "The number of epochs is: 4820\n",
      "The number of epochs is: 4821\n",
      "The number of epochs is: 4822\n",
      "The number of epochs is: 4823\n",
      "The number of epochs is: 4824\n",
      "The number of epochs is: 4825\n",
      "The number of epochs is: 4826\n",
      "The number of epochs is: 4827\n",
      "The number of epochs is: 4828\n",
      "The number of epochs is: 4829\n",
      "The number of epochs is: 4830\n",
      "The number of epochs is: 4831\n",
      "The number of epochs is: 4832\n",
      "The number of epochs is: 4833\n",
      "The number of epochs is: 4834\n",
      "The number of epochs is: 4835\n",
      "The number of epochs is: 4836\n",
      "The number of epochs is: 4837\n",
      "The number of epochs is: 4838\n",
      "The number of epochs is: 4839\n",
      "The number of epochs is: 4840\n",
      "The number of epochs is: 4841\n",
      "The number of epochs is: 4842\n",
      "The number of epochs is: 4843\n",
      "The number of epochs is: 4844\n",
      "The number of epochs is: 4845\n",
      "The number of epochs is: 4846\n",
      "The number of epochs is: 4847\n",
      "The number of epochs is: 4848\n",
      "The number of epochs is: 4849\n",
      "The number of epochs is: 4850\n",
      "The number of epochs is: 4851\n",
      "The number of epochs is: 4852\n",
      "The number of epochs is: 4853\n",
      "The number of epochs is: 4854\n",
      "The number of epochs is: 4855\n",
      "The number of epochs is: 4856\n",
      "The number of epochs is: 4857\n",
      "The number of epochs is: 4858\n",
      "The number of epochs is: 4859\n",
      "The number of epochs is: 4860\n",
      "The number of epochs is: 4861\n",
      "The number of epochs is: 4862\n",
      "The number of epochs is: 4863\n",
      "The number of epochs is: 4864\n",
      "The number of epochs is: 4865\n",
      "The number of epochs is: 4866\n",
      "The number of epochs is: 4867\n",
      "The number of epochs is: 4868\n",
      "The number of epochs is: 4869\n",
      "The number of epochs is: 4870\n",
      "The number of epochs is: 4871\n",
      "The number of epochs is: 4872\n",
      "The number of epochs is: 4873\n",
      "The number of epochs is: 4874\n",
      "The number of epochs is: 4875\n",
      "The number of epochs is: 4876\n",
      "The number of epochs is: 4877\n",
      "The number of epochs is: 4878\n",
      "The number of epochs is: 4879\n",
      "The number of epochs is: 4880\n",
      "The number of epochs is: 4881\n",
      "The number of epochs is: 4882\n",
      "The number of epochs is: 4883\n",
      "The number of epochs is: 4884\n",
      "The number of epochs is: 4885\n",
      "The number of epochs is: 4886\n",
      "The number of epochs is: 4887\n",
      "The number of epochs is: 4888\n",
      "The number of epochs is: 4889\n",
      "The number of epochs is: 4890\n",
      "The number of epochs is: 4891\n",
      "The number of epochs is: 4892\n",
      "The number of epochs is: 4893\n",
      "The number of epochs is: 4894\n",
      "The number of epochs is: 4895\n",
      "The number of epochs is: 4896\n",
      "The number of epochs is: 4897\n",
      "The number of epochs is: 4898\n",
      "The number of epochs is: 4899\n",
      "The number of epochs is: 4900\n",
      "The number of epochs is: 4901\n",
      "The number of epochs is: 4902\n",
      "The number of epochs is: 4903\n",
      "The number of epochs is: 4904\n",
      "The number of epochs is: 4905\n",
      "The number of epochs is: 4906\n",
      "The number of epochs is: 4907\n",
      "The number of epochs is: 4908\n",
      "The number of epochs is: 4909\n",
      "The number of epochs is: 4910\n",
      "The number of epochs is: 4911\n",
      "The number of epochs is: 4912\n",
      "The number of epochs is: 4913\n",
      "The number of epochs is: 4914\n",
      "The number of epochs is: 4915\n",
      "The number of epochs is: 4916\n",
      "The number of epochs is: 4917\n",
      "The number of epochs is: 4918\n",
      "The number of epochs is: 4919\n",
      "The number of epochs is: 4920\n",
      "The number of epochs is: 4921\n",
      "The number of epochs is: 4922\n",
      "The number of epochs is: 4923\n",
      "The number of epochs is: 4924\n",
      "The number of epochs is: 4925\n",
      "The number of epochs is: 4926\n",
      "The number of epochs is: 4927\n",
      "The number of epochs is: 4928\n",
      "The number of epochs is: 4929\n",
      "The number of epochs is: 4930\n",
      "The number of epochs is: 4931\n",
      "The number of epochs is: 4932\n",
      "The number of epochs is: 4933\n",
      "The number of epochs is: 4934\n",
      "The number of epochs is: 4935\n",
      "The number of epochs is: 4936\n",
      "The number of epochs is: 4937\n",
      "The number of epochs is: 4938\n",
      "The number of epochs is: 4939\n",
      "The number of epochs is: 4940\n",
      "The number of epochs is: 4941\n",
      "The number of epochs is: 4942\n",
      "The number of epochs is: 4943\n",
      "The number of epochs is: 4944\n",
      "The number of epochs is: 4945\n",
      "The number of epochs is: 4946\n",
      "The number of epochs is: 4947\n",
      "The number of epochs is: 4948\n",
      "The number of epochs is: 4949\n",
      "The number of epochs is: 4950\n",
      "The number of epochs is: 4951\n",
      "The number of epochs is: 4952\n",
      "The number of epochs is: 4953\n",
      "The number of epochs is: 4954\n",
      "The number of epochs is: 4955\n",
      "The number of epochs is: 4956\n",
      "The number of epochs is: 4957\n",
      "The number of epochs is: 4958\n",
      "The number of epochs is: 4959\n",
      "The number of epochs is: 4960\n",
      "The number of epochs is: 4961\n",
      "The number of epochs is: 4962\n",
      "The number of epochs is: 4963\n",
      "The number of epochs is: 4964\n",
      "The number of epochs is: 4965\n",
      "The number of epochs is: 4966\n",
      "The number of epochs is: 4967\n",
      "The number of epochs is: 4968\n",
      "The number of epochs is: 4969\n",
      "The number of epochs is: 4970\n",
      "The number of epochs is: 4971\n",
      "The number of epochs is: 4972\n",
      "The number of epochs is: 4973\n",
      "The number of epochs is: 4974\n",
      "The number of epochs is: 4975\n",
      "The number of epochs is: 4976\n",
      "The number of epochs is: 4977\n",
      "The number of epochs is: 4978\n",
      "The number of epochs is: 4979\n",
      "The number of epochs is: 4980\n",
      "The number of epochs is: 4981\n",
      "The number of epochs is: 4982\n",
      "The number of epochs is: 4983\n",
      "The number of epochs is: 4984\n",
      "The number of epochs is: 4985\n",
      "The number of epochs is: 4986\n",
      "The number of epochs is: 4987\n",
      "The number of epochs is: 4988\n",
      "The number of epochs is: 4989\n",
      "The number of epochs is: 4990\n",
      "The number of epochs is: 4991\n",
      "The number of epochs is: 4992\n",
      "The number of epochs is: 4993\n",
      "The number of epochs is: 4994\n",
      "The number of epochs is: 4995\n",
      "The number of epochs is: 4996\n",
      "The number of epochs is: 4997\n",
      "The number of epochs is: 4998\n",
      "The number of epochs is: 4999\n",
      "The number of epochs is: 5000\n",
      "10\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAc9ElEQVR4nO3de2xc53nn8e8zvIm68WJdLVKWIju+1BdJobTZtRG4iaA6SWMZcWrLDbZ2EsPxFka63QKBG28TbLIFvDWw7S7SIhWy3iaFYdlxk1pxlch2UtfZpopEiZQt2Y5sKxaH1I2WSN0oiZd59o8ZykNyhhxyLufMmd8HIDiXl+d9Din93nfOeeeMuTsiIhJ9saALEBGR0lDgi4hUCAW+iEiFUOCLiFQIBb6ISIWoDrqAySxYsMBXrFgRdBkiImVjz54977v7wkzPhTrwV6xYQXt7e9BliIiUDTM7nO05HdIREakQCnwRkQqhwBcRqRAKfBGRCqHAFxGpEAp8EZEKocAXEakQCvwCe/mN4/zdv/6GoZFE0KWIiIyhwC+wv3nlHb6/8zDVMQu6FBGRMRT4BXTw+Fn2dvWzeV0rZgp8EQkXBX4BPb2ri5oq4+61LUGXIiIygQK/QC4OjfCjjh423rCEK+bWBV2OiMgECvwC2XHgGP0DQ2xe3xp0KSIiGSnwC2TrrjitzfXcumpB0KWIiGSkwC+A994/z78dOsm9ba3EtDpHREJKgV8AW3fHiRn8XpsO54hIeOUV+Gb2hJm9ZWavmdmPzKwxS7s7zOzXZvaOmT2aT59hMzSS4Lk93Xz8ukUsnj8r6HJERLLKd4b/EnCju98MHAT+dHwDM6sC/hr4JHADcJ+Z3ZBnv6HxszeP8/65S2xetzzoUkREJpVX4Lv7i+4+nLq7E8i0AH098I67H3L3QWArsCmffsNk6+44i+fXcfu1GT9CUkQkNAp5DP+LwE8yPL4MiKfd7049lpGZPWRm7WbW3tvbW8DyCq+n/wL/crCXe9paqa7S6RARCbcpP8TczF4GlmR46jF3fz7V5jFgGHgq34LcfQuwBaCtrc3z3V4xPbs7OY7do5O1IlIGpgx8d98w2fNm9gDwu8An3D1TQPcA6YnYknqsrI0knB+0x7nt6gW0Ns8OuhwRkSnlu0rnDuCrwJ3uPpCl2W7gGjNbaWa1wGZgWz79hsGrB3s5cvoi963XyVoRKQ/5Hnj+NjAPeMnMOs3sOwBmdqWZbQdIndR9BNgBvAk86+4H8uw3cE/v6uKKObVsuH5x0KWIiORkykM6k3H3q7M8fgT4VNr97cD2fPoKkxNnL/Kzt07wpdtWUlutk7UiUh6UVjPw3J5uRhLOvet0slZEyocCf5oSCeeZ3XHWr2xm1cK5QZcjIpIzBf407Tx0ksMnB7hPl0EWkTKjwJ+mp3fHmT+rmk/euDToUkREpkWBPw2nzg+yY/8xPru2hVk1VUGXIyIyLQr8afhRRw+DIwmdrBWRsqTAz5G7s3VXF7e0NnL90vlBlyMiMm0K/Bzt7erj7RPnuE+zexEpUwr8HD29K86c2io+c8uVQZciIjIjCvwcnLk4xAuvHeHO1Vcypy6vNyeLiARGgZ+D5zuPcHEooU+1EpGypsDPwTO7u7huyTxubmkIuhQRkRlT4E9hf89p9vec4b71yzGzoMsREZkxBf4Unt7VRV11jLtWZ/1URhGRsqDAn8TA4DDPdx7h0zctpWF2TdDliIjkRYE/iRdeO8q5S8Ns1qdaiUgEKPAnsXVXF6sWzmHdiqagSxERyZsCP4uDx8+yt6ufzet0slZEokGBn8XWXXFqqozPrtXJWhGJBgV+BheHRvhhRzcbb1jCFXPrgi5HRKQgFPgZ7DhwjP6BITbrU61EJEIU+Bls3RWntbmeW1ctCLoUEZGCUeCP89775/m3Qye5t62VWEwna0UkOhT44zzTHqcqZvxemw7niEi05HWtXzN7AvgMMAi8C3zB3fsztHsPOAuMAMPu3pZPv8UyNJLgB+3d/Pa1i1g8f1bQ5YiIFFS+M/yXgBvd/WbgIPCnk7T9bXdfHdawB/jZmyd4/9wlNutTrUQkgvIKfHd/0d2HU3d3Ai35lxScrbu7WDy/jtuvXRh0KSIiBVfIY/hfBH6S5TkHXjSzPWb2UAH7LJie/gv8y8Fe7mlrpbpKpzZEJHqmPIZvZi8DSzI89Zi7P59q8xgwDDyVZTO3uXuPmS0CXjKzt9z91Sz9PQQ8BLB8eekuWvbs7jgA9+hkrYhE1JSB7+4bJnvezB4Afhf4hLt7lm30pL6fMLMfAeuBjIHv7luALQBtbW0Zt5eT72+CG++GtX8wZdORhPOD9ji3Xb2A1ubZM+5SRCTM8jp2YWZ3AF8F7nT3gSxt5pjZvNHbwEZgfz795uQ3v4C+wzk1ffXtXo6cvsh9ugyyiERYvgervw3MI3mYptPMvgNgZlea2fZUm8XA/zOzfcAu4J/c/ad59js1T4Dltntbd3VxxZxaNly/uMhFiYgEJ691+O5+dZbHjwCfSt0+BNySTz/T5g54ToF/4uxFfvbmCb5420pqq3WyVkSiK5oJN3oqIYfAf25PN8MJ516tvReRiIto4CeS36cI/ETCeWZ3nPUrm1m1cG4JChMRCU5EA38k+X2KT6ra+ZuTHD45wH26DLKIVICIBn5uM/ytu+LMn1XNJ29cWoKiRESCFe3Aj1VlbdJ3fpCf7j/GZ9e2MKsmezsRkaiIduBPMsP/YUcPgyMJnawVkYpRkYHv7mzd1cUtrY1cv3R+CQsTEQlORQb+3q4+3j5xjvs0uxeRChLRwJ98Hf7WXXHm1FbxmVuuLGFRIiLBimjgZ5/hn704xAuvHeXO1Vcypy6vNxqLiJSViAf+xHX4z3ce4cLQCJvX6UJpIlJZIh74E3dv6+4url86n5tbGkpclIhIsCoq8Pf3nGZ/zxk2r2vFpngXrohI1FRU4D+9q4u66hh3rV4WQFEiIsGqmMAfGBxmW+cRPn3TUhpm1wRUmIhIcKIZ+InRi6d9sHv/9NpRzl4aZrM+1UpEKlQ0A//yDP+Da+Rs3R1n1cI5rFvRFFBRIiLBimjgj33j1cHjZ9lzuI/N65brZK2IVKyIBv7Ydfhbd8WpqTI+u1Yna0WkckU88GNcHBrhhx3dbLxhCVfMrQu2LhGRAEU+8HccOEb/wBCb9alWIlLhIh/4z+yO09pcz62rFgRbk4hIwCId+CfODfLLd09yb1srsZhO1opIZYvm5SJTgf+Ld04By1i3opn33j9/+en0hTqGZXw83Zj24xpZtnZZtjvmp8f1l8vPpPefS9/jG+az3clqHNMmx9/vtPdLK6xE8hLpwH+95ywA927ZGWQ1UmR5DyRkG5HzG9Anmxxk63JMvdnLynGAzqXGcdudwX5l3VYev68JPRRoAjTZnCG3SUfm7U58LvMTo7diBhtuWMwX/sNK6mtL95naeQe+mX0L2AQkgBPAA+5+JEO7+4H/mrr73939e/n2nVVqHf6XPraK1bWrkw/hmZpMvD2mjWd8nDHtJ9lu1sd9yvZjnphsW1lqzGWfxptujbm0n9hHYbY7oYucfg/T63v8z2S5mdc+Zeoz43Zz2NbE2rP1kaXePLc77d9V1vry+3+YbZ8mbZe1llza5/h/Ne3xMxeG+Iuf/pr/+6/v8ZWPX82965ZTW138I+yFmOE/4e5/BmBmXwG+Djyc3sDMmoFvAG0k93uPmW1z974C9D9Raobf2jyX1mu09l5Ewmf3e6d44qe/5s+eP8CWXxzijzd8mE2rl1FVxPONeQ8p7n4m7e4cMky+gN8BXnL3U6mQfwm4I9++sxeV/QNQRETCYN2KZp758kf5uy+sY/6sGv7Ls/v45P96lR0Hjk36SjwfBXkNYWZ/bmZx4PMkZ/jjLQPiafe7U49l2tZDZtZuZu29vb0zK8gnXjxNRCRszIzbr13Ejx+5jb/+/bUMjzhf/vs9/OFTe4vSX06JaGYvm9n+DF+bANz9MXdvBZ4CHsmnIHff4u5t7t62cOHCGW4kNcOPle5kiIjITMVixqdvXsqLf/wxrlsyj57+C0XpJ6dj+O6+IcftPQVsJ3m8Pl0PcHva/RbglRy3OX2TfMShiEiYHT45wL3rinNlgLwT0cyuSbu7CXgrQ7MdwEYzazKzJmBj6rHiUOCLSBk6ePwcF4ZGWLO8sSjbL8QqncfN7FqSyzIPk1qhY2ZtwMPu/qC7n0ot39yd+plvuvupAvSdmQJfRMpQRzy5cHFNa3E+tyPvwHf3u7M83g48mHb/SeDJfPvLrSgFvoiUn46ufprn1NLaXF+U7UczEcd9AIqISDno6OpjTWtj0S4jEs1E1Dp8ESkzpweGeLf3fNGO30PkAz+auyci0dPZ3Q/AmuXF+9ztaCaiAl9EykxnVz9mcHNLQ9H6iGYiKvBFpMx0xPv48KJ5zJtVU7Q+opmICnwRKSPuTkdXP6tbG4vaTzQTUYEvImXkN++f5/SFoaKesIWoBn5CF08TkfLR0dUPFPeELUQ18C+vw9fF00Qk/Drifcytq+bqRXOL2k9EA1/r8EWkfHTG+7mltaGoH34CkQ/8aO6eiETHhcER3jx6tmjXz0kXzURU4ItImXi95zQjCS/6CVtQ4IuIBKqjK3mFzGIvyQQFvohIoDq6+lnePJsr5tYVva9oJqICX0TKREe8rySHc0CBLyISmKOnL3D8zCXWlOBwDijwRUQCU6o3XI2KZiLqA1BEpAx0dPVRWx3j+qXzS9JfNBNRb7wSkTLQ0dXPTcsaqK0uTRRHPPCjuXsiUv4GhxO83nO6JMsxR0UzEV0XTxORcHvr2BkuDSdKtkIHIhv4qRl+TBdPE5Fw6oz3A6U7YQtRD3zN8EUkpDq6+lk0r44rG2aVrM9oJqICX0RCrqMr+YYrK+HikrwS0cy+ZWavmVmnmb1oZldmaTeSatNpZtvy6TMnCnwRCbFT5wd57+RASQ/nQP4z/Cfc/WZ3Xw28AHw9S7sL7r469XVnnn1OTevwRSTEOuOlu2BaurwS0d3PpN2dA3h+5RSIZvgiEmIdXf3EDG5uaShpv3knopn9uZnFgc+TfYY/y8zazWynmd01xfYeSrVt7+3tnVlReuOViIRYZ7yf65bMZ3ZtdUn7nTLwzexlM9uf4WsTgLs/5u6twFPAI1k2c5W7twG/D/yVma3K1p+7b3H3NndvW7hw4Qx2iWTga3YvIiGUSDidXf0lXX8/asrhxd035Litp4DtwDcybKMn9f2Qmb0CrAHezb3MaVLgi0hIvdt7jrOXhkt+whbyX6VzTdrdTcBbGdo0mVld6vYC4FbgjXz6nZICX0RC6oMrZDaWvO98DyA9bmbXAgngMPAwgJm1AQ+7+4PA9cDfmlmC5ADzuLsr8EWkInXE+2ior2HlFXNK3ndege/ud2d5vB14MHX7l8BN+fQzbQp8EQmpjq5+bmltJBYr/aKSaKZiQoEvIuFz7tIwB4+fLdknXI0XzVT0BJgunCYi4fJadz8JD+b4PUQ68LUGX0TCZfSEbanfYTsqwoEfzV0TkfLV0dXPhxbOoXF2bSD9RzMVFfgiEjLuTme8jzWtpV9/PyqaqajAF5GQ6e67wPvnBgM7fg8KfBGRkuhIfcJVUMfvQYEvIlISHV19zKqJcd2SeYHVEM1UdFfgi0iodHT1c3NLI9VVwWVTNFNRM3wRCZFLwyO8ceRMoMfvIdKBr3X4IhIOB46cYXAkEegKHYh04Edz10Sk/AR5hcx00UxFBb6IhEhnvJ9ljfUsnj8r0DqimYo+osAXkdDo6OoLdDnmqGimoicgpouniUjwTpy9SHffhcAP50CUA18zfBEJgeOnLwHQ0lQfcCWRDXytwxeRcLiyMXnc/ujpiwFXEtnA17JMEQmH5jm11NdU0d13IehSohz40dw1ESkvZkZLUz3dfQNBl6LAFxEpttbm2cRPaYZfHAp8EQkRzfCLSYEvIiHS0lTPmYvDnL4wFGgd0UxFBb6IhEhL02wAegI+cRvNVFTgi0iIjK7BD/qwTsFS0cz+xMzczBZkef5+M3s79XV/ofrNSOvwRSRERmf48YBn+NWF2IiZtQIbga4szzcD3wDaAAf2mNk2d+8rRP8TaIYvIiHSNLuGObVVkZnh/yXwVZJhnsnvAC+5+6lUyL8E3FGgvidK6OJpIhIeybX4swN/81XeqWhmm4Aed983SbNlQDztfnfqsUzbe8jM2s2svbe3d2ZFaYYvIiGTXJpZBod0zOxlYEmGpx4DvkbycE5BuPsWYAtAW1tbtlcMU2xEgS8i4dLSVM+u904FWkNOge/uGzI9bmY3ASuBfZa8dk0LsNfM1rv7sbSmPcDtafdbgFdmUG9uFPgiEjItTbM5m1qL31BfE0gNeaWiu7/u7ovcfYW7ryB5qGbtuLAH2AFsNLMmM2si+YpgRz59T16YAl9EwqW1Obk0M34quBO3RUtFM2szs+8CuPsp4FvA7tTXN1OPFYcCX0RCZnRpZpDH8QuyLHNUapY/ersdeDDt/pPAk4Xsb5JCFPgiEiphePNVNFNR18MXkZBpqK9hbl11oDP8CAd+NHdNRMrTB9fFV+AXlgJfREIo+eYrHdIpLAW+iITQ6AzffWZvMcpXNFNRgS8iIdTSVM+5S8FdFz+aqei6lo6IhE/QSzOjmYqegFhV0FWIiIwR9NLMiAa+1uGLSPi0jl4XP6APNI9mKmodvoiEUMPsGubNqtYMv6B00lZEQirI6+JHMxUV+CISUkG++SqaqajAF5GQSgb+QCBr8aOZigp8EQmplqbZnB8coX+g9Gvxo5mKCnwRCanW1NLMeAAnbqOZigp8EQmpIN98Fc1U1Dp8EQmpZQG++SqaqagZvoiEVEN9DfNnBXNd/GimoieCrkBEJKug1uJHM/CtKnlYR0QkhFqb6wP5MPNoBn6sChLDQVchIpLR6Ay/1GvxFfgiIiXW0lTPhaERTp0fLGm/EQ386uQ18UVEQiiopZnRDXzN8EUkpD64Lr4CP39WBQnN8EUknFoCerdtQQLfzP7EzNzMFmR5fsTMOlNf2wrR56RiCnwRCa95s2ponF1T8jdfVee7ATNrBTYCXZM0u+Duq/PtK2c6pCMiIRfEZZILMcP/S+CrQHgWvmuVjoiEXEtj6d98lVfgm9kmoMfd903RdJaZtZvZTjO7a4ptPpRq297b2zuzwmLVeretiIRaENfFn/KQjpm9DCzJ8NRjwNdIHs6ZylXu3mNmHwJ+bmavu/u7mRq6+xZgC0BbW9vMfhOa4YtIyLU01XNxKMHJ84MsmFtXkj6nDHx335DpcTO7CVgJ7LPkB4a3AHvNbL27Hxu3jZ7U90Nm9gqwBsgY+AWhY/giEnKtzcm1+PFTAyUL/Bkf0nH31919kbuvcPcVQDewdnzYm1mTmdWlbi8AbgXeyKPmqZlm+CISbksbkkszj/RfLFmfRVmHb2ZtZvbd1N3rgXYz2wf8M/C4uxc38GPVWpYpIqHWlbp42pKGWSXrM+9lmaNSs/zR2+3Ag6nbvwRuKlQ/OdE6fBEJub1dfdRWxbhx2fyS9RnNd9rqpK2IhNzew33c1NJAXXVVyfqMaODr4mkiEl6Xhkd4rec0H7mqqaT9RjfwNcMXkZA6cOQMg8MJ1i5X4OdPF08TkRDbe7gPgLVXNZa032gGvk7aikiI7TncR2tzPYvmlW6FDkQ28HVIR0TCyd1pP9zHR0p8OAcU+CIiJdXdd4Hes5dKfsIWIhv4OqQjIuG0t2v0+L0CvzBiVVqWKSKhtOdwH3Nqq7h28byS9x3RwNchHREJpz2H+1i9vJHqqtLHbzQDXxdPE5EQOn9pmDePngnkhC0U8Fo6oaKLp4XO8TMXiZ8aIBYzqsyoin3wFRu9b0ZVVfJ7LMbldrGYUT2uXSxmQe+SyLTt6+4n4cEcv4eoBv7tj8KtfxR0FZJm++tH+W8/LuxFUi8PGqMDgzFmIBkdGKrGDTKxtIGkyqA6FksOMOMHn1h6uw9+tnr0sdSgFBs3WI3ZzvhaLtcYoyrGmHZjtm0T+65K9RlLr6XKxtaQdYBM28/U46nPsZASGn3D1RrN8AtodnPQFcg4G39rCasWzmXEnUTCGRn98uT3hDsjCUgknOHU46PtEqk2w4nUY/7B98uPJbjcbsSdkZGx7Ua3Mzziae0YU8vQSGJMf+k1ftAvDCcSY/qbUIs7JfzUuhkzY+xgkjZAjg4+yQGECYNKxldnYwZIsra7PCCNG0irYlweCNP7G20/cbAeO/iOPledoV32WpID4fiBtGpcjZknDkx70NxzuI8PL55LQ31Nkf6qk4tm4EvoLGusZ1ljfdBllIz72AEtOTAwdvBJpA1+6YNY2qA0OhAOJxKXfz79Zy5ve9ztsdtmzMA34WcyDpAwMm5gG30+fWAbOyAnuDQ8diBNeIb2GWthwiSgHIy+qhz7ym/cwJY2SHT3DfC5j7QEVq8CX6QILHW4Rf/BZm7CwODjB8ixA+Gkr84yDpBMsu3UQJXxZ5ID4dhXfqlXponxg3X6gAc3Lmvg8//uqsB+p/r3KCKhFIsZMYya0l0uPvKiuSxTREQmUOCLiFQIBb6ISIVQ4IuIVAgFvohIhVDgi4hUCAW+iEiFUOCLiFQI8xBf9MPMeoHDOTZfALxfxHKCov0qL1HdL4juvkVtv65y94WZngh14E+HmbW7e1vQdRSa9qu8RHW/ILr7FtX9ykSHdEREKoQCX0SkQkQp8LcEXUCRaL/KS1T3C6K7b1HdrwkicwxfREQmF6UZvoiITEKBLyJSIco28M2s2cxeMrO3U98zfiqwmY2YWWfqa1up68yVmd1hZr82s3fM7NEMz9eZ2TOp539lZisCKHPactivB8ysN+1v9GAQdU6XmT1pZifMbH+W583M/ndqv18zs7WlrnEmctiv283sdNrf6+ulrnG6zKzVzP7ZzN4wswNm9kcZ2pTl32va3L0sv4C/AB5N3X4U+B9Z2p0LutYc9qUKeBf4EFAL7ANuGNfmD4HvpG5vBp4Juu4C7dcDwLeDrnUG+/YxYC2wP8vznwJ+AhjwUeBXQddcoP26HXgh6DqnuU9LgbWp2/OAgxn+HZbl32u6X2U7wwc2Ad9L3f4ecFdwpeRtPfCOux9y90FgK8n9S5e+v88BnzAzK2GNM5HLfpUld38VODVJk03A9z1pJ9BoZktLU93M5bBfZcfdj7r73tTts8CbwLJxzcry7zVd5Rz4i939aOr2MWBxlnazzKzdzHaa2V2lKW3algHxtPvdTPwHebmNuw8Dp4ErSlLdzOWyXwB3p15GP2dmraUprehy3fdy9O/NbJ+Z/cTMfivoYqYjdSh0DfCrcU9F+e91Wag/xNzMXgaWZHjqsfQ77u5mlm196VXu3mNmHwJ+bmavu/u7ha5VZuzHwNPufsnMvkzyVczHA65JsttL8v/UOTP7FPCPwDXBlpQbM5sL/APwn939TND1BCHUge/uG7I9Z2bHzWypux9NvfQ6kWUbPanvh8zsFZKje9gCvwdIn9m2pB7L1KbbzKqBBuBkacqbsSn3y93T9+G7JM/NREEuf9Oykx6U7r7dzP7GzBa4e6gvPmZmNSTD/il3/2GGJpH8e41Xzod0tgH3p27fDzw/voGZNZlZXer2AuBW4I2SVZi73cA1ZrbSzGpJnpQdv6IofX8/B/zcU2ebQmzK/Rp3nPROksdXo2Ab8Aep1R8fBU6nHYIsW2a2ZPTckZmtJ5khoZ54pOr9P8Cb7v4/szSL5N9rvFDP8KfwOPCsmX2J5CWU7wEwszbgYXd/ELge+FszS5D8h/m4u4cu8N192MweAXaQXNnypLsfMLNvAu3uvo3kP9i/N7N3SJ5U2xxcxbnJcb++YmZ3AsMk9+uBwAqeBjN7muSKlQVm1g18A6gBcPfvANtJrvx4BxgAvhBMpdOTw359DvhPZjYMXAA2l8HE41bgPwKvm1ln6rGvAcuhvP9e06VLK4iIVIhyPqQjIiLToMAXEakQCnwRkQqhwBcRqRAKfBGRCqHAFxGpEAp8EZEK8f8B7OdrDMTpO38AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
<<<<<<< HEAD
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:66: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
>>>>>>> master
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 5701 loss is tensor([-0.2323], grad_fn=<AddBackward0>)\n",
      "epoch: 5702 loss is tensor([-0.1721], grad_fn=<AddBackward0>)\n",
      "epoch: 5703 loss is tensor([-0.2246], grad_fn=<AddBackward0>)\n",
      "epoch: 5704 loss is tensor([-0.2253], grad_fn=<AddBackward0>)\n",
      "epoch: 5705 loss is tensor([-0.1891], grad_fn=<AddBackward0>)\n",
      "epoch: 5706 loss is tensor([-0.1868], grad_fn=<AddBackward0>)\n",
      "epoch: 5707 loss is tensor([-0.2310], grad_fn=<AddBackward0>)\n",
      "epoch: 5708 loss is tensor([-0.2057], grad_fn=<AddBackward0>)\n",
      "epoch: 5709 loss is tensor([-0.2048], grad_fn=<AddBackward0>)\n",
      "epoch: 5710 loss is tensor([-0.2125], grad_fn=<AddBackward0>)\n",
      "epoch: 5711 loss is tensor([-0.2665], grad_fn=<AddBackward0>)\n",
      "epoch: 5712 loss is tensor([-0.2056], grad_fn=<AddBackward0>)\n",
      "epoch: 5713 loss is tensor([-0.1931], grad_fn=<AddBackward0>)\n",
      "epoch: 5714 loss is tensor([-0.2692], grad_fn=<AddBackward0>)\n",
      "epoch: 5715 loss is tensor([-0.2204], grad_fn=<AddBackward0>)\n",
      "epoch: 5716 loss is tensor([-0.1625], grad_fn=<AddBackward0>)\n",
      "epoch: 5717 loss is tensor([-0.2518], grad_fn=<AddBackward0>)\n",
      "epoch: 5718 loss is tensor([-0.2243], grad_fn=<AddBackward0>)\n",
      "epoch: 5719 loss is tensor([-0.2156], grad_fn=<AddBackward0>)\n",
      "epoch: 5720 loss is tensor([-0.2511], grad_fn=<AddBackward0>)\n",
      "epoch: 5721 loss is tensor([-0.2300], grad_fn=<AddBackward0>)\n",
      "epoch: 5722 loss is tensor([-0.2470], grad_fn=<AddBackward0>)\n",
      "epoch: 5723 loss is tensor([-0.1796], grad_fn=<AddBackward0>)\n",
      "epoch: 5724 loss is tensor([-0.2777], grad_fn=<AddBackward0>)\n",
      "epoch: 5725 loss is tensor([-0.2165], grad_fn=<AddBackward0>)\n",
      "epoch: 5726 loss is tensor([-0.1827], grad_fn=<AddBackward0>)\n",
      "epoch: 5727 loss is tensor([-0.2482], grad_fn=<AddBackward0>)\n",
      "epoch: 5728 loss is tensor([-0.2787], grad_fn=<AddBackward0>)\n",
      "epoch: 5729 loss is tensor([-0.1879], grad_fn=<AddBackward0>)\n",
      "epoch: 5730 loss is tensor([-0.2455], grad_fn=<AddBackward0>)\n",
      "epoch: 5731 loss is tensor([-0.2162], grad_fn=<AddBackward0>)\n",
      "epoch: 5732 loss is tensor([-0.2710], grad_fn=<AddBackward0>)\n",
      "epoch: 5733 loss is tensor([-0.2266], grad_fn=<AddBackward0>)\n",
      "epoch: 5734 loss is tensor([-0.2462], grad_fn=<AddBackward0>)\n",
      "epoch: 5735 loss is tensor([-0.2084], grad_fn=<AddBackward0>)\n",
      "epoch: 5736 loss is tensor([-0.1619], grad_fn=<AddBackward0>)\n",
      "epoch: 5737 loss is tensor([-0.2285], grad_fn=<AddBackward0>)\n",
      "epoch: 5738 loss is tensor([-0.2157], grad_fn=<AddBackward0>)\n",
      "epoch: 5739 loss is tensor([-0.1954], grad_fn=<AddBackward0>)\n",
      "epoch: 5740 loss is tensor([-0.1767], grad_fn=<AddBackward0>)\n",
      "epoch: 5741 loss is tensor([-0.1983], grad_fn=<AddBackward0>)\n",
      "epoch: 5742 loss is tensor([-0.1552], grad_fn=<AddBackward0>)\n",
      "epoch: 5743 loss is tensor([-0.2505], grad_fn=<AddBackward0>)\n",
      "epoch: 5744 loss is tensor([-0.2744], grad_fn=<AddBackward0>)\n",
      "epoch: 5745 loss is tensor([-0.2140], grad_fn=<AddBackward0>)\n",
      "epoch: 5746 loss is tensor([-0.2368], grad_fn=<AddBackward0>)\n",
      "epoch: 5747 loss is tensor([-0.2344], grad_fn=<AddBackward0>)\n",
      "epoch: 5748 loss is tensor([-0.2493], grad_fn=<AddBackward0>)\n",
      "epoch: 5749 loss is tensor([-0.2131], grad_fn=<AddBackward0>)\n",
      "epoch: 5750 loss is tensor([-0.2432], grad_fn=<AddBackward0>)\n",
      "epoch: 5751 loss is tensor([-0.2205], grad_fn=<AddBackward0>)\n",
      "epoch: 5752 loss is tensor([-0.2321], grad_fn=<AddBackward0>)\n",
      "epoch: 5753 loss is tensor([-0.2188], grad_fn=<AddBackward0>)\n",
      "epoch: 5754 loss is tensor([-0.2709], grad_fn=<AddBackward0>)\n",
      "epoch: 5755 loss is tensor([-0.2213], grad_fn=<AddBackward0>)\n",
      "epoch: 5756 loss is tensor([-0.2437], grad_fn=<AddBackward0>)\n",
      "epoch: 5757 loss is tensor([-0.2444], grad_fn=<AddBackward0>)\n",
      "epoch: 5758 loss is tensor([-0.1970], grad_fn=<AddBackward0>)\n",
      "epoch: 5759 loss is tensor([-0.2279], grad_fn=<AddBackward0>)\n",
      "epoch: 5760 loss is tensor([-0.2733], grad_fn=<AddBackward0>)\n",
      "epoch: 5761 loss is tensor([-0.2027], grad_fn=<AddBackward0>)\n",
      "epoch: 5762 loss is tensor([-0.2254], grad_fn=<AddBackward0>)\n",
      "epoch: 5763 loss is tensor([-0.2348], grad_fn=<AddBackward0>)\n",
      "epoch: 5764 loss is tensor([-0.1951], grad_fn=<AddBackward0>)\n",
      "epoch: 5765 loss is tensor([-0.1799], grad_fn=<AddBackward0>)\n",
      "epoch: 5766 loss is tensor([-0.2223], grad_fn=<AddBackward0>)\n",
      "epoch: 5767 loss is tensor([-0.1605], grad_fn=<AddBackward0>)\n",
      "epoch: 5768 loss is tensor([-0.2176], grad_fn=<AddBackward0>)\n",
      "epoch: 5769 loss is tensor([-0.2037], grad_fn=<AddBackward0>)\n",
      "epoch: 5770 loss is tensor([-0.2212], grad_fn=<AddBackward0>)\n",
      "epoch: 5771 loss is tensor([-0.2537], grad_fn=<AddBackward0>)\n",
      "epoch: 5772 loss is tensor([-0.2675], grad_fn=<AddBackward0>)\n",
      "epoch: 5773 loss is tensor([-0.2387], grad_fn=<AddBackward0>)\n",
      "epoch: 5774 loss is tensor([-0.1979], grad_fn=<AddBackward0>)\n",
      "epoch: 5775 loss is tensor([-0.2292], grad_fn=<AddBackward0>)\n",
      "epoch: 5776 loss is tensor([-0.2330], grad_fn=<AddBackward0>)\n",
      "epoch: 5777 loss is tensor([-0.1818], grad_fn=<AddBackward0>)\n",
      "epoch: 5778 loss is tensor([-0.2311], grad_fn=<AddBackward0>)\n",
      "epoch: 5779 loss is tensor([-0.2132], grad_fn=<AddBackward0>)\n",
      "epoch: 5780 loss is tensor([-0.2710], grad_fn=<AddBackward0>)\n",
      "epoch: 5781 loss is tensor([-0.2197], grad_fn=<AddBackward0>)\n",
      "epoch: 5782 loss is tensor([-0.2580], grad_fn=<AddBackward0>)\n",
      "epoch: 5783 loss is tensor([-0.1923], grad_fn=<AddBackward0>)\n",
      "epoch: 5784 loss is tensor([-0.2282], grad_fn=<AddBackward0>)\n",
      "epoch: 5785 loss is tensor([-0.2425], grad_fn=<AddBackward0>)\n",
      "epoch: 5786 loss is tensor([-0.1423], grad_fn=<AddBackward0>)\n",
      "epoch: 5787 loss is tensor([-0.0902], grad_fn=<AddBackward0>)\n",
      "epoch: 5788 loss is tensor([-0.2385], grad_fn=<AddBackward0>)\n",
      "epoch: 5789 loss is tensor([-0.2156], grad_fn=<AddBackward0>)\n",
      "epoch: 5790 loss is tensor([-0.1611], grad_fn=<AddBackward0>)\n",
      "epoch: 5791 loss is tensor([-0.1762], grad_fn=<AddBackward0>)\n",
      "epoch: 5792 loss is tensor([-0.2098], grad_fn=<AddBackward0>)\n",
      "epoch: 5793 loss is tensor([-0.2199], grad_fn=<AddBackward0>)\n",
      "epoch: 5794 loss is tensor([-0.1355], grad_fn=<AddBackward0>)\n",
      "epoch: 5795 loss is tensor([-0.2312], grad_fn=<AddBackward0>)\n",
      "epoch: 5796 loss is tensor([-0.2177], grad_fn=<AddBackward0>)\n",
      "epoch: 5797 loss is tensor([-0.2453], grad_fn=<AddBackward0>)\n",
      "epoch: 5798 loss is tensor([-0.2075], grad_fn=<AddBackward0>)\n",
      "epoch: 5799 loss is tensor([-0.2823], grad_fn=<AddBackward0>)\n",
      "epoch: 5800 loss is tensor([-0.2203], grad_fn=<AddBackward0>)\n",
      "28\n"
=======
      "The number of epochs is: 5001\n",
      "The number of epochs is: 5002\n",
      "The number of epochs is: 5003\n",
      "The number of epochs is: 5004\n",
      "The number of epochs is: 5005\n",
      "The number of epochs is: 5006\n",
      "The number of epochs is: 5007\n",
      "The number of epochs is: 5008\n",
      "The number of epochs is: 5009\n",
      "The number of epochs is: 5010\n",
      "The number of epochs is: 5011\n",
      "The number of epochs is: 5012\n",
      "The number of epochs is: 5013\n",
      "The number of epochs is: 5014\n",
      "The number of epochs is: 5015\n",
      "The number of epochs is: 5016\n",
      "The number of epochs is: 5017\n",
      "The number of epochs is: 5018\n",
      "The number of epochs is: 5019\n",
      "The number of epochs is: 5020\n",
      "The number of epochs is: 5021\n",
      "The number of epochs is: 5022\n",
      "The number of epochs is: 5023\n",
      "The number of epochs is: 5024\n",
      "The number of epochs is: 5025\n",
      "The number of epochs is: 5026\n",
      "The number of epochs is: 5027\n",
      "The number of epochs is: 5028\n",
      "The number of epochs is: 5029\n",
      "The number of epochs is: 5030\n",
      "The number of epochs is: 5031\n",
      "The number of epochs is: 5032\n",
      "The number of epochs is: 5033\n",
      "The number of epochs is: 5034\n",
      "The number of epochs is: 5035\n",
      "The number of epochs is: 5036\n",
      "The number of epochs is: 5037\n",
      "The number of epochs is: 5038\n",
      "The number of epochs is: 5039\n",
      "The number of epochs is: 5040\n",
      "The number of epochs is: 5041\n",
      "The number of epochs is: 5042\n",
      "The number of epochs is: 5043\n",
      "The number of epochs is: 5044\n",
      "The number of epochs is: 5045\n",
      "The number of epochs is: 5046\n",
      "The number of epochs is: 5047\n",
      "The number of epochs is: 5048\n",
      "The number of epochs is: 5049\n",
      "The number of epochs is: 5050\n",
      "The number of epochs is: 5051\n",
      "The number of epochs is: 5052\n",
      "The number of epochs is: 5053\n",
      "The number of epochs is: 5054\n",
      "The number of epochs is: 5055\n",
      "The number of epochs is: 5056\n",
      "The number of epochs is: 5057\n",
      "The number of epochs is: 5058\n",
      "The number of epochs is: 5059\n",
      "The number of epochs is: 5060\n",
      "The number of epochs is: 5061\n",
      "The number of epochs is: 5062\n",
      "The number of epochs is: 5063\n",
      "The number of epochs is: 5064\n",
      "The number of epochs is: 5065\n",
      "The number of epochs is: 5066\n",
      "The number of epochs is: 5067\n",
      "The number of epochs is: 5068\n",
      "The number of epochs is: 5069\n",
      "The number of epochs is: 5070\n",
      "The number of epochs is: 5071\n",
      "The number of epochs is: 5072\n",
      "The number of epochs is: 5073\n",
      "The number of epochs is: 5074\n",
      "The number of epochs is: 5075\n",
      "The number of epochs is: 5076\n",
      "The number of epochs is: 5077\n",
      "The number of epochs is: 5078\n",
      "The number of epochs is: 5079\n",
      "The number of epochs is: 5080\n",
      "The number of epochs is: 5081\n",
      "The number of epochs is: 5082\n",
      "The number of epochs is: 5083\n",
      "The number of epochs is: 5084\n",
      "The number of epochs is: 5085\n",
      "The number of epochs is: 5086\n",
      "The number of epochs is: 5087\n",
      "The number of epochs is: 5088\n",
      "The number of epochs is: 5089\n",
      "The number of epochs is: 5090\n",
      "The number of epochs is: 5091\n",
      "The number of epochs is: 5092\n",
      "The number of epochs is: 5093\n",
      "The number of epochs is: 5094\n",
      "The number of epochs is: 5095\n",
      "The number of epochs is: 5096\n",
      "The number of epochs is: 5097\n",
      "The number of epochs is: 5098\n",
      "The number of epochs is: 5099\n",
      "The number of epochs is: 5100\n",
      "The number of epochs is: 5101\n",
      "The number of epochs is: 5102\n",
      "The number of epochs is: 5103\n",
      "The number of epochs is: 5104\n",
      "The number of epochs is: 5105\n",
      "The number of epochs is: 5106\n",
      "The number of epochs is: 5107\n",
      "The number of epochs is: 5108\n",
      "The number of epochs is: 5109\n",
      "The number of epochs is: 5110\n",
      "The number of epochs is: 5111\n",
      "The number of epochs is: 5112\n",
      "The number of epochs is: 5113\n",
      "The number of epochs is: 5114\n",
      "The number of epochs is: 5115\n",
      "The number of epochs is: 5116\n",
      "The number of epochs is: 5117\n",
      "The number of epochs is: 5118\n",
      "The number of epochs is: 5119\n",
      "The number of epochs is: 5120\n",
      "The number of epochs is: 5121\n",
      "The number of epochs is: 5122\n",
      "The number of epochs is: 5123\n",
      "The number of epochs is: 5124\n",
      "The number of epochs is: 5125\n",
      "The number of epochs is: 5126\n",
      "The number of epochs is: 5127\n",
      "The number of epochs is: 5128\n",
      "The number of epochs is: 5129\n",
      "The number of epochs is: 5130\n",
      "The number of epochs is: 5131\n",
      "The number of epochs is: 5132\n",
      "The number of epochs is: 5133\n",
      "The number of epochs is: 5134\n",
      "The number of epochs is: 5135\n",
      "The number of epochs is: 5136\n",
      "The number of epochs is: 5137\n",
      "The number of epochs is: 5138\n",
      "The number of epochs is: 5139\n",
      "The number of epochs is: 5140\n",
      "The number of epochs is: 5141\n",
      "The number of epochs is: 5142\n",
      "The number of epochs is: 5143\n",
      "The number of epochs is: 5144\n",
      "The number of epochs is: 5145\n",
      "The number of epochs is: 5146\n",
      "The number of epochs is: 5147\n",
      "The number of epochs is: 5148\n",
      "The number of epochs is: 5149\n",
      "The number of epochs is: 5150\n",
      "The number of epochs is: 5151\n",
      "The number of epochs is: 5152\n",
      "The number of epochs is: 5153\n",
      "The number of epochs is: 5154\n",
      "The number of epochs is: 5155\n",
      "The number of epochs is: 5156\n",
      "The number of epochs is: 5157\n",
      "The number of epochs is: 5158\n",
      "The number of epochs is: 5159\n",
      "The number of epochs is: 5160\n",
      "The number of epochs is: 5161\n",
      "The number of epochs is: 5162\n",
      "The number of epochs is: 5163\n",
      "The number of epochs is: 5164\n",
      "The number of epochs is: 5165\n",
      "The number of epochs is: 5166\n",
      "The number of epochs is: 5167\n",
      "The number of epochs is: 5168\n",
      "The number of epochs is: 5169\n",
      "The number of epochs is: 5170\n",
      "The number of epochs is: 5171\n",
      "The number of epochs is: 5172\n",
      "The number of epochs is: 5173\n",
      "The number of epochs is: 5174\n",
      "The number of epochs is: 5175\n",
      "The number of epochs is: 5176\n",
      "The number of epochs is: 5177\n",
      "The number of epochs is: 5178\n",
      "The number of epochs is: 5179\n",
      "The number of epochs is: 5180\n",
      "The number of epochs is: 5181\n",
      "The number of epochs is: 5182\n",
      "The number of epochs is: 5183\n",
      "The number of epochs is: 5184\n",
      "The number of epochs is: 5185\n",
      "The number of epochs is: 5186\n",
      "The number of epochs is: 5187\n",
      "The number of epochs is: 5188\n",
      "The number of epochs is: 5189\n",
      "The number of epochs is: 5190\n",
      "The number of epochs is: 5191\n",
      "The number of epochs is: 5192\n",
      "The number of epochs is: 5193\n",
      "The number of epochs is: 5194\n",
      "The number of epochs is: 5195\n",
      "The number of epochs is: 5196\n",
      "The number of epochs is: 5197\n",
      "The number of epochs is: 5198\n",
      "The number of epochs is: 5199\n",
      "The number of epochs is: 5200\n",
      "38\n",
      "Outputting sketch\n"
>>>>>>> master
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5801 loss is tensor([-0.2749], grad_fn=<AddBackward0>)\n",
      "epoch: 5802 loss is tensor([-0.2059], grad_fn=<AddBackward0>)\n",
      "epoch: 5803 loss is tensor([-0.2224], grad_fn=<AddBackward0>)\n",
      "epoch: 5804 loss is tensor([-0.2516], grad_fn=<AddBackward0>)\n",
      "epoch: 5805 loss is tensor([-0.1953], grad_fn=<AddBackward0>)\n",
      "epoch: 5806 loss is tensor([-0.1973], grad_fn=<AddBackward0>)\n",
      "epoch: 5807 loss is tensor([-0.1922], grad_fn=<AddBackward0>)\n",
      "epoch: 5808 loss is tensor([-0.1604], grad_fn=<AddBackward0>)\n",
      "epoch: 5809 loss is tensor([-0.1868], grad_fn=<AddBackward0>)\n",
      "epoch: 5810 loss is tensor([-0.2379], grad_fn=<AddBackward0>)\n",
      "epoch: 5811 loss is tensor([-0.2367], grad_fn=<AddBackward0>)\n",
      "epoch: 5812 loss is tensor([-0.2008], grad_fn=<AddBackward0>)\n",
      "epoch: 5813 loss is tensor([-0.1801], grad_fn=<AddBackward0>)\n",
      "epoch: 5814 loss is tensor([-0.2077], grad_fn=<AddBackward0>)\n",
      "epoch: 5815 loss is tensor([-0.2074], grad_fn=<AddBackward0>)\n",
      "epoch: 5816 loss is tensor([-0.2001], grad_fn=<AddBackward0>)\n",
      "epoch: 5817 loss is tensor([-0.2266], grad_fn=<AddBackward0>)\n",
      "epoch: 5818 loss is tensor([-0.1961], grad_fn=<AddBackward0>)\n",
      "epoch: 5819 loss is tensor([-0.2839], grad_fn=<AddBackward0>)\n",
      "epoch: 5820 loss is tensor([-0.2566], grad_fn=<AddBackward0>)\n",
      "epoch: 5821 loss is tensor([-0.1150], grad_fn=<AddBackward0>)\n",
      "epoch: 5822 loss is tensor([-0.3024], grad_fn=<AddBackward0>)\n",
      "epoch: 5823 loss is tensor([-0.2049], grad_fn=<AddBackward0>)\n",
      "epoch: 5824 loss is tensor([-0.2796], grad_fn=<AddBackward0>)\n",
      "epoch: 5825 loss is tensor([-0.2729], grad_fn=<AddBackward0>)\n",
      "epoch: 5826 loss is tensor([-0.2133], grad_fn=<AddBackward0>)\n",
      "epoch: 5827 loss is tensor([-0.2543], grad_fn=<AddBackward0>)\n",
      "epoch: 5828 loss is tensor([-0.2987], grad_fn=<AddBackward0>)\n",
      "epoch: 5829 loss is tensor([-0.2522], grad_fn=<AddBackward0>)\n",
      "epoch: 5830 loss is tensor([-0.2845], grad_fn=<AddBackward0>)\n",
      "epoch: 5831 loss is tensor([-0.2262], grad_fn=<AddBackward0>)\n",
      "epoch: 5832 loss is tensor([-0.2357], grad_fn=<AddBackward0>)\n",
      "epoch: 5833 loss is tensor([-0.2008], grad_fn=<AddBackward0>)\n",
      "epoch: 5834 loss is tensor([-0.1955], grad_fn=<AddBackward0>)\n",
      "epoch: 5835 loss is tensor([-0.2355], grad_fn=<AddBackward0>)\n",
      "epoch: 5836 loss is tensor([-0.2830], grad_fn=<AddBackward0>)\n",
      "epoch: 5837 loss is tensor([-0.2669], grad_fn=<AddBackward0>)\n",
      "epoch: 5838 loss is tensor([-0.2170], grad_fn=<AddBackward0>)\n",
      "epoch: 5839 loss is tensor([-0.2506], grad_fn=<AddBackward0>)\n",
      "epoch: 5840 loss is tensor([-0.2798], grad_fn=<AddBackward0>)\n",
      "epoch: 5841 loss is tensor([-0.1851], grad_fn=<AddBackward0>)\n",
      "epoch: 5842 loss is tensor([-0.2933], grad_fn=<AddBackward0>)\n",
      "epoch: 5843 loss is tensor([-0.2408], grad_fn=<AddBackward0>)\n",
      "epoch: 5844 loss is tensor([-0.2373], grad_fn=<AddBackward0>)\n",
      "epoch: 5845 loss is tensor([-0.2064], grad_fn=<AddBackward0>)\n",
      "epoch: 5846 loss is tensor([-0.2401], grad_fn=<AddBackward0>)\n",
      "epoch: 5847 loss is tensor([-0.2746], grad_fn=<AddBackward0>)\n",
      "epoch: 5848 loss is tensor([-0.2232], grad_fn=<AddBackward0>)\n",
      "epoch: 5849 loss is tensor([-0.2320], grad_fn=<AddBackward0>)\n",
      "epoch: 5850 loss is tensor([-0.2079], grad_fn=<AddBackward0>)\n",
      "epoch: 5851 loss is tensor([-0.0860], grad_fn=<AddBackward0>)\n",
      "epoch: 5852 loss is tensor([-0.2231], grad_fn=<AddBackward0>)\n",
      "epoch: 5853 loss is tensor([-0.1852], grad_fn=<AddBackward0>)\n",
      "epoch: 5854 loss is tensor([-0.2067], grad_fn=<AddBackward0>)\n",
      "epoch: 5855 loss is tensor([-0.1556], grad_fn=<AddBackward0>)\n",
      "epoch: 5856 loss is tensor([-0.1765], grad_fn=<AddBackward0>)\n",
      "epoch: 5857 loss is tensor([-0.1714], grad_fn=<AddBackward0>)\n",
      "epoch: 5858 loss is tensor([-0.2224], grad_fn=<AddBackward0>)\n",
      "epoch: 5859 loss is tensor([-0.2372], grad_fn=<AddBackward0>)\n",
      "epoch: 5860 loss is tensor([-0.2122], grad_fn=<AddBackward0>)\n",
      "epoch: 5861 loss is tensor([-0.1633], grad_fn=<AddBackward0>)\n",
      "epoch: 5862 loss is tensor([-0.2485], grad_fn=<AddBackward0>)\n",
      "epoch: 5863 loss is tensor([-0.1299], grad_fn=<AddBackward0>)\n",
      "epoch: 5864 loss is tensor([-0.1320], grad_fn=<AddBackward0>)\n",
      "epoch: 5865 loss is tensor([-0.1389], grad_fn=<AddBackward0>)\n",
      "epoch: 5866 loss is tensor([-0.1505], grad_fn=<AddBackward0>)\n",
      "epoch: 5867 loss is tensor([-0.2148], grad_fn=<AddBackward0>)\n",
      "epoch: 5868 loss is tensor([-0.1580], grad_fn=<AddBackward0>)\n",
      "epoch: 5869 loss is tensor([-0.2182], grad_fn=<AddBackward0>)\n",
      "epoch: 5870 loss is tensor([-0.1643], grad_fn=<AddBackward0>)\n",
      "epoch: 5871 loss is tensor([-0.1529], grad_fn=<AddBackward0>)\n",
      "epoch: 5872 loss is tensor([-0.1965], grad_fn=<AddBackward0>)\n",
      "epoch: 5873 loss is tensor([-0.1915], grad_fn=<AddBackward0>)\n",
      "epoch: 5874 loss is tensor([-0.1878], grad_fn=<AddBackward0>)\n",
      "epoch: 5875 loss is tensor([-0.1958], grad_fn=<AddBackward0>)\n",
      "epoch: 5876 loss is tensor([-0.1719], grad_fn=<AddBackward0>)\n",
      "epoch: 5877 loss is tensor([-0.1471], grad_fn=<AddBackward0>)\n",
      "epoch: 5878 loss is tensor([-0.2169], grad_fn=<AddBackward0>)\n",
      "epoch: 5879 loss is tensor([-0.2300], grad_fn=<AddBackward0>)\n",
      "epoch: 5880 loss is tensor([-0.2171], grad_fn=<AddBackward0>)\n",
      "epoch: 5881 loss is tensor([-0.1986], grad_fn=<AddBackward0>)\n",
      "epoch: 5882 loss is tensor([-0.2558], grad_fn=<AddBackward0>)\n",
      "epoch: 5883 loss is tensor([-0.2171], grad_fn=<AddBackward0>)\n",
      "epoch: 5884 loss is tensor([-0.2138], grad_fn=<AddBackward0>)\n",
      "epoch: 5885 loss is tensor([-0.2207], grad_fn=<AddBackward0>)\n",
      "epoch: 5886 loss is tensor([-0.1686], grad_fn=<AddBackward0>)\n",
      "epoch: 5887 loss is tensor([-0.1893], grad_fn=<AddBackward0>)\n",
      "epoch: 5888 loss is tensor([-0.2177], grad_fn=<AddBackward0>)\n",
      "epoch: 5889 loss is tensor([-0.1954], grad_fn=<AddBackward0>)\n",
      "epoch: 5890 loss is tensor([-0.2311], grad_fn=<AddBackward0>)\n",
      "epoch: 5891 loss is tensor([-0.2488], grad_fn=<AddBackward0>)\n",
      "epoch: 5892 loss is tensor([-0.1896], grad_fn=<AddBackward0>)\n",
      "epoch: 5893 loss is tensor([-0.2557], grad_fn=<AddBackward0>)\n",
      "epoch: 5894 loss is tensor([-0.2185], grad_fn=<AddBackward0>)\n",
      "epoch: 5895 loss is tensor([-0.2171], grad_fn=<AddBackward0>)\n",
      "epoch: 5896 loss is tensor([-0.2520], grad_fn=<AddBackward0>)\n",
      "epoch: 5897 loss is tensor([-0.1785], grad_fn=<AddBackward0>)\n",
      "epoch: 5898 loss is tensor([-0.1616], grad_fn=<AddBackward0>)\n",
      "epoch: 5899 loss is tensor([-0.1951], grad_fn=<AddBackward0>)\n",
      "epoch: 5900 loss is tensor([-0.1134], grad_fn=<AddBackward0>)\n",
      "12\n"
     ]
=======
      "/home/muna/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/ipykernel_launcher.py:204: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
>>>>>>> master
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "epoch: 5901 loss is tensor([-0.2270], grad_fn=<AddBackward0>)\n",
      "epoch: 5902 loss is tensor([-0.1672], grad_fn=<AddBackward0>)\n",
      "epoch: 5903 loss is tensor([-0.1895], grad_fn=<AddBackward0>)\n",
      "epoch: 5904 loss is tensor([-0.2208], grad_fn=<AddBackward0>)\n",
      "epoch: 5905 loss is tensor([-0.2577], grad_fn=<AddBackward0>)\n",
      "epoch: 5906 loss is tensor([-0.1757], grad_fn=<AddBackward0>)\n",
      "epoch: 5907 loss is tensor([-0.1962], grad_fn=<AddBackward0>)\n",
      "epoch: 5908 loss is tensor([-0.2216], grad_fn=<AddBackward0>)\n",
      "epoch: 5909 loss is tensor([-0.2398], grad_fn=<AddBackward0>)\n",
      "epoch: 5910 loss is tensor([-0.1710], grad_fn=<AddBackward0>)\n",
      "epoch: 5911 loss is tensor([-0.2399], grad_fn=<AddBackward0>)\n",
      "epoch: 5912 loss is tensor([-0.1724], grad_fn=<AddBackward0>)\n",
      "epoch: 5913 loss is tensor([-0.2195], grad_fn=<AddBackward0>)\n",
      "epoch: 5914 loss is tensor([-0.2203], grad_fn=<AddBackward0>)\n",
      "epoch: 5915 loss is tensor([-0.1563], grad_fn=<AddBackward0>)\n",
      "epoch: 5916 loss is tensor([-0.2114], grad_fn=<AddBackward0>)\n",
      "epoch: 5917 loss is tensor([-0.1842], grad_fn=<AddBackward0>)\n",
      "epoch: 5918 loss is tensor([-0.2188], grad_fn=<AddBackward0>)\n",
      "epoch: 5919 loss is tensor([-0.2513], grad_fn=<AddBackward0>)\n",
      "epoch: 5920 loss is tensor([-0.2153], grad_fn=<AddBackward0>)\n",
      "epoch: 5921 loss is tensor([-0.2330], grad_fn=<AddBackward0>)\n",
      "epoch: 5922 loss is tensor([-0.2168], grad_fn=<AddBackward0>)\n",
      "epoch: 5923 loss is tensor([-0.2381], grad_fn=<AddBackward0>)\n",
      "epoch: 5924 loss is tensor([-0.2117], grad_fn=<AddBackward0>)\n",
      "epoch: 5925 loss is tensor([-0.2012], grad_fn=<AddBackward0>)\n",
      "epoch: 5926 loss is tensor([-0.2383], grad_fn=<AddBackward0>)\n",
      "epoch: 5927 loss is tensor([-0.2593], grad_fn=<AddBackward0>)\n",
      "epoch: 5928 loss is tensor([-0.2549], grad_fn=<AddBackward0>)\n",
      "epoch: 5929 loss is tensor([-0.1846], grad_fn=<AddBackward0>)\n",
      "epoch: 5930 loss is tensor([-0.1717], grad_fn=<AddBackward0>)\n",
      "epoch: 5931 loss is tensor([-0.1725], grad_fn=<AddBackward0>)\n",
      "epoch: 5932 loss is tensor([-0.1455], grad_fn=<AddBackward0>)\n",
      "epoch: 5933 loss is tensor([-0.2358], grad_fn=<AddBackward0>)\n",
      "epoch: 5934 loss is tensor([-0.1427], grad_fn=<AddBackward0>)\n",
      "epoch: 5935 loss is tensor([-0.2028], grad_fn=<AddBackward0>)\n",
      "epoch: 5936 loss is tensor([-0.1751], grad_fn=<AddBackward0>)\n",
      "epoch: 5937 loss is tensor([-0.1768], grad_fn=<AddBackward0>)\n",
      "epoch: 5938 loss is tensor([-0.1803], grad_fn=<AddBackward0>)\n",
      "epoch: 5939 loss is tensor([-0.1782], grad_fn=<AddBackward0>)\n",
      "epoch: 5940 loss is tensor([-0.2507], grad_fn=<AddBackward0>)\n",
      "epoch: 5941 loss is tensor([-0.1441], grad_fn=<AddBackward0>)\n",
      "epoch: 5942 loss is tensor([-0.1961], grad_fn=<AddBackward0>)\n",
      "epoch: 5943 loss is tensor([-0.1460], grad_fn=<AddBackward0>)\n",
      "epoch: 5944 loss is tensor([-0.2048], grad_fn=<AddBackward0>)\n",
      "epoch: 5945 loss is tensor([-0.1629], grad_fn=<AddBackward0>)\n",
      "epoch: 5946 loss is tensor([-0.1139], grad_fn=<AddBackward0>)\n",
      "epoch: 5947 loss is tensor([-0.1692], grad_fn=<AddBackward0>)\n",
      "epoch: 5948 loss is tensor([-0.1629], grad_fn=<AddBackward0>)\n",
      "epoch: 5949 loss is tensor([-0.1889], grad_fn=<AddBackward0>)\n",
      "epoch: 5950 loss is tensor([-0.2381], grad_fn=<AddBackward0>)\n",
      "epoch: 5951 loss is tensor([-0.2014], grad_fn=<AddBackward0>)\n",
      "epoch: 5952 loss is tensor([-0.1407], grad_fn=<AddBackward0>)\n",
      "epoch: 5953 loss is tensor([-0.1100], grad_fn=<AddBackward0>)\n",
      "epoch: 5954 loss is tensor([-0.1596], grad_fn=<AddBackward0>)\n",
      "epoch: 5955 loss is tensor([-0.1632], grad_fn=<AddBackward0>)\n",
      "epoch: 5956 loss is tensor([-0.1527], grad_fn=<AddBackward0>)\n",
      "epoch: 5957 loss is tensor([-0.1641], grad_fn=<AddBackward0>)\n",
      "epoch: 5958 loss is tensor([-0.2203], grad_fn=<AddBackward0>)\n",
      "epoch: 5959 loss is tensor([-0.1951], grad_fn=<AddBackward0>)\n",
      "epoch: 5960 loss is tensor([-0.2203], grad_fn=<AddBackward0>)\n",
      "epoch: 5961 loss is tensor([-0.2070], grad_fn=<AddBackward0>)\n",
      "epoch: 5962 loss is tensor([-0.1887], grad_fn=<AddBackward0>)\n",
      "epoch: 5963 loss is tensor([-0.2014], grad_fn=<AddBackward0>)\n",
      "epoch: 5964 loss is tensor([-0.2104], grad_fn=<AddBackward0>)\n",
      "epoch: 5965 loss is tensor([-0.2471], grad_fn=<AddBackward0>)\n",
      "epoch: 5966 loss is tensor([-0.1825], grad_fn=<AddBackward0>)\n",
      "epoch: 5967 loss is tensor([-0.2408], grad_fn=<AddBackward0>)\n",
      "epoch: 5968 loss is tensor([-0.2156], grad_fn=<AddBackward0>)\n",
      "epoch: 5969 loss is tensor([-0.2293], grad_fn=<AddBackward0>)\n",
      "epoch: 5970 loss is tensor([-0.1705], grad_fn=<AddBackward0>)\n",
      "epoch: 5971 loss is tensor([-0.2243], grad_fn=<AddBackward0>)\n",
      "epoch: 5972 loss is tensor([-0.2489], grad_fn=<AddBackward0>)\n",
      "epoch: 5973 loss is tensor([-0.1335], grad_fn=<AddBackward0>)\n",
      "epoch: 5974 loss is tensor([-0.2535], grad_fn=<AddBackward0>)\n",
      "epoch: 5975 loss is tensor([-0.1509], grad_fn=<AddBackward0>)\n",
      "epoch: 5976 loss is tensor([-0.2136], grad_fn=<AddBackward0>)\n",
      "epoch: 5977 loss is tensor([-0.1846], grad_fn=<AddBackward0>)\n",
      "epoch: 5978 loss is tensor([-0.1885], grad_fn=<AddBackward0>)\n",
      "epoch: 5979 loss is tensor([-0.1993], grad_fn=<AddBackward0>)\n",
      "epoch: 5980 loss is tensor([-0.1226], grad_fn=<AddBackward0>)\n",
      "epoch: 5981 loss is tensor([-0.1966], grad_fn=<AddBackward0>)\n",
      "epoch: 5982 loss is tensor([-0.1639], grad_fn=<AddBackward0>)\n",
      "epoch: 5983 loss is tensor([-0.2033], grad_fn=<AddBackward0>)\n",
      "epoch: 5984 loss is tensor([-0.2290], grad_fn=<AddBackward0>)\n",
      "epoch: 5985 loss is tensor([-0.2145], grad_fn=<AddBackward0>)\n",
      "epoch: 5986 loss is tensor([-0.1847], grad_fn=<AddBackward0>)\n",
      "epoch: 5987 loss is tensor([-0.1783], grad_fn=<AddBackward0>)\n",
      "epoch: 5988 loss is tensor([-0.1561], grad_fn=<AddBackward0>)\n",
      "epoch: 5989 loss is tensor([-0.2064], grad_fn=<AddBackward0>)\n",
      "epoch: 5990 loss is tensor([-0.1577], grad_fn=<AddBackward0>)\n",
      "epoch: 5991 loss is tensor([-0.2166], grad_fn=<AddBackward0>)\n",
      "epoch: 5992 loss is tensor([-0.2166], grad_fn=<AddBackward0>)\n",
      "epoch: 5993 loss is tensor([-0.1133], grad_fn=<AddBackward0>)\n",
      "epoch: 5994 loss is tensor([-0.2278], grad_fn=<AddBackward0>)\n",
      "epoch: 5995 loss is tensor([-0.1586], grad_fn=<AddBackward0>)\n",
      "epoch: 5996 loss is tensor([-0.2299], grad_fn=<AddBackward0>)\n",
      "epoch: 5997 loss is tensor([-0.2083], grad_fn=<AddBackward0>)\n",
      "epoch: 5998 loss is tensor([-0.2264], grad_fn=<AddBackward0>)\n",
      "epoch: 5999 loss is tensor([-0.2330], grad_fn=<AddBackward0>)\n",
      "epoch: 6000 loss is tensor([-0.1867], grad_fn=<AddBackward0>)\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6001 loss is tensor([-0.1838], grad_fn=<AddBackward0>)\n",
      "epoch: 6002 loss is tensor([-0.1860], grad_fn=<AddBackward0>)\n",
      "epoch: 6003 loss is tensor([-0.1622], grad_fn=<AddBackward0>)\n",
      "epoch: 6004 loss is tensor([-0.1783], grad_fn=<AddBackward0>)\n",
      "epoch: 6005 loss is tensor([-0.2108], grad_fn=<AddBackward0>)\n",
      "epoch: 6006 loss is tensor([-0.1630], grad_fn=<AddBackward0>)\n",
      "epoch: 6007 loss is tensor([-0.1466], grad_fn=<AddBackward0>)\n",
      "epoch: 6008 loss is tensor([-0.2100], grad_fn=<AddBackward0>)\n",
      "epoch: 6009 loss is tensor([-0.1528], grad_fn=<AddBackward0>)\n",
      "epoch: 6010 loss is tensor([-0.2504], grad_fn=<AddBackward0>)\n",
      "epoch: 6011 loss is tensor([-0.1088], grad_fn=<AddBackward0>)\n",
      "epoch: 6012 loss is tensor([-0.2066], grad_fn=<AddBackward0>)\n",
      "epoch: 6013 loss is tensor([-0.1264], grad_fn=<AddBackward0>)\n",
      "epoch: 6014 loss is tensor([-0.1738], grad_fn=<AddBackward0>)\n",
      "epoch: 6015 loss is tensor([-0.1796], grad_fn=<AddBackward0>)\n",
      "epoch: 6016 loss is tensor([-0.2032], grad_fn=<AddBackward0>)\n",
      "epoch: 6017 loss is tensor([-0.2561], grad_fn=<AddBackward0>)\n",
      "epoch: 6018 loss is tensor([-0.1415], grad_fn=<AddBackward0>)\n",
      "epoch: 6019 loss is tensor([-0.2736], grad_fn=<AddBackward0>)\n",
      "epoch: 6020 loss is tensor([-0.2442], grad_fn=<AddBackward0>)\n",
      "epoch: 6021 loss is tensor([-0.2433], grad_fn=<AddBackward0>)\n",
      "epoch: 6022 loss is tensor([-0.2299], grad_fn=<AddBackward0>)\n",
      "epoch: 6023 loss is tensor([-0.2515], grad_fn=<AddBackward0>)\n",
      "epoch: 6024 loss is tensor([-0.1914], grad_fn=<AddBackward0>)\n",
      "epoch: 6025 loss is tensor([-0.2120], grad_fn=<AddBackward0>)\n",
      "epoch: 6026 loss is tensor([-0.1720], grad_fn=<AddBackward0>)\n",
      "epoch: 6027 loss is tensor([-0.2225], grad_fn=<AddBackward0>)\n",
      "epoch: 6028 loss is tensor([-0.2383], grad_fn=<AddBackward0>)\n",
      "epoch: 6029 loss is tensor([-0.2420], grad_fn=<AddBackward0>)\n",
      "epoch: 6030 loss is tensor([-0.2396], grad_fn=<AddBackward0>)\n",
      "epoch: 6031 loss is tensor([-0.1984], grad_fn=<AddBackward0>)\n",
      "epoch: 6032 loss is tensor([-0.1816], grad_fn=<AddBackward0>)\n",
      "epoch: 6033 loss is tensor([-0.2348], grad_fn=<AddBackward0>)\n",
      "epoch: 6034 loss is tensor([-0.1819], grad_fn=<AddBackward0>)\n",
      "epoch: 6035 loss is tensor([-0.2134], grad_fn=<AddBackward0>)\n",
      "epoch: 6036 loss is tensor([-0.2326], grad_fn=<AddBackward0>)\n",
      "epoch: 6037 loss is tensor([-0.3010], grad_fn=<AddBackward0>)\n",
      "epoch: 6038 loss is tensor([-0.2198], grad_fn=<AddBackward0>)\n",
      "epoch: 6039 loss is tensor([-0.2327], grad_fn=<AddBackward0>)\n",
      "epoch: 6040 loss is tensor([-0.2913], grad_fn=<AddBackward0>)\n",
      "epoch: 6041 loss is tensor([-0.2518], grad_fn=<AddBackward0>)\n",
      "epoch: 6042 loss is tensor([-0.2889], grad_fn=<AddBackward0>)\n",
      "epoch: 6043 loss is tensor([-0.1935], grad_fn=<AddBackward0>)\n",
      "epoch: 6044 loss is tensor([-0.1688], grad_fn=<AddBackward0>)\n",
      "epoch: 6045 loss is tensor([-0.2121], grad_fn=<AddBackward0>)\n",
      "epoch: 6046 loss is tensor([-0.2258], grad_fn=<AddBackward0>)\n",
      "epoch: 6047 loss is tensor([-0.1659], grad_fn=<AddBackward0>)\n",
      "epoch: 6048 loss is tensor([-0.1528], grad_fn=<AddBackward0>)\n",
      "epoch: 6049 loss is tensor([-0.1826], grad_fn=<AddBackward0>)\n",
      "epoch: 6050 loss is tensor([-0.1180], grad_fn=<AddBackward0>)\n",
      "epoch: 6051 loss is tensor([-0.1575], grad_fn=<AddBackward0>)\n",
      "epoch: 6052 loss is tensor([-0.1757], grad_fn=<AddBackward0>)\n",
      "epoch: 6053 loss is tensor([-0.1871], grad_fn=<AddBackward0>)\n",
      "epoch: 6054 loss is tensor([-0.1886], grad_fn=<AddBackward0>)\n",
      "epoch: 6055 loss is tensor([-0.2011], grad_fn=<AddBackward0>)\n",
      "epoch: 6056 loss is tensor([-0.1944], grad_fn=<AddBackward0>)\n",
      "epoch: 6057 loss is tensor([-0.1305], grad_fn=<AddBackward0>)\n",
      "epoch: 6058 loss is tensor([-0.2005], grad_fn=<AddBackward0>)\n",
      "epoch: 6059 loss is tensor([-0.2028], grad_fn=<AddBackward0>)\n",
      "epoch: 6060 loss is tensor([-0.1759], grad_fn=<AddBackward0>)\n",
      "epoch: 6061 loss is tensor([-0.1790], grad_fn=<AddBackward0>)\n",
      "epoch: 6062 loss is tensor([-0.1426], grad_fn=<AddBackward0>)\n",
      "epoch: 6063 loss is tensor([-0.1668], grad_fn=<AddBackward0>)\n",
      "epoch: 6064 loss is tensor([-0.1726], grad_fn=<AddBackward0>)\n",
      "epoch: 6065 loss is tensor([-0.1806], grad_fn=<AddBackward0>)\n",
      "epoch: 6066 loss is tensor([-0.1481], grad_fn=<AddBackward0>)\n",
      "epoch: 6067 loss is tensor([-0.2170], grad_fn=<AddBackward0>)\n",
      "epoch: 6068 loss is tensor([-0.1473], grad_fn=<AddBackward0>)\n",
      "epoch: 6069 loss is tensor([-0.1885], grad_fn=<AddBackward0>)\n",
      "epoch: 6070 loss is tensor([-0.2014], grad_fn=<AddBackward0>)\n",
      "epoch: 6071 loss is tensor([-0.2146], grad_fn=<AddBackward0>)\n",
      "epoch: 6072 loss is tensor([-0.2594], grad_fn=<AddBackward0>)\n",
      "epoch: 6073 loss is tensor([-0.2029], grad_fn=<AddBackward0>)\n",
      "epoch: 6074 loss is tensor([-0.2174], grad_fn=<AddBackward0>)\n",
      "epoch: 6075 loss is tensor([-0.2161], grad_fn=<AddBackward0>)\n",
      "epoch: 6076 loss is tensor([-0.2515], grad_fn=<AddBackward0>)\n",
      "epoch: 6077 loss is tensor([-0.1793], grad_fn=<AddBackward0>)\n",
      "epoch: 6078 loss is tensor([-0.2456], grad_fn=<AddBackward0>)\n",
      "epoch: 6079 loss is tensor([-0.2141], grad_fn=<AddBackward0>)\n",
      "epoch: 6080 loss is tensor([-0.1552], grad_fn=<AddBackward0>)\n",
      "epoch: 6081 loss is tensor([-0.3052], grad_fn=<AddBackward0>)\n",
      "epoch: 6082 loss is tensor([-0.2523], grad_fn=<AddBackward0>)\n",
      "epoch: 6083 loss is tensor([-0.2699], grad_fn=<AddBackward0>)\n",
      "epoch: 6084 loss is tensor([-0.1941], grad_fn=<AddBackward0>)\n",
      "epoch: 6085 loss is tensor([-0.2323], grad_fn=<AddBackward0>)\n",
      "epoch: 6086 loss is tensor([-0.2231], grad_fn=<AddBackward0>)\n",
      "epoch: 6087 loss is tensor([-0.2399], grad_fn=<AddBackward0>)\n",
      "epoch: 6088 loss is tensor([-0.1871], grad_fn=<AddBackward0>)\n",
      "epoch: 6089 loss is tensor([-0.1527], grad_fn=<AddBackward0>)\n",
      "epoch: 6090 loss is tensor([-0.2286], grad_fn=<AddBackward0>)\n",
      "epoch: 6091 loss is tensor([-0.2045], grad_fn=<AddBackward0>)\n",
      "epoch: 6092 loss is tensor([-0.2331], grad_fn=<AddBackward0>)\n",
      "epoch: 6093 loss is tensor([-0.1946], grad_fn=<AddBackward0>)\n",
      "epoch: 6094 loss is tensor([-0.2398], grad_fn=<AddBackward0>)\n",
      "epoch: 6095 loss is tensor([-0.2522], grad_fn=<AddBackward0>)\n",
      "epoch: 6096 loss is tensor([-0.2789], grad_fn=<AddBackward0>)\n",
      "epoch: 6097 loss is tensor([-0.2080], grad_fn=<AddBackward0>)\n",
      "epoch: 6098 loss is tensor([-0.2484], grad_fn=<AddBackward0>)\n",
      "epoch: 6099 loss is tensor([-0.2533], grad_fn=<AddBackward0>)\n",
      "epoch: 6100 loss is tensor([-0.2326], grad_fn=<AddBackward0>)\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6101 loss is tensor([-0.2596], grad_fn=<AddBackward0>)\n",
      "epoch: 6102 loss is tensor([-0.2416], grad_fn=<AddBackward0>)\n",
      "epoch: 6103 loss is tensor([-0.2757], grad_fn=<AddBackward0>)\n",
      "epoch: 6104 loss is tensor([-0.2316], grad_fn=<AddBackward0>)\n",
      "epoch: 6105 loss is tensor([-0.2940], grad_fn=<AddBackward0>)\n",
      "epoch: 6106 loss is tensor([-0.2433], grad_fn=<AddBackward0>)\n",
      "epoch: 6107 loss is tensor([-0.2556], grad_fn=<AddBackward0>)\n",
      "epoch: 6108 loss is tensor([-0.2096], grad_fn=<AddBackward0>)\n",
      "epoch: 6109 loss is tensor([-0.2610], grad_fn=<AddBackward0>)\n",
      "epoch: 6110 loss is tensor([-0.2179], grad_fn=<AddBackward0>)\n",
      "epoch: 6111 loss is tensor([-0.2206], grad_fn=<AddBackward0>)\n",
      "epoch: 6112 loss is tensor([-0.2767], grad_fn=<AddBackward0>)\n",
      "epoch: 6113 loss is tensor([-0.2798], grad_fn=<AddBackward0>)\n",
      "epoch: 6114 loss is tensor([-0.2675], grad_fn=<AddBackward0>)\n",
      "epoch: 6115 loss is tensor([-0.2348], grad_fn=<AddBackward0>)\n",
      "epoch: 6116 loss is tensor([-0.2082], grad_fn=<AddBackward0>)\n",
      "epoch: 6117 loss is tensor([-0.2585], grad_fn=<AddBackward0>)\n",
      "epoch: 6118 loss is tensor([-0.2746], grad_fn=<AddBackward0>)\n",
      "epoch: 6119 loss is tensor([-0.3118], grad_fn=<AddBackward0>)\n",
      "epoch: 6120 loss is tensor([-0.3002], grad_fn=<AddBackward0>)\n",
      "epoch: 6121 loss is tensor([-0.2670], grad_fn=<AddBackward0>)\n",
      "epoch: 6122 loss is tensor([-0.2675], grad_fn=<AddBackward0>)\n",
      "epoch: 6123 loss is tensor([-0.2992], grad_fn=<AddBackward0>)\n",
      "epoch: 6124 loss is tensor([-0.2771], grad_fn=<AddBackward0>)\n",
      "epoch: 6125 loss is tensor([-0.2665], grad_fn=<AddBackward0>)\n",
      "epoch: 6126 loss is tensor([-0.2517], grad_fn=<AddBackward0>)\n",
      "epoch: 6127 loss is tensor([-0.2105], grad_fn=<AddBackward0>)\n",
      "epoch: 6128 loss is tensor([-0.2820], grad_fn=<AddBackward0>)\n",
      "epoch: 6129 loss is tensor([-0.3246], grad_fn=<AddBackward0>)\n",
      "epoch: 6130 loss is tensor([-0.2351], grad_fn=<AddBackward0>)\n",
      "epoch: 6131 loss is tensor([-0.3440], grad_fn=<AddBackward0>)\n",
      "epoch: 6132 loss is tensor([-0.3001], grad_fn=<AddBackward0>)\n",
      "epoch: 6133 loss is tensor([-0.2220], grad_fn=<AddBackward0>)\n",
      "epoch: 6134 loss is tensor([-0.3180], grad_fn=<AddBackward0>)\n",
      "epoch: 6135 loss is tensor([-0.2886], grad_fn=<AddBackward0>)\n",
      "epoch: 6136 loss is tensor([-0.2442], grad_fn=<AddBackward0>)\n",
      "epoch: 6137 loss is tensor([-0.2861], grad_fn=<AddBackward0>)\n",
      "epoch: 6138 loss is tensor([-0.2442], grad_fn=<AddBackward0>)\n",
      "epoch: 6139 loss is tensor([-0.2551], grad_fn=<AddBackward0>)\n",
      "epoch: 6140 loss is tensor([-0.2549], grad_fn=<AddBackward0>)\n",
      "epoch: 6141 loss is tensor([-0.3025], grad_fn=<AddBackward0>)\n",
      "epoch: 6142 loss is tensor([-0.2547], grad_fn=<AddBackward0>)\n",
      "epoch: 6143 loss is tensor([-0.2546], grad_fn=<AddBackward0>)\n",
      "epoch: 6144 loss is tensor([-0.2285], grad_fn=<AddBackward0>)\n",
      "epoch: 6145 loss is tensor([-0.2966], grad_fn=<AddBackward0>)\n",
      "epoch: 6146 loss is tensor([-0.2289], grad_fn=<AddBackward0>)\n",
      "epoch: 6147 loss is tensor([-0.2733], grad_fn=<AddBackward0>)\n",
      "epoch: 6148 loss is tensor([-0.2558], grad_fn=<AddBackward0>)\n",
      "epoch: 6149 loss is tensor([-0.2609], grad_fn=<AddBackward0>)\n",
      "epoch: 6150 loss is tensor([-0.2549], grad_fn=<AddBackward0>)\n",
      "epoch: 6151 loss is tensor([-0.2732], grad_fn=<AddBackward0>)\n",
      "epoch: 6152 loss is tensor([-0.2922], grad_fn=<AddBackward0>)\n",
      "epoch: 6153 loss is tensor([-0.2599], grad_fn=<AddBackward0>)\n",
      "epoch: 6154 loss is tensor([-0.1977], grad_fn=<AddBackward0>)\n",
      "epoch: 6155 loss is tensor([-0.2954], grad_fn=<AddBackward0>)\n",
      "epoch: 6156 loss is tensor([-0.1709], grad_fn=<AddBackward0>)\n",
      "epoch: 6157 loss is tensor([-0.1466], grad_fn=<AddBackward0>)\n",
      "epoch: 6158 loss is tensor([-0.1966], grad_fn=<AddBackward0>)\n",
      "epoch: 6159 loss is tensor([-0.1963], grad_fn=<AddBackward0>)\n",
      "epoch: 6160 loss is tensor([-0.2171], grad_fn=<AddBackward0>)\n",
      "epoch: 6161 loss is tensor([-0.1990], grad_fn=<AddBackward0>)\n",
      "epoch: 6162 loss is tensor([-0.1839], grad_fn=<AddBackward0>)\n",
      "epoch: 6163 loss is tensor([-0.1814], grad_fn=<AddBackward0>)\n",
      "epoch: 6164 loss is tensor([-0.2654], grad_fn=<AddBackward0>)\n",
      "epoch: 6165 loss is tensor([-0.2535], grad_fn=<AddBackward0>)\n",
      "epoch: 6166 loss is tensor([-0.1948], grad_fn=<AddBackward0>)\n",
      "epoch: 6167 loss is tensor([-0.1796], grad_fn=<AddBackward0>)\n",
      "epoch: 6168 loss is tensor([-0.1910], grad_fn=<AddBackward0>)\n",
      "epoch: 6169 loss is tensor([-0.1593], grad_fn=<AddBackward0>)\n",
      "epoch: 6170 loss is tensor([-0.2554], grad_fn=<AddBackward0>)\n",
      "epoch: 6171 loss is tensor([-0.2402], grad_fn=<AddBackward0>)\n",
      "epoch: 6172 loss is tensor([-0.1539], grad_fn=<AddBackward0>)\n",
      "epoch: 6173 loss is tensor([-0.1111], grad_fn=<AddBackward0>)\n",
      "epoch: 6174 loss is tensor([-0.1292], grad_fn=<AddBackward0>)\n",
      "epoch: 6175 loss is tensor([-0.2155], grad_fn=<AddBackward0>)\n",
      "epoch: 6176 loss is tensor([-0.1567], grad_fn=<AddBackward0>)\n",
      "epoch: 6177 loss is tensor([-0.1218], grad_fn=<AddBackward0>)\n",
      "epoch: 6178 loss is tensor([-0.1742], grad_fn=<AddBackward0>)\n",
      "epoch: 6179 loss is tensor([-0.1740], grad_fn=<AddBackward0>)\n",
      "epoch: 6180 loss is tensor([-0.2091], grad_fn=<AddBackward0>)\n",
      "epoch: 6181 loss is tensor([-0.1899], grad_fn=<AddBackward0>)\n",
      "epoch: 6182 loss is tensor([-0.1931], grad_fn=<AddBackward0>)\n",
      "epoch: 6183 loss is tensor([-0.2216], grad_fn=<AddBackward0>)\n",
      "epoch: 6184 loss is tensor([-0.2117], grad_fn=<AddBackward0>)\n",
      "epoch: 6185 loss is tensor([-0.1606], grad_fn=<AddBackward0>)\n",
      "epoch: 6186 loss is tensor([-0.1852], grad_fn=<AddBackward0>)\n",
      "epoch: 6187 loss is tensor([-0.2526], grad_fn=<AddBackward0>)\n",
      "epoch: 6188 loss is tensor([-0.2430], grad_fn=<AddBackward0>)\n",
      "epoch: 6189 loss is tensor([-0.2357], grad_fn=<AddBackward0>)\n",
      "epoch: 6190 loss is tensor([-0.2280], grad_fn=<AddBackward0>)\n",
      "epoch: 6191 loss is tensor([-0.1872], grad_fn=<AddBackward0>)\n",
      "epoch: 6192 loss is tensor([-0.2297], grad_fn=<AddBackward0>)\n",
      "epoch: 6193 loss is tensor([-0.2869], grad_fn=<AddBackward0>)\n",
      "epoch: 6194 loss is tensor([-0.2797], grad_fn=<AddBackward0>)\n",
      "epoch: 6195 loss is tensor([-0.2857], grad_fn=<AddBackward0>)\n",
      "epoch: 6196 loss is tensor([-0.2365], grad_fn=<AddBackward0>)\n",
      "epoch: 6197 loss is tensor([-0.2697], grad_fn=<AddBackward0>)\n",
      "epoch: 6198 loss is tensor([-0.2337], grad_fn=<AddBackward0>)\n",
      "epoch: 6199 loss is tensor([-0.2634], grad_fn=<AddBackward0>)\n",
      "epoch: 6200 loss is tensor([-0.2830], grad_fn=<AddBackward0>)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6201 loss is tensor([-0.2942], grad_fn=<AddBackward0>)\n",
      "epoch: 6202 loss is tensor([-0.2146], grad_fn=<AddBackward0>)\n",
      "epoch: 6203 loss is tensor([-0.2322], grad_fn=<AddBackward0>)\n",
      "epoch: 6204 loss is tensor([-0.1958], grad_fn=<AddBackward0>)\n",
      "epoch: 6205 loss is tensor([-0.2593], grad_fn=<AddBackward0>)\n",
      "epoch: 6206 loss is tensor([-0.2448], grad_fn=<AddBackward0>)\n",
      "epoch: 6207 loss is tensor([-0.2293], grad_fn=<AddBackward0>)\n",
      "epoch: 6208 loss is tensor([-0.2612], grad_fn=<AddBackward0>)\n",
      "epoch: 6209 loss is tensor([-0.2168], grad_fn=<AddBackward0>)\n",
      "epoch: 6210 loss is tensor([-0.2653], grad_fn=<AddBackward0>)\n",
      "epoch: 6211 loss is tensor([-0.2191], grad_fn=<AddBackward0>)\n",
      "epoch: 6212 loss is tensor([-0.2540], grad_fn=<AddBackward0>)\n",
      "epoch: 6213 loss is tensor([-0.2097], grad_fn=<AddBackward0>)\n",
      "epoch: 6214 loss is tensor([-0.2745], grad_fn=<AddBackward0>)\n",
      "epoch: 6215 loss is tensor([-0.2230], grad_fn=<AddBackward0>)\n",
      "epoch: 6216 loss is tensor([-0.2367], grad_fn=<AddBackward0>)\n",
      "epoch: 6217 loss is tensor([-0.2293], grad_fn=<AddBackward0>)\n",
      "epoch: 6218 loss is tensor([-0.2547], grad_fn=<AddBackward0>)\n",
      "epoch: 6219 loss is tensor([-0.2172], grad_fn=<AddBackward0>)\n",
      "epoch: 6220 loss is tensor([-0.2555], grad_fn=<AddBackward0>)\n",
      "epoch: 6221 loss is tensor([-0.2902], grad_fn=<AddBackward0>)\n",
      "epoch: 6222 loss is tensor([-0.1873], grad_fn=<AddBackward0>)\n",
      "epoch: 6223 loss is tensor([-0.2697], grad_fn=<AddBackward0>)\n",
      "epoch: 6224 loss is tensor([-0.2460], grad_fn=<AddBackward0>)\n",
      "epoch: 6225 loss is tensor([-0.2792], grad_fn=<AddBackward0>)\n",
      "epoch: 6226 loss is tensor([-0.2690], grad_fn=<AddBackward0>)\n",
      "epoch: 6227 loss is tensor([-0.2450], grad_fn=<AddBackward0>)\n",
      "epoch: 6228 loss is tensor([-0.2325], grad_fn=<AddBackward0>)\n",
      "epoch: 6229 loss is tensor([-0.2508], grad_fn=<AddBackward0>)\n",
      "epoch: 6230 loss is tensor([-0.1997], grad_fn=<AddBackward0>)\n",
      "epoch: 6231 loss is tensor([-0.2277], grad_fn=<AddBackward0>)\n",
      "epoch: 6232 loss is tensor([-0.2709], grad_fn=<AddBackward0>)\n",
      "epoch: 6233 loss is tensor([-0.1936], grad_fn=<AddBackward0>)\n",
      "epoch: 6234 loss is tensor([-0.2474], grad_fn=<AddBackward0>)\n",
      "epoch: 6235 loss is tensor([-0.2464], grad_fn=<AddBackward0>)\n",
      "epoch: 6236 loss is tensor([-0.3022], grad_fn=<AddBackward0>)\n",
      "epoch: 6237 loss is tensor([-0.2589], grad_fn=<AddBackward0>)\n",
      "epoch: 6238 loss is tensor([-0.2348], grad_fn=<AddBackward0>)\n",
      "epoch: 6239 loss is tensor([-0.3087], grad_fn=<AddBackward0>)\n",
      "epoch: 6240 loss is tensor([-0.2386], grad_fn=<AddBackward0>)\n",
      "epoch: 6241 loss is tensor([-0.2617], grad_fn=<AddBackward0>)\n",
      "epoch: 6242 loss is tensor([-0.3055], grad_fn=<AddBackward0>)\n",
      "epoch: 6243 loss is tensor([-0.2716], grad_fn=<AddBackward0>)\n",
      "epoch: 6244 loss is tensor([-0.3190], grad_fn=<AddBackward0>)\n",
      "epoch: 6245 loss is tensor([-0.2783], grad_fn=<AddBackward0>)\n",
      "epoch: 6246 loss is tensor([-0.2433], grad_fn=<AddBackward0>)\n",
      "epoch: 6247 loss is tensor([-0.2495], grad_fn=<AddBackward0>)\n",
      "epoch: 6248 loss is tensor([-0.2994], grad_fn=<AddBackward0>)\n",
      "epoch: 6249 loss is tensor([-0.2434], grad_fn=<AddBackward0>)\n",
      "epoch: 6250 loss is tensor([-0.2931], grad_fn=<AddBackward0>)\n",
      "epoch: 6251 loss is tensor([-0.2763], grad_fn=<AddBackward0>)\n",
      "epoch: 6252 loss is tensor([-0.2641], grad_fn=<AddBackward0>)\n",
      "epoch: 6253 loss is tensor([-0.2343], grad_fn=<AddBackward0>)\n",
      "epoch: 6254 loss is tensor([-0.3042], grad_fn=<AddBackward0>)\n",
      "epoch: 6255 loss is tensor([-0.3002], grad_fn=<AddBackward0>)\n",
      "epoch: 6256 loss is tensor([-0.2777], grad_fn=<AddBackward0>)\n",
      "epoch: 6257 loss is tensor([-0.2024], grad_fn=<AddBackward0>)\n",
      "epoch: 6258 loss is tensor([-0.2961], grad_fn=<AddBackward0>)\n",
      "epoch: 6259 loss is tensor([-0.2333], grad_fn=<AddBackward0>)\n",
      "epoch: 6260 loss is tensor([-0.2514], grad_fn=<AddBackward0>)\n",
      "epoch: 6261 loss is tensor([-0.3056], grad_fn=<AddBackward0>)\n",
      "epoch: 6262 loss is tensor([-0.2449], grad_fn=<AddBackward0>)\n",
      "epoch: 6263 loss is tensor([-0.2957], grad_fn=<AddBackward0>)\n",
      "epoch: 6264 loss is tensor([-0.2710], grad_fn=<AddBackward0>)\n",
      "epoch: 6265 loss is tensor([-0.2256], grad_fn=<AddBackward0>)\n",
      "epoch: 6266 loss is tensor([-0.2467], grad_fn=<AddBackward0>)\n",
      "epoch: 6267 loss is tensor([-0.2295], grad_fn=<AddBackward0>)\n",
      "epoch: 6268 loss is tensor([-0.2258], grad_fn=<AddBackward0>)\n",
      "epoch: 6269 loss is tensor([-0.2659], grad_fn=<AddBackward0>)\n",
      "epoch: 6270 loss is tensor([-0.2645], grad_fn=<AddBackward0>)\n",
      "epoch: 6271 loss is tensor([-0.2027], grad_fn=<AddBackward0>)\n",
      "epoch: 6272 loss is tensor([-0.1853], grad_fn=<AddBackward0>)\n",
      "epoch: 6273 loss is tensor([-0.2199], grad_fn=<AddBackward0>)\n",
      "epoch: 6274 loss is tensor([-0.1581], grad_fn=<AddBackward0>)\n",
      "epoch: 6275 loss is tensor([-0.2205], grad_fn=<AddBackward0>)\n",
      "epoch: 6276 loss is tensor([-0.2170], grad_fn=<AddBackward0>)\n",
      "epoch: 6277 loss is tensor([-0.1923], grad_fn=<AddBackward0>)\n",
      "epoch: 6278 loss is tensor([-0.2632], grad_fn=<AddBackward0>)\n",
      "epoch: 6279 loss is tensor([-0.1955], grad_fn=<AddBackward0>)\n",
      "epoch: 6280 loss is tensor([-0.2515], grad_fn=<AddBackward0>)\n",
      "epoch: 6281 loss is tensor([-0.2615], grad_fn=<AddBackward0>)\n",
      "epoch: 6282 loss is tensor([-0.1988], grad_fn=<AddBackward0>)\n",
      "epoch: 6283 loss is tensor([-0.2066], grad_fn=<AddBackward0>)\n",
      "epoch: 6284 loss is tensor([-0.2884], grad_fn=<AddBackward0>)\n",
      "epoch: 6285 loss is tensor([-0.2177], grad_fn=<AddBackward0>)\n",
      "epoch: 6286 loss is tensor([-0.2458], grad_fn=<AddBackward0>)\n",
      "epoch: 6287 loss is tensor([-0.2977], grad_fn=<AddBackward0>)\n",
      "epoch: 6288 loss is tensor([-0.2562], grad_fn=<AddBackward0>)\n",
      "epoch: 6289 loss is tensor([-0.2657], grad_fn=<AddBackward0>)\n",
      "epoch: 6290 loss is tensor([-0.2866], grad_fn=<AddBackward0>)\n",
      "epoch: 6291 loss is tensor([-0.2300], grad_fn=<AddBackward0>)\n",
      "epoch: 6292 loss is tensor([-0.2846], grad_fn=<AddBackward0>)\n",
      "epoch: 6293 loss is tensor([-0.2477], grad_fn=<AddBackward0>)\n",
      "epoch: 6294 loss is tensor([-0.2459], grad_fn=<AddBackward0>)\n",
      "epoch: 6295 loss is tensor([-0.2870], grad_fn=<AddBackward0>)\n",
      "epoch: 6296 loss is tensor([-0.2374], grad_fn=<AddBackward0>)\n",
      "epoch: 6297 loss is tensor([-0.1876], grad_fn=<AddBackward0>)\n",
      "epoch: 6298 loss is tensor([-0.2799], grad_fn=<AddBackward0>)\n",
      "epoch: 6299 loss is tensor([-0.2824], grad_fn=<AddBackward0>)\n",
      "epoch: 6300 loss is tensor([-0.2982], grad_fn=<AddBackward0>)\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6301 loss is tensor([-0.2905], grad_fn=<AddBackward0>)\n",
      "epoch: 6302 loss is tensor([-0.2636], grad_fn=<AddBackward0>)\n",
      "epoch: 6303 loss is tensor([-0.2652], grad_fn=<AddBackward0>)\n",
      "epoch: 6304 loss is tensor([-0.2831], grad_fn=<AddBackward0>)\n",
      "epoch: 6305 loss is tensor([-0.2672], grad_fn=<AddBackward0>)\n",
      "epoch: 6306 loss is tensor([-0.2343], grad_fn=<AddBackward0>)\n",
      "epoch: 6307 loss is tensor([-0.2429], grad_fn=<AddBackward0>)\n",
      "epoch: 6308 loss is tensor([-0.2364], grad_fn=<AddBackward0>)\n",
      "epoch: 6309 loss is tensor([-0.3610], grad_fn=<AddBackward0>)\n",
      "epoch: 6310 loss is tensor([-0.2348], grad_fn=<AddBackward0>)\n",
      "epoch: 6311 loss is tensor([-0.3381], grad_fn=<AddBackward0>)\n",
      "epoch: 6312 loss is tensor([-0.2829], grad_fn=<AddBackward0>)\n",
      "epoch: 6313 loss is tensor([-0.3359], grad_fn=<AddBackward0>)\n",
      "epoch: 6314 loss is tensor([-0.2306], grad_fn=<AddBackward0>)\n",
      "epoch: 6315 loss is tensor([-0.2467], grad_fn=<AddBackward0>)\n",
      "epoch: 6316 loss is tensor([-0.2957], grad_fn=<AddBackward0>)\n",
      "epoch: 6317 loss is tensor([-0.2712], grad_fn=<AddBackward0>)\n",
      "epoch: 6318 loss is tensor([-0.2749], grad_fn=<AddBackward0>)\n",
      "epoch: 6319 loss is tensor([-0.2796], grad_fn=<AddBackward0>)\n",
      "epoch: 6320 loss is tensor([-0.2531], grad_fn=<AddBackward0>)\n",
      "epoch: 6321 loss is tensor([-0.2790], grad_fn=<AddBackward0>)\n",
      "epoch: 6322 loss is tensor([-0.2864], grad_fn=<AddBackward0>)\n",
      "epoch: 6323 loss is tensor([-0.3233], grad_fn=<AddBackward0>)\n",
      "epoch: 6324 loss is tensor([-0.2845], grad_fn=<AddBackward0>)\n",
      "epoch: 6325 loss is tensor([-0.2352], grad_fn=<AddBackward0>)\n",
      "epoch: 6326 loss is tensor([-0.2695], grad_fn=<AddBackward0>)\n",
      "epoch: 6327 loss is tensor([-0.2893], grad_fn=<AddBackward0>)\n",
      "epoch: 6328 loss is tensor([-0.2782], grad_fn=<AddBackward0>)\n",
      "epoch: 6329 loss is tensor([-0.2652], grad_fn=<AddBackward0>)\n",
      "epoch: 6330 loss is tensor([-0.2912], grad_fn=<AddBackward0>)\n",
      "epoch: 6331 loss is tensor([-0.2616], grad_fn=<AddBackward0>)\n",
      "epoch: 6332 loss is tensor([-0.2616], grad_fn=<AddBackward0>)\n",
      "epoch: 6333 loss is tensor([-0.2932], grad_fn=<AddBackward0>)\n",
      "epoch: 6334 loss is tensor([-0.3202], grad_fn=<AddBackward0>)\n",
      "epoch: 6335 loss is tensor([-0.2495], grad_fn=<AddBackward0>)\n",
      "epoch: 6336 loss is tensor([-0.2668], grad_fn=<AddBackward0>)\n",
      "epoch: 6337 loss is tensor([-0.3714], grad_fn=<AddBackward0>)\n",
      "epoch: 6338 loss is tensor([-0.3343], grad_fn=<AddBackward0>)\n",
      "epoch: 6339 loss is tensor([-0.2727], grad_fn=<AddBackward0>)\n",
      "epoch: 6340 loss is tensor([-0.3249], grad_fn=<AddBackward0>)\n",
      "epoch: 6341 loss is tensor([-0.2531], grad_fn=<AddBackward0>)\n",
      "epoch: 6342 loss is tensor([-0.3156], grad_fn=<AddBackward0>)\n",
      "epoch: 6343 loss is tensor([-0.2671], grad_fn=<AddBackward0>)\n",
      "epoch: 6344 loss is tensor([-0.2607], grad_fn=<AddBackward0>)\n",
      "epoch: 6345 loss is tensor([-0.2876], grad_fn=<AddBackward0>)\n",
      "epoch: 6346 loss is tensor([-0.2465], grad_fn=<AddBackward0>)\n",
      "epoch: 6347 loss is tensor([-0.1896], grad_fn=<AddBackward0>)\n",
      "epoch: 6348 loss is tensor([-0.2572], grad_fn=<AddBackward0>)\n",
      "epoch: 6349 loss is tensor([-0.1604], grad_fn=<AddBackward0>)\n",
      "epoch: 6350 loss is tensor([-0.2024], grad_fn=<AddBackward0>)\n",
      "epoch: 6351 loss is tensor([-0.2612], grad_fn=<AddBackward0>)\n",
      "epoch: 6352 loss is tensor([-0.2147], grad_fn=<AddBackward0>)\n",
      "epoch: 6353 loss is tensor([-0.2091], grad_fn=<AddBackward0>)\n",
      "epoch: 6354 loss is tensor([-0.2031], grad_fn=<AddBackward0>)\n",
      "epoch: 6355 loss is tensor([-0.2608], grad_fn=<AddBackward0>)\n",
      "epoch: 6356 loss is tensor([-0.2761], grad_fn=<AddBackward0>)\n",
      "epoch: 6357 loss is tensor([-0.2270], grad_fn=<AddBackward0>)\n",
      "epoch: 6358 loss is tensor([-0.2350], grad_fn=<AddBackward0>)\n",
      "epoch: 6359 loss is tensor([-0.2229], grad_fn=<AddBackward0>)\n",
      "epoch: 6360 loss is tensor([-0.2678], grad_fn=<AddBackward0>)\n",
      "epoch: 6361 loss is tensor([-0.2843], grad_fn=<AddBackward0>)\n",
      "epoch: 6362 loss is tensor([-0.2759], grad_fn=<AddBackward0>)\n",
      "epoch: 6363 loss is tensor([-0.2966], grad_fn=<AddBackward0>)\n",
      "epoch: 6364 loss is tensor([-0.2818], grad_fn=<AddBackward0>)\n",
      "epoch: 6365 loss is tensor([-0.2867], grad_fn=<AddBackward0>)\n",
      "epoch: 6366 loss is tensor([-0.2413], grad_fn=<AddBackward0>)\n",
      "epoch: 6367 loss is tensor([-0.2093], grad_fn=<AddBackward0>)\n",
      "epoch: 6368 loss is tensor([-0.2006], grad_fn=<AddBackward0>)\n",
      "epoch: 6369 loss is tensor([-0.2650], grad_fn=<AddBackward0>)\n",
      "epoch: 6370 loss is tensor([-0.2345], grad_fn=<AddBackward0>)\n",
      "epoch: 6371 loss is tensor([-0.2288], grad_fn=<AddBackward0>)\n",
      "epoch: 6372 loss is tensor([-0.2271], grad_fn=<AddBackward0>)\n",
      "epoch: 6373 loss is tensor([-0.2611], grad_fn=<AddBackward0>)\n",
      "epoch: 6374 loss is tensor([-0.2184], grad_fn=<AddBackward0>)\n",
      "epoch: 6375 loss is tensor([-0.2615], grad_fn=<AddBackward0>)\n",
      "epoch: 6376 loss is tensor([-0.2682], grad_fn=<AddBackward0>)\n",
      "epoch: 6377 loss is tensor([-0.2752], grad_fn=<AddBackward0>)\n",
      "epoch: 6378 loss is tensor([-0.2476], grad_fn=<AddBackward0>)\n",
      "epoch: 6379 loss is tensor([-0.2930], grad_fn=<AddBackward0>)\n",
      "epoch: 6380 loss is tensor([-0.2331], grad_fn=<AddBackward0>)\n",
      "epoch: 6381 loss is tensor([-0.2462], grad_fn=<AddBackward0>)\n",
      "epoch: 6382 loss is tensor([-0.2742], grad_fn=<AddBackward0>)\n",
      "epoch: 6383 loss is tensor([-0.2537], grad_fn=<AddBackward0>)\n",
      "epoch: 6384 loss is tensor([-0.2757], grad_fn=<AddBackward0>)\n",
      "epoch: 6385 loss is tensor([-0.3302], grad_fn=<AddBackward0>)\n",
      "epoch: 6386 loss is tensor([-0.2832], grad_fn=<AddBackward0>)\n",
      "epoch: 6387 loss is tensor([-0.2216], grad_fn=<AddBackward0>)\n",
      "epoch: 6388 loss is tensor([-0.2313], grad_fn=<AddBackward0>)\n",
      "epoch: 6389 loss is tensor([-0.2639], grad_fn=<AddBackward0>)\n",
      "epoch: 6390 loss is tensor([-0.2878], grad_fn=<AddBackward0>)\n",
      "epoch: 6391 loss is tensor([-0.3012], grad_fn=<AddBackward0>)\n",
      "epoch: 6392 loss is tensor([-0.2415], grad_fn=<AddBackward0>)\n",
      "epoch: 6393 loss is tensor([-0.3215], grad_fn=<AddBackward0>)\n",
      "epoch: 6394 loss is tensor([-0.2384], grad_fn=<AddBackward0>)\n",
      "epoch: 6395 loss is tensor([-0.2562], grad_fn=<AddBackward0>)\n",
      "epoch: 6396 loss is tensor([-0.2911], grad_fn=<AddBackward0>)\n",
      "epoch: 6397 loss is tensor([-0.2135], grad_fn=<AddBackward0>)\n",
      "epoch: 6398 loss is tensor([-0.2164], grad_fn=<AddBackward0>)\n",
      "epoch: 6399 loss is tensor([-0.2031], grad_fn=<AddBackward0>)\n",
      "epoch: 6400 loss is tensor([-0.3293], grad_fn=<AddBackward0>)\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6401 loss is tensor([-0.2716], grad_fn=<AddBackward0>)\n",
      "epoch: 6402 loss is tensor([-0.2673], grad_fn=<AddBackward0>)\n",
      "epoch: 6403 loss is tensor([-0.3087], grad_fn=<AddBackward0>)\n",
      "epoch: 6404 loss is tensor([-0.2292], grad_fn=<AddBackward0>)\n",
      "epoch: 6405 loss is tensor([-0.2881], grad_fn=<AddBackward0>)\n",
      "epoch: 6406 loss is tensor([-0.2928], grad_fn=<AddBackward0>)\n",
      "epoch: 6407 loss is tensor([-0.3370], grad_fn=<AddBackward0>)\n",
      "epoch: 6408 loss is tensor([-0.2661], grad_fn=<AddBackward0>)\n",
      "epoch: 6409 loss is tensor([-0.2624], grad_fn=<AddBackward0>)\n",
      "epoch: 6410 loss is tensor([-0.2510], grad_fn=<AddBackward0>)\n",
      "epoch: 6411 loss is tensor([-0.2923], grad_fn=<AddBackward0>)\n",
      "epoch: 6412 loss is tensor([-0.2175], grad_fn=<AddBackward0>)\n",
      "epoch: 6413 loss is tensor([-0.2496], grad_fn=<AddBackward0>)\n",
      "epoch: 6414 loss is tensor([-0.1470], grad_fn=<AddBackward0>)\n",
      "epoch: 6415 loss is tensor([-0.2672], grad_fn=<AddBackward0>)\n",
      "epoch: 6416 loss is tensor([-0.2079], grad_fn=<AddBackward0>)\n",
      "epoch: 6417 loss is tensor([-0.2754], grad_fn=<AddBackward0>)\n",
      "epoch: 6418 loss is tensor([-0.2378], grad_fn=<AddBackward0>)\n",
      "epoch: 6419 loss is tensor([-0.2318], grad_fn=<AddBackward0>)\n",
      "epoch: 6420 loss is tensor([-0.2144], grad_fn=<AddBackward0>)\n",
      "epoch: 6421 loss is tensor([-0.2397], grad_fn=<AddBackward0>)\n",
      "epoch: 6422 loss is tensor([-0.2234], grad_fn=<AddBackward0>)\n",
      "epoch: 6423 loss is tensor([-0.3058], grad_fn=<AddBackward0>)\n",
      "epoch: 6424 loss is tensor([-0.2648], grad_fn=<AddBackward0>)\n",
      "epoch: 6425 loss is tensor([-0.2832], grad_fn=<AddBackward0>)\n",
      "epoch: 6426 loss is tensor([-0.2442], grad_fn=<AddBackward0>)\n",
      "epoch: 6427 loss is tensor([-0.1893], grad_fn=<AddBackward0>)\n",
      "epoch: 6428 loss is tensor([-0.2356], grad_fn=<AddBackward0>)\n",
      "epoch: 6429 loss is tensor([-0.1920], grad_fn=<AddBackward0>)\n",
      "epoch: 6430 loss is tensor([-0.2018], grad_fn=<AddBackward0>)\n",
      "epoch: 6431 loss is tensor([-0.2737], grad_fn=<AddBackward0>)\n",
      "epoch: 6432 loss is tensor([-0.1988], grad_fn=<AddBackward0>)\n",
      "epoch: 6433 loss is tensor([-0.2502], grad_fn=<AddBackward0>)\n",
      "epoch: 6434 loss is tensor([-0.1659], grad_fn=<AddBackward0>)\n",
      "epoch: 6435 loss is tensor([-0.1888], grad_fn=<AddBackward0>)\n",
      "epoch: 6436 loss is tensor([-0.2491], grad_fn=<AddBackward0>)\n",
      "epoch: 6437 loss is tensor([-0.3148], grad_fn=<AddBackward0>)\n",
      "epoch: 6438 loss is tensor([-0.3060], grad_fn=<AddBackward0>)\n",
      "epoch: 6439 loss is tensor([-0.2610], grad_fn=<AddBackward0>)\n",
      "epoch: 6440 loss is tensor([-0.2726], grad_fn=<AddBackward0>)\n",
      "epoch: 6441 loss is tensor([-0.2244], grad_fn=<AddBackward0>)\n",
      "epoch: 6442 loss is tensor([-0.2327], grad_fn=<AddBackward0>)\n",
      "epoch: 6443 loss is tensor([-0.1679], grad_fn=<AddBackward0>)\n",
      "epoch: 6444 loss is tensor([-0.2248], grad_fn=<AddBackward0>)\n",
      "epoch: 6445 loss is tensor([-0.2630], grad_fn=<AddBackward0>)\n",
      "epoch: 6446 loss is tensor([-0.2013], grad_fn=<AddBackward0>)\n",
      "epoch: 6447 loss is tensor([-0.2337], grad_fn=<AddBackward0>)\n",
      "epoch: 6448 loss is tensor([-0.2660], grad_fn=<AddBackward0>)\n",
      "epoch: 6449 loss is tensor([-0.2851], grad_fn=<AddBackward0>)\n",
      "epoch: 6450 loss is tensor([-0.2188], grad_fn=<AddBackward0>)\n",
      "epoch: 6451 loss is tensor([-0.2651], grad_fn=<AddBackward0>)\n",
      "epoch: 6452 loss is tensor([-0.2600], grad_fn=<AddBackward0>)\n",
      "epoch: 6453 loss is tensor([-0.2255], grad_fn=<AddBackward0>)\n",
      "epoch: 6454 loss is tensor([-0.2558], grad_fn=<AddBackward0>)\n",
      "epoch: 6455 loss is tensor([-0.2510], grad_fn=<AddBackward0>)\n",
      "epoch: 6456 loss is tensor([-0.2332], grad_fn=<AddBackward0>)\n",
      "epoch: 6457 loss is tensor([-0.2777], grad_fn=<AddBackward0>)\n",
      "epoch: 6458 loss is tensor([-0.2474], grad_fn=<AddBackward0>)\n",
      "epoch: 6459 loss is tensor([-0.2695], grad_fn=<AddBackward0>)\n",
      "epoch: 6460 loss is tensor([-0.2474], grad_fn=<AddBackward0>)\n",
      "epoch: 6461 loss is tensor([-0.2942], grad_fn=<AddBackward0>)\n",
      "epoch: 6462 loss is tensor([-0.2924], grad_fn=<AddBackward0>)\n",
      "epoch: 6463 loss is tensor([-0.2450], grad_fn=<AddBackward0>)\n",
      "epoch: 6464 loss is tensor([-0.2310], grad_fn=<AddBackward0>)\n",
      "epoch: 6465 loss is tensor([-0.3155], grad_fn=<AddBackward0>)\n",
      "epoch: 6466 loss is tensor([-0.2675], grad_fn=<AddBackward0>)\n",
      "epoch: 6467 loss is tensor([-0.2910], grad_fn=<AddBackward0>)\n",
      "epoch: 6468 loss is tensor([-0.2520], grad_fn=<AddBackward0>)\n",
      "epoch: 6469 loss is tensor([-0.2825], grad_fn=<AddBackward0>)\n",
      "epoch: 6470 loss is tensor([-0.2770], grad_fn=<AddBackward0>)\n",
      "epoch: 6471 loss is tensor([-0.2540], grad_fn=<AddBackward0>)\n",
      "epoch: 6472 loss is tensor([-0.2747], grad_fn=<AddBackward0>)\n",
      "epoch: 6473 loss is tensor([-0.2588], grad_fn=<AddBackward0>)\n",
      "epoch: 6474 loss is tensor([-0.2821], grad_fn=<AddBackward0>)\n",
      "epoch: 6475 loss is tensor([-0.2394], grad_fn=<AddBackward0>)\n",
      "epoch: 6476 loss is tensor([-0.2405], grad_fn=<AddBackward0>)\n",
      "epoch: 6477 loss is tensor([-0.2733], grad_fn=<AddBackward0>)\n",
      "epoch: 6478 loss is tensor([-0.2855], grad_fn=<AddBackward0>)\n",
      "epoch: 6479 loss is tensor([-0.3050], grad_fn=<AddBackward0>)\n",
      "epoch: 6480 loss is tensor([-0.2478], grad_fn=<AddBackward0>)\n",
      "epoch: 6481 loss is tensor([-0.3028], grad_fn=<AddBackward0>)\n",
      "epoch: 6482 loss is tensor([-0.2835], grad_fn=<AddBackward0>)\n",
      "epoch: 6483 loss is tensor([-0.2855], grad_fn=<AddBackward0>)\n",
      "epoch: 6484 loss is tensor([-0.2297], grad_fn=<AddBackward0>)\n",
      "epoch: 6485 loss is tensor([-0.3412], grad_fn=<AddBackward0>)\n",
      "epoch: 6486 loss is tensor([-0.2580], grad_fn=<AddBackward0>)\n",
      "epoch: 6487 loss is tensor([-0.2363], grad_fn=<AddBackward0>)\n",
      "epoch: 6488 loss is tensor([-0.2727], grad_fn=<AddBackward0>)\n",
      "epoch: 6489 loss is tensor([-0.2364], grad_fn=<AddBackward0>)\n",
      "epoch: 6490 loss is tensor([-0.3102], grad_fn=<AddBackward0>)\n",
      "epoch: 6491 loss is tensor([-0.2795], grad_fn=<AddBackward0>)\n",
      "epoch: 6492 loss is tensor([-0.2737], grad_fn=<AddBackward0>)\n",
      "epoch: 6493 loss is tensor([-0.2854], grad_fn=<AddBackward0>)\n",
      "epoch: 6494 loss is tensor([-0.3099], grad_fn=<AddBackward0>)\n",
      "epoch: 6495 loss is tensor([-0.3415], grad_fn=<AddBackward0>)\n",
      "epoch: 6496 loss is tensor([-0.2561], grad_fn=<AddBackward0>)\n",
      "epoch: 6497 loss is tensor([-0.3300], grad_fn=<AddBackward0>)\n",
      "epoch: 6498 loss is tensor([-0.2659], grad_fn=<AddBackward0>)\n",
      "epoch: 6499 loss is tensor([-0.3276], grad_fn=<AddBackward0>)\n",
      "epoch: 6500 loss is tensor([-0.2965], grad_fn=<AddBackward0>)\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6501 loss is tensor([-0.2251], grad_fn=<AddBackward0>)\n",
      "epoch: 6502 loss is tensor([-0.3340], grad_fn=<AddBackward0>)\n",
      "epoch: 6503 loss is tensor([-0.3255], grad_fn=<AddBackward0>)\n",
      "epoch: 6504 loss is tensor([-0.2972], grad_fn=<AddBackward0>)\n",
      "epoch: 6505 loss is tensor([-0.3279], grad_fn=<AddBackward0>)\n",
      "epoch: 6506 loss is tensor([-0.3102], grad_fn=<AddBackward0>)\n",
      "epoch: 6507 loss is tensor([-0.2816], grad_fn=<AddBackward0>)\n",
      "epoch: 6508 loss is tensor([-0.3013], grad_fn=<AddBackward0>)\n",
      "epoch: 6509 loss is tensor([-0.2180], grad_fn=<AddBackward0>)\n",
      "epoch: 6510 loss is tensor([-0.2983], grad_fn=<AddBackward0>)\n",
      "epoch: 6511 loss is tensor([-0.2744], grad_fn=<AddBackward0>)\n",
      "epoch: 6512 loss is tensor([-0.2152], grad_fn=<AddBackward0>)\n",
      "epoch: 6513 loss is tensor([-0.2737], grad_fn=<AddBackward0>)\n",
      "epoch: 6514 loss is tensor([-0.2912], grad_fn=<AddBackward0>)\n",
      "epoch: 6515 loss is tensor([-0.3209], grad_fn=<AddBackward0>)\n",
      "epoch: 6516 loss is tensor([-0.3238], grad_fn=<AddBackward0>)\n",
      "epoch: 6517 loss is tensor([-0.2945], grad_fn=<AddBackward0>)\n",
      "epoch: 6518 loss is tensor([-0.2917], grad_fn=<AddBackward0>)\n",
      "epoch: 6519 loss is tensor([-0.2329], grad_fn=<AddBackward0>)\n",
      "epoch: 6520 loss is tensor([-0.2432], grad_fn=<AddBackward0>)\n",
      "epoch: 6521 loss is tensor([-0.2554], grad_fn=<AddBackward0>)\n",
      "epoch: 6522 loss is tensor([-0.2347], grad_fn=<AddBackward0>)\n",
      "epoch: 6523 loss is tensor([-0.2650], grad_fn=<AddBackward0>)\n",
      "epoch: 6524 loss is tensor([-0.2606], grad_fn=<AddBackward0>)\n",
      "epoch: 6525 loss is tensor([-0.2849], grad_fn=<AddBackward0>)\n",
      "epoch: 6526 loss is tensor([-0.2028], grad_fn=<AddBackward0>)\n",
      "epoch: 6527 loss is tensor([-0.2733], grad_fn=<AddBackward0>)\n",
      "epoch: 6528 loss is tensor([-0.2328], grad_fn=<AddBackward0>)\n",
      "epoch: 6529 loss is tensor([-0.2707], grad_fn=<AddBackward0>)\n",
      "epoch: 6530 loss is tensor([-0.2359], grad_fn=<AddBackward0>)\n",
      "epoch: 6531 loss is tensor([-0.2912], grad_fn=<AddBackward0>)\n",
      "epoch: 6532 loss is tensor([-0.2063], grad_fn=<AddBackward0>)\n",
      "epoch: 6533 loss is tensor([-0.2789], grad_fn=<AddBackward0>)\n",
      "epoch: 6534 loss is tensor([-0.3391], grad_fn=<AddBackward0>)\n",
      "epoch: 6535 loss is tensor([-0.3149], grad_fn=<AddBackward0>)\n",
      "epoch: 6536 loss is tensor([-0.2732], grad_fn=<AddBackward0>)\n",
      "epoch: 6537 loss is tensor([-0.2666], grad_fn=<AddBackward0>)\n",
      "epoch: 6538 loss is tensor([-0.2507], grad_fn=<AddBackward0>)\n",
      "epoch: 6539 loss is tensor([-0.2678], grad_fn=<AddBackward0>)\n",
      "epoch: 6540 loss is tensor([-0.2701], grad_fn=<AddBackward0>)\n",
      "epoch: 6541 loss is tensor([-0.2704], grad_fn=<AddBackward0>)\n",
      "epoch: 6542 loss is tensor([-0.3072], grad_fn=<AddBackward0>)\n",
      "epoch: 6543 loss is tensor([-0.2674], grad_fn=<AddBackward0>)\n",
      "epoch: 6544 loss is tensor([-0.2865], grad_fn=<AddBackward0>)\n",
      "epoch: 6545 loss is tensor([-0.2689], grad_fn=<AddBackward0>)\n",
      "epoch: 6546 loss is tensor([-0.2482], grad_fn=<AddBackward0>)\n",
      "epoch: 6547 loss is tensor([-0.2708], grad_fn=<AddBackward0>)\n",
      "epoch: 6548 loss is tensor([-0.3108], grad_fn=<AddBackward0>)\n",
      "epoch: 6549 loss is tensor([-0.2514], grad_fn=<AddBackward0>)\n",
      "epoch: 6550 loss is tensor([-0.2371], grad_fn=<AddBackward0>)\n",
      "epoch: 6551 loss is tensor([-0.2404], grad_fn=<AddBackward0>)\n",
      "epoch: 6552 loss is tensor([-0.2598], grad_fn=<AddBackward0>)\n",
      "epoch: 6553 loss is tensor([-0.2514], grad_fn=<AddBackward0>)\n",
      "epoch: 6554 loss is tensor([-0.2851], grad_fn=<AddBackward0>)\n",
      "epoch: 6555 loss is tensor([-0.2303], grad_fn=<AddBackward0>)\n",
      "epoch: 6556 loss is tensor([-0.2603], grad_fn=<AddBackward0>)\n",
      "epoch: 6557 loss is tensor([-0.2589], grad_fn=<AddBackward0>)\n",
      "epoch: 6558 loss is tensor([-0.2561], grad_fn=<AddBackward0>)\n",
      "epoch: 6559 loss is tensor([-0.2618], grad_fn=<AddBackward0>)\n",
      "epoch: 6560 loss is tensor([-0.2342], grad_fn=<AddBackward0>)\n",
      "epoch: 6561 loss is tensor([-0.2534], grad_fn=<AddBackward0>)\n",
      "epoch: 6562 loss is tensor([-0.2424], grad_fn=<AddBackward0>)\n",
      "epoch: 6563 loss is tensor([-0.2572], grad_fn=<AddBackward0>)\n",
      "epoch: 6564 loss is tensor([-0.1656], grad_fn=<AddBackward0>)\n",
      "epoch: 6565 loss is tensor([-0.3030], grad_fn=<AddBackward0>)\n",
      "epoch: 6566 loss is tensor([-0.2504], grad_fn=<AddBackward0>)\n",
      "epoch: 6567 loss is tensor([-0.2473], grad_fn=<AddBackward0>)\n",
      "epoch: 6568 loss is tensor([-0.2182], grad_fn=<AddBackward0>)\n",
      "epoch: 6569 loss is tensor([-0.2983], grad_fn=<AddBackward0>)\n",
      "epoch: 6570 loss is tensor([-0.2203], grad_fn=<AddBackward0>)\n",
      "epoch: 6571 loss is tensor([-0.3092], grad_fn=<AddBackward0>)\n",
      "epoch: 6572 loss is tensor([-0.3158], grad_fn=<AddBackward0>)\n",
      "epoch: 6573 loss is tensor([-0.2402], grad_fn=<AddBackward0>)\n",
      "epoch: 6574 loss is tensor([-0.3166], grad_fn=<AddBackward0>)\n",
      "epoch: 6575 loss is tensor([-0.3126], grad_fn=<AddBackward0>)\n",
      "epoch: 6576 loss is tensor([-0.2252], grad_fn=<AddBackward0>)\n",
      "epoch: 6577 loss is tensor([-0.2896], grad_fn=<AddBackward0>)\n",
      "epoch: 6578 loss is tensor([-0.3189], grad_fn=<AddBackward0>)\n",
      "epoch: 6579 loss is tensor([-0.2730], grad_fn=<AddBackward0>)\n",
      "epoch: 6580 loss is tensor([-0.2985], grad_fn=<AddBackward0>)\n",
      "epoch: 6581 loss is tensor([-0.2767], grad_fn=<AddBackward0>)\n",
      "epoch: 6582 loss is tensor([-0.3116], grad_fn=<AddBackward0>)\n",
      "epoch: 6583 loss is tensor([-0.3103], grad_fn=<AddBackward0>)\n",
      "epoch: 6584 loss is tensor([-0.2864], grad_fn=<AddBackward0>)\n",
      "epoch: 6585 loss is tensor([-0.2640], grad_fn=<AddBackward0>)\n",
      "epoch: 6586 loss is tensor([-0.2846], grad_fn=<AddBackward0>)\n",
      "epoch: 6587 loss is tensor([-0.2693], grad_fn=<AddBackward0>)\n",
      "epoch: 6588 loss is tensor([-0.2025], grad_fn=<AddBackward0>)\n",
      "epoch: 6589 loss is tensor([-0.2575], grad_fn=<AddBackward0>)\n",
      "epoch: 6590 loss is tensor([-0.2807], grad_fn=<AddBackward0>)\n",
      "epoch: 6591 loss is tensor([-0.2835], grad_fn=<AddBackward0>)\n",
      "epoch: 6592 loss is tensor([-0.2133], grad_fn=<AddBackward0>)\n",
      "epoch: 6593 loss is tensor([-0.2690], grad_fn=<AddBackward0>)\n",
      "epoch: 6594 loss is tensor([-0.2847], grad_fn=<AddBackward0>)\n",
      "epoch: 6595 loss is tensor([-0.2941], grad_fn=<AddBackward0>)\n",
      "epoch: 6596 loss is tensor([-0.2214], grad_fn=<AddBackward0>)\n",
      "epoch: 6597 loss is tensor([-0.2533], grad_fn=<AddBackward0>)\n",
      "epoch: 6598 loss is tensor([-0.2446], grad_fn=<AddBackward0>)\n",
      "epoch: 6599 loss is tensor([-0.2283], grad_fn=<AddBackward0>)\n",
      "epoch: 6600 loss is tensor([-0.2186], grad_fn=<AddBackward0>)\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6601 loss is tensor([-0.2479], grad_fn=<AddBackward0>)\n",
      "epoch: 6602 loss is tensor([-0.2267], grad_fn=<AddBackward0>)\n",
      "epoch: 6603 loss is tensor([-0.2673], grad_fn=<AddBackward0>)\n",
      "epoch: 6604 loss is tensor([-0.2298], grad_fn=<AddBackward0>)\n",
      "epoch: 6605 loss is tensor([-0.1967], grad_fn=<AddBackward0>)\n",
      "epoch: 6606 loss is tensor([-0.2738], grad_fn=<AddBackward0>)\n",
      "epoch: 6607 loss is tensor([-0.2287], grad_fn=<AddBackward0>)\n",
      "epoch: 6608 loss is tensor([-0.1940], grad_fn=<AddBackward0>)\n",
      "epoch: 6609 loss is tensor([-0.2539], grad_fn=<AddBackward0>)\n",
      "epoch: 6610 loss is tensor([-0.2636], grad_fn=<AddBackward0>)\n",
      "epoch: 6611 loss is tensor([-0.2390], grad_fn=<AddBackward0>)\n",
      "epoch: 6612 loss is tensor([-0.2322], grad_fn=<AddBackward0>)\n",
      "epoch: 6613 loss is tensor([-0.2824], grad_fn=<AddBackward0>)\n",
      "epoch: 6614 loss is tensor([-0.2713], grad_fn=<AddBackward0>)\n",
      "epoch: 6615 loss is tensor([-0.2079], grad_fn=<AddBackward0>)\n",
      "epoch: 6616 loss is tensor([-0.2411], grad_fn=<AddBackward0>)\n",
      "epoch: 6617 loss is tensor([-0.2790], grad_fn=<AddBackward0>)\n",
      "epoch: 6618 loss is tensor([-0.2267], grad_fn=<AddBackward0>)\n",
      "epoch: 6619 loss is tensor([-0.2767], grad_fn=<AddBackward0>)\n",
      "epoch: 6620 loss is tensor([-0.2488], grad_fn=<AddBackward0>)\n",
      "epoch: 6621 loss is tensor([-0.2976], grad_fn=<AddBackward0>)\n",
      "epoch: 6622 loss is tensor([-0.2988], grad_fn=<AddBackward0>)\n",
      "epoch: 6623 loss is tensor([-0.2562], grad_fn=<AddBackward0>)\n",
      "epoch: 6624 loss is tensor([-0.2479], grad_fn=<AddBackward0>)\n",
      "epoch: 6625 loss is tensor([-0.2766], grad_fn=<AddBackward0>)\n",
      "epoch: 6626 loss is tensor([-0.2251], grad_fn=<AddBackward0>)\n",
      "epoch: 6627 loss is tensor([-0.2486], grad_fn=<AddBackward0>)\n",
      "epoch: 6628 loss is tensor([-0.2751], grad_fn=<AddBackward0>)\n",
      "epoch: 6629 loss is tensor([-0.2715], grad_fn=<AddBackward0>)\n",
      "epoch: 6630 loss is tensor([-0.2202], grad_fn=<AddBackward0>)\n",
      "epoch: 6631 loss is tensor([-0.2220], grad_fn=<AddBackward0>)\n",
      "epoch: 6632 loss is tensor([-0.2401], grad_fn=<AddBackward0>)\n",
      "epoch: 6633 loss is tensor([-0.2655], grad_fn=<AddBackward0>)\n",
      "epoch: 6634 loss is tensor([-0.2374], grad_fn=<AddBackward0>)\n",
      "epoch: 6635 loss is tensor([-0.2359], grad_fn=<AddBackward0>)\n",
      "epoch: 6636 loss is tensor([-0.2300], grad_fn=<AddBackward0>)\n",
      "epoch: 6637 loss is tensor([-0.1713], grad_fn=<AddBackward0>)\n",
      "epoch: 6638 loss is tensor([-0.2768], grad_fn=<AddBackward0>)\n",
      "epoch: 6639 loss is tensor([-0.2117], grad_fn=<AddBackward0>)\n",
      "epoch: 6640 loss is tensor([-0.2708], grad_fn=<AddBackward0>)\n",
      "epoch: 6641 loss is tensor([-0.2690], grad_fn=<AddBackward0>)\n",
      "epoch: 6642 loss is tensor([-0.2443], grad_fn=<AddBackward0>)\n",
      "epoch: 6643 loss is tensor([-0.2698], grad_fn=<AddBackward0>)\n",
      "epoch: 6644 loss is tensor([-0.2430], grad_fn=<AddBackward0>)\n",
      "epoch: 6645 loss is tensor([-0.2987], grad_fn=<AddBackward0>)\n",
      "epoch: 6646 loss is tensor([-0.2314], grad_fn=<AddBackward0>)\n",
      "epoch: 6647 loss is tensor([-0.2556], grad_fn=<AddBackward0>)\n",
      "epoch: 6648 loss is tensor([-0.3185], grad_fn=<AddBackward0>)\n",
      "epoch: 6649 loss is tensor([-0.2503], grad_fn=<AddBackward0>)\n",
      "epoch: 6650 loss is tensor([-0.2680], grad_fn=<AddBackward0>)\n",
      "epoch: 6651 loss is tensor([-0.2539], grad_fn=<AddBackward0>)\n",
      "epoch: 6652 loss is tensor([-0.3193], grad_fn=<AddBackward0>)\n",
      "epoch: 6653 loss is tensor([-0.2394], grad_fn=<AddBackward0>)\n",
      "epoch: 6654 loss is tensor([-0.2782], grad_fn=<AddBackward0>)\n",
      "epoch: 6655 loss is tensor([-0.2289], grad_fn=<AddBackward0>)\n",
      "epoch: 6656 loss is tensor([-0.2771], grad_fn=<AddBackward0>)\n",
      "epoch: 6657 loss is tensor([-0.2975], grad_fn=<AddBackward0>)\n",
      "epoch: 6658 loss is tensor([-0.2589], grad_fn=<AddBackward0>)\n",
      "epoch: 6659 loss is tensor([-0.2880], grad_fn=<AddBackward0>)\n",
      "epoch: 6660 loss is tensor([-0.2616], grad_fn=<AddBackward0>)\n",
      "epoch: 6661 loss is tensor([-0.3121], grad_fn=<AddBackward0>)\n",
      "epoch: 6662 loss is tensor([-0.2326], grad_fn=<AddBackward0>)\n",
      "epoch: 6663 loss is tensor([-0.2434], grad_fn=<AddBackward0>)\n",
      "epoch: 6664 loss is tensor([-0.2762], grad_fn=<AddBackward0>)\n",
      "epoch: 6665 loss is tensor([-0.2806], grad_fn=<AddBackward0>)\n",
      "epoch: 6666 loss is tensor([-0.2813], grad_fn=<AddBackward0>)\n",
      "epoch: 6667 loss is tensor([-0.3054], grad_fn=<AddBackward0>)\n",
      "epoch: 6668 loss is tensor([-0.2496], grad_fn=<AddBackward0>)\n",
      "epoch: 6669 loss is tensor([-0.2941], grad_fn=<AddBackward0>)\n",
      "epoch: 6670 loss is tensor([-0.2192], grad_fn=<AddBackward0>)\n",
      "epoch: 6671 loss is tensor([-0.2859], grad_fn=<AddBackward0>)\n",
      "epoch: 6672 loss is tensor([-0.2375], grad_fn=<AddBackward0>)\n",
      "epoch: 6673 loss is tensor([-0.2705], grad_fn=<AddBackward0>)\n",
      "epoch: 6674 loss is tensor([-0.2834], grad_fn=<AddBackward0>)\n",
      "epoch: 6675 loss is tensor([-0.2462], grad_fn=<AddBackward0>)\n",
      "epoch: 6676 loss is tensor([-0.2834], grad_fn=<AddBackward0>)\n",
      "epoch: 6677 loss is tensor([-0.2803], grad_fn=<AddBackward0>)\n",
      "epoch: 6678 loss is tensor([-0.2919], grad_fn=<AddBackward0>)\n",
      "epoch: 6679 loss is tensor([-0.2229], grad_fn=<AddBackward0>)\n",
      "epoch: 6680 loss is tensor([-0.2812], grad_fn=<AddBackward0>)\n",
      "epoch: 6681 loss is tensor([-0.2845], grad_fn=<AddBackward0>)\n",
      "epoch: 6682 loss is tensor([-0.2338], grad_fn=<AddBackward0>)\n",
      "epoch: 6683 loss is tensor([-0.2820], grad_fn=<AddBackward0>)\n",
      "epoch: 6684 loss is tensor([-0.2434], grad_fn=<AddBackward0>)\n",
      "epoch: 6685 loss is tensor([-0.2185], grad_fn=<AddBackward0>)\n",
      "epoch: 6686 loss is tensor([-0.2626], grad_fn=<AddBackward0>)\n",
      "epoch: 6687 loss is tensor([-0.2780], grad_fn=<AddBackward0>)\n",
      "epoch: 6688 loss is tensor([-0.2501], grad_fn=<AddBackward0>)\n",
      "epoch: 6689 loss is tensor([-0.2318], grad_fn=<AddBackward0>)\n",
      "epoch: 6690 loss is tensor([-0.2229], grad_fn=<AddBackward0>)\n",
      "epoch: 6691 loss is tensor([-0.2669], grad_fn=<AddBackward0>)\n",
      "epoch: 6692 loss is tensor([-0.2376], grad_fn=<AddBackward0>)\n",
      "epoch: 6693 loss is tensor([-0.2587], grad_fn=<AddBackward0>)\n",
      "epoch: 6694 loss is tensor([-0.2144], grad_fn=<AddBackward0>)\n",
      "epoch: 6695 loss is tensor([-0.1731], grad_fn=<AddBackward0>)\n",
      "epoch: 6696 loss is tensor([-0.2382], grad_fn=<AddBackward0>)\n",
      "epoch: 6697 loss is tensor([-0.2009], grad_fn=<AddBackward0>)\n",
      "epoch: 6698 loss is tensor([-0.2449], grad_fn=<AddBackward0>)\n",
      "epoch: 6699 loss is tensor([-0.2088], grad_fn=<AddBackward0>)\n",
      "epoch: 6700 loss is tensor([-0.2180], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6701 loss is tensor([-0.2282], grad_fn=<AddBackward0>)\n",
      "epoch: 6702 loss is tensor([-0.2036], grad_fn=<AddBackward0>)\n",
      "epoch: 6703 loss is tensor([-0.2810], grad_fn=<AddBackward0>)\n",
      "epoch: 6704 loss is tensor([-0.2554], grad_fn=<AddBackward0>)\n",
      "epoch: 6705 loss is tensor([-0.3000], grad_fn=<AddBackward0>)\n",
      "epoch: 6706 loss is tensor([-0.2398], grad_fn=<AddBackward0>)\n",
      "epoch: 6707 loss is tensor([-0.1976], grad_fn=<AddBackward0>)\n",
      "epoch: 6708 loss is tensor([-0.2846], grad_fn=<AddBackward0>)\n",
      "epoch: 6709 loss is tensor([-0.2294], grad_fn=<AddBackward0>)\n",
      "epoch: 6710 loss is tensor([-0.3321], grad_fn=<AddBackward0>)\n",
      "epoch: 6711 loss is tensor([-0.3203], grad_fn=<AddBackward0>)\n",
      "epoch: 6712 loss is tensor([-0.2529], grad_fn=<AddBackward0>)\n",
      "epoch: 6713 loss is tensor([-0.2911], grad_fn=<AddBackward0>)\n",
      "epoch: 6714 loss is tensor([-0.2970], grad_fn=<AddBackward0>)\n",
      "epoch: 6715 loss is tensor([-0.2661], grad_fn=<AddBackward0>)\n",
      "epoch: 6716 loss is tensor([-0.3319], grad_fn=<AddBackward0>)\n",
      "epoch: 6717 loss is tensor([-0.3147], grad_fn=<AddBackward0>)\n",
      "epoch: 6718 loss is tensor([-0.3154], grad_fn=<AddBackward0>)\n",
      "epoch: 6719 loss is tensor([-0.3396], grad_fn=<AddBackward0>)\n",
      "epoch: 6720 loss is tensor([-0.3100], grad_fn=<AddBackward0>)\n",
      "epoch: 6721 loss is tensor([-0.3447], grad_fn=<AddBackward0>)\n",
      "epoch: 6722 loss is tensor([-0.3059], grad_fn=<AddBackward0>)\n",
      "epoch: 6723 loss is tensor([-0.3705], grad_fn=<AddBackward0>)\n",
      "epoch: 6724 loss is tensor([-0.3263], grad_fn=<AddBackward0>)\n",
      "epoch: 6725 loss is tensor([-0.2836], grad_fn=<AddBackward0>)\n",
      "epoch: 6726 loss is tensor([-0.3327], grad_fn=<AddBackward0>)\n",
      "epoch: 6727 loss is tensor([-0.3015], grad_fn=<AddBackward0>)\n",
      "epoch: 6728 loss is tensor([-0.2554], grad_fn=<AddBackward0>)\n",
      "epoch: 6729 loss is tensor([-0.3083], grad_fn=<AddBackward0>)\n",
      "epoch: 6730 loss is tensor([-0.2708], grad_fn=<AddBackward0>)\n",
      "epoch: 6731 loss is tensor([-0.2660], grad_fn=<AddBackward0>)\n",
      "epoch: 6732 loss is tensor([-0.3142], grad_fn=<AddBackward0>)\n",
      "epoch: 6733 loss is tensor([-0.2745], grad_fn=<AddBackward0>)\n",
      "epoch: 6734 loss is tensor([-0.2458], grad_fn=<AddBackward0>)\n",
      "epoch: 6735 loss is tensor([-0.1996], grad_fn=<AddBackward0>)\n",
      "epoch: 6736 loss is tensor([-0.2593], grad_fn=<AddBackward0>)\n",
      "epoch: 6737 loss is tensor([-0.1878], grad_fn=<AddBackward0>)\n",
      "epoch: 6738 loss is tensor([-0.2273], grad_fn=<AddBackward0>)\n",
      "epoch: 6739 loss is tensor([-0.2570], grad_fn=<AddBackward0>)\n",
      "epoch: 6740 loss is tensor([-0.2351], grad_fn=<AddBackward0>)\n",
      "epoch: 6741 loss is tensor([-0.2224], grad_fn=<AddBackward0>)\n",
      "epoch: 6742 loss is tensor([-0.1966], grad_fn=<AddBackward0>)\n",
      "epoch: 6743 loss is tensor([-0.2025], grad_fn=<AddBackward0>)\n",
      "epoch: 6744 loss is tensor([-0.2528], grad_fn=<AddBackward0>)\n",
      "epoch: 6745 loss is tensor([-0.2070], grad_fn=<AddBackward0>)\n",
      "epoch: 6746 loss is tensor([-0.2706], grad_fn=<AddBackward0>)\n",
      "epoch: 6747 loss is tensor([-0.2252], grad_fn=<AddBackward0>)\n",
      "epoch: 6748 loss is tensor([-0.2457], grad_fn=<AddBackward0>)\n",
      "epoch: 6749 loss is tensor([-0.2364], grad_fn=<AddBackward0>)\n",
      "epoch: 6750 loss is tensor([-0.2432], grad_fn=<AddBackward0>)\n",
      "epoch: 6751 loss is tensor([-0.2499], grad_fn=<AddBackward0>)\n",
      "epoch: 6752 loss is tensor([-0.2873], grad_fn=<AddBackward0>)\n",
      "epoch: 6753 loss is tensor([-0.2711], grad_fn=<AddBackward0>)\n",
      "epoch: 6754 loss is tensor([-0.2175], grad_fn=<AddBackward0>)\n",
      "epoch: 6755 loss is tensor([-0.2217], grad_fn=<AddBackward0>)\n",
      "epoch: 6756 loss is tensor([-0.2456], grad_fn=<AddBackward0>)\n",
      "epoch: 6757 loss is tensor([-0.1351], grad_fn=<AddBackward0>)\n",
      "epoch: 6758 loss is tensor([-0.2329], grad_fn=<AddBackward0>)\n",
      "epoch: 6759 loss is tensor([-0.2046], grad_fn=<AddBackward0>)\n",
      "epoch: 6760 loss is tensor([-0.2245], grad_fn=<AddBackward0>)\n",
      "epoch: 6761 loss is tensor([-0.2417], grad_fn=<AddBackward0>)\n",
      "epoch: 6762 loss is tensor([-0.2971], grad_fn=<AddBackward0>)\n",
      "epoch: 6763 loss is tensor([-0.2623], grad_fn=<AddBackward0>)\n",
      "epoch: 6764 loss is tensor([-0.2560], grad_fn=<AddBackward0>)\n",
      "epoch: 6765 loss is tensor([-0.3217], grad_fn=<AddBackward0>)\n",
      "epoch: 6766 loss is tensor([-0.2588], grad_fn=<AddBackward0>)\n",
      "epoch: 6767 loss is tensor([-0.1694], grad_fn=<AddBackward0>)\n",
      "epoch: 6768 loss is tensor([-0.2899], grad_fn=<AddBackward0>)\n",
      "epoch: 6769 loss is tensor([-0.2937], grad_fn=<AddBackward0>)\n",
      "epoch: 6770 loss is tensor([-0.2822], grad_fn=<AddBackward0>)\n",
      "epoch: 6771 loss is tensor([-0.2469], grad_fn=<AddBackward0>)\n",
      "epoch: 6772 loss is tensor([-0.2674], grad_fn=<AddBackward0>)\n",
      "epoch: 6773 loss is tensor([-0.2760], grad_fn=<AddBackward0>)\n",
      "epoch: 6774 loss is tensor([-0.2594], grad_fn=<AddBackward0>)\n",
      "epoch: 6775 loss is tensor([-0.3299], grad_fn=<AddBackward0>)\n",
      "epoch: 6776 loss is tensor([-0.2236], grad_fn=<AddBackward0>)\n",
      "epoch: 6777 loss is tensor([-0.2528], grad_fn=<AddBackward0>)\n",
      "epoch: 6778 loss is tensor([-0.2884], grad_fn=<AddBackward0>)\n",
      "epoch: 6779 loss is tensor([-0.2870], grad_fn=<AddBackward0>)\n",
      "epoch: 6780 loss is tensor([-0.2584], grad_fn=<AddBackward0>)\n",
      "epoch: 6781 loss is tensor([-0.2840], grad_fn=<AddBackward0>)\n",
      "epoch: 6782 loss is tensor([-0.2374], grad_fn=<AddBackward0>)\n",
      "epoch: 6783 loss is tensor([-0.2390], grad_fn=<AddBackward0>)\n",
      "epoch: 6784 loss is tensor([-0.2916], grad_fn=<AddBackward0>)\n",
      "epoch: 6785 loss is tensor([-0.2627], grad_fn=<AddBackward0>)\n",
      "epoch: 6786 loss is tensor([-0.2853], grad_fn=<AddBackward0>)\n",
      "epoch: 6787 loss is tensor([-0.2914], grad_fn=<AddBackward0>)\n",
      "epoch: 6788 loss is tensor([-0.2533], grad_fn=<AddBackward0>)\n",
      "epoch: 6789 loss is tensor([-0.2487], grad_fn=<AddBackward0>)\n",
      "epoch: 6790 loss is tensor([-0.2623], grad_fn=<AddBackward0>)\n",
      "epoch: 6791 loss is tensor([-0.2597], grad_fn=<AddBackward0>)\n",
      "epoch: 6792 loss is tensor([-0.2845], grad_fn=<AddBackward0>)\n",
      "epoch: 6793 loss is tensor([-0.3001], grad_fn=<AddBackward0>)\n",
      "epoch: 6794 loss is tensor([-0.3011], grad_fn=<AddBackward0>)\n",
      "epoch: 6795 loss is tensor([-0.2711], grad_fn=<AddBackward0>)\n",
      "epoch: 6796 loss is tensor([-0.3012], grad_fn=<AddBackward0>)\n",
      "epoch: 6797 loss is tensor([-0.3037], grad_fn=<AddBackward0>)\n",
      "epoch: 6798 loss is tensor([-0.3303], grad_fn=<AddBackward0>)\n",
      "epoch: 6799 loss is tensor([-0.2920], grad_fn=<AddBackward0>)\n",
      "epoch: 6800 loss is tensor([-0.2247], grad_fn=<AddBackward0>)\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6801 loss is tensor([-0.2686], grad_fn=<AddBackward0>)\n",
      "epoch: 6802 loss is tensor([-0.3047], grad_fn=<AddBackward0>)\n",
      "epoch: 6803 loss is tensor([-0.2732], grad_fn=<AddBackward0>)\n",
      "epoch: 6804 loss is tensor([-0.2874], grad_fn=<AddBackward0>)\n",
      "epoch: 6805 loss is tensor([-0.2699], grad_fn=<AddBackward0>)\n",
      "epoch: 6806 loss is tensor([-0.3322], grad_fn=<AddBackward0>)\n",
      "epoch: 6807 loss is tensor([-0.2382], grad_fn=<AddBackward0>)\n",
      "epoch: 6808 loss is tensor([-0.2902], grad_fn=<AddBackward0>)\n",
      "epoch: 6809 loss is tensor([-0.3326], grad_fn=<AddBackward0>)\n",
      "epoch: 6810 loss is tensor([-0.2843], grad_fn=<AddBackward0>)\n",
      "epoch: 6811 loss is tensor([-0.3084], grad_fn=<AddBackward0>)\n",
      "epoch: 6812 loss is tensor([-0.2623], grad_fn=<AddBackward0>)\n",
      "epoch: 6813 loss is tensor([-0.3386], grad_fn=<AddBackward0>)\n",
      "epoch: 6814 loss is tensor([-0.2815], grad_fn=<AddBackward0>)\n",
      "epoch: 6815 loss is tensor([-0.2815], grad_fn=<AddBackward0>)\n",
      "epoch: 6816 loss is tensor([-0.3248], grad_fn=<AddBackward0>)\n",
      "epoch: 6817 loss is tensor([-0.2835], grad_fn=<AddBackward0>)\n",
      "epoch: 6818 loss is tensor([-0.2259], grad_fn=<AddBackward0>)\n",
      "epoch: 6819 loss is tensor([-0.3440], grad_fn=<AddBackward0>)\n",
      "epoch: 6820 loss is tensor([-0.2337], grad_fn=<AddBackward0>)\n",
      "epoch: 6821 loss is tensor([-0.3010], grad_fn=<AddBackward0>)\n",
      "epoch: 6822 loss is tensor([-0.3275], grad_fn=<AddBackward0>)\n",
      "epoch: 6823 loss is tensor([-0.3200], grad_fn=<AddBackward0>)\n",
      "epoch: 6824 loss is tensor([-0.3031], grad_fn=<AddBackward0>)\n",
      "epoch: 6825 loss is tensor([-0.3151], grad_fn=<AddBackward0>)\n",
      "epoch: 6826 loss is tensor([-0.3198], grad_fn=<AddBackward0>)\n",
      "epoch: 6827 loss is tensor([-0.3061], grad_fn=<AddBackward0>)\n",
      "epoch: 6828 loss is tensor([-0.3149], grad_fn=<AddBackward0>)\n",
      "epoch: 6829 loss is tensor([-0.3274], grad_fn=<AddBackward0>)\n",
      "epoch: 6830 loss is tensor([-0.2835], grad_fn=<AddBackward0>)\n",
      "epoch: 6831 loss is tensor([-0.3551], grad_fn=<AddBackward0>)\n",
      "epoch: 6832 loss is tensor([-0.3000], grad_fn=<AddBackward0>)\n",
      "epoch: 6833 loss is tensor([-0.2870], grad_fn=<AddBackward0>)\n",
      "epoch: 6834 loss is tensor([-0.2819], grad_fn=<AddBackward0>)\n",
      "epoch: 6835 loss is tensor([-0.3457], grad_fn=<AddBackward0>)\n",
      "epoch: 6836 loss is tensor([-0.3037], grad_fn=<AddBackward0>)\n",
      "epoch: 6837 loss is tensor([-0.3240], grad_fn=<AddBackward0>)\n",
      "epoch: 6838 loss is tensor([-0.2755], grad_fn=<AddBackward0>)\n",
      "epoch: 6839 loss is tensor([-0.2688], grad_fn=<AddBackward0>)\n",
      "epoch: 6840 loss is tensor([-0.3024], grad_fn=<AddBackward0>)\n",
      "epoch: 6841 loss is tensor([-0.2225], grad_fn=<AddBackward0>)\n",
      "epoch: 6842 loss is tensor([-0.2606], grad_fn=<AddBackward0>)\n",
      "epoch: 6843 loss is tensor([-0.3172], grad_fn=<AddBackward0>)\n",
      "epoch: 6844 loss is tensor([-0.2901], grad_fn=<AddBackward0>)\n",
      "epoch: 6845 loss is tensor([-0.3107], grad_fn=<AddBackward0>)\n",
      "epoch: 6846 loss is tensor([-0.2892], grad_fn=<AddBackward0>)\n",
      "epoch: 6847 loss is tensor([-0.2874], grad_fn=<AddBackward0>)\n",
      "epoch: 6848 loss is tensor([-0.2750], grad_fn=<AddBackward0>)\n",
      "epoch: 6849 loss is tensor([-0.2819], grad_fn=<AddBackward0>)\n",
      "epoch: 6850 loss is tensor([-0.3592], grad_fn=<AddBackward0>)\n",
      "epoch: 6851 loss is tensor([-0.3234], grad_fn=<AddBackward0>)\n",
      "epoch: 6852 loss is tensor([-0.3067], grad_fn=<AddBackward0>)\n",
      "epoch: 6853 loss is tensor([-0.2598], grad_fn=<AddBackward0>)\n",
      "epoch: 6854 loss is tensor([-0.3025], grad_fn=<AddBackward0>)\n",
      "epoch: 6855 loss is tensor([-0.2776], grad_fn=<AddBackward0>)\n",
      "epoch: 6856 loss is tensor([-0.3402], grad_fn=<AddBackward0>)\n",
      "epoch: 6857 loss is tensor([-0.2918], grad_fn=<AddBackward0>)\n",
      "epoch: 6858 loss is tensor([-0.3135], grad_fn=<AddBackward0>)\n",
      "epoch: 6859 loss is tensor([-0.3326], grad_fn=<AddBackward0>)\n",
      "epoch: 6860 loss is tensor([-0.3621], grad_fn=<AddBackward0>)\n",
      "epoch: 6861 loss is tensor([-0.3089], grad_fn=<AddBackward0>)\n",
      "epoch: 6862 loss is tensor([-0.3542], grad_fn=<AddBackward0>)\n",
      "epoch: 6863 loss is tensor([-0.2544], grad_fn=<AddBackward0>)\n",
      "epoch: 6864 loss is tensor([-0.2702], grad_fn=<AddBackward0>)\n",
      "epoch: 6865 loss is tensor([-0.3383], grad_fn=<AddBackward0>)\n",
      "epoch: 6866 loss is tensor([-0.2407], grad_fn=<AddBackward0>)\n",
      "epoch: 6867 loss is tensor([-0.3138], grad_fn=<AddBackward0>)\n",
      "epoch: 6868 loss is tensor([-0.3242], grad_fn=<AddBackward0>)\n",
      "epoch: 6869 loss is tensor([-0.3690], grad_fn=<AddBackward0>)\n",
      "epoch: 6870 loss is tensor([-0.2902], grad_fn=<AddBackward0>)\n",
      "epoch: 6871 loss is tensor([-0.3365], grad_fn=<AddBackward0>)\n",
      "epoch: 6872 loss is tensor([-0.3022], grad_fn=<AddBackward0>)\n",
      "epoch: 6873 loss is tensor([-0.2646], grad_fn=<AddBackward0>)\n",
      "epoch: 6874 loss is tensor([-0.2932], grad_fn=<AddBackward0>)\n",
      "epoch: 6875 loss is tensor([-0.3184], grad_fn=<AddBackward0>)\n",
      "epoch: 6876 loss is tensor([-0.2914], grad_fn=<AddBackward0>)\n",
      "epoch: 6877 loss is tensor([-0.3056], grad_fn=<AddBackward0>)\n",
      "epoch: 6878 loss is tensor([-0.2858], grad_fn=<AddBackward0>)\n",
      "epoch: 6879 loss is tensor([-0.2943], grad_fn=<AddBackward0>)\n",
      "epoch: 6880 loss is tensor([-0.3207], grad_fn=<AddBackward0>)\n",
      "epoch: 6881 loss is tensor([-0.2464], grad_fn=<AddBackward0>)\n",
      "epoch: 6882 loss is tensor([-0.3342], grad_fn=<AddBackward0>)\n",
      "epoch: 6883 loss is tensor([-0.2779], grad_fn=<AddBackward0>)\n",
      "epoch: 6884 loss is tensor([-0.2959], grad_fn=<AddBackward0>)\n",
      "epoch: 6885 loss is tensor([-0.2748], grad_fn=<AddBackward0>)\n",
      "epoch: 6886 loss is tensor([-0.3307], grad_fn=<AddBackward0>)\n",
      "epoch: 6887 loss is tensor([-0.3309], grad_fn=<AddBackward0>)\n",
      "epoch: 6888 loss is tensor([-0.3315], grad_fn=<AddBackward0>)\n",
      "epoch: 6889 loss is tensor([-0.3319], grad_fn=<AddBackward0>)\n",
      "epoch: 6890 loss is tensor([-0.3075], grad_fn=<AddBackward0>)\n",
      "epoch: 6891 loss is tensor([-0.2850], grad_fn=<AddBackward0>)\n",
      "epoch: 6892 loss is tensor([-0.2899], grad_fn=<AddBackward0>)\n",
      "epoch: 6893 loss is tensor([-0.2750], grad_fn=<AddBackward0>)\n",
      "epoch: 6894 loss is tensor([-0.3061], grad_fn=<AddBackward0>)\n",
      "epoch: 6895 loss is tensor([-0.3005], grad_fn=<AddBackward0>)\n",
      "epoch: 6896 loss is tensor([-0.3136], grad_fn=<AddBackward0>)\n",
      "epoch: 6897 loss is tensor([-0.2878], grad_fn=<AddBackward0>)\n",
      "epoch: 6898 loss is tensor([-0.3171], grad_fn=<AddBackward0>)\n",
      "epoch: 6899 loss is tensor([-0.3352], grad_fn=<AddBackward0>)\n",
      "epoch: 6900 loss is tensor([-0.3456], grad_fn=<AddBackward0>)\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6901 loss is tensor([-0.2944], grad_fn=<AddBackward0>)\n",
      "epoch: 6902 loss is tensor([-0.2790], grad_fn=<AddBackward0>)\n",
      "epoch: 6903 loss is tensor([-0.2961], grad_fn=<AddBackward0>)\n",
      "epoch: 6904 loss is tensor([-0.2941], grad_fn=<AddBackward0>)\n",
      "epoch: 6905 loss is tensor([-0.3300], grad_fn=<AddBackward0>)\n",
      "epoch: 6906 loss is tensor([-0.2971], grad_fn=<AddBackward0>)\n",
      "epoch: 6907 loss is tensor([-0.3020], grad_fn=<AddBackward0>)\n",
      "epoch: 6908 loss is tensor([-0.2926], grad_fn=<AddBackward0>)\n",
      "epoch: 6909 loss is tensor([-0.3218], grad_fn=<AddBackward0>)\n",
      "epoch: 6910 loss is tensor([-0.3423], grad_fn=<AddBackward0>)\n",
      "epoch: 6911 loss is tensor([-0.2850], grad_fn=<AddBackward0>)\n",
      "epoch: 6912 loss is tensor([-0.2975], grad_fn=<AddBackward0>)\n",
      "epoch: 6913 loss is tensor([-0.3007], grad_fn=<AddBackward0>)\n",
      "epoch: 6914 loss is tensor([-0.3059], grad_fn=<AddBackward0>)\n",
      "epoch: 6915 loss is tensor([-0.2794], grad_fn=<AddBackward0>)\n",
      "epoch: 6916 loss is tensor([-0.3556], grad_fn=<AddBackward0>)\n",
      "epoch: 6917 loss is tensor([-0.2782], grad_fn=<AddBackward0>)\n",
      "epoch: 6918 loss is tensor([-0.3482], grad_fn=<AddBackward0>)\n",
      "epoch: 6919 loss is tensor([-0.3065], grad_fn=<AddBackward0>)\n",
      "epoch: 6920 loss is tensor([-0.3226], grad_fn=<AddBackward0>)\n",
      "epoch: 6921 loss is tensor([-0.3047], grad_fn=<AddBackward0>)\n",
      "epoch: 6922 loss is tensor([-0.3105], grad_fn=<AddBackward0>)\n",
      "epoch: 6923 loss is tensor([-0.3462], grad_fn=<AddBackward0>)\n",
      "epoch: 6924 loss is tensor([-0.3071], grad_fn=<AddBackward0>)\n",
      "epoch: 6925 loss is tensor([-0.3410], grad_fn=<AddBackward0>)\n",
      "epoch: 6926 loss is tensor([-0.2947], grad_fn=<AddBackward0>)\n",
      "epoch: 6927 loss is tensor([-0.2889], grad_fn=<AddBackward0>)\n",
      "epoch: 6928 loss is tensor([-0.2443], grad_fn=<AddBackward0>)\n",
      "epoch: 6929 loss is tensor([-0.2888], grad_fn=<AddBackward0>)\n",
      "epoch: 6930 loss is tensor([-0.3048], grad_fn=<AddBackward0>)\n",
      "epoch: 6931 loss is tensor([-0.3255], grad_fn=<AddBackward0>)\n",
      "epoch: 6932 loss is tensor([-0.2903], grad_fn=<AddBackward0>)\n",
      "epoch: 6933 loss is tensor([-0.2837], grad_fn=<AddBackward0>)\n",
      "epoch: 6934 loss is tensor([-0.2678], grad_fn=<AddBackward0>)\n",
      "epoch: 6935 loss is tensor([-0.2402], grad_fn=<AddBackward0>)\n",
      "epoch: 6936 loss is tensor([-0.3197], grad_fn=<AddBackward0>)\n",
      "epoch: 6937 loss is tensor([-0.3066], grad_fn=<AddBackward0>)\n",
      "epoch: 6938 loss is tensor([-0.2689], grad_fn=<AddBackward0>)\n",
      "epoch: 6939 loss is tensor([-0.2302], grad_fn=<AddBackward0>)\n",
      "epoch: 6940 loss is tensor([-0.2968], grad_fn=<AddBackward0>)\n",
      "epoch: 6941 loss is tensor([-0.3360], grad_fn=<AddBackward0>)\n",
      "epoch: 6942 loss is tensor([-0.3099], grad_fn=<AddBackward0>)\n",
      "epoch: 6943 loss is tensor([-0.3107], grad_fn=<AddBackward0>)\n",
      "epoch: 6944 loss is tensor([-0.3426], grad_fn=<AddBackward0>)\n",
      "epoch: 6945 loss is tensor([-0.3369], grad_fn=<AddBackward0>)\n",
      "epoch: 6946 loss is tensor([-0.3067], grad_fn=<AddBackward0>)\n",
      "epoch: 6947 loss is tensor([-0.2639], grad_fn=<AddBackward0>)\n",
      "epoch: 6948 loss is tensor([-0.3047], grad_fn=<AddBackward0>)\n",
      "epoch: 6949 loss is tensor([-0.2881], grad_fn=<AddBackward0>)\n",
      "epoch: 6950 loss is tensor([-0.2283], grad_fn=<AddBackward0>)\n",
      "epoch: 6951 loss is tensor([-0.2701], grad_fn=<AddBackward0>)\n",
      "epoch: 6952 loss is tensor([-0.2482], grad_fn=<AddBackward0>)\n",
      "epoch: 6953 loss is tensor([-0.1690], grad_fn=<AddBackward0>)\n",
      "epoch: 6954 loss is tensor([-0.2644], grad_fn=<AddBackward0>)\n",
      "epoch: 6955 loss is tensor([-0.2630], grad_fn=<AddBackward0>)\n",
      "epoch: 6956 loss is tensor([-0.2534], grad_fn=<AddBackward0>)\n",
      "epoch: 6957 loss is tensor([-0.2949], grad_fn=<AddBackward0>)\n",
      "epoch: 6958 loss is tensor([-0.2923], grad_fn=<AddBackward0>)\n",
      "epoch: 6959 loss is tensor([-0.2215], grad_fn=<AddBackward0>)\n",
      "epoch: 6960 loss is tensor([-0.2965], grad_fn=<AddBackward0>)\n",
      "epoch: 6961 loss is tensor([-0.2773], grad_fn=<AddBackward0>)\n",
      "epoch: 6962 loss is tensor([-0.2621], grad_fn=<AddBackward0>)\n",
      "epoch: 6963 loss is tensor([-0.3397], grad_fn=<AddBackward0>)\n",
      "epoch: 6964 loss is tensor([-0.2819], grad_fn=<AddBackward0>)\n",
      "epoch: 6965 loss is tensor([-0.3211], grad_fn=<AddBackward0>)\n",
      "epoch: 6966 loss is tensor([-0.2740], grad_fn=<AddBackward0>)\n",
      "epoch: 6967 loss is tensor([-0.3385], grad_fn=<AddBackward0>)\n",
      "epoch: 6968 loss is tensor([-0.3366], grad_fn=<AddBackward0>)\n",
      "epoch: 6969 loss is tensor([-0.2905], grad_fn=<AddBackward0>)\n",
      "epoch: 6970 loss is tensor([-0.3479], grad_fn=<AddBackward0>)\n",
      "epoch: 6971 loss is tensor([-0.3285], grad_fn=<AddBackward0>)\n",
      "epoch: 6972 loss is tensor([-0.3578], grad_fn=<AddBackward0>)\n",
      "epoch: 6973 loss is tensor([-0.3345], grad_fn=<AddBackward0>)\n",
      "epoch: 6974 loss is tensor([-0.2895], grad_fn=<AddBackward0>)\n",
      "epoch: 6975 loss is tensor([-0.3331], grad_fn=<AddBackward0>)\n",
      "epoch: 6976 loss is tensor([-0.3008], grad_fn=<AddBackward0>)\n",
      "epoch: 6977 loss is tensor([-0.3042], grad_fn=<AddBackward0>)\n",
      "epoch: 6978 loss is tensor([-0.3358], grad_fn=<AddBackward0>)\n",
      "epoch: 6979 loss is tensor([-0.3756], grad_fn=<AddBackward0>)\n",
      "epoch: 6980 loss is tensor([-0.3870], grad_fn=<AddBackward0>)\n",
      "epoch: 6981 loss is tensor([-0.3237], grad_fn=<AddBackward0>)\n",
      "epoch: 6982 loss is tensor([-0.2387], grad_fn=<AddBackward0>)\n",
      "epoch: 6983 loss is tensor([-0.3505], grad_fn=<AddBackward0>)\n",
      "epoch: 6984 loss is tensor([-0.3502], grad_fn=<AddBackward0>)\n",
      "epoch: 6985 loss is tensor([-0.3271], grad_fn=<AddBackward0>)\n",
      "epoch: 6986 loss is tensor([-0.3101], grad_fn=<AddBackward0>)\n",
      "epoch: 6987 loss is tensor([-0.3012], grad_fn=<AddBackward0>)\n",
      "epoch: 6988 loss is tensor([-0.3207], grad_fn=<AddBackward0>)\n",
      "epoch: 6989 loss is tensor([-0.3584], grad_fn=<AddBackward0>)\n",
      "epoch: 6990 loss is tensor([-0.3512], grad_fn=<AddBackward0>)\n",
      "epoch: 6991 loss is tensor([-0.2988], grad_fn=<AddBackward0>)\n",
      "epoch: 6992 loss is tensor([-0.2187], grad_fn=<AddBackward0>)\n",
      "epoch: 6993 loss is tensor([-0.2969], grad_fn=<AddBackward0>)\n",
      "epoch: 6994 loss is tensor([-0.2802], grad_fn=<AddBackward0>)\n",
      "epoch: 6995 loss is tensor([-0.2404], grad_fn=<AddBackward0>)\n",
      "epoch: 6996 loss is tensor([-0.3670], grad_fn=<AddBackward0>)\n",
      "epoch: 6997 loss is tensor([-0.3104], grad_fn=<AddBackward0>)\n",
      "epoch: 6998 loss is tensor([-0.2363], grad_fn=<AddBackward0>)\n",
      "epoch: 6999 loss is tensor([-0.3099], grad_fn=<AddBackward0>)\n",
      "epoch: 7000 loss is tensor([-0.3580], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7001 loss is tensor([-0.3033], grad_fn=<AddBackward0>)\n",
      "epoch: 7002 loss is tensor([-0.3849], grad_fn=<AddBackward0>)\n",
      "epoch: 7003 loss is tensor([-0.3221], grad_fn=<AddBackward0>)\n",
      "epoch: 7004 loss is tensor([-0.3372], grad_fn=<AddBackward0>)\n",
      "epoch: 7005 loss is tensor([-0.3030], grad_fn=<AddBackward0>)\n",
      "epoch: 7006 loss is tensor([-0.3112], grad_fn=<AddBackward0>)\n",
      "epoch: 7007 loss is tensor([-0.3021], grad_fn=<AddBackward0>)\n",
      "epoch: 7008 loss is tensor([-0.3768], grad_fn=<AddBackward0>)\n",
      "epoch: 7009 loss is tensor([-0.2676], grad_fn=<AddBackward0>)\n",
      "epoch: 7010 loss is tensor([-0.3582], grad_fn=<AddBackward0>)\n",
      "epoch: 7011 loss is tensor([-0.2765], grad_fn=<AddBackward0>)\n",
      "epoch: 7012 loss is tensor([-0.3184], grad_fn=<AddBackward0>)\n",
      "epoch: 7013 loss is tensor([-0.3116], grad_fn=<AddBackward0>)\n",
      "epoch: 7014 loss is tensor([-0.3106], grad_fn=<AddBackward0>)\n",
      "epoch: 7015 loss is tensor([-0.2837], grad_fn=<AddBackward0>)\n",
      "epoch: 7016 loss is tensor([-0.3215], grad_fn=<AddBackward0>)\n",
      "epoch: 7017 loss is tensor([-0.2791], grad_fn=<AddBackward0>)\n",
      "epoch: 7018 loss is tensor([-0.2521], grad_fn=<AddBackward0>)\n",
      "epoch: 7019 loss is tensor([-0.3737], grad_fn=<AddBackward0>)\n",
      "epoch: 7020 loss is tensor([-0.3197], grad_fn=<AddBackward0>)\n",
      "epoch: 7021 loss is tensor([-0.3458], grad_fn=<AddBackward0>)\n",
      "epoch: 7022 loss is tensor([-0.3109], grad_fn=<AddBackward0>)\n",
      "epoch: 7023 loss is tensor([-0.2388], grad_fn=<AddBackward0>)\n",
      "epoch: 7024 loss is tensor([-0.3030], grad_fn=<AddBackward0>)\n",
      "epoch: 7025 loss is tensor([-0.2737], grad_fn=<AddBackward0>)\n",
      "epoch: 7026 loss is tensor([-0.2930], grad_fn=<AddBackward0>)\n",
      "epoch: 7027 loss is tensor([-0.2740], grad_fn=<AddBackward0>)\n",
      "epoch: 7028 loss is tensor([-0.1841], grad_fn=<AddBackward0>)\n",
      "epoch: 7029 loss is tensor([-0.2605], grad_fn=<AddBackward0>)\n",
      "epoch: 7030 loss is tensor([-0.2515], grad_fn=<AddBackward0>)\n",
      "epoch: 7031 loss is tensor([-0.2065], grad_fn=<AddBackward0>)\n",
      "epoch: 7032 loss is tensor([-0.2859], grad_fn=<AddBackward0>)\n",
      "epoch: 7033 loss is tensor([-0.3024], grad_fn=<AddBackward0>)\n",
      "epoch: 7034 loss is tensor([-0.2345], grad_fn=<AddBackward0>)\n",
      "epoch: 7035 loss is tensor([-0.2767], grad_fn=<AddBackward0>)\n",
      "epoch: 7036 loss is tensor([-0.2409], grad_fn=<AddBackward0>)\n",
      "epoch: 7037 loss is tensor([-0.2887], grad_fn=<AddBackward0>)\n",
      "epoch: 7038 loss is tensor([-0.2488], grad_fn=<AddBackward0>)\n",
      "epoch: 7039 loss is tensor([-0.2162], grad_fn=<AddBackward0>)\n",
      "epoch: 7040 loss is tensor([-0.2978], grad_fn=<AddBackward0>)\n",
      "epoch: 7041 loss is tensor([-0.2708], grad_fn=<AddBackward0>)\n",
      "epoch: 7042 loss is tensor([-0.2584], grad_fn=<AddBackward0>)\n",
      "epoch: 7043 loss is tensor([-0.2897], grad_fn=<AddBackward0>)\n",
      "epoch: 7044 loss is tensor([-0.2501], grad_fn=<AddBackward0>)\n",
      "epoch: 7045 loss is tensor([-0.2593], grad_fn=<AddBackward0>)\n",
      "epoch: 7046 loss is tensor([-0.2593], grad_fn=<AddBackward0>)\n",
      "epoch: 7047 loss is tensor([-0.2776], grad_fn=<AddBackward0>)\n",
      "epoch: 7048 loss is tensor([-0.2837], grad_fn=<AddBackward0>)\n",
      "epoch: 7049 loss is tensor([-0.3224], grad_fn=<AddBackward0>)\n",
      "epoch: 7050 loss is tensor([-0.2874], grad_fn=<AddBackward0>)\n",
      "epoch: 7051 loss is tensor([-0.3013], grad_fn=<AddBackward0>)\n",
      "epoch: 7052 loss is tensor([-0.3217], grad_fn=<AddBackward0>)\n",
      "epoch: 7053 loss is tensor([-0.2941], grad_fn=<AddBackward0>)\n",
      "epoch: 7054 loss is tensor([-0.3158], grad_fn=<AddBackward0>)\n",
      "epoch: 7055 loss is tensor([-0.2565], grad_fn=<AddBackward0>)\n",
      "epoch: 7056 loss is tensor([-0.1814], grad_fn=<AddBackward0>)\n",
      "epoch: 7057 loss is tensor([-0.2673], grad_fn=<AddBackward0>)\n",
      "epoch: 7058 loss is tensor([-0.2316], grad_fn=<AddBackward0>)\n",
      "epoch: 7059 loss is tensor([-0.2401], grad_fn=<AddBackward0>)\n",
      "epoch: 7060 loss is tensor([-0.1977], grad_fn=<AddBackward0>)\n",
      "epoch: 7061 loss is tensor([-0.2153], grad_fn=<AddBackward0>)\n",
      "epoch: 7062 loss is tensor([-0.2492], grad_fn=<AddBackward0>)\n",
      "epoch: 7063 loss is tensor([-0.1797], grad_fn=<AddBackward0>)\n",
      "epoch: 7064 loss is tensor([-0.2360], grad_fn=<AddBackward0>)\n",
      "epoch: 7065 loss is tensor([-0.2227], grad_fn=<AddBackward0>)\n",
      "epoch: 7066 loss is tensor([-0.2298], grad_fn=<AddBackward0>)\n",
      "epoch: 7067 loss is tensor([-0.2530], grad_fn=<AddBackward0>)\n",
      "epoch: 7068 loss is tensor([-0.2945], grad_fn=<AddBackward0>)\n",
      "epoch: 7069 loss is tensor([-0.2514], grad_fn=<AddBackward0>)\n",
      "epoch: 7070 loss is tensor([-0.2307], grad_fn=<AddBackward0>)\n",
      "epoch: 7071 loss is tensor([-0.2780], grad_fn=<AddBackward0>)\n",
      "epoch: 7072 loss is tensor([-0.3005], grad_fn=<AddBackward0>)\n",
      "epoch: 7073 loss is tensor([-0.2894], grad_fn=<AddBackward0>)\n",
      "epoch: 7074 loss is tensor([-0.2933], grad_fn=<AddBackward0>)\n",
      "epoch: 7075 loss is tensor([-0.2854], grad_fn=<AddBackward0>)\n",
      "epoch: 7076 loss is tensor([-0.3232], grad_fn=<AddBackward0>)\n",
      "epoch: 7077 loss is tensor([-0.1996], grad_fn=<AddBackward0>)\n",
      "epoch: 7078 loss is tensor([-0.2519], grad_fn=<AddBackward0>)\n",
      "epoch: 7079 loss is tensor([-0.2224], grad_fn=<AddBackward0>)\n",
      "epoch: 7080 loss is tensor([-0.2621], grad_fn=<AddBackward0>)\n",
      "epoch: 7081 loss is tensor([-0.2558], grad_fn=<AddBackward0>)\n",
      "epoch: 7082 loss is tensor([-0.2591], grad_fn=<AddBackward0>)\n",
      "epoch: 7083 loss is tensor([-0.2932], grad_fn=<AddBackward0>)\n",
      "epoch: 7084 loss is tensor([-0.2304], grad_fn=<AddBackward0>)\n",
      "epoch: 7085 loss is tensor([-0.2648], grad_fn=<AddBackward0>)\n",
      "epoch: 7086 loss is tensor([-0.2306], grad_fn=<AddBackward0>)\n",
      "epoch: 7087 loss is tensor([-0.2761], grad_fn=<AddBackward0>)\n",
      "epoch: 7088 loss is tensor([-0.2440], grad_fn=<AddBackward0>)\n",
      "epoch: 7089 loss is tensor([-0.2281], grad_fn=<AddBackward0>)\n",
      "epoch: 7090 loss is tensor([-0.2838], grad_fn=<AddBackward0>)\n",
      "epoch: 7091 loss is tensor([-0.2668], grad_fn=<AddBackward0>)\n",
      "epoch: 7092 loss is tensor([-0.2129], grad_fn=<AddBackward0>)\n",
      "epoch: 7093 loss is tensor([-0.2019], grad_fn=<AddBackward0>)\n",
      "epoch: 7094 loss is tensor([-0.2626], grad_fn=<AddBackward0>)\n",
      "epoch: 7095 loss is tensor([-0.3044], grad_fn=<AddBackward0>)\n",
      "epoch: 7096 loss is tensor([-0.2515], grad_fn=<AddBackward0>)\n",
      "epoch: 7097 loss is tensor([-0.2952], grad_fn=<AddBackward0>)\n",
      "epoch: 7098 loss is tensor([-0.2785], grad_fn=<AddBackward0>)\n",
      "epoch: 7099 loss is tensor([-0.2740], grad_fn=<AddBackward0>)\n",
      "epoch: 7100 loss is tensor([-0.3345], grad_fn=<AddBackward0>)\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7101 loss is tensor([-0.2324], grad_fn=<AddBackward0>)\n",
      "epoch: 7102 loss is tensor([-0.3084], grad_fn=<AddBackward0>)\n",
      "epoch: 7103 loss is tensor([-0.3210], grad_fn=<AddBackward0>)\n",
      "epoch: 7104 loss is tensor([-0.2581], grad_fn=<AddBackward0>)\n",
      "epoch: 7105 loss is tensor([-0.3101], grad_fn=<AddBackward0>)\n",
      "epoch: 7106 loss is tensor([-0.2437], grad_fn=<AddBackward0>)\n",
      "epoch: 7107 loss is tensor([-0.2753], grad_fn=<AddBackward0>)\n",
      "epoch: 7108 loss is tensor([-0.3008], grad_fn=<AddBackward0>)\n",
      "epoch: 7109 loss is tensor([-0.2386], grad_fn=<AddBackward0>)\n",
      "epoch: 7110 loss is tensor([-0.2726], grad_fn=<AddBackward0>)\n",
      "epoch: 7111 loss is tensor([-0.2795], grad_fn=<AddBackward0>)\n",
      "epoch: 7112 loss is tensor([-0.2734], grad_fn=<AddBackward0>)\n",
      "epoch: 7113 loss is tensor([-0.2989], grad_fn=<AddBackward0>)\n",
      "epoch: 7114 loss is tensor([-0.2797], grad_fn=<AddBackward0>)\n",
      "epoch: 7115 loss is tensor([-0.2306], grad_fn=<AddBackward0>)\n",
      "epoch: 7116 loss is tensor([-0.2875], grad_fn=<AddBackward0>)\n",
      "epoch: 7117 loss is tensor([-0.2903], grad_fn=<AddBackward0>)\n",
      "epoch: 7118 loss is tensor([-0.2888], grad_fn=<AddBackward0>)\n",
      "epoch: 7119 loss is tensor([-0.2935], grad_fn=<AddBackward0>)\n",
      "epoch: 7120 loss is tensor([-0.2875], grad_fn=<AddBackward0>)\n",
      "epoch: 7121 loss is tensor([-0.2325], grad_fn=<AddBackward0>)\n",
      "epoch: 7122 loss is tensor([-0.3231], grad_fn=<AddBackward0>)\n",
      "epoch: 7123 loss is tensor([-0.2698], grad_fn=<AddBackward0>)\n",
      "epoch: 7124 loss is tensor([-0.2535], grad_fn=<AddBackward0>)\n",
      "epoch: 7125 loss is tensor([-0.2460], grad_fn=<AddBackward0>)\n",
      "epoch: 7126 loss is tensor([-0.2366], grad_fn=<AddBackward0>)\n",
      "epoch: 7127 loss is tensor([-0.3238], grad_fn=<AddBackward0>)\n",
      "epoch: 7128 loss is tensor([-0.2910], grad_fn=<AddBackward0>)\n",
      "epoch: 7129 loss is tensor([-0.2897], grad_fn=<AddBackward0>)\n",
      "epoch: 7130 loss is tensor([-0.3078], grad_fn=<AddBackward0>)\n",
      "epoch: 7131 loss is tensor([-0.2988], grad_fn=<AddBackward0>)\n",
      "epoch: 7132 loss is tensor([-0.3000], grad_fn=<AddBackward0>)\n",
      "epoch: 7133 loss is tensor([-0.2474], grad_fn=<AddBackward0>)\n",
      "epoch: 7134 loss is tensor([-0.2981], grad_fn=<AddBackward0>)\n",
      "epoch: 7135 loss is tensor([-0.2713], grad_fn=<AddBackward0>)\n",
      "epoch: 7136 loss is tensor([-0.2971], grad_fn=<AddBackward0>)\n",
      "epoch: 7137 loss is tensor([-0.2488], grad_fn=<AddBackward0>)\n",
      "epoch: 7138 loss is tensor([-0.2637], grad_fn=<AddBackward0>)\n",
      "epoch: 7139 loss is tensor([-0.3211], grad_fn=<AddBackward0>)\n",
      "epoch: 7140 loss is tensor([-0.2884], grad_fn=<AddBackward0>)\n",
      "epoch: 7141 loss is tensor([-0.3556], grad_fn=<AddBackward0>)\n",
      "epoch: 7142 loss is tensor([-0.2912], grad_fn=<AddBackward0>)\n",
      "epoch: 7143 loss is tensor([-0.3327], grad_fn=<AddBackward0>)\n",
      "epoch: 7144 loss is tensor([-0.3225], grad_fn=<AddBackward0>)\n",
      "epoch: 7145 loss is tensor([-0.3304], grad_fn=<AddBackward0>)\n",
      "epoch: 7146 loss is tensor([-0.2877], grad_fn=<AddBackward0>)\n",
      "epoch: 7147 loss is tensor([-0.3324], grad_fn=<AddBackward0>)\n",
      "epoch: 7148 loss is tensor([-0.2392], grad_fn=<AddBackward0>)\n",
      "epoch: 7149 loss is tensor([-0.2447], grad_fn=<AddBackward0>)\n",
      "epoch: 7150 loss is tensor([-0.2944], grad_fn=<AddBackward0>)\n",
      "epoch: 7151 loss is tensor([-0.3389], grad_fn=<AddBackward0>)\n",
      "epoch: 7152 loss is tensor([-0.3394], grad_fn=<AddBackward0>)\n",
      "epoch: 7153 loss is tensor([-0.2845], grad_fn=<AddBackward0>)\n",
      "epoch: 7154 loss is tensor([-0.3142], grad_fn=<AddBackward0>)\n",
      "epoch: 7155 loss is tensor([-0.2151], grad_fn=<AddBackward0>)\n",
      "epoch: 7156 loss is tensor([-0.1447], grad_fn=<AddBackward0>)\n",
      "epoch: 7157 loss is tensor([-0.1672], grad_fn=<AddBackward0>)\n",
      "epoch: 7158 loss is tensor([-0.2546], grad_fn=<AddBackward0>)\n",
      "epoch: 7159 loss is tensor([-0.2036], grad_fn=<AddBackward0>)\n",
      "epoch: 7160 loss is tensor([-0.1789], grad_fn=<AddBackward0>)\n",
      "epoch: 7161 loss is tensor([-0.2343], grad_fn=<AddBackward0>)\n",
      "epoch: 7162 loss is tensor([-0.2250], grad_fn=<AddBackward0>)\n",
      "epoch: 7163 loss is tensor([-0.2977], grad_fn=<AddBackward0>)\n",
      "epoch: 7164 loss is tensor([-0.2195], grad_fn=<AddBackward0>)\n",
      "epoch: 7165 loss is tensor([-0.2530], grad_fn=<AddBackward0>)\n",
      "epoch: 7166 loss is tensor([-0.2791], grad_fn=<AddBackward0>)\n",
      "epoch: 7167 loss is tensor([-0.2354], grad_fn=<AddBackward0>)\n",
      "epoch: 7168 loss is tensor([-0.2560], grad_fn=<AddBackward0>)\n",
      "epoch: 7169 loss is tensor([-0.3237], grad_fn=<AddBackward0>)\n",
      "epoch: 7170 loss is tensor([-0.2608], grad_fn=<AddBackward0>)\n",
      "epoch: 7171 loss is tensor([-0.3014], grad_fn=<AddBackward0>)\n",
      "epoch: 7172 loss is tensor([-0.3227], grad_fn=<AddBackward0>)\n",
      "epoch: 7173 loss is tensor([-0.3642], grad_fn=<AddBackward0>)\n",
      "epoch: 7174 loss is tensor([-0.2686], grad_fn=<AddBackward0>)\n",
      "epoch: 7175 loss is tensor([-0.2164], grad_fn=<AddBackward0>)\n",
      "epoch: 7176 loss is tensor([-0.2488], grad_fn=<AddBackward0>)\n",
      "epoch: 7177 loss is tensor([-0.1382], grad_fn=<AddBackward0>)\n",
      "epoch: 7178 loss is tensor([-0.2226], grad_fn=<AddBackward0>)\n",
      "epoch: 7179 loss is tensor([-0.2444], grad_fn=<AddBackward0>)\n",
      "epoch: 7180 loss is tensor([-0.2155], grad_fn=<AddBackward0>)\n",
      "epoch: 7181 loss is tensor([-0.2409], grad_fn=<AddBackward0>)\n",
      "epoch: 7182 loss is tensor([-0.2524], grad_fn=<AddBackward0>)\n",
      "epoch: 7183 loss is tensor([-0.2523], grad_fn=<AddBackward0>)\n",
      "epoch: 7184 loss is tensor([-0.2419], grad_fn=<AddBackward0>)\n",
      "epoch: 7185 loss is tensor([-0.3041], grad_fn=<AddBackward0>)\n",
      "epoch: 7186 loss is tensor([-0.2597], grad_fn=<AddBackward0>)\n",
      "epoch: 7187 loss is tensor([-0.2266], grad_fn=<AddBackward0>)\n",
      "epoch: 7188 loss is tensor([-0.2171], grad_fn=<AddBackward0>)\n",
      "epoch: 7189 loss is tensor([-0.2468], grad_fn=<AddBackward0>)\n",
      "epoch: 7190 loss is tensor([-0.2027], grad_fn=<AddBackward0>)\n",
      "epoch: 7191 loss is tensor([-0.2026], grad_fn=<AddBackward0>)\n",
      "epoch: 7192 loss is tensor([-0.2939], grad_fn=<AddBackward0>)\n",
      "epoch: 7193 loss is tensor([-0.2467], grad_fn=<AddBackward0>)\n",
      "epoch: 7194 loss is tensor([-0.2772], grad_fn=<AddBackward0>)\n",
      "epoch: 7195 loss is tensor([-0.2981], grad_fn=<AddBackward0>)\n",
      "epoch: 7196 loss is tensor([-0.2499], grad_fn=<AddBackward0>)\n",
      "epoch: 7197 loss is tensor([-0.2753], grad_fn=<AddBackward0>)\n",
      "epoch: 7198 loss is tensor([-0.2685], grad_fn=<AddBackward0>)\n",
      "epoch: 7199 loss is tensor([-0.2934], grad_fn=<AddBackward0>)\n",
      "epoch: 7200 loss is tensor([-0.1922], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7201 loss is tensor([-0.3091], grad_fn=<AddBackward0>)\n",
      "epoch: 7202 loss is tensor([-0.3405], grad_fn=<AddBackward0>)\n",
      "epoch: 7203 loss is tensor([-0.1983], grad_fn=<AddBackward0>)\n",
      "epoch: 7204 loss is tensor([-0.3092], grad_fn=<AddBackward0>)\n",
      "epoch: 7205 loss is tensor([-0.2717], grad_fn=<AddBackward0>)\n",
      "epoch: 7206 loss is tensor([-0.2934], grad_fn=<AddBackward0>)\n",
      "epoch: 7207 loss is tensor([-0.3195], grad_fn=<AddBackward0>)\n",
      "epoch: 7208 loss is tensor([-0.3169], grad_fn=<AddBackward0>)\n",
      "epoch: 7209 loss is tensor([-0.2630], grad_fn=<AddBackward0>)\n",
      "epoch: 7210 loss is tensor([-0.2939], grad_fn=<AddBackward0>)\n",
      "epoch: 7211 loss is tensor([-0.3152], grad_fn=<AddBackward0>)\n",
      "epoch: 7212 loss is tensor([-0.3021], grad_fn=<AddBackward0>)\n",
      "epoch: 7213 loss is tensor([-0.3095], grad_fn=<AddBackward0>)\n",
      "epoch: 7214 loss is tensor([-0.2867], grad_fn=<AddBackward0>)\n",
      "epoch: 7215 loss is tensor([-0.3173], grad_fn=<AddBackward0>)\n",
      "epoch: 7216 loss is tensor([-0.2586], grad_fn=<AddBackward0>)\n",
      "epoch: 7217 loss is tensor([-0.3813], grad_fn=<AddBackward0>)\n",
      "epoch: 7218 loss is tensor([-0.3122], grad_fn=<AddBackward0>)\n",
      "epoch: 7219 loss is tensor([-0.3374], grad_fn=<AddBackward0>)\n",
      "epoch: 7220 loss is tensor([-0.3222], grad_fn=<AddBackward0>)\n",
      "epoch: 7221 loss is tensor([-0.3186], grad_fn=<AddBackward0>)\n",
      "epoch: 7222 loss is tensor([-0.3208], grad_fn=<AddBackward0>)\n",
      "epoch: 7223 loss is tensor([-0.3431], grad_fn=<AddBackward0>)\n",
      "epoch: 7224 loss is tensor([-0.3341], grad_fn=<AddBackward0>)\n",
      "epoch: 7225 loss is tensor([-0.3359], grad_fn=<AddBackward0>)\n",
      "epoch: 7226 loss is tensor([-0.2755], grad_fn=<AddBackward0>)\n",
      "epoch: 7227 loss is tensor([-0.2836], grad_fn=<AddBackward0>)\n",
      "epoch: 7228 loss is tensor([-0.2904], grad_fn=<AddBackward0>)\n",
      "epoch: 7229 loss is tensor([-0.3183], grad_fn=<AddBackward0>)\n",
      "epoch: 7230 loss is tensor([-0.3604], grad_fn=<AddBackward0>)\n",
      "epoch: 7231 loss is tensor([-0.3713], grad_fn=<AddBackward0>)\n",
      "epoch: 7232 loss is tensor([-0.3127], grad_fn=<AddBackward0>)\n",
      "epoch: 7233 loss is tensor([-0.3312], grad_fn=<AddBackward0>)\n",
      "epoch: 7234 loss is tensor([-0.1899], grad_fn=<AddBackward0>)\n",
      "epoch: 7235 loss is tensor([-0.2152], grad_fn=<AddBackward0>)\n",
      "epoch: 7236 loss is tensor([-0.3531], grad_fn=<AddBackward0>)\n",
      "epoch: 7237 loss is tensor([-0.2424], grad_fn=<AddBackward0>)\n",
      "epoch: 7238 loss is tensor([-0.1298], grad_fn=<AddBackward0>)\n",
      "epoch: 7239 loss is tensor([-0.2693], grad_fn=<AddBackward0>)\n",
      "epoch: 7240 loss is tensor([-0.2824], grad_fn=<AddBackward0>)\n",
      "epoch: 7241 loss is tensor([-0.2048], grad_fn=<AddBackward0>)\n",
      "epoch: 7242 loss is tensor([-0.1869], grad_fn=<AddBackward0>)\n",
      "epoch: 7243 loss is tensor([-0.1867], grad_fn=<AddBackward0>)\n",
      "epoch: 7244 loss is tensor([-0.2602], grad_fn=<AddBackward0>)\n",
      "epoch: 7245 loss is tensor([-0.2076], grad_fn=<AddBackward0>)\n",
      "epoch: 7246 loss is tensor([-0.2330], grad_fn=<AddBackward0>)\n",
      "epoch: 7247 loss is tensor([-0.2416], grad_fn=<AddBackward0>)\n",
      "epoch: 7248 loss is tensor([-0.2581], grad_fn=<AddBackward0>)\n",
      "epoch: 7249 loss is tensor([-0.2449], grad_fn=<AddBackward0>)\n",
      "epoch: 7250 loss is tensor([-0.2670], grad_fn=<AddBackward0>)\n",
      "epoch: 7251 loss is tensor([-0.2521], grad_fn=<AddBackward0>)\n",
      "epoch: 7252 loss is tensor([-0.2469], grad_fn=<AddBackward0>)\n",
      "epoch: 7253 loss is tensor([-0.3192], grad_fn=<AddBackward0>)\n",
      "epoch: 7254 loss is tensor([-0.2691], grad_fn=<AddBackward0>)\n",
      "epoch: 7255 loss is tensor([-0.2977], grad_fn=<AddBackward0>)\n",
      "epoch: 7256 loss is tensor([-0.2439], grad_fn=<AddBackward0>)\n",
      "epoch: 7257 loss is tensor([-0.2581], grad_fn=<AddBackward0>)\n",
      "epoch: 7258 loss is tensor([-0.2895], grad_fn=<AddBackward0>)\n",
      "epoch: 7259 loss is tensor([-0.2632], grad_fn=<AddBackward0>)\n",
      "epoch: 7260 loss is tensor([-0.3188], grad_fn=<AddBackward0>)\n",
      "epoch: 7261 loss is tensor([-0.3292], grad_fn=<AddBackward0>)\n",
      "epoch: 7262 loss is tensor([-0.3281], grad_fn=<AddBackward0>)\n",
      "epoch: 7263 loss is tensor([-0.3190], grad_fn=<AddBackward0>)\n",
      "epoch: 7264 loss is tensor([-0.3510], grad_fn=<AddBackward0>)\n",
      "epoch: 7265 loss is tensor([-0.3045], grad_fn=<AddBackward0>)\n",
      "epoch: 7266 loss is tensor([-0.3018], grad_fn=<AddBackward0>)\n",
      "epoch: 7267 loss is tensor([-0.3073], grad_fn=<AddBackward0>)\n",
      "epoch: 7268 loss is tensor([-0.2794], grad_fn=<AddBackward0>)\n",
      "epoch: 7269 loss is tensor([-0.2855], grad_fn=<AddBackward0>)\n",
      "epoch: 7270 loss is tensor([-0.3010], grad_fn=<AddBackward0>)\n",
      "epoch: 7271 loss is tensor([-0.3510], grad_fn=<AddBackward0>)\n",
      "epoch: 7272 loss is tensor([-0.2806], grad_fn=<AddBackward0>)\n",
      "epoch: 7273 loss is tensor([-0.2895], grad_fn=<AddBackward0>)\n",
      "epoch: 7274 loss is tensor([-0.3201], grad_fn=<AddBackward0>)\n",
      "epoch: 7275 loss is tensor([-0.2561], grad_fn=<AddBackward0>)\n",
      "epoch: 7276 loss is tensor([-0.2747], grad_fn=<AddBackward0>)\n",
      "epoch: 7277 loss is tensor([-0.3339], grad_fn=<AddBackward0>)\n",
      "epoch: 7278 loss is tensor([-0.2838], grad_fn=<AddBackward0>)\n",
      "epoch: 7279 loss is tensor([-0.2900], grad_fn=<AddBackward0>)\n",
      "epoch: 7280 loss is tensor([-0.3074], grad_fn=<AddBackward0>)\n",
      "epoch: 7281 loss is tensor([-0.2798], grad_fn=<AddBackward0>)\n",
      "epoch: 7282 loss is tensor([-0.2781], grad_fn=<AddBackward0>)\n",
      "epoch: 7283 loss is tensor([-0.3447], grad_fn=<AddBackward0>)\n",
      "epoch: 7284 loss is tensor([-0.3367], grad_fn=<AddBackward0>)\n",
      "epoch: 7285 loss is tensor([-0.3382], grad_fn=<AddBackward0>)\n",
      "epoch: 7286 loss is tensor([-0.3722], grad_fn=<AddBackward0>)\n",
      "epoch: 7287 loss is tensor([-0.3148], grad_fn=<AddBackward0>)\n",
      "epoch: 7288 loss is tensor([-0.3328], grad_fn=<AddBackward0>)\n",
      "epoch: 7289 loss is tensor([-0.3614], grad_fn=<AddBackward0>)\n",
      "epoch: 7290 loss is tensor([-0.3474], grad_fn=<AddBackward0>)\n",
      "epoch: 7291 loss is tensor([-0.3394], grad_fn=<AddBackward0>)\n",
      "epoch: 7292 loss is tensor([-0.2446], grad_fn=<AddBackward0>)\n",
      "epoch: 7293 loss is tensor([-0.2283], grad_fn=<AddBackward0>)\n",
      "epoch: 7294 loss is tensor([-0.2349], grad_fn=<AddBackward0>)\n",
      "epoch: 7295 loss is tensor([-0.3470], grad_fn=<AddBackward0>)\n",
      "epoch: 7296 loss is tensor([-0.3146], grad_fn=<AddBackward0>)\n",
      "epoch: 7297 loss is tensor([-0.2718], grad_fn=<AddBackward0>)\n",
      "epoch: 7298 loss is tensor([-0.2701], grad_fn=<AddBackward0>)\n",
      "epoch: 7299 loss is tensor([-0.3169], grad_fn=<AddBackward0>)\n",
      "epoch: 7300 loss is tensor([-0.2591], grad_fn=<AddBackward0>)\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7301 loss is tensor([-0.3078], grad_fn=<AddBackward0>)\n",
      "epoch: 7302 loss is tensor([-0.3541], grad_fn=<AddBackward0>)\n",
      "epoch: 7303 loss is tensor([-0.2775], grad_fn=<AddBackward0>)\n",
      "epoch: 7304 loss is tensor([-0.2963], grad_fn=<AddBackward0>)\n",
      "epoch: 7305 loss is tensor([-0.2962], grad_fn=<AddBackward0>)\n",
      "epoch: 7306 loss is tensor([-0.3252], grad_fn=<AddBackward0>)\n",
      "epoch: 7307 loss is tensor([-0.2720], grad_fn=<AddBackward0>)\n",
      "epoch: 7308 loss is tensor([-0.3015], grad_fn=<AddBackward0>)\n",
      "epoch: 7309 loss is tensor([-0.2762], grad_fn=<AddBackward0>)\n",
      "epoch: 7310 loss is tensor([-0.2931], grad_fn=<AddBackward0>)\n",
      "epoch: 7311 loss is tensor([-0.3362], grad_fn=<AddBackward0>)\n",
      "epoch: 7312 loss is tensor([-0.2823], grad_fn=<AddBackward0>)\n",
      "epoch: 7313 loss is tensor([-0.2813], grad_fn=<AddBackward0>)\n",
      "epoch: 7314 loss is tensor([-0.2942], grad_fn=<AddBackward0>)\n",
      "epoch: 7315 loss is tensor([-0.2982], grad_fn=<AddBackward0>)\n",
      "epoch: 7316 loss is tensor([-0.2951], grad_fn=<AddBackward0>)\n",
      "epoch: 7317 loss is tensor([-0.2949], grad_fn=<AddBackward0>)\n",
      "epoch: 7318 loss is tensor([-0.2799], grad_fn=<AddBackward0>)\n",
      "epoch: 7319 loss is tensor([-0.2294], grad_fn=<AddBackward0>)\n",
      "epoch: 7320 loss is tensor([-0.2578], grad_fn=<AddBackward0>)\n",
      "epoch: 7321 loss is tensor([-0.2928], grad_fn=<AddBackward0>)\n",
      "epoch: 7322 loss is tensor([-0.2538], grad_fn=<AddBackward0>)\n",
      "epoch: 7323 loss is tensor([-0.2337], grad_fn=<AddBackward0>)\n",
      "epoch: 7324 loss is tensor([-0.3259], grad_fn=<AddBackward0>)\n",
      "epoch: 7325 loss is tensor([-0.2903], grad_fn=<AddBackward0>)\n",
      "epoch: 7326 loss is tensor([-0.2733], grad_fn=<AddBackward0>)\n",
      "epoch: 7327 loss is tensor([-0.2217], grad_fn=<AddBackward0>)\n",
      "epoch: 7328 loss is tensor([-0.3133], grad_fn=<AddBackward0>)\n",
      "epoch: 7329 loss is tensor([-0.2746], grad_fn=<AddBackward0>)\n",
      "epoch: 7330 loss is tensor([-0.2670], grad_fn=<AddBackward0>)\n",
      "epoch: 7331 loss is tensor([-0.2954], grad_fn=<AddBackward0>)\n",
      "epoch: 7332 loss is tensor([-0.3035], grad_fn=<AddBackward0>)\n",
      "epoch: 7333 loss is tensor([-0.2330], grad_fn=<AddBackward0>)\n",
      "epoch: 7334 loss is tensor([-0.2371], grad_fn=<AddBackward0>)\n",
      "epoch: 7335 loss is tensor([-0.2686], grad_fn=<AddBackward0>)\n",
      "epoch: 7336 loss is tensor([-0.2548], grad_fn=<AddBackward0>)\n",
      "epoch: 7337 loss is tensor([-0.2404], grad_fn=<AddBackward0>)\n",
      "epoch: 7338 loss is tensor([-0.3284], grad_fn=<AddBackward0>)\n",
      "epoch: 7339 loss is tensor([-0.2244], grad_fn=<AddBackward0>)\n",
      "epoch: 7340 loss is tensor([-0.2297], grad_fn=<AddBackward0>)\n",
      "epoch: 7341 loss is tensor([-0.2755], grad_fn=<AddBackward0>)\n",
      "epoch: 7342 loss is tensor([-0.2584], grad_fn=<AddBackward0>)\n",
      "epoch: 7343 loss is tensor([-0.2790], grad_fn=<AddBackward0>)\n",
      "epoch: 7344 loss is tensor([-0.2343], grad_fn=<AddBackward0>)\n",
      "epoch: 7345 loss is tensor([-0.2527], grad_fn=<AddBackward0>)\n",
      "epoch: 7346 loss is tensor([-0.2496], grad_fn=<AddBackward0>)\n",
      "epoch: 7347 loss is tensor([-0.2874], grad_fn=<AddBackward0>)\n",
      "epoch: 7348 loss is tensor([-0.2954], grad_fn=<AddBackward0>)\n",
      "epoch: 7349 loss is tensor([-0.3200], grad_fn=<AddBackward0>)\n",
      "epoch: 7350 loss is tensor([-0.2828], grad_fn=<AddBackward0>)\n",
      "epoch: 7351 loss is tensor([-0.2976], grad_fn=<AddBackward0>)\n",
      "epoch: 7352 loss is tensor([-0.2738], grad_fn=<AddBackward0>)\n",
      "epoch: 7353 loss is tensor([-0.2907], grad_fn=<AddBackward0>)\n",
      "epoch: 7354 loss is tensor([-0.3317], grad_fn=<AddBackward0>)\n",
      "epoch: 7355 loss is tensor([-0.3102], grad_fn=<AddBackward0>)\n",
      "epoch: 7356 loss is tensor([-0.3715], grad_fn=<AddBackward0>)\n",
      "epoch: 7357 loss is tensor([-0.3180], grad_fn=<AddBackward0>)\n",
      "epoch: 7358 loss is tensor([-0.3444], grad_fn=<AddBackward0>)\n",
      "epoch: 7359 loss is tensor([-0.3040], grad_fn=<AddBackward0>)\n",
      "epoch: 7360 loss is tensor([-0.3085], grad_fn=<AddBackward0>)\n",
      "epoch: 7361 loss is tensor([-0.2432], grad_fn=<AddBackward0>)\n",
      "epoch: 7362 loss is tensor([-0.3499], grad_fn=<AddBackward0>)\n",
      "epoch: 7363 loss is tensor([-0.3084], grad_fn=<AddBackward0>)\n",
      "epoch: 7364 loss is tensor([-0.3183], grad_fn=<AddBackward0>)\n",
      "epoch: 7365 loss is tensor([-0.2480], grad_fn=<AddBackward0>)\n",
      "epoch: 7366 loss is tensor([-0.3041], grad_fn=<AddBackward0>)\n",
      "epoch: 7367 loss is tensor([-0.2965], grad_fn=<AddBackward0>)\n",
      "epoch: 7368 loss is tensor([-0.2677], grad_fn=<AddBackward0>)\n",
      "epoch: 7369 loss is tensor([-0.2639], grad_fn=<AddBackward0>)\n",
      "epoch: 7370 loss is tensor([-0.3449], grad_fn=<AddBackward0>)\n",
      "epoch: 7371 loss is tensor([-0.2790], grad_fn=<AddBackward0>)\n",
      "epoch: 7372 loss is tensor([-0.2984], grad_fn=<AddBackward0>)\n",
      "epoch: 7373 loss is tensor([-0.2736], grad_fn=<AddBackward0>)\n",
      "epoch: 7374 loss is tensor([-0.2411], grad_fn=<AddBackward0>)\n",
      "epoch: 7375 loss is tensor([-0.2782], grad_fn=<AddBackward0>)\n",
      "epoch: 7376 loss is tensor([-0.2355], grad_fn=<AddBackward0>)\n",
      "epoch: 7377 loss is tensor([-0.2946], grad_fn=<AddBackward0>)\n",
      "epoch: 7378 loss is tensor([-0.3480], grad_fn=<AddBackward0>)\n",
      "epoch: 7379 loss is tensor([-0.3538], grad_fn=<AddBackward0>)\n",
      "epoch: 7380 loss is tensor([-0.3176], grad_fn=<AddBackward0>)\n",
      "epoch: 7381 loss is tensor([-0.3374], grad_fn=<AddBackward0>)\n",
      "epoch: 7382 loss is tensor([-0.3401], grad_fn=<AddBackward0>)\n",
      "epoch: 7383 loss is tensor([-0.3208], grad_fn=<AddBackward0>)\n",
      "epoch: 7384 loss is tensor([-0.3410], grad_fn=<AddBackward0>)\n",
      "epoch: 7385 loss is tensor([-0.2569], grad_fn=<AddBackward0>)\n",
      "epoch: 7386 loss is tensor([-0.3445], grad_fn=<AddBackward0>)\n",
      "epoch: 7387 loss is tensor([-0.3371], grad_fn=<AddBackward0>)\n",
      "epoch: 7388 loss is tensor([-0.3274], grad_fn=<AddBackward0>)\n",
      "epoch: 7389 loss is tensor([-0.3063], grad_fn=<AddBackward0>)\n",
      "epoch: 7390 loss is tensor([-0.3060], grad_fn=<AddBackward0>)\n",
      "epoch: 7391 loss is tensor([-0.3171], grad_fn=<AddBackward0>)\n",
      "epoch: 7392 loss is tensor([-0.3100], grad_fn=<AddBackward0>)\n",
      "epoch: 7393 loss is tensor([-0.2572], grad_fn=<AddBackward0>)\n",
      "epoch: 7394 loss is tensor([-0.2928], grad_fn=<AddBackward0>)\n",
      "epoch: 7395 loss is tensor([-0.2266], grad_fn=<AddBackward0>)\n",
      "epoch: 7396 loss is tensor([-0.3025], grad_fn=<AddBackward0>)\n",
      "epoch: 7397 loss is tensor([-0.3227], grad_fn=<AddBackward0>)\n",
      "epoch: 7398 loss is tensor([-0.2828], grad_fn=<AddBackward0>)\n",
      "epoch: 7399 loss is tensor([-0.2864], grad_fn=<AddBackward0>)\n",
      "epoch: 7400 loss is tensor([-0.3200], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7401 loss is tensor([-0.3218], grad_fn=<AddBackward0>)\n",
      "epoch: 7402 loss is tensor([-0.3501], grad_fn=<AddBackward0>)\n",
      "epoch: 7403 loss is tensor([-0.3026], grad_fn=<AddBackward0>)\n",
      "epoch: 7404 loss is tensor([-0.3455], grad_fn=<AddBackward0>)\n",
      "epoch: 7405 loss is tensor([-0.3297], grad_fn=<AddBackward0>)\n",
      "epoch: 7406 loss is tensor([-0.3645], grad_fn=<AddBackward0>)\n",
      "epoch: 7407 loss is tensor([-0.3325], grad_fn=<AddBackward0>)\n",
      "epoch: 7408 loss is tensor([-0.3732], grad_fn=<AddBackward0>)\n",
      "epoch: 7409 loss is tensor([-0.3589], grad_fn=<AddBackward0>)\n",
      "epoch: 7410 loss is tensor([-0.3863], grad_fn=<AddBackward0>)\n",
      "epoch: 7411 loss is tensor([-0.3467], grad_fn=<AddBackward0>)\n",
      "epoch: 7412 loss is tensor([-0.3403], grad_fn=<AddBackward0>)\n",
      "epoch: 7413 loss is tensor([-0.3430], grad_fn=<AddBackward0>)\n",
      "epoch: 7414 loss is tensor([-0.3644], grad_fn=<AddBackward0>)\n",
      "epoch: 7415 loss is tensor([-0.3197], grad_fn=<AddBackward0>)\n",
      "epoch: 7416 loss is tensor([-0.3137], grad_fn=<AddBackward0>)\n",
      "epoch: 7417 loss is tensor([-0.3455], grad_fn=<AddBackward0>)\n",
      "epoch: 7418 loss is tensor([-0.2756], grad_fn=<AddBackward0>)\n",
      "epoch: 7419 loss is tensor([-0.3200], grad_fn=<AddBackward0>)\n",
      "epoch: 7420 loss is tensor([-0.3382], grad_fn=<AddBackward0>)\n",
      "epoch: 7421 loss is tensor([-0.2970], grad_fn=<AddBackward0>)\n",
      "epoch: 7422 loss is tensor([-0.2900], grad_fn=<AddBackward0>)\n",
      "epoch: 7423 loss is tensor([-0.3719], grad_fn=<AddBackward0>)\n",
      "epoch: 7424 loss is tensor([-0.3343], grad_fn=<AddBackward0>)\n",
      "epoch: 7425 loss is tensor([-0.3248], grad_fn=<AddBackward0>)\n",
      "epoch: 7426 loss is tensor([-0.3319], grad_fn=<AddBackward0>)\n",
      "epoch: 7427 loss is tensor([-0.3090], grad_fn=<AddBackward0>)\n",
      "epoch: 7428 loss is tensor([-0.3316], grad_fn=<AddBackward0>)\n",
      "epoch: 7429 loss is tensor([-0.3567], grad_fn=<AddBackward0>)\n",
      "epoch: 7430 loss is tensor([-0.3225], grad_fn=<AddBackward0>)\n",
      "epoch: 7431 loss is tensor([-0.2719], grad_fn=<AddBackward0>)\n",
      "epoch: 7432 loss is tensor([-0.3176], grad_fn=<AddBackward0>)\n",
      "epoch: 7433 loss is tensor([-0.2930], grad_fn=<AddBackward0>)\n",
      "epoch: 7434 loss is tensor([-0.2590], grad_fn=<AddBackward0>)\n",
      "epoch: 7435 loss is tensor([-0.2842], grad_fn=<AddBackward0>)\n",
      "epoch: 7436 loss is tensor([-0.2984], grad_fn=<AddBackward0>)\n",
      "epoch: 7437 loss is tensor([-0.2750], grad_fn=<AddBackward0>)\n",
      "epoch: 7438 loss is tensor([-0.2134], grad_fn=<AddBackward0>)\n",
      "epoch: 7439 loss is tensor([-0.3023], grad_fn=<AddBackward0>)\n",
      "epoch: 7440 loss is tensor([-0.2734], grad_fn=<AddBackward0>)\n",
      "epoch: 7441 loss is tensor([-0.2437], grad_fn=<AddBackward0>)\n",
      "epoch: 7442 loss is tensor([-0.3055], grad_fn=<AddBackward0>)\n",
      "epoch: 7443 loss is tensor([-0.3557], grad_fn=<AddBackward0>)\n",
      "epoch: 7444 loss is tensor([-0.2836], grad_fn=<AddBackward0>)\n",
      "epoch: 7445 loss is tensor([-0.3164], grad_fn=<AddBackward0>)\n",
      "epoch: 7446 loss is tensor([-0.2986], grad_fn=<AddBackward0>)\n",
      "epoch: 7447 loss is tensor([-0.2363], grad_fn=<AddBackward0>)\n",
      "epoch: 7448 loss is tensor([-0.3349], grad_fn=<AddBackward0>)\n",
      "epoch: 7449 loss is tensor([-0.2748], grad_fn=<AddBackward0>)\n",
      "epoch: 7450 loss is tensor([-0.3408], grad_fn=<AddBackward0>)\n",
      "epoch: 7451 loss is tensor([-0.3244], grad_fn=<AddBackward0>)\n",
      "epoch: 7452 loss is tensor([-0.3153], grad_fn=<AddBackward0>)\n",
      "epoch: 7453 loss is tensor([-0.2910], grad_fn=<AddBackward0>)\n",
      "epoch: 7454 loss is tensor([-0.3382], grad_fn=<AddBackward0>)\n",
      "epoch: 7455 loss is tensor([-0.3242], grad_fn=<AddBackward0>)\n",
      "epoch: 7456 loss is tensor([-0.2221], grad_fn=<AddBackward0>)\n",
      "epoch: 7457 loss is tensor([-0.2512], grad_fn=<AddBackward0>)\n",
      "epoch: 7458 loss is tensor([-0.2586], grad_fn=<AddBackward0>)\n",
      "epoch: 7459 loss is tensor([-0.2279], grad_fn=<AddBackward0>)\n",
      "epoch: 7460 loss is tensor([-0.2044], grad_fn=<AddBackward0>)\n",
      "epoch: 7461 loss is tensor([-0.2625], grad_fn=<AddBackward0>)\n",
      "epoch: 7462 loss is tensor([-0.2557], grad_fn=<AddBackward0>)\n",
      "epoch: 7463 loss is tensor([-0.2503], grad_fn=<AddBackward0>)\n",
      "epoch: 7464 loss is tensor([-0.2760], grad_fn=<AddBackward0>)\n",
      "epoch: 7465 loss is tensor([-0.2641], grad_fn=<AddBackward0>)\n",
      "epoch: 7466 loss is tensor([-0.2701], grad_fn=<AddBackward0>)\n",
      "epoch: 7467 loss is tensor([-0.2513], grad_fn=<AddBackward0>)\n",
      "epoch: 7468 loss is tensor([-0.2982], grad_fn=<AddBackward0>)\n",
      "epoch: 7469 loss is tensor([-0.2550], grad_fn=<AddBackward0>)\n",
      "epoch: 7470 loss is tensor([-0.2813], grad_fn=<AddBackward0>)\n",
      "epoch: 7471 loss is tensor([-0.2564], grad_fn=<AddBackward0>)\n",
      "epoch: 7472 loss is tensor([-0.1791], grad_fn=<AddBackward0>)\n",
      "epoch: 7473 loss is tensor([-0.2205], grad_fn=<AddBackward0>)\n",
      "epoch: 7474 loss is tensor([-0.2447], grad_fn=<AddBackward0>)\n",
      "epoch: 7475 loss is tensor([-0.2810], grad_fn=<AddBackward0>)\n",
      "epoch: 7476 loss is tensor([-0.2793], grad_fn=<AddBackward0>)\n",
      "epoch: 7477 loss is tensor([-0.2403], grad_fn=<AddBackward0>)\n",
      "epoch: 7478 loss is tensor([-0.2700], grad_fn=<AddBackward0>)\n",
      "epoch: 7479 loss is tensor([-0.2649], grad_fn=<AddBackward0>)\n",
      "epoch: 7480 loss is tensor([-0.2596], grad_fn=<AddBackward0>)\n",
      "epoch: 7481 loss is tensor([-0.3434], grad_fn=<AddBackward0>)\n",
      "epoch: 7482 loss is tensor([-0.3455], grad_fn=<AddBackward0>)\n",
      "epoch: 7483 loss is tensor([-0.3197], grad_fn=<AddBackward0>)\n",
      "epoch: 7484 loss is tensor([-0.2454], grad_fn=<AddBackward0>)\n",
      "epoch: 7485 loss is tensor([-0.2666], grad_fn=<AddBackward0>)\n",
      "epoch: 7486 loss is tensor([-0.2457], grad_fn=<AddBackward0>)\n",
      "epoch: 7487 loss is tensor([-0.2951], grad_fn=<AddBackward0>)\n",
      "epoch: 7488 loss is tensor([-0.2572], grad_fn=<AddBackward0>)\n",
      "epoch: 7489 loss is tensor([-0.2301], grad_fn=<AddBackward0>)\n",
      "epoch: 7490 loss is tensor([-0.2562], grad_fn=<AddBackward0>)\n",
      "epoch: 7491 loss is tensor([-0.3050], grad_fn=<AddBackward0>)\n",
      "epoch: 7492 loss is tensor([-0.2847], grad_fn=<AddBackward0>)\n",
      "epoch: 7493 loss is tensor([-0.3730], grad_fn=<AddBackward0>)\n",
      "epoch: 7494 loss is tensor([-0.2990], grad_fn=<AddBackward0>)\n",
      "epoch: 7495 loss is tensor([-0.3268], grad_fn=<AddBackward0>)\n",
      "epoch: 7496 loss is tensor([-0.2743], grad_fn=<AddBackward0>)\n",
      "epoch: 7497 loss is tensor([-0.2783], grad_fn=<AddBackward0>)\n",
      "epoch: 7498 loss is tensor([-0.3150], grad_fn=<AddBackward0>)\n",
      "epoch: 7499 loss is tensor([-0.2190], grad_fn=<AddBackward0>)\n",
      "epoch: 7500 loss is tensor([-0.2868], grad_fn=<AddBackward0>)\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7501 loss is tensor([-0.2691], grad_fn=<AddBackward0>)\n",
      "epoch: 7502 loss is tensor([-0.2710], grad_fn=<AddBackward0>)\n",
      "epoch: 7503 loss is tensor([-0.2684], grad_fn=<AddBackward0>)\n",
      "epoch: 7504 loss is tensor([-0.2860], grad_fn=<AddBackward0>)\n",
      "epoch: 7505 loss is tensor([-0.2752], grad_fn=<AddBackward0>)\n",
      "epoch: 7506 loss is tensor([-0.2452], grad_fn=<AddBackward0>)\n",
      "epoch: 7507 loss is tensor([-0.3019], grad_fn=<AddBackward0>)\n",
      "epoch: 7508 loss is tensor([-0.2946], grad_fn=<AddBackward0>)\n",
      "epoch: 7509 loss is tensor([-0.2338], grad_fn=<AddBackward0>)\n",
      "epoch: 7510 loss is tensor([-0.2238], grad_fn=<AddBackward0>)\n",
      "epoch: 7511 loss is tensor([-0.2903], grad_fn=<AddBackward0>)\n",
      "epoch: 7512 loss is tensor([-0.2039], grad_fn=<AddBackward0>)\n",
      "epoch: 7513 loss is tensor([-0.2064], grad_fn=<AddBackward0>)\n",
      "epoch: 7514 loss is tensor([-0.1619], grad_fn=<AddBackward0>)\n",
      "epoch: 7515 loss is tensor([-0.3046], grad_fn=<AddBackward0>)\n",
      "epoch: 7516 loss is tensor([-0.2813], grad_fn=<AddBackward0>)\n",
      "epoch: 7517 loss is tensor([-0.2758], grad_fn=<AddBackward0>)\n",
      "epoch: 7518 loss is tensor([-0.2737], grad_fn=<AddBackward0>)\n",
      "epoch: 7519 loss is tensor([-0.2357], grad_fn=<AddBackward0>)\n",
      "epoch: 7520 loss is tensor([-0.2946], grad_fn=<AddBackward0>)\n",
      "epoch: 7521 loss is tensor([-0.3673], grad_fn=<AddBackward0>)\n",
      "epoch: 7522 loss is tensor([-0.3469], grad_fn=<AddBackward0>)\n",
      "epoch: 7523 loss is tensor([-0.2653], grad_fn=<AddBackward0>)\n",
      "epoch: 7524 loss is tensor([-0.3282], grad_fn=<AddBackward0>)\n",
      "epoch: 7525 loss is tensor([-0.3076], grad_fn=<AddBackward0>)\n",
      "epoch: 7526 loss is tensor([-0.2459], grad_fn=<AddBackward0>)\n",
      "epoch: 7527 loss is tensor([-0.2614], grad_fn=<AddBackward0>)\n",
      "epoch: 7528 loss is tensor([-0.2448], grad_fn=<AddBackward0>)\n",
      "epoch: 7529 loss is tensor([-0.2648], grad_fn=<AddBackward0>)\n",
      "epoch: 7530 loss is tensor([-0.2404], grad_fn=<AddBackward0>)\n",
      "epoch: 7531 loss is tensor([-0.2972], grad_fn=<AddBackward0>)\n",
      "epoch: 7532 loss is tensor([-0.2843], grad_fn=<AddBackward0>)\n",
      "epoch: 7533 loss is tensor([-0.2415], grad_fn=<AddBackward0>)\n",
      "epoch: 7534 loss is tensor([-0.2493], grad_fn=<AddBackward0>)\n",
      "epoch: 7535 loss is tensor([-0.3065], grad_fn=<AddBackward0>)\n",
      "epoch: 7536 loss is tensor([-0.2576], grad_fn=<AddBackward0>)\n",
      "epoch: 7537 loss is tensor([-0.3009], grad_fn=<AddBackward0>)\n",
      "epoch: 7538 loss is tensor([-0.2914], grad_fn=<AddBackward0>)\n",
      "epoch: 7539 loss is tensor([-0.2856], grad_fn=<AddBackward0>)\n",
      "epoch: 7540 loss is tensor([-0.2851], grad_fn=<AddBackward0>)\n",
      "epoch: 7541 loss is tensor([-0.2740], grad_fn=<AddBackward0>)\n",
      "epoch: 7542 loss is tensor([-0.3066], grad_fn=<AddBackward0>)\n",
      "epoch: 7543 loss is tensor([-0.2805], grad_fn=<AddBackward0>)\n",
      "epoch: 7544 loss is tensor([-0.3629], grad_fn=<AddBackward0>)\n",
      "epoch: 7545 loss is tensor([-0.3639], grad_fn=<AddBackward0>)\n",
      "epoch: 7546 loss is tensor([-0.3005], grad_fn=<AddBackward0>)\n",
      "epoch: 7547 loss is tensor([-0.3442], grad_fn=<AddBackward0>)\n",
      "epoch: 7548 loss is tensor([-0.2816], grad_fn=<AddBackward0>)\n",
      "epoch: 7549 loss is tensor([-0.2902], grad_fn=<AddBackward0>)\n",
      "epoch: 7550 loss is tensor([-0.3105], grad_fn=<AddBackward0>)\n",
      "epoch: 7551 loss is tensor([-0.3183], grad_fn=<AddBackward0>)\n",
      "epoch: 7552 loss is tensor([-0.3336], grad_fn=<AddBackward0>)\n",
      "epoch: 7553 loss is tensor([-0.2744], grad_fn=<AddBackward0>)\n",
      "epoch: 7554 loss is tensor([-0.3143], grad_fn=<AddBackward0>)\n",
      "epoch: 7555 loss is tensor([-0.3177], grad_fn=<AddBackward0>)\n",
      "epoch: 7556 loss is tensor([-0.2470], grad_fn=<AddBackward0>)\n",
      "epoch: 7557 loss is tensor([-0.3190], grad_fn=<AddBackward0>)\n",
      "epoch: 7558 loss is tensor([-0.2609], grad_fn=<AddBackward0>)\n",
      "epoch: 7559 loss is tensor([-0.2932], grad_fn=<AddBackward0>)\n",
      "epoch: 7560 loss is tensor([-0.2822], grad_fn=<AddBackward0>)\n",
      "epoch: 7561 loss is tensor([-0.2806], grad_fn=<AddBackward0>)\n",
      "epoch: 7562 loss is tensor([-0.2779], grad_fn=<AddBackward0>)\n",
      "epoch: 7563 loss is tensor([-0.3359], grad_fn=<AddBackward0>)\n",
      "epoch: 7564 loss is tensor([-0.2906], grad_fn=<AddBackward0>)\n",
      "epoch: 7565 loss is tensor([-0.3013], grad_fn=<AddBackward0>)\n",
      "epoch: 7566 loss is tensor([-0.2834], grad_fn=<AddBackward0>)\n",
      "epoch: 7567 loss is tensor([-0.3179], grad_fn=<AddBackward0>)\n",
      "epoch: 7568 loss is tensor([-0.2987], grad_fn=<AddBackward0>)\n",
      "epoch: 7569 loss is tensor([-0.2266], grad_fn=<AddBackward0>)\n",
      "epoch: 7570 loss is tensor([-0.3139], grad_fn=<AddBackward0>)\n",
      "epoch: 7571 loss is tensor([-0.2547], grad_fn=<AddBackward0>)\n",
      "epoch: 7572 loss is tensor([-0.2807], grad_fn=<AddBackward0>)\n",
      "epoch: 7573 loss is tensor([-0.3404], grad_fn=<AddBackward0>)\n",
      "epoch: 7574 loss is tensor([-0.2777], grad_fn=<AddBackward0>)\n",
      "epoch: 7575 loss is tensor([-0.2789], grad_fn=<AddBackward0>)\n",
      "epoch: 7576 loss is tensor([-0.2793], grad_fn=<AddBackward0>)\n",
      "epoch: 7577 loss is tensor([-0.2706], grad_fn=<AddBackward0>)\n",
      "epoch: 7578 loss is tensor([-0.2120], grad_fn=<AddBackward0>)\n",
      "epoch: 7579 loss is tensor([-0.2771], grad_fn=<AddBackward0>)\n",
      "epoch: 7580 loss is tensor([-0.3242], grad_fn=<AddBackward0>)\n",
      "epoch: 7581 loss is tensor([-0.2318], grad_fn=<AddBackward0>)\n",
      "epoch: 7582 loss is tensor([-0.2962], grad_fn=<AddBackward0>)\n",
      "epoch: 7583 loss is tensor([-0.2860], grad_fn=<AddBackward0>)\n",
      "epoch: 7584 loss is tensor([-0.2720], grad_fn=<AddBackward0>)\n",
      "epoch: 7585 loss is tensor([-0.2455], grad_fn=<AddBackward0>)\n",
      "epoch: 7586 loss is tensor([-0.2971], grad_fn=<AddBackward0>)\n",
      "epoch: 7587 loss is tensor([-0.3747], grad_fn=<AddBackward0>)\n",
      "epoch: 7588 loss is tensor([-0.2946], grad_fn=<AddBackward0>)\n",
      "epoch: 7589 loss is tensor([-0.3122], grad_fn=<AddBackward0>)\n",
      "epoch: 7590 loss is tensor([-0.3338], grad_fn=<AddBackward0>)\n",
      "epoch: 7591 loss is tensor([-0.3329], grad_fn=<AddBackward0>)\n",
      "epoch: 7592 loss is tensor([-0.2973], grad_fn=<AddBackward0>)\n",
      "epoch: 7593 loss is tensor([-0.2400], grad_fn=<AddBackward0>)\n",
      "epoch: 7594 loss is tensor([-0.3597], grad_fn=<AddBackward0>)\n",
      "epoch: 7595 loss is tensor([-0.3008], grad_fn=<AddBackward0>)\n",
      "epoch: 7596 loss is tensor([-0.3121], grad_fn=<AddBackward0>)\n",
      "epoch: 7597 loss is tensor([-0.3552], grad_fn=<AddBackward0>)\n",
      "epoch: 7598 loss is tensor([-0.3541], grad_fn=<AddBackward0>)\n",
      "epoch: 7599 loss is tensor([-0.3405], grad_fn=<AddBackward0>)\n",
      "epoch: 7600 loss is tensor([-0.3555], grad_fn=<AddBackward0>)\n",
      "52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7601 loss is tensor([-0.3327], grad_fn=<AddBackward0>)\n",
      "epoch: 7602 loss is tensor([-0.3040], grad_fn=<AddBackward0>)\n",
      "epoch: 7603 loss is tensor([-0.3072], grad_fn=<AddBackward0>)\n",
      "epoch: 7604 loss is tensor([-0.3919], grad_fn=<AddBackward0>)\n",
      "epoch: 7605 loss is tensor([-0.3718], grad_fn=<AddBackward0>)\n",
      "epoch: 7606 loss is tensor([-0.3246], grad_fn=<AddBackward0>)\n",
      "epoch: 7607 loss is tensor([-0.3478], grad_fn=<AddBackward0>)\n",
      "epoch: 7608 loss is tensor([-0.2892], grad_fn=<AddBackward0>)\n",
      "epoch: 7609 loss is tensor([-0.3450], grad_fn=<AddBackward0>)\n",
      "epoch: 7610 loss is tensor([-0.3254], grad_fn=<AddBackward0>)\n",
      "epoch: 7611 loss is tensor([-0.3186], grad_fn=<AddBackward0>)\n",
      "epoch: 7612 loss is tensor([-0.3123], grad_fn=<AddBackward0>)\n",
      "epoch: 7613 loss is tensor([-0.3038], grad_fn=<AddBackward0>)\n",
      "epoch: 7614 loss is tensor([-0.2901], grad_fn=<AddBackward0>)\n",
      "epoch: 7615 loss is tensor([-0.2498], grad_fn=<AddBackward0>)\n",
      "epoch: 7616 loss is tensor([-0.2635], grad_fn=<AddBackward0>)\n",
      "epoch: 7617 loss is tensor([-0.2763], grad_fn=<AddBackward0>)\n",
      "epoch: 7618 loss is tensor([-0.3353], grad_fn=<AddBackward0>)\n",
      "epoch: 7619 loss is tensor([-0.2888], grad_fn=<AddBackward0>)\n",
      "epoch: 7620 loss is tensor([-0.3657], grad_fn=<AddBackward0>)\n",
      "epoch: 7621 loss is tensor([-0.3222], grad_fn=<AddBackward0>)\n",
      "epoch: 7622 loss is tensor([-0.3187], grad_fn=<AddBackward0>)\n",
      "epoch: 7623 loss is tensor([-0.3548], grad_fn=<AddBackward0>)\n",
      "epoch: 7624 loss is tensor([-0.2651], grad_fn=<AddBackward0>)\n",
      "epoch: 7625 loss is tensor([-0.2996], grad_fn=<AddBackward0>)\n",
      "epoch: 7626 loss is tensor([-0.3079], grad_fn=<AddBackward0>)\n",
      "epoch: 7627 loss is tensor([-0.3023], grad_fn=<AddBackward0>)\n",
      "epoch: 7628 loss is tensor([-0.3531], grad_fn=<AddBackward0>)\n",
      "epoch: 7629 loss is tensor([-0.2904], grad_fn=<AddBackward0>)\n",
      "epoch: 7630 loss is tensor([-0.4081], grad_fn=<AddBackward0>)\n",
      "epoch: 7631 loss is tensor([-0.3146], grad_fn=<AddBackward0>)\n",
      "epoch: 7632 loss is tensor([-0.2994], grad_fn=<AddBackward0>)\n",
      "epoch: 7633 loss is tensor([-0.3062], grad_fn=<AddBackward0>)\n",
      "epoch: 7634 loss is tensor([-0.3705], grad_fn=<AddBackward0>)\n",
      "epoch: 7635 loss is tensor([-0.3325], grad_fn=<AddBackward0>)\n",
      "epoch: 7636 loss is tensor([-0.2497], grad_fn=<AddBackward0>)\n",
      "epoch: 7637 loss is tensor([-0.3119], grad_fn=<AddBackward0>)\n",
      "epoch: 7638 loss is tensor([-0.2750], grad_fn=<AddBackward0>)\n",
      "epoch: 7639 loss is tensor([-0.2180], grad_fn=<AddBackward0>)\n",
      "epoch: 7640 loss is tensor([-0.2699], grad_fn=<AddBackward0>)\n",
      "epoch: 7641 loss is tensor([-0.2601], grad_fn=<AddBackward0>)\n",
      "epoch: 7642 loss is tensor([-0.2612], grad_fn=<AddBackward0>)\n",
      "epoch: 7643 loss is tensor([-0.2437], grad_fn=<AddBackward0>)\n",
      "epoch: 7644 loss is tensor([-0.2995], grad_fn=<AddBackward0>)\n",
      "epoch: 7645 loss is tensor([-0.2797], grad_fn=<AddBackward0>)\n",
      "epoch: 7646 loss is tensor([-0.2426], grad_fn=<AddBackward0>)\n",
      "epoch: 7647 loss is tensor([-0.2426], grad_fn=<AddBackward0>)\n",
      "epoch: 7648 loss is tensor([-0.3177], grad_fn=<AddBackward0>)\n",
      "epoch: 7649 loss is tensor([-0.3068], grad_fn=<AddBackward0>)\n",
      "epoch: 7650 loss is tensor([-0.2463], grad_fn=<AddBackward0>)\n",
      "epoch: 7651 loss is tensor([-0.2752], grad_fn=<AddBackward0>)\n",
      "epoch: 7652 loss is tensor([-0.3397], grad_fn=<AddBackward0>)\n",
      "epoch: 7653 loss is tensor([-0.2841], grad_fn=<AddBackward0>)\n",
      "epoch: 7654 loss is tensor([-0.3399], grad_fn=<AddBackward0>)\n",
      "epoch: 7655 loss is tensor([-0.2876], grad_fn=<AddBackward0>)\n",
      "epoch: 7656 loss is tensor([-0.3490], grad_fn=<AddBackward0>)\n",
      "epoch: 7657 loss is tensor([-0.2642], grad_fn=<AddBackward0>)\n",
      "epoch: 7658 loss is tensor([-0.2940], grad_fn=<AddBackward0>)\n",
      "epoch: 7659 loss is tensor([-0.2447], grad_fn=<AddBackward0>)\n",
      "epoch: 7660 loss is tensor([-0.2434], grad_fn=<AddBackward0>)\n",
      "epoch: 7661 loss is tensor([-0.3461], grad_fn=<AddBackward0>)\n",
      "epoch: 7662 loss is tensor([-0.3023], grad_fn=<AddBackward0>)\n",
      "epoch: 7663 loss is tensor([-0.2957], grad_fn=<AddBackward0>)\n",
      "epoch: 7664 loss is tensor([-0.3175], grad_fn=<AddBackward0>)\n",
      "epoch: 7665 loss is tensor([-0.3322], grad_fn=<AddBackward0>)\n",
      "epoch: 7666 loss is tensor([-0.2622], grad_fn=<AddBackward0>)\n",
      "epoch: 7667 loss is tensor([-0.3378], grad_fn=<AddBackward0>)\n",
      "epoch: 7668 loss is tensor([-0.2615], grad_fn=<AddBackward0>)\n",
      "epoch: 7669 loss is tensor([-0.2559], grad_fn=<AddBackward0>)\n",
      "epoch: 7670 loss is tensor([-0.2812], grad_fn=<AddBackward0>)\n",
      "epoch: 7671 loss is tensor([-0.3460], grad_fn=<AddBackward0>)\n",
      "epoch: 7672 loss is tensor([-0.3283], grad_fn=<AddBackward0>)\n",
      "epoch: 7673 loss is tensor([-0.3058], grad_fn=<AddBackward0>)\n",
      "epoch: 7674 loss is tensor([-0.2606], grad_fn=<AddBackward0>)\n",
      "epoch: 7675 loss is tensor([-0.3259], grad_fn=<AddBackward0>)\n",
      "epoch: 7676 loss is tensor([-0.2750], grad_fn=<AddBackward0>)\n",
      "epoch: 7677 loss is tensor([-0.2883], grad_fn=<AddBackward0>)\n",
      "epoch: 7678 loss is tensor([-0.3437], grad_fn=<AddBackward0>)\n",
      "epoch: 7679 loss is tensor([-0.3667], grad_fn=<AddBackward0>)\n",
      "epoch: 7680 loss is tensor([-0.3355], grad_fn=<AddBackward0>)\n",
      "epoch: 7681 loss is tensor([-0.3253], grad_fn=<AddBackward0>)\n",
      "epoch: 7682 loss is tensor([-0.2961], grad_fn=<AddBackward0>)\n",
      "epoch: 7683 loss is tensor([-0.3475], grad_fn=<AddBackward0>)\n",
      "epoch: 7684 loss is tensor([-0.3841], grad_fn=<AddBackward0>)\n",
      "epoch: 7685 loss is tensor([-0.3833], grad_fn=<AddBackward0>)\n",
      "epoch: 7686 loss is tensor([-0.3225], grad_fn=<AddBackward0>)\n",
      "epoch: 7687 loss is tensor([-0.3316], grad_fn=<AddBackward0>)\n",
      "epoch: 7688 loss is tensor([-0.3580], grad_fn=<AddBackward0>)\n",
      "epoch: 7689 loss is tensor([-0.3981], grad_fn=<AddBackward0>)\n",
      "epoch: 7690 loss is tensor([-0.3438], grad_fn=<AddBackward0>)\n",
      "epoch: 7691 loss is tensor([-0.3843], grad_fn=<AddBackward0>)\n",
      "epoch: 7692 loss is tensor([-0.3619], grad_fn=<AddBackward0>)\n",
      "epoch: 7693 loss is tensor([-0.3428], grad_fn=<AddBackward0>)\n",
      "epoch: 7694 loss is tensor([-0.3441], grad_fn=<AddBackward0>)\n",
      "epoch: 7695 loss is tensor([-0.3739], grad_fn=<AddBackward0>)\n",
      "epoch: 7696 loss is tensor([-0.3614], grad_fn=<AddBackward0>)\n",
      "epoch: 7697 loss is tensor([-0.3309], grad_fn=<AddBackward0>)\n",
      "epoch: 7698 loss is tensor([-0.3103], grad_fn=<AddBackward0>)\n",
      "epoch: 7699 loss is tensor([-0.2868], grad_fn=<AddBackward0>)\n",
      "epoch: 7700 loss is tensor([-0.3555], grad_fn=<AddBackward0>)\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7701 loss is tensor([-0.3028], grad_fn=<AddBackward0>)\n",
      "epoch: 7702 loss is tensor([-0.3429], grad_fn=<AddBackward0>)\n",
      "epoch: 7703 loss is tensor([-0.3661], grad_fn=<AddBackward0>)\n",
      "epoch: 7704 loss is tensor([-0.2903], grad_fn=<AddBackward0>)\n",
      "epoch: 7705 loss is tensor([-0.3466], grad_fn=<AddBackward0>)\n",
      "epoch: 7706 loss is tensor([-0.3416], grad_fn=<AddBackward0>)\n",
      "epoch: 7707 loss is tensor([-0.3515], grad_fn=<AddBackward0>)\n",
      "epoch: 7708 loss is tensor([-0.3362], grad_fn=<AddBackward0>)\n",
      "epoch: 7709 loss is tensor([-0.3551], grad_fn=<AddBackward0>)\n",
      "epoch: 7710 loss is tensor([-0.4065], grad_fn=<AddBackward0>)\n",
      "epoch: 7711 loss is tensor([-0.3045], grad_fn=<AddBackward0>)\n",
      "epoch: 7712 loss is tensor([-0.3794], grad_fn=<AddBackward0>)\n",
      "epoch: 7713 loss is tensor([-0.3038], grad_fn=<AddBackward0>)\n",
      "epoch: 7714 loss is tensor([-0.3599], grad_fn=<AddBackward0>)\n",
      "epoch: 7715 loss is tensor([-0.3480], grad_fn=<AddBackward0>)\n",
      "epoch: 7716 loss is tensor([-0.4047], grad_fn=<AddBackward0>)\n",
      "epoch: 7717 loss is tensor([-0.3310], grad_fn=<AddBackward0>)\n",
      "epoch: 7718 loss is tensor([-0.3237], grad_fn=<AddBackward0>)\n",
      "epoch: 7719 loss is tensor([-0.3259], grad_fn=<AddBackward0>)\n",
      "epoch: 7720 loss is tensor([-0.2721], grad_fn=<AddBackward0>)\n",
      "epoch: 7721 loss is tensor([-0.3456], grad_fn=<AddBackward0>)\n",
      "epoch: 7722 loss is tensor([-0.2715], grad_fn=<AddBackward0>)\n",
      "epoch: 7723 loss is tensor([-0.1943], grad_fn=<AddBackward0>)\n",
      "epoch: 7724 loss is tensor([-0.1589], grad_fn=<AddBackward0>)\n",
      "epoch: 7725 loss is tensor([-0.2330], grad_fn=<AddBackward0>)\n",
      "epoch: 7726 loss is tensor([-0.2229], grad_fn=<AddBackward0>)\n",
      "epoch: 7727 loss is tensor([-0.2682], grad_fn=<AddBackward0>)\n",
      "epoch: 7728 loss is tensor([-0.2786], grad_fn=<AddBackward0>)\n",
      "epoch: 7729 loss is tensor([-0.2157], grad_fn=<AddBackward0>)\n",
      "epoch: 7730 loss is tensor([-0.3255], grad_fn=<AddBackward0>)\n",
      "epoch: 7731 loss is tensor([-0.2983], grad_fn=<AddBackward0>)\n",
      "epoch: 7732 loss is tensor([-0.2401], grad_fn=<AddBackward0>)\n",
      "epoch: 7733 loss is tensor([-0.2926], grad_fn=<AddBackward0>)\n",
      "epoch: 7734 loss is tensor([-0.3197], grad_fn=<AddBackward0>)\n",
      "epoch: 7735 loss is tensor([-0.2587], grad_fn=<AddBackward0>)\n",
      "epoch: 7736 loss is tensor([-0.2774], grad_fn=<AddBackward0>)\n",
      "epoch: 7737 loss is tensor([-0.3400], grad_fn=<AddBackward0>)\n",
      "epoch: 7738 loss is tensor([-0.3616], grad_fn=<AddBackward0>)\n",
      "epoch: 7739 loss is tensor([-0.2071], grad_fn=<AddBackward0>)\n",
      "epoch: 7740 loss is tensor([-0.3002], grad_fn=<AddBackward0>)\n",
      "epoch: 7741 loss is tensor([-0.3071], grad_fn=<AddBackward0>)\n",
      "epoch: 7742 loss is tensor([-0.2609], grad_fn=<AddBackward0>)\n",
      "epoch: 7743 loss is tensor([-0.3041], grad_fn=<AddBackward0>)\n",
      "epoch: 7744 loss is tensor([-0.2764], grad_fn=<AddBackward0>)\n",
      "epoch: 7745 loss is tensor([-0.2784], grad_fn=<AddBackward0>)\n",
      "epoch: 7746 loss is tensor([-0.3005], grad_fn=<AddBackward0>)\n",
      "epoch: 7747 loss is tensor([-0.3340], grad_fn=<AddBackward0>)\n",
      "epoch: 7748 loss is tensor([-0.2240], grad_fn=<AddBackward0>)\n",
      "epoch: 7749 loss is tensor([-0.2493], grad_fn=<AddBackward0>)\n",
      "epoch: 7750 loss is tensor([-0.3437], grad_fn=<AddBackward0>)\n",
      "epoch: 7751 loss is tensor([-0.3365], grad_fn=<AddBackward0>)\n",
      "epoch: 7752 loss is tensor([-0.2683], grad_fn=<AddBackward0>)\n",
      "epoch: 7753 loss is tensor([-0.3303], grad_fn=<AddBackward0>)\n",
      "epoch: 7754 loss is tensor([-0.3135], grad_fn=<AddBackward0>)\n",
      "epoch: 7755 loss is tensor([-0.2622], grad_fn=<AddBackward0>)\n",
      "epoch: 7756 loss is tensor([-0.3293], grad_fn=<AddBackward0>)\n",
      "epoch: 7757 loss is tensor([-0.3718], grad_fn=<AddBackward0>)\n",
      "epoch: 7758 loss is tensor([-0.2483], grad_fn=<AddBackward0>)\n",
      "epoch: 7759 loss is tensor([-0.3622], grad_fn=<AddBackward0>)\n",
      "epoch: 7760 loss is tensor([-0.3016], grad_fn=<AddBackward0>)\n",
      "epoch: 7761 loss is tensor([-0.3399], grad_fn=<AddBackward0>)\n",
      "epoch: 7762 loss is tensor([-0.3553], grad_fn=<AddBackward0>)\n",
      "epoch: 7763 loss is tensor([-0.3636], grad_fn=<AddBackward0>)\n",
      "epoch: 7764 loss is tensor([-0.3466], grad_fn=<AddBackward0>)\n",
      "epoch: 7765 loss is tensor([-0.3548], grad_fn=<AddBackward0>)\n",
      "epoch: 7766 loss is tensor([-0.3576], grad_fn=<AddBackward0>)\n",
      "epoch: 7767 loss is tensor([-0.3716], grad_fn=<AddBackward0>)\n",
      "epoch: 7768 loss is tensor([-0.3402], grad_fn=<AddBackward0>)\n",
      "epoch: 7769 loss is tensor([-0.3479], grad_fn=<AddBackward0>)\n",
      "epoch: 7770 loss is tensor([-0.4220], grad_fn=<AddBackward0>)\n",
      "epoch: 7771 loss is tensor([-0.3272], grad_fn=<AddBackward0>)\n",
      "epoch: 7772 loss is tensor([-0.3238], grad_fn=<AddBackward0>)\n",
      "epoch: 7773 loss is tensor([-0.3035], grad_fn=<AddBackward0>)\n",
      "epoch: 7774 loss is tensor([-0.3528], grad_fn=<AddBackward0>)\n",
      "epoch: 7775 loss is tensor([-0.3627], grad_fn=<AddBackward0>)\n",
      "epoch: 7776 loss is tensor([-0.3073], grad_fn=<AddBackward0>)\n",
      "epoch: 7777 loss is tensor([-0.3616], grad_fn=<AddBackward0>)\n",
      "epoch: 7778 loss is tensor([-0.3442], grad_fn=<AddBackward0>)\n",
      "epoch: 7779 loss is tensor([-0.3301], grad_fn=<AddBackward0>)\n",
      "epoch: 7780 loss is tensor([-0.3083], grad_fn=<AddBackward0>)\n",
      "epoch: 7781 loss is tensor([-0.2979], grad_fn=<AddBackward0>)\n",
      "epoch: 7782 loss is tensor([-0.3520], grad_fn=<AddBackward0>)\n",
      "epoch: 7783 loss is tensor([-0.3559], grad_fn=<AddBackward0>)\n",
      "epoch: 7784 loss is tensor([-0.2723], grad_fn=<AddBackward0>)\n",
      "epoch: 7785 loss is tensor([-0.2904], grad_fn=<AddBackward0>)\n",
      "epoch: 7786 loss is tensor([-0.2773], grad_fn=<AddBackward0>)\n",
      "epoch: 7787 loss is tensor([-0.3273], grad_fn=<AddBackward0>)\n",
      "epoch: 7788 loss is tensor([-0.3566], grad_fn=<AddBackward0>)\n",
      "epoch: 7789 loss is tensor([-0.2748], grad_fn=<AddBackward0>)\n",
      "epoch: 7790 loss is tensor([-0.3047], grad_fn=<AddBackward0>)\n",
      "epoch: 7791 loss is tensor([-0.3303], grad_fn=<AddBackward0>)\n",
      "epoch: 7792 loss is tensor([-0.3405], grad_fn=<AddBackward0>)\n",
      "epoch: 7793 loss is tensor([-0.3622], grad_fn=<AddBackward0>)\n",
      "epoch: 7794 loss is tensor([-0.3174], grad_fn=<AddBackward0>)\n",
      "epoch: 7795 loss is tensor([-0.2841], grad_fn=<AddBackward0>)\n",
      "epoch: 7796 loss is tensor([-0.3321], grad_fn=<AddBackward0>)\n",
      "epoch: 7797 loss is tensor([-0.3083], grad_fn=<AddBackward0>)\n",
      "epoch: 7798 loss is tensor([-0.2197], grad_fn=<AddBackward0>)\n",
      "epoch: 7799 loss is tensor([-0.2961], grad_fn=<AddBackward0>)\n",
      "epoch: 7800 loss is tensor([-0.3698], grad_fn=<AddBackward0>)\n",
      "58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7801 loss is tensor([-0.2777], grad_fn=<AddBackward0>)\n",
      "epoch: 7802 loss is tensor([-0.3372], grad_fn=<AddBackward0>)\n",
      "epoch: 7803 loss is tensor([-0.2871], grad_fn=<AddBackward0>)\n",
      "epoch: 7804 loss is tensor([-0.2914], grad_fn=<AddBackward0>)\n",
      "epoch: 7805 loss is tensor([-0.3189], grad_fn=<AddBackward0>)\n",
      "epoch: 7806 loss is tensor([-0.3351], grad_fn=<AddBackward0>)\n",
      "epoch: 7807 loss is tensor([-0.3498], grad_fn=<AddBackward0>)\n",
      "epoch: 7808 loss is tensor([-0.3320], grad_fn=<AddBackward0>)\n",
      "epoch: 7809 loss is tensor([-0.3512], grad_fn=<AddBackward0>)\n",
      "epoch: 7810 loss is tensor([-0.3316], grad_fn=<AddBackward0>)\n",
      "epoch: 7811 loss is tensor([-0.3472], grad_fn=<AddBackward0>)\n",
      "epoch: 7812 loss is tensor([-0.2706], grad_fn=<AddBackward0>)\n",
      "epoch: 7813 loss is tensor([-0.3086], grad_fn=<AddBackward0>)\n",
      "epoch: 7814 loss is tensor([-0.3015], grad_fn=<AddBackward0>)\n",
      "epoch: 7815 loss is tensor([-0.1982], grad_fn=<AddBackward0>)\n",
      "epoch: 7816 loss is tensor([-0.2301], grad_fn=<AddBackward0>)\n",
      "epoch: 7817 loss is tensor([-0.2928], grad_fn=<AddBackward0>)\n",
      "epoch: 7818 loss is tensor([-0.2613], grad_fn=<AddBackward0>)\n",
      "epoch: 7819 loss is tensor([-0.3406], grad_fn=<AddBackward0>)\n",
      "epoch: 7820 loss is tensor([-0.3459], grad_fn=<AddBackward0>)\n",
      "epoch: 7821 loss is tensor([-0.3260], grad_fn=<AddBackward0>)\n",
      "epoch: 7822 loss is tensor([-0.3277], grad_fn=<AddBackward0>)\n",
      "epoch: 7823 loss is tensor([-0.3642], grad_fn=<AddBackward0>)\n",
      "epoch: 7824 loss is tensor([-0.3329], grad_fn=<AddBackward0>)\n",
      "epoch: 7825 loss is tensor([-0.3433], grad_fn=<AddBackward0>)\n",
      "epoch: 7826 loss is tensor([-0.3411], grad_fn=<AddBackward0>)\n",
      "epoch: 7827 loss is tensor([-0.2869], grad_fn=<AddBackward0>)\n",
      "epoch: 7828 loss is tensor([-0.3243], grad_fn=<AddBackward0>)\n",
      "epoch: 7829 loss is tensor([-0.2977], grad_fn=<AddBackward0>)\n",
      "epoch: 7830 loss is tensor([-0.3388], grad_fn=<AddBackward0>)\n",
      "epoch: 7831 loss is tensor([-0.3900], grad_fn=<AddBackward0>)\n",
      "epoch: 7832 loss is tensor([-0.2870], grad_fn=<AddBackward0>)\n",
      "epoch: 7833 loss is tensor([-0.3093], grad_fn=<AddBackward0>)\n",
      "epoch: 7834 loss is tensor([-0.3563], grad_fn=<AddBackward0>)\n",
      "epoch: 7835 loss is tensor([-0.3399], grad_fn=<AddBackward0>)\n",
      "epoch: 7836 loss is tensor([-0.3079], grad_fn=<AddBackward0>)\n",
      "epoch: 7837 loss is tensor([-0.3580], grad_fn=<AddBackward0>)\n",
      "epoch: 7838 loss is tensor([-0.3152], grad_fn=<AddBackward0>)\n",
      "epoch: 7839 loss is tensor([-0.2643], grad_fn=<AddBackward0>)\n",
      "epoch: 7840 loss is tensor([-0.2713], grad_fn=<AddBackward0>)\n",
      "epoch: 7841 loss is tensor([-0.3443], grad_fn=<AddBackward0>)\n",
      "epoch: 7842 loss is tensor([-0.3332], grad_fn=<AddBackward0>)\n",
      "epoch: 7843 loss is tensor([-0.3246], grad_fn=<AddBackward0>)\n",
      "epoch: 7844 loss is tensor([-0.3657], grad_fn=<AddBackward0>)\n",
      "epoch: 7845 loss is tensor([-0.3864], grad_fn=<AddBackward0>)\n",
      "epoch: 7846 loss is tensor([-0.3283], grad_fn=<AddBackward0>)\n",
      "epoch: 7847 loss is tensor([-0.3300], grad_fn=<AddBackward0>)\n",
      "epoch: 7848 loss is tensor([-0.3001], grad_fn=<AddBackward0>)\n",
      "epoch: 7849 loss is tensor([-0.3270], grad_fn=<AddBackward0>)\n",
      "epoch: 7850 loss is tensor([-0.4011], grad_fn=<AddBackward0>)\n",
      "epoch: 7851 loss is tensor([-0.3481], grad_fn=<AddBackward0>)\n",
      "epoch: 7852 loss is tensor([-0.3011], grad_fn=<AddBackward0>)\n",
      "epoch: 7853 loss is tensor([-0.3431], grad_fn=<AddBackward0>)\n",
      "epoch: 7854 loss is tensor([-0.2936], grad_fn=<AddBackward0>)\n",
      "epoch: 7855 loss is tensor([-0.3025], grad_fn=<AddBackward0>)\n",
      "epoch: 7856 loss is tensor([-0.3321], grad_fn=<AddBackward0>)\n",
      "epoch: 7857 loss is tensor([-0.3823], grad_fn=<AddBackward0>)\n",
      "epoch: 7858 loss is tensor([-0.3238], grad_fn=<AddBackward0>)\n",
      "epoch: 7859 loss is tensor([-0.3252], grad_fn=<AddBackward0>)\n",
      "epoch: 7860 loss is tensor([-0.3621], grad_fn=<AddBackward0>)\n",
      "epoch: 7861 loss is tensor([-0.3363], grad_fn=<AddBackward0>)\n",
      "epoch: 7862 loss is tensor([-0.3334], grad_fn=<AddBackward0>)\n",
      "epoch: 7863 loss is tensor([-0.3830], grad_fn=<AddBackward0>)\n",
      "epoch: 7864 loss is tensor([-0.3028], grad_fn=<AddBackward0>)\n",
      "epoch: 7865 loss is tensor([-0.3078], grad_fn=<AddBackward0>)\n",
      "epoch: 7866 loss is tensor([-0.3787], grad_fn=<AddBackward0>)\n",
      "epoch: 7867 loss is tensor([-0.2997], grad_fn=<AddBackward0>)\n",
      "epoch: 7868 loss is tensor([-0.3724], grad_fn=<AddBackward0>)\n",
      "epoch: 7869 loss is tensor([-0.3687], grad_fn=<AddBackward0>)\n",
      "epoch: 7870 loss is tensor([-0.3373], grad_fn=<AddBackward0>)\n",
      "epoch: 7871 loss is tensor([-0.3474], grad_fn=<AddBackward0>)\n",
      "epoch: 7872 loss is tensor([-0.2963], grad_fn=<AddBackward0>)\n",
      "epoch: 7873 loss is tensor([-0.3404], grad_fn=<AddBackward0>)\n",
      "epoch: 7874 loss is tensor([-0.3672], grad_fn=<AddBackward0>)\n",
      "epoch: 7875 loss is tensor([-0.3521], grad_fn=<AddBackward0>)\n",
      "epoch: 7876 loss is tensor([-0.3728], grad_fn=<AddBackward0>)\n",
      "epoch: 7877 loss is tensor([-0.3116], grad_fn=<AddBackward0>)\n",
      "epoch: 7878 loss is tensor([-0.3497], grad_fn=<AddBackward0>)\n",
      "epoch: 7879 loss is tensor([-0.3247], grad_fn=<AddBackward0>)\n",
      "epoch: 7880 loss is tensor([-0.2686], grad_fn=<AddBackward0>)\n",
      "epoch: 7881 loss is tensor([-0.3162], grad_fn=<AddBackward0>)\n",
      "epoch: 7882 loss is tensor([-0.3509], grad_fn=<AddBackward0>)\n",
      "epoch: 7883 loss is tensor([-0.2863], grad_fn=<AddBackward0>)\n",
      "epoch: 7884 loss is tensor([-0.3172], grad_fn=<AddBackward0>)\n",
      "epoch: 7885 loss is tensor([-0.2486], grad_fn=<AddBackward0>)\n",
      "epoch: 7886 loss is tensor([-0.3396], grad_fn=<AddBackward0>)\n",
      "epoch: 7887 loss is tensor([-0.4059], grad_fn=<AddBackward0>)\n",
      "epoch: 7888 loss is tensor([-0.3664], grad_fn=<AddBackward0>)\n",
      "epoch: 7889 loss is tensor([-0.2866], grad_fn=<AddBackward0>)\n",
      "epoch: 7890 loss is tensor([-0.3491], grad_fn=<AddBackward0>)\n",
      "epoch: 7891 loss is tensor([-0.3489], grad_fn=<AddBackward0>)\n",
      "epoch: 7892 loss is tensor([-0.3292], grad_fn=<AddBackward0>)\n",
      "epoch: 7893 loss is tensor([-0.3326], grad_fn=<AddBackward0>)\n",
      "epoch: 7894 loss is tensor([-0.2902], grad_fn=<AddBackward0>)\n",
      "epoch: 7895 loss is tensor([-0.3344], grad_fn=<AddBackward0>)\n",
      "epoch: 7896 loss is tensor([-0.3153], grad_fn=<AddBackward0>)\n",
      "epoch: 7897 loss is tensor([-0.2939], grad_fn=<AddBackward0>)\n",
      "epoch: 7898 loss is tensor([-0.3252], grad_fn=<AddBackward0>)\n",
      "epoch: 7899 loss is tensor([-0.3050], grad_fn=<AddBackward0>)\n",
      "epoch: 7900 loss is tensor([-0.2746], grad_fn=<AddBackward0>)\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7901 loss is tensor([-0.3626], grad_fn=<AddBackward0>)\n",
      "epoch: 7902 loss is tensor([-0.3091], grad_fn=<AddBackward0>)\n",
      "epoch: 7903 loss is tensor([-0.3091], grad_fn=<AddBackward0>)\n",
      "epoch: 7904 loss is tensor([-0.3784], grad_fn=<AddBackward0>)\n",
      "epoch: 7905 loss is tensor([-0.3231], grad_fn=<AddBackward0>)\n",
      "epoch: 7906 loss is tensor([-0.3542], grad_fn=<AddBackward0>)\n",
      "epoch: 7907 loss is tensor([-0.3328], grad_fn=<AddBackward0>)\n",
      "epoch: 7908 loss is tensor([-0.3921], grad_fn=<AddBackward0>)\n",
      "epoch: 7909 loss is tensor([-0.3441], grad_fn=<AddBackward0>)\n",
      "epoch: 7910 loss is tensor([-0.3622], grad_fn=<AddBackward0>)\n",
      "epoch: 7911 loss is tensor([-0.3797], grad_fn=<AddBackward0>)\n",
      "epoch: 7912 loss is tensor([-0.3392], grad_fn=<AddBackward0>)\n",
      "epoch: 7913 loss is tensor([-0.3700], grad_fn=<AddBackward0>)\n",
      "epoch: 7914 loss is tensor([-0.3417], grad_fn=<AddBackward0>)\n",
      "epoch: 7915 loss is tensor([-0.3233], grad_fn=<AddBackward0>)\n",
      "epoch: 7916 loss is tensor([-0.3209], grad_fn=<AddBackward0>)\n",
      "epoch: 7917 loss is tensor([-0.3824], grad_fn=<AddBackward0>)\n",
      "epoch: 7918 loss is tensor([-0.2892], grad_fn=<AddBackward0>)\n",
      "epoch: 7919 loss is tensor([-0.3343], grad_fn=<AddBackward0>)\n",
      "epoch: 7920 loss is tensor([-0.3326], grad_fn=<AddBackward0>)\n",
      "epoch: 7921 loss is tensor([-0.3703], grad_fn=<AddBackward0>)\n",
      "epoch: 7922 loss is tensor([-0.3354], grad_fn=<AddBackward0>)\n",
      "epoch: 7923 loss is tensor([-0.3343], grad_fn=<AddBackward0>)\n",
      "epoch: 7924 loss is tensor([-0.3408], grad_fn=<AddBackward0>)\n",
      "epoch: 7925 loss is tensor([-0.3594], grad_fn=<AddBackward0>)\n",
      "epoch: 7926 loss is tensor([-0.2649], grad_fn=<AddBackward0>)\n",
      "epoch: 7927 loss is tensor([-0.4009], grad_fn=<AddBackward0>)\n",
      "epoch: 7928 loss is tensor([-0.2939], grad_fn=<AddBackward0>)\n",
      "epoch: 7929 loss is tensor([-0.3442], grad_fn=<AddBackward0>)\n",
      "epoch: 7930 loss is tensor([-0.3182], grad_fn=<AddBackward0>)\n",
      "epoch: 7931 loss is tensor([-0.3120], grad_fn=<AddBackward0>)\n",
      "epoch: 7932 loss is tensor([-0.3048], grad_fn=<AddBackward0>)\n",
      "epoch: 7933 loss is tensor([-0.3833], grad_fn=<AddBackward0>)\n",
      "epoch: 7934 loss is tensor([-0.3210], grad_fn=<AddBackward0>)\n",
      "epoch: 7935 loss is tensor([-0.3365], grad_fn=<AddBackward0>)\n",
      "epoch: 7936 loss is tensor([-0.3393], grad_fn=<AddBackward0>)\n",
      "epoch: 7937 loss is tensor([-0.3427], grad_fn=<AddBackward0>)\n",
      "epoch: 7938 loss is tensor([-0.3404], grad_fn=<AddBackward0>)\n",
      "epoch: 7939 loss is tensor([-0.3687], grad_fn=<AddBackward0>)\n",
      "epoch: 7940 loss is tensor([-0.3298], grad_fn=<AddBackward0>)\n",
      "epoch: 7941 loss is tensor([-0.3226], grad_fn=<AddBackward0>)\n",
      "epoch: 7942 loss is tensor([-0.3583], grad_fn=<AddBackward0>)\n",
      "epoch: 7943 loss is tensor([-0.3662], grad_fn=<AddBackward0>)\n",
      "epoch: 7944 loss is tensor([-0.3455], grad_fn=<AddBackward0>)\n",
      "epoch: 7945 loss is tensor([-0.3648], grad_fn=<AddBackward0>)\n",
      "epoch: 7946 loss is tensor([-0.3548], grad_fn=<AddBackward0>)\n",
      "epoch: 7947 loss is tensor([-0.3555], grad_fn=<AddBackward0>)\n",
      "epoch: 7948 loss is tensor([-0.3256], grad_fn=<AddBackward0>)\n",
      "epoch: 7949 loss is tensor([-0.3227], grad_fn=<AddBackward0>)\n",
      "epoch: 7950 loss is tensor([-0.3918], grad_fn=<AddBackward0>)\n",
      "epoch: 7951 loss is tensor([-0.3644], grad_fn=<AddBackward0>)\n",
      "epoch: 7952 loss is tensor([-0.3455], grad_fn=<AddBackward0>)\n",
      "epoch: 7953 loss is tensor([-0.3678], grad_fn=<AddBackward0>)\n",
      "epoch: 7954 loss is tensor([-0.3247], grad_fn=<AddBackward0>)\n",
      "epoch: 7955 loss is tensor([-0.3227], grad_fn=<AddBackward0>)\n",
      "epoch: 7956 loss is tensor([-0.2941], grad_fn=<AddBackward0>)\n",
      "epoch: 7957 loss is tensor([-0.3511], grad_fn=<AddBackward0>)\n",
      "epoch: 7958 loss is tensor([-0.3513], grad_fn=<AddBackward0>)\n",
      "epoch: 7959 loss is tensor([-0.3142], grad_fn=<AddBackward0>)\n",
      "epoch: 7960 loss is tensor([-0.3393], grad_fn=<AddBackward0>)\n",
      "epoch: 7961 loss is tensor([-0.3855], grad_fn=<AddBackward0>)\n",
      "epoch: 7962 loss is tensor([-0.3823], grad_fn=<AddBackward0>)\n",
      "epoch: 7963 loss is tensor([-0.3925], grad_fn=<AddBackward0>)\n",
      "epoch: 7964 loss is tensor([-0.3009], grad_fn=<AddBackward0>)\n",
      "epoch: 7965 loss is tensor([-0.3781], grad_fn=<AddBackward0>)\n",
      "epoch: 7966 loss is tensor([-0.3052], grad_fn=<AddBackward0>)\n",
      "epoch: 7967 loss is tensor([-0.3190], grad_fn=<AddBackward0>)\n",
      "epoch: 7968 loss is tensor([-0.2898], grad_fn=<AddBackward0>)\n",
      "epoch: 7969 loss is tensor([-0.3483], grad_fn=<AddBackward0>)\n",
      "epoch: 7970 loss is tensor([-0.3128], grad_fn=<AddBackward0>)\n",
      "epoch: 7971 loss is tensor([-0.3510], grad_fn=<AddBackward0>)\n",
      "epoch: 7972 loss is tensor([-0.3275], grad_fn=<AddBackward0>)\n",
      "epoch: 7973 loss is tensor([-0.3633], grad_fn=<AddBackward0>)\n",
      "epoch: 7974 loss is tensor([-0.3467], grad_fn=<AddBackward0>)\n",
      "epoch: 7975 loss is tensor([-0.3938], grad_fn=<AddBackward0>)\n",
      "epoch: 7976 loss is tensor([-0.3587], grad_fn=<AddBackward0>)\n",
      "epoch: 7977 loss is tensor([-0.3241], grad_fn=<AddBackward0>)\n",
      "epoch: 7978 loss is tensor([-0.3507], grad_fn=<AddBackward0>)\n",
      "epoch: 7979 loss is tensor([-0.3414], grad_fn=<AddBackward0>)\n",
      "epoch: 7980 loss is tensor([-0.3830], grad_fn=<AddBackward0>)\n",
      "epoch: 7981 loss is tensor([-0.3825], grad_fn=<AddBackward0>)\n",
      "epoch: 7982 loss is tensor([-0.4155], grad_fn=<AddBackward0>)\n",
      "epoch: 7983 loss is tensor([-0.3620], grad_fn=<AddBackward0>)\n",
      "epoch: 7984 loss is tensor([-0.3414], grad_fn=<AddBackward0>)\n",
      "epoch: 7985 loss is tensor([-0.3508], grad_fn=<AddBackward0>)\n",
      "epoch: 7986 loss is tensor([-0.3501], grad_fn=<AddBackward0>)\n",
      "epoch: 7987 loss is tensor([-0.3789], grad_fn=<AddBackward0>)\n",
      "epoch: 7988 loss is tensor([-0.3495], grad_fn=<AddBackward0>)\n",
      "epoch: 7989 loss is tensor([-0.3746], grad_fn=<AddBackward0>)\n",
      "epoch: 7990 loss is tensor([-0.3895], grad_fn=<AddBackward0>)\n",
      "epoch: 7991 loss is tensor([-0.3837], grad_fn=<AddBackward0>)\n",
      "epoch: 7992 loss is tensor([-0.3417], grad_fn=<AddBackward0>)\n",
      "epoch: 7993 loss is tensor([-0.3792], grad_fn=<AddBackward0>)\n",
      "epoch: 7994 loss is tensor([-0.3431], grad_fn=<AddBackward0>)\n",
      "epoch: 7995 loss is tensor([-0.3214], grad_fn=<AddBackward0>)\n",
      "epoch: 7996 loss is tensor([-0.3131], grad_fn=<AddBackward0>)\n",
      "epoch: 7997 loss is tensor([-0.3310], grad_fn=<AddBackward0>)\n",
      "epoch: 7998 loss is tensor([-0.3548], grad_fn=<AddBackward0>)\n",
      "epoch: 7999 loss is tensor([-0.3289], grad_fn=<AddBackward0>)\n",
      "epoch: 8000 loss is tensor([-0.3311], grad_fn=<AddBackward0>)\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8001 loss is tensor([-0.3761], grad_fn=<AddBackward0>)\n",
      "epoch: 8002 loss is tensor([-0.3161], grad_fn=<AddBackward0>)\n",
      "epoch: 8003 loss is tensor([-0.3479], grad_fn=<AddBackward0>)\n",
      "epoch: 8004 loss is tensor([-0.4182], grad_fn=<AddBackward0>)\n",
      "epoch: 8005 loss is tensor([-0.3803], grad_fn=<AddBackward0>)\n",
      "epoch: 8006 loss is tensor([-0.3335], grad_fn=<AddBackward0>)\n",
      "epoch: 8007 loss is tensor([-0.3555], grad_fn=<AddBackward0>)\n",
      "epoch: 8008 loss is tensor([-0.2922], grad_fn=<AddBackward0>)\n",
      "epoch: 8009 loss is tensor([-0.3482], grad_fn=<AddBackward0>)\n",
      "epoch: 8010 loss is tensor([-0.3769], grad_fn=<AddBackward0>)\n",
      "epoch: 8011 loss is tensor([-0.3564], grad_fn=<AddBackward0>)\n",
      "epoch: 8012 loss is tensor([-0.4098], grad_fn=<AddBackward0>)\n",
      "epoch: 8013 loss is tensor([-0.3709], grad_fn=<AddBackward0>)\n",
      "epoch: 8014 loss is tensor([-0.4256], grad_fn=<AddBackward0>)\n",
      "epoch: 8015 loss is tensor([-0.3777], grad_fn=<AddBackward0>)\n",
      "epoch: 8016 loss is tensor([-0.3678], grad_fn=<AddBackward0>)\n",
      "epoch: 8017 loss is tensor([-0.3780], grad_fn=<AddBackward0>)\n",
      "epoch: 8018 loss is tensor([-0.3444], grad_fn=<AddBackward0>)\n",
      "epoch: 8019 loss is tensor([-0.3102], grad_fn=<AddBackward0>)\n",
      "epoch: 8020 loss is tensor([-0.3723], grad_fn=<AddBackward0>)\n",
      "epoch: 8021 loss is tensor([-0.3525], grad_fn=<AddBackward0>)\n",
      "epoch: 8022 loss is tensor([-0.3744], grad_fn=<AddBackward0>)\n",
      "epoch: 8023 loss is tensor([-0.3833], grad_fn=<AddBackward0>)\n",
      "epoch: 8024 loss is tensor([-0.3640], grad_fn=<AddBackward0>)\n",
      "epoch: 8025 loss is tensor([-0.4260], grad_fn=<AddBackward0>)\n",
      "epoch: 8026 loss is tensor([-0.3881], grad_fn=<AddBackward0>)\n",
      "epoch: 8027 loss is tensor([-0.3915], grad_fn=<AddBackward0>)\n",
      "epoch: 8028 loss is tensor([-0.3192], grad_fn=<AddBackward0>)\n",
      "epoch: 8029 loss is tensor([-0.3465], grad_fn=<AddBackward0>)\n",
      "epoch: 8030 loss is tensor([-0.4025], grad_fn=<AddBackward0>)\n",
      "epoch: 8031 loss is tensor([-0.3936], grad_fn=<AddBackward0>)\n",
      "epoch: 8032 loss is tensor([-0.3555], grad_fn=<AddBackward0>)\n",
      "epoch: 8033 loss is tensor([-0.3990], grad_fn=<AddBackward0>)\n",
      "epoch: 8034 loss is tensor([-0.3933], grad_fn=<AddBackward0>)\n",
      "epoch: 8035 loss is tensor([-0.3981], grad_fn=<AddBackward0>)\n",
      "epoch: 8036 loss is tensor([-0.3872], grad_fn=<AddBackward0>)\n",
      "epoch: 8037 loss is tensor([-0.3342], grad_fn=<AddBackward0>)\n",
      "epoch: 8038 loss is tensor([-0.3601], grad_fn=<AddBackward0>)\n",
      "epoch: 8039 loss is tensor([-0.4060], grad_fn=<AddBackward0>)\n",
      "epoch: 8040 loss is tensor([-0.3421], grad_fn=<AddBackward0>)\n",
      "epoch: 8041 loss is tensor([-0.2869], grad_fn=<AddBackward0>)\n",
      "epoch: 8042 loss is tensor([-0.3758], grad_fn=<AddBackward0>)\n",
      "epoch: 8043 loss is tensor([-0.3435], grad_fn=<AddBackward0>)\n",
      "epoch: 8044 loss is tensor([-0.3592], grad_fn=<AddBackward0>)\n",
      "epoch: 8045 loss is tensor([-0.4076], grad_fn=<AddBackward0>)\n",
      "epoch: 8046 loss is tensor([-0.3746], grad_fn=<AddBackward0>)\n",
      "epoch: 8047 loss is tensor([-0.3612], grad_fn=<AddBackward0>)\n",
      "epoch: 8048 loss is tensor([-0.3565], grad_fn=<AddBackward0>)\n",
      "epoch: 8049 loss is tensor([-0.3492], grad_fn=<AddBackward0>)\n",
      "epoch: 8050 loss is tensor([-0.3553], grad_fn=<AddBackward0>)\n",
      "epoch: 8051 loss is tensor([-0.3579], grad_fn=<AddBackward0>)\n",
      "epoch: 8052 loss is tensor([-0.4127], grad_fn=<AddBackward0>)\n",
      "epoch: 8053 loss is tensor([-0.3565], grad_fn=<AddBackward0>)\n",
      "epoch: 8054 loss is tensor([-0.3268], grad_fn=<AddBackward0>)\n",
      "epoch: 8055 loss is tensor([-0.2989], grad_fn=<AddBackward0>)\n",
      "epoch: 8056 loss is tensor([-0.2653], grad_fn=<AddBackward0>)\n",
      "epoch: 8057 loss is tensor([-0.3446], grad_fn=<AddBackward0>)\n",
      "epoch: 8058 loss is tensor([-0.3255], grad_fn=<AddBackward0>)\n",
      "epoch: 8059 loss is tensor([-0.2514], grad_fn=<AddBackward0>)\n",
      "epoch: 8060 loss is tensor([-0.2549], grad_fn=<AddBackward0>)\n",
      "epoch: 8061 loss is tensor([-0.3090], grad_fn=<AddBackward0>)\n",
      "epoch: 8062 loss is tensor([-0.3117], grad_fn=<AddBackward0>)\n",
      "epoch: 8063 loss is tensor([-0.2941], grad_fn=<AddBackward0>)\n",
      "epoch: 8064 loss is tensor([-0.3284], grad_fn=<AddBackward0>)\n",
      "epoch: 8065 loss is tensor([-0.3093], grad_fn=<AddBackward0>)\n",
      "epoch: 8066 loss is tensor([-0.3273], grad_fn=<AddBackward0>)\n",
      "epoch: 8067 loss is tensor([-0.3289], grad_fn=<AddBackward0>)\n",
      "epoch: 8068 loss is tensor([-0.2858], grad_fn=<AddBackward0>)\n",
      "epoch: 8069 loss is tensor([-0.3604], grad_fn=<AddBackward0>)\n",
      "epoch: 8070 loss is tensor([-0.3845], grad_fn=<AddBackward0>)\n",
      "epoch: 8071 loss is tensor([-0.3392], grad_fn=<AddBackward0>)\n",
      "epoch: 8072 loss is tensor([-0.2948], grad_fn=<AddBackward0>)\n",
      "epoch: 8073 loss is tensor([-0.2804], grad_fn=<AddBackward0>)\n",
      "epoch: 8074 loss is tensor([-0.2765], grad_fn=<AddBackward0>)\n",
      "epoch: 8075 loss is tensor([-0.3362], grad_fn=<AddBackward0>)\n",
      "epoch: 8076 loss is tensor([-0.3578], grad_fn=<AddBackward0>)\n",
      "epoch: 8077 loss is tensor([-0.3602], grad_fn=<AddBackward0>)\n",
      "epoch: 8078 loss is tensor([-0.3623], grad_fn=<AddBackward0>)\n",
      "epoch: 8079 loss is tensor([-0.3636], grad_fn=<AddBackward0>)\n",
      "epoch: 8080 loss is tensor([-0.3876], grad_fn=<AddBackward0>)\n",
      "epoch: 8081 loss is tensor([-0.2682], grad_fn=<AddBackward0>)\n",
      "epoch: 8082 loss is tensor([-0.3218], grad_fn=<AddBackward0>)\n",
      "epoch: 8083 loss is tensor([-0.3141], grad_fn=<AddBackward0>)\n",
      "epoch: 8084 loss is tensor([-0.2826], grad_fn=<AddBackward0>)\n",
      "epoch: 8085 loss is tensor([-0.3147], grad_fn=<AddBackward0>)\n",
      "epoch: 8086 loss is tensor([-0.2547], grad_fn=<AddBackward0>)\n",
      "epoch: 8087 loss is tensor([-0.3867], grad_fn=<AddBackward0>)\n",
      "epoch: 8088 loss is tensor([-0.2789], grad_fn=<AddBackward0>)\n",
      "epoch: 8089 loss is tensor([-0.3555], grad_fn=<AddBackward0>)\n",
      "epoch: 8090 loss is tensor([-0.3434], grad_fn=<AddBackward0>)\n",
      "epoch: 8091 loss is tensor([-0.3706], grad_fn=<AddBackward0>)\n",
      "epoch: 8092 loss is tensor([-0.3428], grad_fn=<AddBackward0>)\n",
      "epoch: 8093 loss is tensor([-0.3601], grad_fn=<AddBackward0>)\n",
      "epoch: 8094 loss is tensor([-0.3399], grad_fn=<AddBackward0>)\n",
      "epoch: 8095 loss is tensor([-0.3854], grad_fn=<AddBackward0>)\n",
      "epoch: 8096 loss is tensor([-0.3114], grad_fn=<AddBackward0>)\n",
      "epoch: 8097 loss is tensor([-0.3400], grad_fn=<AddBackward0>)\n",
      "epoch: 8098 loss is tensor([-0.3402], grad_fn=<AddBackward0>)\n",
      "epoch: 8099 loss is tensor([-0.3552], grad_fn=<AddBackward0>)\n",
      "epoch: 8100 loss is tensor([-0.3130], grad_fn=<AddBackward0>)\n",
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8101 loss is tensor([-0.3775], grad_fn=<AddBackward0>)\n",
      "epoch: 8102 loss is tensor([-0.3368], grad_fn=<AddBackward0>)\n",
      "epoch: 8103 loss is tensor([-0.3720], grad_fn=<AddBackward0>)\n",
      "epoch: 8104 loss is tensor([-0.3578], grad_fn=<AddBackward0>)\n",
      "epoch: 8105 loss is tensor([-0.3663], grad_fn=<AddBackward0>)\n",
      "epoch: 8106 loss is tensor([-0.3432], grad_fn=<AddBackward0>)\n",
      "epoch: 8107 loss is tensor([-0.3606], grad_fn=<AddBackward0>)\n",
      "epoch: 8108 loss is tensor([-0.4054], grad_fn=<AddBackward0>)\n",
      "epoch: 8109 loss is tensor([-0.3689], grad_fn=<AddBackward0>)\n",
      "epoch: 8110 loss is tensor([-0.3474], grad_fn=<AddBackward0>)\n",
      "epoch: 8111 loss is tensor([-0.3312], grad_fn=<AddBackward0>)\n",
      "epoch: 8112 loss is tensor([-0.3462], grad_fn=<AddBackward0>)\n",
      "epoch: 8113 loss is tensor([-0.3240], grad_fn=<AddBackward0>)\n",
      "epoch: 8114 loss is tensor([-0.3981], grad_fn=<AddBackward0>)\n",
      "epoch: 8115 loss is tensor([-0.3609], grad_fn=<AddBackward0>)\n",
      "epoch: 8116 loss is tensor([-0.3267], grad_fn=<AddBackward0>)\n",
      "epoch: 8117 loss is tensor([-0.3526], grad_fn=<AddBackward0>)\n",
      "epoch: 8118 loss is tensor([-0.3564], grad_fn=<AddBackward0>)\n",
      "epoch: 8119 loss is tensor([-0.3864], grad_fn=<AddBackward0>)\n",
      "epoch: 8120 loss is tensor([-0.3673], grad_fn=<AddBackward0>)\n",
      "epoch: 8121 loss is tensor([-0.3799], grad_fn=<AddBackward0>)\n",
      "epoch: 8122 loss is tensor([-0.3599], grad_fn=<AddBackward0>)\n",
      "epoch: 8123 loss is tensor([-0.3457], grad_fn=<AddBackward0>)\n",
      "epoch: 8124 loss is tensor([-0.3329], grad_fn=<AddBackward0>)\n",
      "epoch: 8125 loss is tensor([-0.3978], grad_fn=<AddBackward0>)\n",
      "epoch: 8126 loss is tensor([-0.3882], grad_fn=<AddBackward0>)\n",
      "epoch: 8127 loss is tensor([-0.3436], grad_fn=<AddBackward0>)\n",
      "epoch: 8128 loss is tensor([-0.3806], grad_fn=<AddBackward0>)\n",
      "epoch: 8129 loss is tensor([-0.3482], grad_fn=<AddBackward0>)\n",
      "epoch: 8130 loss is tensor([-0.3126], grad_fn=<AddBackward0>)\n",
      "epoch: 8131 loss is tensor([-0.3310], grad_fn=<AddBackward0>)\n",
      "epoch: 8132 loss is tensor([-0.3532], grad_fn=<AddBackward0>)\n",
      "epoch: 8133 loss is tensor([-0.3737], grad_fn=<AddBackward0>)\n",
      "epoch: 8134 loss is tensor([-0.3664], grad_fn=<AddBackward0>)\n",
      "epoch: 8135 loss is tensor([-0.3974], grad_fn=<AddBackward0>)\n",
      "epoch: 8136 loss is tensor([-0.3691], grad_fn=<AddBackward0>)\n",
      "epoch: 8137 loss is tensor([-0.4284], grad_fn=<AddBackward0>)\n",
      "epoch: 8138 loss is tensor([-0.4053], grad_fn=<AddBackward0>)\n",
      "epoch: 8139 loss is tensor([-0.4018], grad_fn=<AddBackward0>)\n",
      "epoch: 8140 loss is tensor([-0.3509], grad_fn=<AddBackward0>)\n",
      "epoch: 8141 loss is tensor([-0.3962], grad_fn=<AddBackward0>)\n",
      "epoch: 8142 loss is tensor([-0.4012], grad_fn=<AddBackward0>)\n",
      "epoch: 8143 loss is tensor([-0.3482], grad_fn=<AddBackward0>)\n",
      "epoch: 8144 loss is tensor([-0.3527], grad_fn=<AddBackward0>)\n",
      "epoch: 8145 loss is tensor([-0.3160], grad_fn=<AddBackward0>)\n",
      "epoch: 8146 loss is tensor([-0.3045], grad_fn=<AddBackward0>)\n",
      "epoch: 8147 loss is tensor([-0.3896], grad_fn=<AddBackward0>)\n",
      "epoch: 8148 loss is tensor([-0.3207], grad_fn=<AddBackward0>)\n",
      "epoch: 8149 loss is tensor([-0.4247], grad_fn=<AddBackward0>)\n",
      "epoch: 8150 loss is tensor([-0.3473], grad_fn=<AddBackward0>)\n",
      "epoch: 8151 loss is tensor([-0.3163], grad_fn=<AddBackward0>)\n",
      "epoch: 8152 loss is tensor([-0.3768], grad_fn=<AddBackward0>)\n",
      "epoch: 8153 loss is tensor([-0.3819], grad_fn=<AddBackward0>)\n",
      "epoch: 8154 loss is tensor([-0.3891], grad_fn=<AddBackward0>)\n",
      "epoch: 8155 loss is tensor([-0.3453], grad_fn=<AddBackward0>)\n",
      "epoch: 8156 loss is tensor([-0.2341], grad_fn=<AddBackward0>)\n",
      "epoch: 8157 loss is tensor([-0.2192], grad_fn=<AddBackward0>)\n",
      "epoch: 8158 loss is tensor([-0.3456], grad_fn=<AddBackward0>)\n",
      "epoch: 8159 loss is tensor([-0.2440], grad_fn=<AddBackward0>)\n",
      "epoch: 8160 loss is tensor([-0.2433], grad_fn=<AddBackward0>)\n",
      "epoch: 8161 loss is tensor([-0.2921], grad_fn=<AddBackward0>)\n",
      "epoch: 8162 loss is tensor([-0.2433], grad_fn=<AddBackward0>)\n",
      "epoch: 8163 loss is tensor([-0.3482], grad_fn=<AddBackward0>)\n",
      "epoch: 8164 loss is tensor([-0.3300], grad_fn=<AddBackward0>)\n",
      "epoch: 8165 loss is tensor([-0.3756], grad_fn=<AddBackward0>)\n",
      "epoch: 8166 loss is tensor([-0.3468], grad_fn=<AddBackward0>)\n",
      "epoch: 8167 loss is tensor([-0.3655], grad_fn=<AddBackward0>)\n",
      "epoch: 8168 loss is tensor([-0.3389], grad_fn=<AddBackward0>)\n",
      "epoch: 8169 loss is tensor([-0.3392], grad_fn=<AddBackward0>)\n",
      "epoch: 8170 loss is tensor([-0.3908], grad_fn=<AddBackward0>)\n",
      "epoch: 8171 loss is tensor([-0.3485], grad_fn=<AddBackward0>)\n",
      "epoch: 8172 loss is tensor([-0.4034], grad_fn=<AddBackward0>)\n",
      "epoch: 8173 loss is tensor([-0.3785], grad_fn=<AddBackward0>)\n",
      "epoch: 8174 loss is tensor([-0.3973], grad_fn=<AddBackward0>)\n",
      "epoch: 8175 loss is tensor([-0.3699], grad_fn=<AddBackward0>)\n",
      "epoch: 8176 loss is tensor([-0.3626], grad_fn=<AddBackward0>)\n",
      "epoch: 8177 loss is tensor([-0.3802], grad_fn=<AddBackward0>)\n",
      "epoch: 8178 loss is tensor([-0.3482], grad_fn=<AddBackward0>)\n",
      "epoch: 8179 loss is tensor([-0.3210], grad_fn=<AddBackward0>)\n",
      "epoch: 8180 loss is tensor([-0.3117], grad_fn=<AddBackward0>)\n",
      "epoch: 8181 loss is tensor([-0.3164], grad_fn=<AddBackward0>)\n",
      "epoch: 8182 loss is tensor([-0.2934], grad_fn=<AddBackward0>)\n",
      "epoch: 8183 loss is tensor([-0.3533], grad_fn=<AddBackward0>)\n",
      "epoch: 8184 loss is tensor([-0.3366], grad_fn=<AddBackward0>)\n",
      "epoch: 8185 loss is tensor([-0.3882], grad_fn=<AddBackward0>)\n",
      "epoch: 8186 loss is tensor([-0.3536], grad_fn=<AddBackward0>)\n",
      "epoch: 8187 loss is tensor([-0.4175], grad_fn=<AddBackward0>)\n",
      "epoch: 8188 loss is tensor([-0.3180], grad_fn=<AddBackward0>)\n",
      "epoch: 8189 loss is tensor([-0.3680], grad_fn=<AddBackward0>)\n",
      "epoch: 8190 loss is tensor([-0.3621], grad_fn=<AddBackward0>)\n",
      "epoch: 8191 loss is tensor([-0.3352], grad_fn=<AddBackward0>)\n",
      "epoch: 8192 loss is tensor([-0.3930], grad_fn=<AddBackward0>)\n",
      "epoch: 8193 loss is tensor([-0.3621], grad_fn=<AddBackward0>)\n",
      "epoch: 8194 loss is tensor([-0.3863], grad_fn=<AddBackward0>)\n",
      "epoch: 8195 loss is tensor([-0.3623], grad_fn=<AddBackward0>)\n",
      "epoch: 8196 loss is tensor([-0.3161], grad_fn=<AddBackward0>)\n",
      "epoch: 8197 loss is tensor([-0.3975], grad_fn=<AddBackward0>)\n",
      "epoch: 8198 loss is tensor([-0.3389], grad_fn=<AddBackward0>)\n",
      "epoch: 8199 loss is tensor([-0.3039], grad_fn=<AddBackward0>)\n",
      "epoch: 8200 loss is tensor([-0.3705], grad_fn=<AddBackward0>)\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8201 loss is tensor([-0.3170], grad_fn=<AddBackward0>)\n",
      "epoch: 8202 loss is tensor([-0.3169], grad_fn=<AddBackward0>)\n",
      "epoch: 8203 loss is tensor([-0.2780], grad_fn=<AddBackward0>)\n",
      "epoch: 8204 loss is tensor([-0.3547], grad_fn=<AddBackward0>)\n",
      "epoch: 8205 loss is tensor([-0.3042], grad_fn=<AddBackward0>)\n",
      "epoch: 8206 loss is tensor([-0.3418], grad_fn=<AddBackward0>)\n",
      "epoch: 8207 loss is tensor([-0.3123], grad_fn=<AddBackward0>)\n",
      "epoch: 8208 loss is tensor([-0.2213], grad_fn=<AddBackward0>)\n",
      "epoch: 8209 loss is tensor([-0.2657], grad_fn=<AddBackward0>)\n",
      "epoch: 8210 loss is tensor([-0.2953], grad_fn=<AddBackward0>)\n",
      "epoch: 8211 loss is tensor([-0.3125], grad_fn=<AddBackward0>)\n",
      "epoch: 8212 loss is tensor([-0.3558], grad_fn=<AddBackward0>)\n",
      "epoch: 8213 loss is tensor([-0.2991], grad_fn=<AddBackward0>)\n",
      "epoch: 8214 loss is tensor([-0.3959], grad_fn=<AddBackward0>)\n",
      "epoch: 8215 loss is tensor([-0.3165], grad_fn=<AddBackward0>)\n",
      "epoch: 8216 loss is tensor([-0.3233], grad_fn=<AddBackward0>)\n",
      "epoch: 8217 loss is tensor([-0.3779], grad_fn=<AddBackward0>)\n",
      "epoch: 8218 loss is tensor([-0.3712], grad_fn=<AddBackward0>)\n",
      "epoch: 8219 loss is tensor([-0.3928], grad_fn=<AddBackward0>)\n",
      "epoch: 8220 loss is tensor([-0.3481], grad_fn=<AddBackward0>)\n",
      "epoch: 8221 loss is tensor([-0.3018], grad_fn=<AddBackward0>)\n",
      "epoch: 8222 loss is tensor([-0.3701], grad_fn=<AddBackward0>)\n",
      "epoch: 8223 loss is tensor([-0.3448], grad_fn=<AddBackward0>)\n",
      "epoch: 8224 loss is tensor([-0.3395], grad_fn=<AddBackward0>)\n",
      "epoch: 8225 loss is tensor([-0.3494], grad_fn=<AddBackward0>)\n",
      "epoch: 8226 loss is tensor([-0.2824], grad_fn=<AddBackward0>)\n",
      "epoch: 8227 loss is tensor([-0.3573], grad_fn=<AddBackward0>)\n",
      "epoch: 8228 loss is tensor([-0.3098], grad_fn=<AddBackward0>)\n",
      "epoch: 8229 loss is tensor([-0.2871], grad_fn=<AddBackward0>)\n",
      "epoch: 8230 loss is tensor([-0.3476], grad_fn=<AddBackward0>)\n",
      "epoch: 8231 loss is tensor([-0.3636], grad_fn=<AddBackward0>)\n",
      "epoch: 8232 loss is tensor([-0.3556], grad_fn=<AddBackward0>)\n",
      "epoch: 8233 loss is tensor([-0.3162], grad_fn=<AddBackward0>)\n",
      "epoch: 8234 loss is tensor([-0.3316], grad_fn=<AddBackward0>)\n",
      "epoch: 8235 loss is tensor([-0.3243], grad_fn=<AddBackward0>)\n",
      "epoch: 8236 loss is tensor([-0.3558], grad_fn=<AddBackward0>)\n",
      "epoch: 8237 loss is tensor([-0.3126], grad_fn=<AddBackward0>)\n",
      "epoch: 8238 loss is tensor([-0.3662], grad_fn=<AddBackward0>)\n",
      "epoch: 8239 loss is tensor([-0.3535], grad_fn=<AddBackward0>)\n",
      "epoch: 8240 loss is tensor([-0.3615], grad_fn=<AddBackward0>)\n",
      "epoch: 8241 loss is tensor([-0.3729], grad_fn=<AddBackward0>)\n",
      "epoch: 8242 loss is tensor([-0.3082], grad_fn=<AddBackward0>)\n",
      "epoch: 8243 loss is tensor([-0.3663], grad_fn=<AddBackward0>)\n",
      "epoch: 8244 loss is tensor([-0.2981], grad_fn=<AddBackward0>)\n",
      "epoch: 8245 loss is tensor([-0.3084], grad_fn=<AddBackward0>)\n",
      "epoch: 8246 loss is tensor([-0.2961], grad_fn=<AddBackward0>)\n",
      "epoch: 8247 loss is tensor([-0.2994], grad_fn=<AddBackward0>)\n",
      "epoch: 8248 loss is tensor([-0.3018], grad_fn=<AddBackward0>)\n",
      "epoch: 8249 loss is tensor([-0.3328], grad_fn=<AddBackward0>)\n",
      "epoch: 8250 loss is tensor([-0.3193], grad_fn=<AddBackward0>)\n",
      "epoch: 8251 loss is tensor([-0.3492], grad_fn=<AddBackward0>)\n",
      "epoch: 8252 loss is tensor([-0.2896], grad_fn=<AddBackward0>)\n",
      "epoch: 8253 loss is tensor([-0.3154], grad_fn=<AddBackward0>)\n",
      "epoch: 8254 loss is tensor([-0.3594], grad_fn=<AddBackward0>)\n",
      "epoch: 8255 loss is tensor([-0.3376], grad_fn=<AddBackward0>)\n",
      "epoch: 8256 loss is tensor([-0.3607], grad_fn=<AddBackward0>)\n",
      "epoch: 8257 loss is tensor([-0.3715], grad_fn=<AddBackward0>)\n",
      "epoch: 8258 loss is tensor([-0.3642], grad_fn=<AddBackward0>)\n",
      "epoch: 8259 loss is tensor([-0.3598], grad_fn=<AddBackward0>)\n",
      "epoch: 8260 loss is tensor([-0.3327], grad_fn=<AddBackward0>)\n",
      "epoch: 8261 loss is tensor([-0.3276], grad_fn=<AddBackward0>)\n",
      "epoch: 8262 loss is tensor([-0.3778], grad_fn=<AddBackward0>)\n",
      "epoch: 8263 loss is tensor([-0.3205], grad_fn=<AddBackward0>)\n",
      "epoch: 8264 loss is tensor([-0.3116], grad_fn=<AddBackward0>)\n",
      "epoch: 8265 loss is tensor([-0.3412], grad_fn=<AddBackward0>)\n",
      "epoch: 8266 loss is tensor([-0.3497], grad_fn=<AddBackward0>)\n",
      "epoch: 8267 loss is tensor([-0.3649], grad_fn=<AddBackward0>)\n",
      "epoch: 8268 loss is tensor([-0.3503], grad_fn=<AddBackward0>)\n",
      "epoch: 8269 loss is tensor([-0.4126], grad_fn=<AddBackward0>)\n",
      "epoch: 8270 loss is tensor([-0.3497], grad_fn=<AddBackward0>)\n",
      "epoch: 8271 loss is tensor([-0.3870], grad_fn=<AddBackward0>)\n",
      "epoch: 8272 loss is tensor([-0.3390], grad_fn=<AddBackward0>)\n",
      "epoch: 8273 loss is tensor([-0.4186], grad_fn=<AddBackward0>)\n",
      "epoch: 8274 loss is tensor([-0.3843], grad_fn=<AddBackward0>)\n",
      "epoch: 8275 loss is tensor([-0.3774], grad_fn=<AddBackward0>)\n",
      "epoch: 8276 loss is tensor([-0.3389], grad_fn=<AddBackward0>)\n",
      "epoch: 8277 loss is tensor([-0.4077], grad_fn=<AddBackward0>)\n",
      "epoch: 8278 loss is tensor([-0.4320], grad_fn=<AddBackward0>)\n",
      "epoch: 8279 loss is tensor([-0.3746], grad_fn=<AddBackward0>)\n",
      "epoch: 8280 loss is tensor([-0.3810], grad_fn=<AddBackward0>)\n",
      "epoch: 8281 loss is tensor([-0.4090], grad_fn=<AddBackward0>)\n",
      "epoch: 8282 loss is tensor([-0.3635], grad_fn=<AddBackward0>)\n",
      "epoch: 8283 loss is tensor([-0.3276], grad_fn=<AddBackward0>)\n",
      "epoch: 8284 loss is tensor([-0.3731], grad_fn=<AddBackward0>)\n",
      "epoch: 8285 loss is tensor([-0.4079], grad_fn=<AddBackward0>)\n",
      "epoch: 8286 loss is tensor([-0.3900], grad_fn=<AddBackward0>)\n",
      "epoch: 8287 loss is tensor([-0.3770], grad_fn=<AddBackward0>)\n",
      "epoch: 8288 loss is tensor([-0.4099], grad_fn=<AddBackward0>)\n",
      "epoch: 8289 loss is tensor([-0.3385], grad_fn=<AddBackward0>)\n",
      "epoch: 8290 loss is tensor([-0.3509], grad_fn=<AddBackward0>)\n",
      "epoch: 8291 loss is tensor([-0.3458], grad_fn=<AddBackward0>)\n",
      "epoch: 8292 loss is tensor([-0.3585], grad_fn=<AddBackward0>)\n",
      "epoch: 8293 loss is tensor([-0.3710], grad_fn=<AddBackward0>)\n",
      "epoch: 8294 loss is tensor([-0.3884], grad_fn=<AddBackward0>)\n",
      "epoch: 8295 loss is tensor([-0.3281], grad_fn=<AddBackward0>)\n",
      "epoch: 8296 loss is tensor([-0.3022], grad_fn=<AddBackward0>)\n",
      "epoch: 8297 loss is tensor([-0.3634], grad_fn=<AddBackward0>)\n",
      "epoch: 8298 loss is tensor([-0.3915], grad_fn=<AddBackward0>)\n",
      "epoch: 8299 loss is tensor([-0.4086], grad_fn=<AddBackward0>)\n",
      "epoch: 8300 loss is tensor([-0.3622], grad_fn=<AddBackward0>)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8301 loss is tensor([-0.3754], grad_fn=<AddBackward0>)\n",
      "epoch: 8302 loss is tensor([-0.3543], grad_fn=<AddBackward0>)\n",
      "epoch: 8303 loss is tensor([-0.3309], grad_fn=<AddBackward0>)\n",
      "epoch: 8304 loss is tensor([-0.3879], grad_fn=<AddBackward0>)\n",
      "epoch: 8305 loss is tensor([-0.3463], grad_fn=<AddBackward0>)\n",
      "epoch: 8306 loss is tensor([-0.3724], grad_fn=<AddBackward0>)\n",
      "epoch: 8307 loss is tensor([-0.3513], grad_fn=<AddBackward0>)\n",
      "epoch: 8308 loss is tensor([-0.3554], grad_fn=<AddBackward0>)\n",
      "epoch: 8309 loss is tensor([-0.2912], grad_fn=<AddBackward0>)\n",
      "epoch: 8310 loss is tensor([-0.2752], grad_fn=<AddBackward0>)\n",
      "epoch: 8311 loss is tensor([-0.3427], grad_fn=<AddBackward0>)\n",
      "epoch: 8312 loss is tensor([-0.3020], grad_fn=<AddBackward0>)\n",
      "epoch: 8313 loss is tensor([-0.3006], grad_fn=<AddBackward0>)\n",
      "epoch: 8314 loss is tensor([-0.3248], grad_fn=<AddBackward0>)\n",
      "epoch: 8315 loss is tensor([-0.3372], grad_fn=<AddBackward0>)\n",
      "epoch: 8316 loss is tensor([-0.2829], grad_fn=<AddBackward0>)\n",
      "epoch: 8317 loss is tensor([-0.3710], grad_fn=<AddBackward0>)\n",
      "epoch: 8318 loss is tensor([-0.3980], grad_fn=<AddBackward0>)\n",
      "epoch: 8319 loss is tensor([-0.4064], grad_fn=<AddBackward0>)\n",
      "epoch: 8320 loss is tensor([-0.3757], grad_fn=<AddBackward0>)\n",
      "epoch: 8321 loss is tensor([-0.3682], grad_fn=<AddBackward0>)\n",
      "epoch: 8322 loss is tensor([-0.4031], grad_fn=<AddBackward0>)\n",
      "epoch: 8323 loss is tensor([-0.3577], grad_fn=<AddBackward0>)\n",
      "epoch: 8324 loss is tensor([-0.3961], grad_fn=<AddBackward0>)\n",
      "epoch: 8325 loss is tensor([-0.3923], grad_fn=<AddBackward0>)\n",
      "epoch: 8326 loss is tensor([-0.3522], grad_fn=<AddBackward0>)\n",
      "epoch: 8327 loss is tensor([-0.3317], grad_fn=<AddBackward0>)\n",
      "epoch: 8328 loss is tensor([-0.3416], grad_fn=<AddBackward0>)\n",
      "epoch: 8329 loss is tensor([-0.3605], grad_fn=<AddBackward0>)\n",
      "epoch: 8330 loss is tensor([-0.3048], grad_fn=<AddBackward0>)\n",
      "epoch: 8331 loss is tensor([-0.3692], grad_fn=<AddBackward0>)\n",
      "epoch: 8332 loss is tensor([-0.3521], grad_fn=<AddBackward0>)\n",
      "epoch: 8333 loss is tensor([-0.3758], grad_fn=<AddBackward0>)\n",
      "epoch: 8334 loss is tensor([-0.3944], grad_fn=<AddBackward0>)\n",
      "epoch: 8335 loss is tensor([-0.4325], grad_fn=<AddBackward0>)\n",
      "epoch: 8336 loss is tensor([-0.4123], grad_fn=<AddBackward0>)\n",
      "epoch: 8337 loss is tensor([-0.4120], grad_fn=<AddBackward0>)\n",
      "epoch: 8338 loss is tensor([-0.3425], grad_fn=<AddBackward0>)\n",
      "epoch: 8339 loss is tensor([-0.4048], grad_fn=<AddBackward0>)\n",
      "epoch: 8340 loss is tensor([-0.4310], grad_fn=<AddBackward0>)\n",
      "epoch: 8341 loss is tensor([-0.4367], grad_fn=<AddBackward0>)\n",
      "epoch: 8342 loss is tensor([-0.4144], grad_fn=<AddBackward0>)\n",
      "epoch: 8343 loss is tensor([-0.3708], grad_fn=<AddBackward0>)\n",
      "epoch: 8344 loss is tensor([-0.3112], grad_fn=<AddBackward0>)\n",
      "epoch: 8345 loss is tensor([-0.3195], grad_fn=<AddBackward0>)\n",
      "epoch: 8346 loss is tensor([-0.3466], grad_fn=<AddBackward0>)\n",
      "epoch: 8347 loss is tensor([-0.3070], grad_fn=<AddBackward0>)\n",
      "epoch: 8348 loss is tensor([-0.3886], grad_fn=<AddBackward0>)\n",
      "epoch: 8349 loss is tensor([-0.3850], grad_fn=<AddBackward0>)\n",
      "epoch: 8350 loss is tensor([-0.3655], grad_fn=<AddBackward0>)\n",
      "epoch: 8351 loss is tensor([-0.3121], grad_fn=<AddBackward0>)\n",
      "epoch: 8352 loss is tensor([-0.3287], grad_fn=<AddBackward0>)\n",
      "epoch: 8353 loss is tensor([-0.3827], grad_fn=<AddBackward0>)\n",
      "epoch: 8354 loss is tensor([-0.3221], grad_fn=<AddBackward0>)\n",
      "epoch: 8355 loss is tensor([-0.2832], grad_fn=<AddBackward0>)\n",
      "epoch: 8356 loss is tensor([-0.4061], grad_fn=<AddBackward0>)\n",
      "epoch: 8357 loss is tensor([-0.3073], grad_fn=<AddBackward0>)\n",
      "epoch: 8358 loss is tensor([-0.3290], grad_fn=<AddBackward0>)\n",
      "epoch: 8359 loss is tensor([-0.3504], grad_fn=<AddBackward0>)\n",
      "epoch: 8360 loss is tensor([-0.3559], grad_fn=<AddBackward0>)\n",
      "epoch: 8361 loss is tensor([-0.3584], grad_fn=<AddBackward0>)\n",
      "epoch: 8362 loss is tensor([-0.3881], grad_fn=<AddBackward0>)\n",
      "epoch: 8363 loss is tensor([-0.3330], grad_fn=<AddBackward0>)\n",
      "epoch: 8364 loss is tensor([-0.3184], grad_fn=<AddBackward0>)\n",
      "epoch: 8365 loss is tensor([-0.3387], grad_fn=<AddBackward0>)\n",
      "epoch: 8366 loss is tensor([-0.3231], grad_fn=<AddBackward0>)\n",
      "epoch: 8367 loss is tensor([-0.2783], grad_fn=<AddBackward0>)\n",
      "epoch: 8368 loss is tensor([-0.2442], grad_fn=<AddBackward0>)\n",
      "epoch: 8369 loss is tensor([-0.2427], grad_fn=<AddBackward0>)\n",
      "epoch: 8370 loss is tensor([-0.3779], grad_fn=<AddBackward0>)\n",
      "epoch: 8371 loss is tensor([-0.3888], grad_fn=<AddBackward0>)\n",
      "epoch: 8372 loss is tensor([-0.2906], grad_fn=<AddBackward0>)\n",
      "epoch: 8373 loss is tensor([-0.2731], grad_fn=<AddBackward0>)\n",
      "epoch: 8374 loss is tensor([-0.3824], grad_fn=<AddBackward0>)\n",
      "epoch: 8375 loss is tensor([-0.3160], grad_fn=<AddBackward0>)\n",
      "epoch: 8376 loss is tensor([-0.3390], grad_fn=<AddBackward0>)\n",
      "epoch: 8377 loss is tensor([-0.3207], grad_fn=<AddBackward0>)\n",
      "epoch: 8378 loss is tensor([-0.3215], grad_fn=<AddBackward0>)\n",
      "epoch: 8379 loss is tensor([-0.3977], grad_fn=<AddBackward0>)\n",
      "epoch: 8380 loss is tensor([-0.3357], grad_fn=<AddBackward0>)\n",
      "epoch: 8381 loss is tensor([-0.2947], grad_fn=<AddBackward0>)\n",
      "epoch: 8382 loss is tensor([-0.2996], grad_fn=<AddBackward0>)\n",
      "epoch: 8383 loss is tensor([-0.3017], grad_fn=<AddBackward0>)\n",
      "epoch: 8384 loss is tensor([-0.3752], grad_fn=<AddBackward0>)\n",
      "epoch: 8385 loss is tensor([-0.3578], grad_fn=<AddBackward0>)\n",
      "epoch: 8386 loss is tensor([-0.3642], grad_fn=<AddBackward0>)\n",
      "epoch: 8387 loss is tensor([-0.3639], grad_fn=<AddBackward0>)\n",
      "epoch: 8388 loss is tensor([-0.3820], grad_fn=<AddBackward0>)\n",
      "epoch: 8389 loss is tensor([-0.3620], grad_fn=<AddBackward0>)\n",
      "epoch: 8390 loss is tensor([-0.3831], grad_fn=<AddBackward0>)\n",
      "epoch: 8391 loss is tensor([-0.3640], grad_fn=<AddBackward0>)\n",
      "epoch: 8392 loss is tensor([-0.3309], grad_fn=<AddBackward0>)\n",
      "epoch: 8393 loss is tensor([-0.3371], grad_fn=<AddBackward0>)\n",
      "epoch: 8394 loss is tensor([-0.3506], grad_fn=<AddBackward0>)\n",
      "epoch: 8395 loss is tensor([-0.2896], grad_fn=<AddBackward0>)\n",
      "epoch: 8396 loss is tensor([-0.3464], grad_fn=<AddBackward0>)\n",
      "epoch: 8397 loss is tensor([-0.3958], grad_fn=<AddBackward0>)\n",
      "epoch: 8398 loss is tensor([-0.3932], grad_fn=<AddBackward0>)\n",
      "epoch: 8399 loss is tensor([-0.3134], grad_fn=<AddBackward0>)\n",
      "epoch: 8400 loss is tensor([-0.3856], grad_fn=<AddBackward0>)\n",
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8401 loss is tensor([-0.3605], grad_fn=<AddBackward0>)\n",
      "epoch: 8402 loss is tensor([-0.3191], grad_fn=<AddBackward0>)\n",
      "epoch: 8403 loss is tensor([-0.3361], grad_fn=<AddBackward0>)\n",
      "epoch: 8404 loss is tensor([-0.3983], grad_fn=<AddBackward0>)\n",
      "epoch: 8405 loss is tensor([-0.3371], grad_fn=<AddBackward0>)\n",
      "epoch: 8406 loss is tensor([-0.3330], grad_fn=<AddBackward0>)\n",
      "epoch: 8407 loss is tensor([-0.3444], grad_fn=<AddBackward0>)\n",
      "epoch: 8408 loss is tensor([-0.4025], grad_fn=<AddBackward0>)\n",
      "epoch: 8409 loss is tensor([-0.3184], grad_fn=<AddBackward0>)\n",
      "epoch: 8410 loss is tensor([-0.3825], grad_fn=<AddBackward0>)\n",
      "epoch: 8411 loss is tensor([-0.3895], grad_fn=<AddBackward0>)\n",
      "epoch: 8412 loss is tensor([-0.3417], grad_fn=<AddBackward0>)\n",
      "epoch: 8413 loss is tensor([-0.3668], grad_fn=<AddBackward0>)\n",
      "epoch: 8414 loss is tensor([-0.3881], grad_fn=<AddBackward0>)\n",
      "epoch: 8415 loss is tensor([-0.3822], grad_fn=<AddBackward0>)\n",
      "epoch: 8416 loss is tensor([-0.4010], grad_fn=<AddBackward0>)\n",
      "epoch: 8417 loss is tensor([-0.3676], grad_fn=<AddBackward0>)\n",
      "epoch: 8418 loss is tensor([-0.3476], grad_fn=<AddBackward0>)\n",
      "epoch: 8419 loss is tensor([-0.3329], grad_fn=<AddBackward0>)\n",
      "epoch: 8420 loss is tensor([-0.3863], grad_fn=<AddBackward0>)\n",
      "epoch: 8421 loss is tensor([-0.4120], grad_fn=<AddBackward0>)\n",
      "epoch: 8422 loss is tensor([-0.3866], grad_fn=<AddBackward0>)\n",
      "epoch: 8423 loss is tensor([-0.3213], grad_fn=<AddBackward0>)\n",
      "epoch: 8424 loss is tensor([-0.2935], grad_fn=<AddBackward0>)\n",
      "epoch: 8425 loss is tensor([-0.4096], grad_fn=<AddBackward0>)\n",
      "epoch: 8426 loss is tensor([-0.3714], grad_fn=<AddBackward0>)\n",
      "epoch: 8427 loss is tensor([-0.3422], grad_fn=<AddBackward0>)\n",
      "epoch: 8428 loss is tensor([-0.3553], grad_fn=<AddBackward0>)\n",
      "epoch: 8429 loss is tensor([-0.3574], grad_fn=<AddBackward0>)\n",
      "epoch: 8430 loss is tensor([-0.3761], grad_fn=<AddBackward0>)\n",
      "epoch: 8431 loss is tensor([-0.3363], grad_fn=<AddBackward0>)\n",
      "epoch: 8432 loss is tensor([-0.2808], grad_fn=<AddBackward0>)\n",
      "epoch: 8433 loss is tensor([-0.3859], grad_fn=<AddBackward0>)\n",
      "epoch: 8434 loss is tensor([-0.3651], grad_fn=<AddBackward0>)\n",
      "epoch: 8435 loss is tensor([-0.3601], grad_fn=<AddBackward0>)\n",
      "epoch: 8436 loss is tensor([-0.3600], grad_fn=<AddBackward0>)\n",
      "epoch: 8437 loss is tensor([-0.3228], grad_fn=<AddBackward0>)\n",
      "epoch: 8438 loss is tensor([-0.3825], grad_fn=<AddBackward0>)\n",
      "epoch: 8439 loss is tensor([-0.3930], grad_fn=<AddBackward0>)\n",
      "epoch: 8440 loss is tensor([-0.3695], grad_fn=<AddBackward0>)\n",
      "epoch: 8441 loss is tensor([-0.3850], grad_fn=<AddBackward0>)\n",
      "epoch: 8442 loss is tensor([-0.3083], grad_fn=<AddBackward0>)\n",
      "epoch: 8443 loss is tensor([-0.3093], grad_fn=<AddBackward0>)\n",
      "epoch: 8444 loss is tensor([-0.3735], grad_fn=<AddBackward0>)\n",
      "epoch: 8445 loss is tensor([-0.3495], grad_fn=<AddBackward0>)\n",
      "epoch: 8446 loss is tensor([-0.4151], grad_fn=<AddBackward0>)\n",
      "epoch: 8447 loss is tensor([-0.2888], grad_fn=<AddBackward0>)\n",
      "epoch: 8448 loss is tensor([-0.2941], grad_fn=<AddBackward0>)\n",
      "epoch: 8449 loss is tensor([-0.3348], grad_fn=<AddBackward0>)\n",
      "epoch: 8450 loss is tensor([-0.3199], grad_fn=<AddBackward0>)\n",
      "epoch: 8451 loss is tensor([-0.3287], grad_fn=<AddBackward0>)\n",
      "epoch: 8452 loss is tensor([-0.3317], grad_fn=<AddBackward0>)\n",
      "epoch: 8453 loss is tensor([-0.3828], grad_fn=<AddBackward0>)\n",
      "epoch: 8454 loss is tensor([-0.3523], grad_fn=<AddBackward0>)\n",
      "epoch: 8455 loss is tensor([-0.3902], grad_fn=<AddBackward0>)\n",
      "epoch: 8456 loss is tensor([-0.3856], grad_fn=<AddBackward0>)\n",
      "epoch: 8457 loss is tensor([-0.2954], grad_fn=<AddBackward0>)\n",
      "epoch: 8458 loss is tensor([-0.3252], grad_fn=<AddBackward0>)\n",
      "epoch: 8459 loss is tensor([-0.2601], grad_fn=<AddBackward0>)\n",
      "epoch: 8460 loss is tensor([-0.3048], grad_fn=<AddBackward0>)\n",
      "epoch: 8461 loss is tensor([-0.4004], grad_fn=<AddBackward0>)\n",
      "epoch: 8462 loss is tensor([-0.3066], grad_fn=<AddBackward0>)\n",
      "epoch: 8463 loss is tensor([-0.3084], grad_fn=<AddBackward0>)\n",
      "epoch: 8464 loss is tensor([-0.3884], grad_fn=<AddBackward0>)\n",
      "epoch: 8465 loss is tensor([-0.3356], grad_fn=<AddBackward0>)\n",
      "epoch: 8466 loss is tensor([-0.3207], grad_fn=<AddBackward0>)\n",
      "epoch: 8467 loss is tensor([-0.3113], grad_fn=<AddBackward0>)\n",
      "epoch: 8468 loss is tensor([-0.3477], grad_fn=<AddBackward0>)\n",
      "epoch: 8469 loss is tensor([-0.3464], grad_fn=<AddBackward0>)\n",
      "epoch: 8470 loss is tensor([-0.3343], grad_fn=<AddBackward0>)\n",
      "epoch: 8471 loss is tensor([-0.3873], grad_fn=<AddBackward0>)\n",
      "epoch: 8472 loss is tensor([-0.3317], grad_fn=<AddBackward0>)\n",
      "epoch: 8473 loss is tensor([-0.3936], grad_fn=<AddBackward0>)\n",
      "epoch: 8474 loss is tensor([-0.3537], grad_fn=<AddBackward0>)\n",
      "epoch: 8475 loss is tensor([-0.3330], grad_fn=<AddBackward0>)\n",
      "epoch: 8476 loss is tensor([-0.3050], grad_fn=<AddBackward0>)\n",
      "epoch: 8477 loss is tensor([-0.3102], grad_fn=<AddBackward0>)\n",
      "epoch: 8478 loss is tensor([-0.3275], grad_fn=<AddBackward0>)\n",
      "epoch: 8479 loss is tensor([-0.3223], grad_fn=<AddBackward0>)\n",
      "epoch: 8480 loss is tensor([-0.3534], grad_fn=<AddBackward0>)\n",
      "epoch: 8481 loss is tensor([-0.3924], grad_fn=<AddBackward0>)\n",
      "epoch: 8482 loss is tensor([-0.3270], grad_fn=<AddBackward0>)\n",
      "epoch: 8483 loss is tensor([-0.3584], grad_fn=<AddBackward0>)\n",
      "epoch: 8484 loss is tensor([-0.2287], grad_fn=<AddBackward0>)\n",
      "epoch: 8485 loss is tensor([-0.3510], grad_fn=<AddBackward0>)\n",
      "epoch: 8486 loss is tensor([-0.3487], grad_fn=<AddBackward0>)\n",
      "epoch: 8487 loss is tensor([-0.2559], grad_fn=<AddBackward0>)\n",
      "epoch: 8488 loss is tensor([-0.3013], grad_fn=<AddBackward0>)\n",
      "epoch: 8489 loss is tensor([-0.3496], grad_fn=<AddBackward0>)\n",
      "epoch: 8490 loss is tensor([-0.2786], grad_fn=<AddBackward0>)\n",
      "epoch: 8491 loss is tensor([-0.2775], grad_fn=<AddBackward0>)\n",
      "epoch: 8492 loss is tensor([-0.2152], grad_fn=<AddBackward0>)\n",
      "epoch: 8493 loss is tensor([-0.3020], grad_fn=<AddBackward0>)\n",
      "epoch: 8494 loss is tensor([-0.2660], grad_fn=<AddBackward0>)\n",
      "epoch: 8495 loss is tensor([-0.2381], grad_fn=<AddBackward0>)\n",
      "epoch: 8496 loss is tensor([-0.2137], grad_fn=<AddBackward0>)\n",
      "epoch: 8497 loss is tensor([-0.2324], grad_fn=<AddBackward0>)\n",
      "epoch: 8498 loss is tensor([-0.2888], grad_fn=<AddBackward0>)\n",
      "epoch: 8499 loss is tensor([-0.2974], grad_fn=<AddBackward0>)\n",
      "epoch: 8500 loss is tensor([-0.2732], grad_fn=<AddBackward0>)\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8501 loss is tensor([-0.2836], grad_fn=<AddBackward0>)\n",
      "epoch: 8502 loss is tensor([-0.3225], grad_fn=<AddBackward0>)\n",
      "epoch: 8503 loss is tensor([-0.3257], grad_fn=<AddBackward0>)\n",
      "epoch: 8504 loss is tensor([-0.2869], grad_fn=<AddBackward0>)\n",
      "epoch: 8505 loss is tensor([-0.3411], grad_fn=<AddBackward0>)\n",
      "epoch: 8506 loss is tensor([-0.3203], grad_fn=<AddBackward0>)\n",
      "epoch: 8507 loss is tensor([-0.3302], grad_fn=<AddBackward0>)\n",
      "epoch: 8508 loss is tensor([-0.3190], grad_fn=<AddBackward0>)\n",
      "epoch: 8509 loss is tensor([-0.3617], grad_fn=<AddBackward0>)\n",
      "epoch: 8510 loss is tensor([-0.3343], grad_fn=<AddBackward0>)\n",
      "epoch: 8511 loss is tensor([-0.3325], grad_fn=<AddBackward0>)\n",
      "epoch: 8512 loss is tensor([-0.3665], grad_fn=<AddBackward0>)\n",
      "epoch: 8513 loss is tensor([-0.3760], grad_fn=<AddBackward0>)\n",
      "epoch: 8514 loss is tensor([-0.3384], grad_fn=<AddBackward0>)\n",
      "epoch: 8515 loss is tensor([-0.3362], grad_fn=<AddBackward0>)\n",
      "epoch: 8516 loss is tensor([-0.3647], grad_fn=<AddBackward0>)\n",
      "epoch: 8517 loss is tensor([-0.3115], grad_fn=<AddBackward0>)\n",
      "epoch: 8518 loss is tensor([-0.3963], grad_fn=<AddBackward0>)\n",
      "epoch: 8519 loss is tensor([-0.3227], grad_fn=<AddBackward0>)\n",
      "epoch: 8520 loss is tensor([-0.3144], grad_fn=<AddBackward0>)\n",
      "epoch: 8521 loss is tensor([-0.3480], grad_fn=<AddBackward0>)\n",
      "epoch: 8522 loss is tensor([-0.3043], grad_fn=<AddBackward0>)\n",
      "epoch: 8523 loss is tensor([-0.2834], grad_fn=<AddBackward0>)\n",
      "epoch: 8524 loss is tensor([-0.3703], grad_fn=<AddBackward0>)\n",
      "epoch: 8525 loss is tensor([-0.3189], grad_fn=<AddBackward0>)\n",
      "epoch: 8526 loss is tensor([-0.2891], grad_fn=<AddBackward0>)\n",
      "epoch: 8527 loss is tensor([-0.3616], grad_fn=<AddBackward0>)\n",
      "epoch: 8528 loss is tensor([-0.3300], grad_fn=<AddBackward0>)\n",
      "epoch: 8529 loss is tensor([-0.3673], grad_fn=<AddBackward0>)\n",
      "epoch: 8530 loss is tensor([-0.3525], grad_fn=<AddBackward0>)\n",
      "epoch: 8531 loss is tensor([-0.2602], grad_fn=<AddBackward0>)\n",
      "epoch: 8532 loss is tensor([-0.3283], grad_fn=<AddBackward0>)\n",
      "epoch: 8533 loss is tensor([-0.3338], grad_fn=<AddBackward0>)\n",
      "epoch: 8534 loss is tensor([-0.3283], grad_fn=<AddBackward0>)\n",
      "epoch: 8535 loss is tensor([-0.3377], grad_fn=<AddBackward0>)\n",
      "epoch: 8536 loss is tensor([-0.3053], grad_fn=<AddBackward0>)\n",
      "epoch: 8537 loss is tensor([-0.3683], grad_fn=<AddBackward0>)\n",
      "epoch: 8538 loss is tensor([-0.3833], grad_fn=<AddBackward0>)\n",
      "epoch: 8539 loss is tensor([-0.4305], grad_fn=<AddBackward0>)\n",
      "epoch: 8540 loss is tensor([-0.3790], grad_fn=<AddBackward0>)\n",
      "epoch: 8541 loss is tensor([-0.3836], grad_fn=<AddBackward0>)\n",
      "epoch: 8542 loss is tensor([-0.4046], grad_fn=<AddBackward0>)\n",
      "epoch: 8543 loss is tensor([-0.3451], grad_fn=<AddBackward0>)\n",
      "epoch: 8544 loss is tensor([-0.2653], grad_fn=<AddBackward0>)\n",
      "epoch: 8545 loss is tensor([-0.3181], grad_fn=<AddBackward0>)\n",
      "epoch: 8546 loss is tensor([-0.3251], grad_fn=<AddBackward0>)\n",
      "epoch: 8547 loss is tensor([-0.3683], grad_fn=<AddBackward0>)\n",
      "epoch: 8548 loss is tensor([-0.3646], grad_fn=<AddBackward0>)\n",
      "epoch: 8549 loss is tensor([-0.3361], grad_fn=<AddBackward0>)\n",
      "epoch: 8550 loss is tensor([-0.4091], grad_fn=<AddBackward0>)\n",
      "epoch: 8551 loss is tensor([-0.3891], grad_fn=<AddBackward0>)\n",
      "epoch: 8552 loss is tensor([-0.3655], grad_fn=<AddBackward0>)\n",
      "epoch: 8553 loss is tensor([-0.3112], grad_fn=<AddBackward0>)\n",
      "epoch: 8554 loss is tensor([-0.3795], grad_fn=<AddBackward0>)\n",
      "epoch: 8555 loss is tensor([-0.3981], grad_fn=<AddBackward0>)\n",
      "epoch: 8556 loss is tensor([-0.3365], grad_fn=<AddBackward0>)\n",
      "epoch: 8557 loss is tensor([-0.3823], grad_fn=<AddBackward0>)\n",
      "epoch: 8558 loss is tensor([-0.3474], grad_fn=<AddBackward0>)\n",
      "epoch: 8559 loss is tensor([-0.3951], grad_fn=<AddBackward0>)\n",
      "epoch: 8560 loss is tensor([-0.3400], grad_fn=<AddBackward0>)\n",
      "epoch: 8561 loss is tensor([-0.3741], grad_fn=<AddBackward0>)\n",
      "epoch: 8562 loss is tensor([-0.3913], grad_fn=<AddBackward0>)\n",
      "epoch: 8563 loss is tensor([-0.3697], grad_fn=<AddBackward0>)\n",
      "epoch: 8564 loss is tensor([-0.3459], grad_fn=<AddBackward0>)\n",
      "epoch: 8565 loss is tensor([-0.3927], grad_fn=<AddBackward0>)\n",
      "epoch: 8566 loss is tensor([-0.4245], grad_fn=<AddBackward0>)\n",
      "epoch: 8567 loss is tensor([-0.4182], grad_fn=<AddBackward0>)\n",
      "epoch: 8568 loss is tensor([-0.4016], grad_fn=<AddBackward0>)\n",
      "epoch: 8569 loss is tensor([-0.4008], grad_fn=<AddBackward0>)\n",
      "epoch: 8570 loss is tensor([-0.3676], grad_fn=<AddBackward0>)\n",
      "epoch: 8571 loss is tensor([-0.3948], grad_fn=<AddBackward0>)\n",
      "epoch: 8572 loss is tensor([-0.3409], grad_fn=<AddBackward0>)\n",
      "epoch: 8573 loss is tensor([-0.4073], grad_fn=<AddBackward0>)\n",
      "epoch: 8574 loss is tensor([-0.3663], grad_fn=<AddBackward0>)\n",
      "epoch: 8575 loss is tensor([-0.3665], grad_fn=<AddBackward0>)\n",
      "epoch: 8576 loss is tensor([-0.4023], grad_fn=<AddBackward0>)\n",
      "epoch: 8577 loss is tensor([-0.4098], grad_fn=<AddBackward0>)\n",
      "epoch: 8578 loss is tensor([-0.4594], grad_fn=<AddBackward0>)\n",
      "epoch: 8579 loss is tensor([-0.4091], grad_fn=<AddBackward0>)\n",
      "epoch: 8580 loss is tensor([-0.4296], grad_fn=<AddBackward0>)\n",
      "epoch: 8581 loss is tensor([-0.4063], grad_fn=<AddBackward0>)\n",
      "epoch: 8582 loss is tensor([-0.3941], grad_fn=<AddBackward0>)\n",
      "epoch: 8583 loss is tensor([-0.3557], grad_fn=<AddBackward0>)\n",
      "epoch: 8584 loss is tensor([-0.3863], grad_fn=<AddBackward0>)\n",
      "epoch: 8585 loss is tensor([-0.4032], grad_fn=<AddBackward0>)\n",
      "epoch: 8586 loss is tensor([-0.4244], grad_fn=<AddBackward0>)\n",
      "epoch: 8587 loss is tensor([-0.4087], grad_fn=<AddBackward0>)\n",
      "epoch: 8588 loss is tensor([-0.3245], grad_fn=<AddBackward0>)\n",
      "epoch: 8589 loss is tensor([-0.3867], grad_fn=<AddBackward0>)\n",
      "epoch: 8590 loss is tensor([-0.4161], grad_fn=<AddBackward0>)\n",
      "epoch: 8591 loss is tensor([-0.3968], grad_fn=<AddBackward0>)\n",
      "epoch: 8592 loss is tensor([-0.4476], grad_fn=<AddBackward0>)\n",
      "epoch: 8593 loss is tensor([-0.3767], grad_fn=<AddBackward0>)\n",
      "epoch: 8594 loss is tensor([-0.4414], grad_fn=<AddBackward0>)\n",
      "epoch: 8595 loss is tensor([-0.3960], grad_fn=<AddBackward0>)\n",
      "epoch: 8596 loss is tensor([-0.3617], grad_fn=<AddBackward0>)\n",
      "epoch: 8597 loss is tensor([-0.3673], grad_fn=<AddBackward0>)\n",
      "epoch: 8598 loss is tensor([-0.3842], grad_fn=<AddBackward0>)\n",
      "epoch: 8599 loss is tensor([-0.3826], grad_fn=<AddBackward0>)\n",
      "epoch: 8600 loss is tensor([-0.3070], grad_fn=<AddBackward0>)\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8601 loss is tensor([-0.3079], grad_fn=<AddBackward0>)\n",
      "epoch: 8602 loss is tensor([-0.3927], grad_fn=<AddBackward0>)\n",
      "epoch: 8603 loss is tensor([-0.3323], grad_fn=<AddBackward0>)\n",
      "epoch: 8604 loss is tensor([-0.3130], grad_fn=<AddBackward0>)\n",
      "epoch: 8605 loss is tensor([-0.3679], grad_fn=<AddBackward0>)\n",
      "epoch: 8606 loss is tensor([-0.4116], grad_fn=<AddBackward0>)\n",
      "epoch: 8607 loss is tensor([-0.4070], grad_fn=<AddBackward0>)\n",
      "epoch: 8608 loss is tensor([-0.3823], grad_fn=<AddBackward0>)\n",
      "epoch: 8609 loss is tensor([-0.3613], grad_fn=<AddBackward0>)\n",
      "epoch: 8610 loss is tensor([-0.4036], grad_fn=<AddBackward0>)\n",
      "epoch: 8611 loss is tensor([-0.4123], grad_fn=<AddBackward0>)\n",
      "epoch: 8612 loss is tensor([-0.3770], grad_fn=<AddBackward0>)\n",
      "epoch: 8613 loss is tensor([-0.4193], grad_fn=<AddBackward0>)\n",
      "epoch: 8614 loss is tensor([-0.3723], grad_fn=<AddBackward0>)\n",
      "epoch: 8615 loss is tensor([-0.3731], grad_fn=<AddBackward0>)\n",
      "epoch: 8616 loss is tensor([-0.4008], grad_fn=<AddBackward0>)\n",
      "epoch: 8617 loss is tensor([-0.4001], grad_fn=<AddBackward0>)\n",
      "epoch: 8618 loss is tensor([-0.3856], grad_fn=<AddBackward0>)\n",
      "epoch: 8619 loss is tensor([-0.3720], grad_fn=<AddBackward0>)\n",
      "epoch: 8620 loss is tensor([-0.4545], grad_fn=<AddBackward0>)\n",
      "epoch: 8621 loss is tensor([-0.4156], grad_fn=<AddBackward0>)\n",
      "epoch: 8622 loss is tensor([-0.3992], grad_fn=<AddBackward0>)\n",
      "epoch: 8623 loss is tensor([-0.3961], grad_fn=<AddBackward0>)\n",
      "epoch: 8624 loss is tensor([-0.4057], grad_fn=<AddBackward0>)\n",
      "epoch: 8625 loss is tensor([-0.3484], grad_fn=<AddBackward0>)\n",
      "epoch: 8626 loss is tensor([-0.3622], grad_fn=<AddBackward0>)\n",
      "epoch: 8627 loss is tensor([-0.3893], grad_fn=<AddBackward0>)\n",
      "epoch: 8628 loss is tensor([-0.3938], grad_fn=<AddBackward0>)\n",
      "epoch: 8629 loss is tensor([-0.4281], grad_fn=<AddBackward0>)\n",
      "epoch: 8630 loss is tensor([-0.3430], grad_fn=<AddBackward0>)\n",
      "epoch: 8631 loss is tensor([-0.3745], grad_fn=<AddBackward0>)\n",
      "epoch: 8632 loss is tensor([-0.3867], grad_fn=<AddBackward0>)\n",
      "epoch: 8633 loss is tensor([-0.3384], grad_fn=<AddBackward0>)\n",
      "epoch: 8634 loss is tensor([-0.4272], grad_fn=<AddBackward0>)\n",
      "epoch: 8635 loss is tensor([-0.4071], grad_fn=<AddBackward0>)\n",
      "epoch: 8636 loss is tensor([-0.4371], grad_fn=<AddBackward0>)\n",
      "epoch: 8637 loss is tensor([-0.3598], grad_fn=<AddBackward0>)\n",
      "epoch: 8638 loss is tensor([-0.3692], grad_fn=<AddBackward0>)\n",
      "epoch: 8639 loss is tensor([-0.3890], grad_fn=<AddBackward0>)\n",
      "epoch: 8640 loss is tensor([-0.4061], grad_fn=<AddBackward0>)\n",
      "epoch: 8641 loss is tensor([-0.4282], grad_fn=<AddBackward0>)\n",
      "epoch: 8642 loss is tensor([-0.3828], grad_fn=<AddBackward0>)\n",
      "epoch: 8643 loss is tensor([-0.4204], grad_fn=<AddBackward0>)\n",
      "epoch: 8644 loss is tensor([-0.3931], grad_fn=<AddBackward0>)\n",
      "epoch: 8645 loss is tensor([-0.4358], grad_fn=<AddBackward0>)\n",
      "epoch: 8646 loss is tensor([-0.4151], grad_fn=<AddBackward0>)\n",
      "epoch: 8647 loss is tensor([-0.3437], grad_fn=<AddBackward0>)\n",
      "epoch: 8648 loss is tensor([-0.3287], grad_fn=<AddBackward0>)\n",
      "epoch: 8649 loss is tensor([-0.3254], grad_fn=<AddBackward0>)\n",
      "epoch: 8650 loss is tensor([-0.4142], grad_fn=<AddBackward0>)\n",
      "epoch: 8651 loss is tensor([-0.3965], grad_fn=<AddBackward0>)\n",
      "epoch: 8652 loss is tensor([-0.4046], grad_fn=<AddBackward0>)\n",
      "epoch: 8653 loss is tensor([-0.3636], grad_fn=<AddBackward0>)\n",
      "epoch: 8654 loss is tensor([-0.3672], grad_fn=<AddBackward0>)\n",
      "epoch: 8655 loss is tensor([-0.3476], grad_fn=<AddBackward0>)\n",
      "epoch: 8656 loss is tensor([-0.3554], grad_fn=<AddBackward0>)\n",
      "epoch: 8657 loss is tensor([-0.4307], grad_fn=<AddBackward0>)\n",
      "epoch: 8658 loss is tensor([-0.3352], grad_fn=<AddBackward0>)\n",
      "epoch: 8659 loss is tensor([-0.3256], grad_fn=<AddBackward0>)\n",
      "epoch: 8660 loss is tensor([-0.3610], grad_fn=<AddBackward0>)\n",
      "epoch: 8661 loss is tensor([-0.3516], grad_fn=<AddBackward0>)\n",
      "epoch: 8662 loss is tensor([-0.4047], grad_fn=<AddBackward0>)\n",
      "epoch: 8663 loss is tensor([-0.3931], grad_fn=<AddBackward0>)\n",
      "epoch: 8664 loss is tensor([-0.4245], grad_fn=<AddBackward0>)\n",
      "epoch: 8665 loss is tensor([-0.3934], grad_fn=<AddBackward0>)\n",
      "epoch: 8666 loss is tensor([-0.4126], grad_fn=<AddBackward0>)\n",
      "epoch: 8667 loss is tensor([-0.4078], grad_fn=<AddBackward0>)\n",
      "epoch: 8668 loss is tensor([-0.4080], grad_fn=<AddBackward0>)\n",
      "epoch: 8669 loss is tensor([-0.4249], grad_fn=<AddBackward0>)\n",
      "epoch: 8670 loss is tensor([-0.3427], grad_fn=<AddBackward0>)\n",
      "epoch: 8671 loss is tensor([-0.4142], grad_fn=<AddBackward0>)\n",
      "epoch: 8672 loss is tensor([-0.3486], grad_fn=<AddBackward0>)\n",
      "epoch: 8673 loss is tensor([-0.4306], grad_fn=<AddBackward0>)\n",
      "epoch: 8674 loss is tensor([-0.4005], grad_fn=<AddBackward0>)\n",
      "epoch: 8675 loss is tensor([-0.3651], grad_fn=<AddBackward0>)\n",
      "epoch: 8676 loss is tensor([-0.3920], grad_fn=<AddBackward0>)\n",
      "epoch: 8677 loss is tensor([-0.3533], grad_fn=<AddBackward0>)\n",
      "epoch: 8678 loss is tensor([-0.3935], grad_fn=<AddBackward0>)\n",
      "epoch: 8679 loss is tensor([-0.3643], grad_fn=<AddBackward0>)\n",
      "epoch: 8680 loss is tensor([-0.4231], grad_fn=<AddBackward0>)\n",
      "epoch: 8681 loss is tensor([-0.3877], grad_fn=<AddBackward0>)\n",
      "epoch: 8682 loss is tensor([-0.3614], grad_fn=<AddBackward0>)\n",
      "epoch: 8683 loss is tensor([-0.3869], grad_fn=<AddBackward0>)\n",
      "epoch: 8684 loss is tensor([-0.4263], grad_fn=<AddBackward0>)\n",
      "epoch: 8685 loss is tensor([-0.4558], grad_fn=<AddBackward0>)\n",
      "epoch: 8686 loss is tensor([-0.4153], grad_fn=<AddBackward0>)\n",
      "epoch: 8687 loss is tensor([-0.3552], grad_fn=<AddBackward0>)\n",
      "epoch: 8688 loss is tensor([-0.3537], grad_fn=<AddBackward0>)\n",
      "epoch: 8689 loss is tensor([-0.4264], grad_fn=<AddBackward0>)\n",
      "epoch: 8690 loss is tensor([-0.3417], grad_fn=<AddBackward0>)\n",
      "epoch: 8691 loss is tensor([-0.3596], grad_fn=<AddBackward0>)\n",
      "epoch: 8692 loss is tensor([-0.3866], grad_fn=<AddBackward0>)\n",
      "epoch: 8693 loss is tensor([-0.4430], grad_fn=<AddBackward0>)\n",
      "epoch: 8694 loss is tensor([-0.3670], grad_fn=<AddBackward0>)\n",
      "epoch: 8695 loss is tensor([-0.3836], grad_fn=<AddBackward0>)\n",
      "epoch: 8696 loss is tensor([-0.3906], grad_fn=<AddBackward0>)\n",
      "epoch: 8697 loss is tensor([-0.3852], grad_fn=<AddBackward0>)\n",
      "epoch: 8698 loss is tensor([-0.3831], grad_fn=<AddBackward0>)\n",
      "epoch: 8699 loss is tensor([-0.3358], grad_fn=<AddBackward0>)\n",
      "epoch: 8700 loss is tensor([-0.3822], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8701 loss is tensor([-0.3359], grad_fn=<AddBackward0>)\n",
      "epoch: 8702 loss is tensor([-0.3224], grad_fn=<AddBackward0>)\n",
      "epoch: 8703 loss is tensor([-0.3132], grad_fn=<AddBackward0>)\n",
      "epoch: 8704 loss is tensor([-0.2287], grad_fn=<AddBackward0>)\n",
      "epoch: 8705 loss is tensor([-0.2957], grad_fn=<AddBackward0>)\n",
      "epoch: 8706 loss is tensor([-0.2899], grad_fn=<AddBackward0>)\n",
      "epoch: 8707 loss is tensor([-0.2997], grad_fn=<AddBackward0>)\n",
      "epoch: 8708 loss is tensor([-0.3389], grad_fn=<AddBackward0>)\n",
      "epoch: 8709 loss is tensor([-0.3725], grad_fn=<AddBackward0>)\n",
      "epoch: 8710 loss is tensor([-0.3251], grad_fn=<AddBackward0>)\n",
      "epoch: 8711 loss is tensor([-0.3493], grad_fn=<AddBackward0>)\n",
      "epoch: 8712 loss is tensor([-0.3352], grad_fn=<AddBackward0>)\n",
      "epoch: 8713 loss is tensor([-0.3229], grad_fn=<AddBackward0>)\n",
      "epoch: 8714 loss is tensor([-0.3511], grad_fn=<AddBackward0>)\n",
      "epoch: 8715 loss is tensor([-0.3125], grad_fn=<AddBackward0>)\n",
      "epoch: 8716 loss is tensor([-0.2925], grad_fn=<AddBackward0>)\n",
      "epoch: 8717 loss is tensor([-0.3506], grad_fn=<AddBackward0>)\n",
      "epoch: 8718 loss is tensor([-0.3791], grad_fn=<AddBackward0>)\n",
      "epoch: 8719 loss is tensor([-0.3886], grad_fn=<AddBackward0>)\n",
      "epoch: 8720 loss is tensor([-0.3336], grad_fn=<AddBackward0>)\n",
      "epoch: 8721 loss is tensor([-0.3565], grad_fn=<AddBackward0>)\n",
      "epoch: 8722 loss is tensor([-0.4364], grad_fn=<AddBackward0>)\n",
      "epoch: 8723 loss is tensor([-0.3834], grad_fn=<AddBackward0>)\n",
      "epoch: 8724 loss is tensor([-0.3591], grad_fn=<AddBackward0>)\n",
      "epoch: 8725 loss is tensor([-0.4218], grad_fn=<AddBackward0>)\n",
      "epoch: 8726 loss is tensor([-0.4029], grad_fn=<AddBackward0>)\n",
      "epoch: 8727 loss is tensor([-0.3924], grad_fn=<AddBackward0>)\n",
      "epoch: 8728 loss is tensor([-0.3689], grad_fn=<AddBackward0>)\n",
      "epoch: 8729 loss is tensor([-0.3115], grad_fn=<AddBackward0>)\n",
      "epoch: 8730 loss is tensor([-0.3473], grad_fn=<AddBackward0>)\n",
      "epoch: 8731 loss is tensor([-0.3571], grad_fn=<AddBackward0>)\n",
      "epoch: 8732 loss is tensor([-0.3522], grad_fn=<AddBackward0>)\n",
      "epoch: 8733 loss is tensor([-0.3177], grad_fn=<AddBackward0>)\n",
      "epoch: 8734 loss is tensor([-0.3944], grad_fn=<AddBackward0>)\n",
      "epoch: 8735 loss is tensor([-0.4353], grad_fn=<AddBackward0>)\n",
      "epoch: 8736 loss is tensor([-0.4037], grad_fn=<AddBackward0>)\n",
      "epoch: 8737 loss is tensor([-0.3187], grad_fn=<AddBackward0>)\n",
      "epoch: 8738 loss is tensor([-0.3060], grad_fn=<AddBackward0>)\n",
      "epoch: 8739 loss is tensor([-0.3788], grad_fn=<AddBackward0>)\n",
      "epoch: 8740 loss is tensor([-0.3980], grad_fn=<AddBackward0>)\n",
      "epoch: 8741 loss is tensor([-0.3680], grad_fn=<AddBackward0>)\n",
      "epoch: 8742 loss is tensor([-0.3541], grad_fn=<AddBackward0>)\n",
      "epoch: 8743 loss is tensor([-0.3685], grad_fn=<AddBackward0>)\n",
      "epoch: 8744 loss is tensor([-0.3314], grad_fn=<AddBackward0>)\n",
      "epoch: 8745 loss is tensor([-0.3629], grad_fn=<AddBackward0>)\n",
      "epoch: 8746 loss is tensor([-0.3555], grad_fn=<AddBackward0>)\n",
      "epoch: 8747 loss is tensor([-0.3803], grad_fn=<AddBackward0>)\n",
      "epoch: 8748 loss is tensor([-0.3251], grad_fn=<AddBackward0>)\n",
      "epoch: 8749 loss is tensor([-0.4265], grad_fn=<AddBackward0>)\n",
      "epoch: 8750 loss is tensor([-0.3761], grad_fn=<AddBackward0>)\n",
      "epoch: 8751 loss is tensor([-0.3944], grad_fn=<AddBackward0>)\n",
      "epoch: 8752 loss is tensor([-0.3918], grad_fn=<AddBackward0>)\n",
      "epoch: 8753 loss is tensor([-0.3748], grad_fn=<AddBackward0>)\n",
      "epoch: 8754 loss is tensor([-0.2664], grad_fn=<AddBackward0>)\n",
      "epoch: 8755 loss is tensor([-0.3650], grad_fn=<AddBackward0>)\n",
      "epoch: 8756 loss is tensor([-0.2855], grad_fn=<AddBackward0>)\n",
      "epoch: 8757 loss is tensor([-0.3364], grad_fn=<AddBackward0>)\n",
      "epoch: 8758 loss is tensor([-0.3497], grad_fn=<AddBackward0>)\n",
      "epoch: 8759 loss is tensor([-0.3285], grad_fn=<AddBackward0>)\n",
      "epoch: 8760 loss is tensor([-0.3712], grad_fn=<AddBackward0>)\n",
      "epoch: 8761 loss is tensor([-0.3534], grad_fn=<AddBackward0>)\n",
      "epoch: 8762 loss is tensor([-0.3663], grad_fn=<AddBackward0>)\n",
      "epoch: 8763 loss is tensor([-0.3918], grad_fn=<AddBackward0>)\n",
      "epoch: 8764 loss is tensor([-0.3749], grad_fn=<AddBackward0>)\n",
      "epoch: 8765 loss is tensor([-0.3792], grad_fn=<AddBackward0>)\n",
      "epoch: 8766 loss is tensor([-0.3971], grad_fn=<AddBackward0>)\n",
      "epoch: 8767 loss is tensor([-0.3903], grad_fn=<AddBackward0>)\n",
      "epoch: 8768 loss is tensor([-0.3613], grad_fn=<AddBackward0>)\n",
      "epoch: 8769 loss is tensor([-0.3295], grad_fn=<AddBackward0>)\n",
      "epoch: 8770 loss is tensor([-0.3985], grad_fn=<AddBackward0>)\n",
      "epoch: 8771 loss is tensor([-0.3991], grad_fn=<AddBackward0>)\n",
      "epoch: 8772 loss is tensor([-0.4028], grad_fn=<AddBackward0>)\n",
      "epoch: 8773 loss is tensor([-0.3266], grad_fn=<AddBackward0>)\n",
      "epoch: 8774 loss is tensor([-0.4012], grad_fn=<AddBackward0>)\n",
      "epoch: 8775 loss is tensor([-0.3794], grad_fn=<AddBackward0>)\n",
      "epoch: 8776 loss is tensor([-0.4061], grad_fn=<AddBackward0>)\n",
      "epoch: 8777 loss is tensor([-0.4188], grad_fn=<AddBackward0>)\n",
      "epoch: 8778 loss is tensor([-0.4072], grad_fn=<AddBackward0>)\n",
      "epoch: 8779 loss is tensor([-0.4014], grad_fn=<AddBackward0>)\n",
      "epoch: 8780 loss is tensor([-0.3818], grad_fn=<AddBackward0>)\n",
      "epoch: 8781 loss is tensor([-0.3610], grad_fn=<AddBackward0>)\n",
      "epoch: 8782 loss is tensor([-0.3486], grad_fn=<AddBackward0>)\n",
      "epoch: 8783 loss is tensor([-0.3907], grad_fn=<AddBackward0>)\n",
      "epoch: 8784 loss is tensor([-0.4260], grad_fn=<AddBackward0>)\n",
      "epoch: 8785 loss is tensor([-0.3415], grad_fn=<AddBackward0>)\n",
      "epoch: 8786 loss is tensor([-0.3849], grad_fn=<AddBackward0>)\n",
      "epoch: 8787 loss is tensor([-0.3091], grad_fn=<AddBackward0>)\n",
      "epoch: 8788 loss is tensor([-0.3782], grad_fn=<AddBackward0>)\n",
      "epoch: 8789 loss is tensor([-0.3990], grad_fn=<AddBackward0>)\n",
      "epoch: 8790 loss is tensor([-0.2376], grad_fn=<AddBackward0>)\n",
      "epoch: 8791 loss is tensor([-0.2803], grad_fn=<AddBackward0>)\n",
      "epoch: 8792 loss is tensor([-0.3050], grad_fn=<AddBackward0>)\n",
      "epoch: 8793 loss is tensor([-0.2910], grad_fn=<AddBackward0>)\n",
      "epoch: 8794 loss is tensor([-0.3326], grad_fn=<AddBackward0>)\n",
      "epoch: 8795 loss is tensor([-0.3232], grad_fn=<AddBackward0>)\n",
      "epoch: 8796 loss is tensor([-0.3684], grad_fn=<AddBackward0>)\n",
      "epoch: 8797 loss is tensor([-0.3636], grad_fn=<AddBackward0>)\n",
      "epoch: 8798 loss is tensor([-0.2803], grad_fn=<AddBackward0>)\n",
      "epoch: 8799 loss is tensor([-0.3396], grad_fn=<AddBackward0>)\n",
      "epoch: 8800 loss is tensor([-0.3813], grad_fn=<AddBackward0>)\n",
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8801 loss is tensor([-0.3295], grad_fn=<AddBackward0>)\n",
      "epoch: 8802 loss is tensor([-0.3866], grad_fn=<AddBackward0>)\n",
      "epoch: 8803 loss is tensor([-0.3722], grad_fn=<AddBackward0>)\n",
      "epoch: 8804 loss is tensor([-0.3602], grad_fn=<AddBackward0>)\n",
      "epoch: 8805 loss is tensor([-0.3935], grad_fn=<AddBackward0>)\n",
      "epoch: 8806 loss is tensor([-0.3646], grad_fn=<AddBackward0>)\n",
      "epoch: 8807 loss is tensor([-0.3681], grad_fn=<AddBackward0>)\n",
      "epoch: 8808 loss is tensor([-0.3888], grad_fn=<AddBackward0>)\n",
      "epoch: 8809 loss is tensor([-0.3392], grad_fn=<AddBackward0>)\n",
      "epoch: 8810 loss is tensor([-0.3766], grad_fn=<AddBackward0>)\n",
      "epoch: 8811 loss is tensor([-0.3824], grad_fn=<AddBackward0>)\n",
      "epoch: 8812 loss is tensor([-0.3701], grad_fn=<AddBackward0>)\n",
      "epoch: 8813 loss is tensor([-0.3772], grad_fn=<AddBackward0>)\n",
      "epoch: 8814 loss is tensor([-0.3390], grad_fn=<AddBackward0>)\n",
      "epoch: 8815 loss is tensor([-0.4598], grad_fn=<AddBackward0>)\n",
      "epoch: 8816 loss is tensor([-0.3913], grad_fn=<AddBackward0>)\n",
      "epoch: 8817 loss is tensor([-0.3906], grad_fn=<AddBackward0>)\n",
      "epoch: 8818 loss is tensor([-0.4280], grad_fn=<AddBackward0>)\n",
      "epoch: 8819 loss is tensor([-0.4198], grad_fn=<AddBackward0>)\n",
      "epoch: 8820 loss is tensor([-0.3549], grad_fn=<AddBackward0>)\n",
      "epoch: 8821 loss is tensor([-0.3911], grad_fn=<AddBackward0>)\n",
      "epoch: 8822 loss is tensor([-0.3637], grad_fn=<AddBackward0>)\n",
      "epoch: 8823 loss is tensor([-0.4175], grad_fn=<AddBackward0>)\n",
      "epoch: 8824 loss is tensor([-0.3967], grad_fn=<AddBackward0>)\n",
      "epoch: 8825 loss is tensor([-0.3728], grad_fn=<AddBackward0>)\n",
      "epoch: 8826 loss is tensor([-0.4103], grad_fn=<AddBackward0>)\n",
      "epoch: 8827 loss is tensor([-0.4287], grad_fn=<AddBackward0>)\n",
      "epoch: 8828 loss is tensor([-0.4353], grad_fn=<AddBackward0>)\n",
      "epoch: 8829 loss is tensor([-0.3923], grad_fn=<AddBackward0>)\n",
      "epoch: 8830 loss is tensor([-0.3783], grad_fn=<AddBackward0>)\n",
      "epoch: 8831 loss is tensor([-0.3435], grad_fn=<AddBackward0>)\n",
      "epoch: 8832 loss is tensor([-0.3734], grad_fn=<AddBackward0>)\n",
      "epoch: 8833 loss is tensor([-0.4058], grad_fn=<AddBackward0>)\n",
      "epoch: 8834 loss is tensor([-0.3941], grad_fn=<AddBackward0>)\n",
      "epoch: 8835 loss is tensor([-0.4018], grad_fn=<AddBackward0>)\n",
      "epoch: 8836 loss is tensor([-0.3889], grad_fn=<AddBackward0>)\n",
      "epoch: 8837 loss is tensor([-0.3526], grad_fn=<AddBackward0>)\n",
      "epoch: 8838 loss is tensor([-0.4128], grad_fn=<AddBackward0>)\n",
      "epoch: 8839 loss is tensor([-0.3734], grad_fn=<AddBackward0>)\n",
      "epoch: 8840 loss is tensor([-0.3932], grad_fn=<AddBackward0>)\n",
      "epoch: 8841 loss is tensor([-0.4027], grad_fn=<AddBackward0>)\n",
      "epoch: 8842 loss is tensor([-0.4086], grad_fn=<AddBackward0>)\n",
      "epoch: 8843 loss is tensor([-0.3716], grad_fn=<AddBackward0>)\n",
      "epoch: 8844 loss is tensor([-0.4286], grad_fn=<AddBackward0>)\n",
      "epoch: 8845 loss is tensor([-0.4026], grad_fn=<AddBackward0>)\n",
      "epoch: 8846 loss is tensor([-0.3782], grad_fn=<AddBackward0>)\n",
      "epoch: 8847 loss is tensor([-0.3961], grad_fn=<AddBackward0>)\n",
      "epoch: 8848 loss is tensor([-0.3904], grad_fn=<AddBackward0>)\n",
      "epoch: 8849 loss is tensor([-0.3787], grad_fn=<AddBackward0>)\n",
      "epoch: 8850 loss is tensor([-0.3883], grad_fn=<AddBackward0>)\n",
      "epoch: 8851 loss is tensor([-0.4170], grad_fn=<AddBackward0>)\n",
      "epoch: 8852 loss is tensor([-0.3865], grad_fn=<AddBackward0>)\n",
      "epoch: 8853 loss is tensor([-0.3766], grad_fn=<AddBackward0>)\n",
      "epoch: 8854 loss is tensor([-0.3860], grad_fn=<AddBackward0>)\n",
      "epoch: 8855 loss is tensor([-0.4248], grad_fn=<AddBackward0>)\n",
      "epoch: 8856 loss is tensor([-0.3741], grad_fn=<AddBackward0>)\n",
      "epoch: 8857 loss is tensor([-0.4274], grad_fn=<AddBackward0>)\n",
      "epoch: 8858 loss is tensor([-0.3823], grad_fn=<AddBackward0>)\n",
      "epoch: 8859 loss is tensor([-0.3503], grad_fn=<AddBackward0>)\n",
      "epoch: 8860 loss is tensor([-0.4036], grad_fn=<AddBackward0>)\n",
      "epoch: 8861 loss is tensor([-0.3361], grad_fn=<AddBackward0>)\n",
      "epoch: 8862 loss is tensor([-0.4261], grad_fn=<AddBackward0>)\n",
      "epoch: 8863 loss is tensor([-0.3878], grad_fn=<AddBackward0>)\n",
      "epoch: 8864 loss is tensor([-0.2809], grad_fn=<AddBackward0>)\n",
      "epoch: 8865 loss is tensor([-0.3900], grad_fn=<AddBackward0>)\n",
      "epoch: 8866 loss is tensor([-0.3699], grad_fn=<AddBackward0>)\n",
      "epoch: 8867 loss is tensor([-0.3295], grad_fn=<AddBackward0>)\n",
      "epoch: 8868 loss is tensor([-0.3612], grad_fn=<AddBackward0>)\n",
      "epoch: 8869 loss is tensor([-0.3944], grad_fn=<AddBackward0>)\n",
      "epoch: 8870 loss is tensor([-0.3698], grad_fn=<AddBackward0>)\n",
      "epoch: 8871 loss is tensor([-0.4010], grad_fn=<AddBackward0>)\n",
      "epoch: 8872 loss is tensor([-0.4016], grad_fn=<AddBackward0>)\n",
      "epoch: 8873 loss is tensor([-0.3984], grad_fn=<AddBackward0>)\n",
      "epoch: 8874 loss is tensor([-0.3889], grad_fn=<AddBackward0>)\n",
      "epoch: 8875 loss is tensor([-0.4061], grad_fn=<AddBackward0>)\n",
      "epoch: 8876 loss is tensor([-0.4350], grad_fn=<AddBackward0>)\n",
      "epoch: 8877 loss is tensor([-0.3810], grad_fn=<AddBackward0>)\n",
      "epoch: 8878 loss is tensor([-0.3767], grad_fn=<AddBackward0>)\n",
      "epoch: 8879 loss is tensor([-0.4037], grad_fn=<AddBackward0>)\n",
      "epoch: 8880 loss is tensor([-0.3832], grad_fn=<AddBackward0>)\n",
      "epoch: 8881 loss is tensor([-0.3490], grad_fn=<AddBackward0>)\n",
      "epoch: 8882 loss is tensor([-0.3649], grad_fn=<AddBackward0>)\n",
      "epoch: 8883 loss is tensor([-0.4433], grad_fn=<AddBackward0>)\n",
      "epoch: 8884 loss is tensor([-0.4585], grad_fn=<AddBackward0>)\n",
      "epoch: 8885 loss is tensor([-0.4340], grad_fn=<AddBackward0>)\n",
      "epoch: 8886 loss is tensor([-0.4232], grad_fn=<AddBackward0>)\n",
      "epoch: 8887 loss is tensor([-0.3371], grad_fn=<AddBackward0>)\n",
      "epoch: 8888 loss is tensor([-0.3234], grad_fn=<AddBackward0>)\n",
      "epoch: 8889 loss is tensor([-0.3557], grad_fn=<AddBackward0>)\n",
      "epoch: 8890 loss is tensor([-0.4170], grad_fn=<AddBackward0>)\n",
      "epoch: 8891 loss is tensor([-0.3963], grad_fn=<AddBackward0>)\n",
      "epoch: 8892 loss is tensor([-0.3715], grad_fn=<AddBackward0>)\n",
      "epoch: 8893 loss is tensor([-0.2579], grad_fn=<AddBackward0>)\n",
      "epoch: 8894 loss is tensor([-0.3519], grad_fn=<AddBackward0>)\n",
      "epoch: 8895 loss is tensor([-0.3856], grad_fn=<AddBackward0>)\n",
      "epoch: 8896 loss is tensor([-0.4059], grad_fn=<AddBackward0>)\n",
      "epoch: 8897 loss is tensor([-0.4051], grad_fn=<AddBackward0>)\n",
      "epoch: 8898 loss is tensor([-0.4032], grad_fn=<AddBackward0>)\n",
      "epoch: 8899 loss is tensor([-0.4025], grad_fn=<AddBackward0>)\n",
      "epoch: 8900 loss is tensor([-0.4391], grad_fn=<AddBackward0>)\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8901 loss is tensor([-0.3800], grad_fn=<AddBackward0>)\n",
      "epoch: 8902 loss is tensor([-0.3198], grad_fn=<AddBackward0>)\n",
      "epoch: 8903 loss is tensor([-0.3991], grad_fn=<AddBackward0>)\n",
      "epoch: 8904 loss is tensor([-0.4152], grad_fn=<AddBackward0>)\n",
      "epoch: 8905 loss is tensor([-0.3914], grad_fn=<AddBackward0>)\n",
      "epoch: 8906 loss is tensor([-0.3897], grad_fn=<AddBackward0>)\n",
      "epoch: 8907 loss is tensor([-0.4303], grad_fn=<AddBackward0>)\n",
      "epoch: 8908 loss is tensor([-0.4383], grad_fn=<AddBackward0>)\n",
      "epoch: 8909 loss is tensor([-0.3708], grad_fn=<AddBackward0>)\n",
      "epoch: 8910 loss is tensor([-0.4142], grad_fn=<AddBackward0>)\n",
      "epoch: 8911 loss is tensor([-0.4020], grad_fn=<AddBackward0>)\n",
      "epoch: 8912 loss is tensor([-0.4086], grad_fn=<AddBackward0>)\n",
      "epoch: 8913 loss is tensor([-0.4041], grad_fn=<AddBackward0>)\n",
      "epoch: 8914 loss is tensor([-0.3799], grad_fn=<AddBackward0>)\n",
      "epoch: 8915 loss is tensor([-0.4193], grad_fn=<AddBackward0>)\n",
      "epoch: 8916 loss is tensor([-0.3865], grad_fn=<AddBackward0>)\n",
      "epoch: 8917 loss is tensor([-0.4573], grad_fn=<AddBackward0>)\n",
      "epoch: 8918 loss is tensor([-0.4194], grad_fn=<AddBackward0>)\n",
      "epoch: 8919 loss is tensor([-0.3793], grad_fn=<AddBackward0>)\n",
      "epoch: 8920 loss is tensor([-0.3921], grad_fn=<AddBackward0>)\n",
      "epoch: 8921 loss is tensor([-0.4238], grad_fn=<AddBackward0>)\n",
      "epoch: 8922 loss is tensor([-0.3062], grad_fn=<AddBackward0>)\n",
      "epoch: 8923 loss is tensor([-0.3582], grad_fn=<AddBackward0>)\n",
      "epoch: 8924 loss is tensor([-0.3960], grad_fn=<AddBackward0>)\n",
      "epoch: 8925 loss is tensor([-0.3860], grad_fn=<AddBackward0>)\n",
      "epoch: 8926 loss is tensor([-0.3834], grad_fn=<AddBackward0>)\n",
      "epoch: 8927 loss is tensor([-0.4531], grad_fn=<AddBackward0>)\n",
      "epoch: 8928 loss is tensor([-0.3406], grad_fn=<AddBackward0>)\n",
      "epoch: 8929 loss is tensor([-0.4011], grad_fn=<AddBackward0>)\n",
      "epoch: 8930 loss is tensor([-0.4512], grad_fn=<AddBackward0>)\n",
      "epoch: 8931 loss is tensor([-0.4364], grad_fn=<AddBackward0>)\n",
      "epoch: 8932 loss is tensor([-0.3525], grad_fn=<AddBackward0>)\n",
      "epoch: 8933 loss is tensor([-0.3768], grad_fn=<AddBackward0>)\n",
      "epoch: 8934 loss is tensor([-0.3966], grad_fn=<AddBackward0>)\n",
      "epoch: 8935 loss is tensor([-0.3603], grad_fn=<AddBackward0>)\n",
      "epoch: 8936 loss is tensor([-0.3607], grad_fn=<AddBackward0>)\n",
      "epoch: 8937 loss is tensor([-0.3666], grad_fn=<AddBackward0>)\n",
      "epoch: 8938 loss is tensor([-0.3632], grad_fn=<AddBackward0>)\n",
      "epoch: 8939 loss is tensor([-0.3790], grad_fn=<AddBackward0>)\n",
      "epoch: 8940 loss is tensor([-0.4091], grad_fn=<AddBackward0>)\n",
      "epoch: 8941 loss is tensor([-0.3756], grad_fn=<AddBackward0>)\n",
      "epoch: 8942 loss is tensor([-0.3810], grad_fn=<AddBackward0>)\n",
      "epoch: 8943 loss is tensor([-0.3522], grad_fn=<AddBackward0>)\n",
      "epoch: 8944 loss is tensor([-0.4382], grad_fn=<AddBackward0>)\n",
      "epoch: 8945 loss is tensor([-0.3785], grad_fn=<AddBackward0>)\n",
      "epoch: 8946 loss is tensor([-0.4301], grad_fn=<AddBackward0>)\n",
      "epoch: 8947 loss is tensor([-0.3996], grad_fn=<AddBackward0>)\n",
      "epoch: 8948 loss is tensor([-0.3964], grad_fn=<AddBackward0>)\n",
      "epoch: 8949 loss is tensor([-0.3538], grad_fn=<AddBackward0>)\n",
      "epoch: 8950 loss is tensor([-0.3778], grad_fn=<AddBackward0>)\n",
      "epoch: 8951 loss is tensor([-0.4497], grad_fn=<AddBackward0>)\n",
      "epoch: 8952 loss is tensor([-0.3345], grad_fn=<AddBackward0>)\n",
      "epoch: 8953 loss is tensor([-0.3906], grad_fn=<AddBackward0>)\n",
      "epoch: 8954 loss is tensor([-0.3746], grad_fn=<AddBackward0>)\n",
      "epoch: 8955 loss is tensor([-0.4665], grad_fn=<AddBackward0>)\n",
      "epoch: 8956 loss is tensor([-0.4019], grad_fn=<AddBackward0>)\n",
      "epoch: 8957 loss is tensor([-0.3930], grad_fn=<AddBackward0>)\n",
      "epoch: 8958 loss is tensor([-0.4274], grad_fn=<AddBackward0>)\n",
      "epoch: 8959 loss is tensor([-0.4025], grad_fn=<AddBackward0>)\n",
      "epoch: 8960 loss is tensor([-0.4098], grad_fn=<AddBackward0>)\n",
      "epoch: 8961 loss is tensor([-0.4253], grad_fn=<AddBackward0>)\n",
      "epoch: 8962 loss is tensor([-0.3863], grad_fn=<AddBackward0>)\n",
      "epoch: 8963 loss is tensor([-0.3995], grad_fn=<AddBackward0>)\n",
      "epoch: 8964 loss is tensor([-0.4107], grad_fn=<AddBackward0>)\n",
      "epoch: 8965 loss is tensor([-0.4387], grad_fn=<AddBackward0>)\n",
      "epoch: 8966 loss is tensor([-0.4225], grad_fn=<AddBackward0>)\n",
      "epoch: 8967 loss is tensor([-0.4171], grad_fn=<AddBackward0>)\n",
      "epoch: 8968 loss is tensor([-0.3779], grad_fn=<AddBackward0>)\n",
      "epoch: 8969 loss is tensor([-0.4163], grad_fn=<AddBackward0>)\n",
      "epoch: 8970 loss is tensor([-0.4360], grad_fn=<AddBackward0>)\n",
      "epoch: 8971 loss is tensor([-0.4063], grad_fn=<AddBackward0>)\n",
      "epoch: 8972 loss is tensor([-0.3474], grad_fn=<AddBackward0>)\n",
      "epoch: 8973 loss is tensor([-0.3799], grad_fn=<AddBackward0>)\n",
      "epoch: 8974 loss is tensor([-0.3742], grad_fn=<AddBackward0>)\n",
      "epoch: 8975 loss is tensor([-0.3829], grad_fn=<AddBackward0>)\n",
      "epoch: 8976 loss is tensor([-0.4119], grad_fn=<AddBackward0>)\n",
      "epoch: 8977 loss is tensor([-0.4261], grad_fn=<AddBackward0>)\n",
      "epoch: 8978 loss is tensor([-0.3891], grad_fn=<AddBackward0>)\n",
      "epoch: 8979 loss is tensor([-0.3845], grad_fn=<AddBackward0>)\n",
      "epoch: 8980 loss is tensor([-0.3936], grad_fn=<AddBackward0>)\n",
      "epoch: 8981 loss is tensor([-0.4127], grad_fn=<AddBackward0>)\n",
      "epoch: 8982 loss is tensor([-0.4207], grad_fn=<AddBackward0>)\n",
      "epoch: 8983 loss is tensor([-0.3288], grad_fn=<AddBackward0>)\n",
      "epoch: 8984 loss is tensor([-0.4086], grad_fn=<AddBackward0>)\n",
      "epoch: 8985 loss is tensor([-0.4164], grad_fn=<AddBackward0>)\n",
      "epoch: 8986 loss is tensor([-0.4081], grad_fn=<AddBackward0>)\n",
      "epoch: 8987 loss is tensor([-0.4427], grad_fn=<AddBackward0>)\n",
      "epoch: 8988 loss is tensor([-0.4143], grad_fn=<AddBackward0>)\n",
      "epoch: 8989 loss is tensor([-0.3892], grad_fn=<AddBackward0>)\n",
      "epoch: 8990 loss is tensor([-0.3701], grad_fn=<AddBackward0>)\n",
      "epoch: 8991 loss is tensor([-0.4009], grad_fn=<AddBackward0>)\n",
      "epoch: 8992 loss is tensor([-0.3974], grad_fn=<AddBackward0>)\n",
      "epoch: 8993 loss is tensor([-0.4423], grad_fn=<AddBackward0>)\n",
      "epoch: 8994 loss is tensor([-0.3541], grad_fn=<AddBackward0>)\n",
      "epoch: 8995 loss is tensor([-0.4531], grad_fn=<AddBackward0>)\n",
      "epoch: 8996 loss is tensor([-0.3930], grad_fn=<AddBackward0>)\n",
      "epoch: 8997 loss is tensor([-0.3244], grad_fn=<AddBackward0>)\n",
      "epoch: 8998 loss is tensor([-0.3980], grad_fn=<AddBackward0>)\n",
      "epoch: 8999 loss is tensor([-0.4268], grad_fn=<AddBackward0>)\n",
      "epoch: 9000 loss is tensor([-0.4157], grad_fn=<AddBackward0>)\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9001 loss is tensor([-0.4938], grad_fn=<AddBackward0>)\n",
      "epoch: 9002 loss is tensor([-0.4176], grad_fn=<AddBackward0>)\n",
      "epoch: 9003 loss is tensor([-0.4354], grad_fn=<AddBackward0>)\n",
      "epoch: 9004 loss is tensor([-0.4381], grad_fn=<AddBackward0>)\n",
      "epoch: 9005 loss is tensor([-0.4348], grad_fn=<AddBackward0>)\n",
      "epoch: 9006 loss is tensor([-0.4276], grad_fn=<AddBackward0>)\n",
      "epoch: 9007 loss is tensor([-0.3863], grad_fn=<AddBackward0>)\n",
      "epoch: 9008 loss is tensor([-0.4424], grad_fn=<AddBackward0>)\n",
      "epoch: 9009 loss is tensor([-0.4311], grad_fn=<AddBackward0>)\n",
      "epoch: 9010 loss is tensor([-0.4497], grad_fn=<AddBackward0>)\n",
      "epoch: 9011 loss is tensor([-0.4213], grad_fn=<AddBackward0>)\n",
      "epoch: 9012 loss is tensor([-0.3951], grad_fn=<AddBackward0>)\n",
      "epoch: 9013 loss is tensor([-0.3749], grad_fn=<AddBackward0>)\n",
      "epoch: 9014 loss is tensor([-0.3687], grad_fn=<AddBackward0>)\n",
      "epoch: 9015 loss is tensor([-0.3422], grad_fn=<AddBackward0>)\n",
      "epoch: 9016 loss is tensor([-0.3933], grad_fn=<AddBackward0>)\n",
      "epoch: 9017 loss is tensor([-0.4070], grad_fn=<AddBackward0>)\n",
      "epoch: 9018 loss is tensor([-0.3470], grad_fn=<AddBackward0>)\n",
      "epoch: 9019 loss is tensor([-0.4399], grad_fn=<AddBackward0>)\n",
      "epoch: 9020 loss is tensor([-0.3931], grad_fn=<AddBackward0>)\n",
      "epoch: 9021 loss is tensor([-0.4101], grad_fn=<AddBackward0>)\n",
      "epoch: 9022 loss is tensor([-0.3556], grad_fn=<AddBackward0>)\n",
      "epoch: 9023 loss is tensor([-0.3998], grad_fn=<AddBackward0>)\n",
      "epoch: 9024 loss is tensor([-0.4077], grad_fn=<AddBackward0>)\n",
      "epoch: 9025 loss is tensor([-0.4193], grad_fn=<AddBackward0>)\n",
      "epoch: 9026 loss is tensor([-0.4169], grad_fn=<AddBackward0>)\n",
      "epoch: 9027 loss is tensor([-0.3837], grad_fn=<AddBackward0>)\n",
      "epoch: 9028 loss is tensor([-0.3987], grad_fn=<AddBackward0>)\n",
      "epoch: 9029 loss is tensor([-0.3363], grad_fn=<AddBackward0>)\n",
      "epoch: 9030 loss is tensor([-0.3837], grad_fn=<AddBackward0>)\n",
      "epoch: 9031 loss is tensor([-0.4233], grad_fn=<AddBackward0>)\n",
      "epoch: 9032 loss is tensor([-0.4574], grad_fn=<AddBackward0>)\n",
      "epoch: 9033 loss is tensor([-0.4103], grad_fn=<AddBackward0>)\n",
      "epoch: 9034 loss is tensor([-0.3790], grad_fn=<AddBackward0>)\n",
      "epoch: 9035 loss is tensor([-0.4082], grad_fn=<AddBackward0>)\n",
      "epoch: 9036 loss is tensor([-0.3966], grad_fn=<AddBackward0>)\n",
      "epoch: 9037 loss is tensor([-0.3476], grad_fn=<AddBackward0>)\n",
      "epoch: 9038 loss is tensor([-0.3842], grad_fn=<AddBackward0>)\n",
      "epoch: 9039 loss is tensor([-0.4486], grad_fn=<AddBackward0>)\n",
      "epoch: 9040 loss is tensor([-0.3982], grad_fn=<AddBackward0>)\n",
      "epoch: 9041 loss is tensor([-0.4218], grad_fn=<AddBackward0>)\n",
      "epoch: 9042 loss is tensor([-0.4133], grad_fn=<AddBackward0>)\n",
      "epoch: 9043 loss is tensor([-0.4613], grad_fn=<AddBackward0>)\n",
      "epoch: 9044 loss is tensor([-0.3695], grad_fn=<AddBackward0>)\n",
      "epoch: 9045 loss is tensor([-0.3536], grad_fn=<AddBackward0>)\n",
      "epoch: 9046 loss is tensor([-0.3771], grad_fn=<AddBackward0>)\n",
      "epoch: 9047 loss is tensor([-0.4220], grad_fn=<AddBackward0>)\n",
      "epoch: 9048 loss is tensor([-0.4402], grad_fn=<AddBackward0>)\n",
      "epoch: 9049 loss is tensor([-0.3906], grad_fn=<AddBackward0>)\n",
      "epoch: 9050 loss is tensor([-0.3289], grad_fn=<AddBackward0>)\n",
      "epoch: 9051 loss is tensor([-0.4117], grad_fn=<AddBackward0>)\n",
      "epoch: 9052 loss is tensor([-0.3150], grad_fn=<AddBackward0>)\n",
      "epoch: 9053 loss is tensor([-0.3546], grad_fn=<AddBackward0>)\n",
      "epoch: 9054 loss is tensor([-0.3409], grad_fn=<AddBackward0>)\n",
      "epoch: 9055 loss is tensor([-0.3796], grad_fn=<AddBackward0>)\n",
      "epoch: 9056 loss is tensor([-0.4299], grad_fn=<AddBackward0>)\n",
      "epoch: 9057 loss is tensor([-0.4057], grad_fn=<AddBackward0>)\n",
      "epoch: 9058 loss is tensor([-0.4106], grad_fn=<AddBackward0>)\n",
      "epoch: 9059 loss is tensor([-0.4070], grad_fn=<AddBackward0>)\n",
      "epoch: 9060 loss is tensor([-0.4321], grad_fn=<AddBackward0>)\n",
      "epoch: 9061 loss is tensor([-0.4197], grad_fn=<AddBackward0>)\n",
      "epoch: 9062 loss is tensor([-0.4309], grad_fn=<AddBackward0>)\n",
      "epoch: 9063 loss is tensor([-0.4285], grad_fn=<AddBackward0>)\n",
      "epoch: 9064 loss is tensor([-0.4475], grad_fn=<AddBackward0>)\n",
      "epoch: 9065 loss is tensor([-0.4038], grad_fn=<AddBackward0>)\n",
      "epoch: 9066 loss is tensor([-0.4368], grad_fn=<AddBackward0>)\n",
      "epoch: 9067 loss is tensor([-0.4447], grad_fn=<AddBackward0>)\n",
      "epoch: 9068 loss is tensor([-0.4080], grad_fn=<AddBackward0>)\n",
      "epoch: 9069 loss is tensor([-0.4208], grad_fn=<AddBackward0>)\n",
      "epoch: 9070 loss is tensor([-0.4189], grad_fn=<AddBackward0>)\n",
      "epoch: 9071 loss is tensor([-0.4392], grad_fn=<AddBackward0>)\n",
      "epoch: 9072 loss is tensor([-0.4299], grad_fn=<AddBackward0>)\n",
      "epoch: 9073 loss is tensor([-0.4328], grad_fn=<AddBackward0>)\n",
      "epoch: 9074 loss is tensor([-0.3896], grad_fn=<AddBackward0>)\n",
      "epoch: 9075 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 9076 loss is tensor([-0.4450], grad_fn=<AddBackward0>)\n",
      "epoch: 9077 loss is tensor([-0.3495], grad_fn=<AddBackward0>)\n",
      "epoch: 9078 loss is tensor([-0.2965], grad_fn=<AddBackward0>)\n",
      "epoch: 9079 loss is tensor([-0.3127], grad_fn=<AddBackward0>)\n",
      "epoch: 9080 loss is tensor([-0.4095], grad_fn=<AddBackward0>)\n",
      "epoch: 9081 loss is tensor([-0.3829], grad_fn=<AddBackward0>)\n",
      "epoch: 9082 loss is tensor([-0.3231], grad_fn=<AddBackward0>)\n",
      "epoch: 9083 loss is tensor([-0.3371], grad_fn=<AddBackward0>)\n",
      "epoch: 9084 loss is tensor([-0.4104], grad_fn=<AddBackward0>)\n",
      "epoch: 9085 loss is tensor([-0.3984], grad_fn=<AddBackward0>)\n",
      "epoch: 9086 loss is tensor([-0.4047], grad_fn=<AddBackward0>)\n",
      "epoch: 9087 loss is tensor([-0.3714], grad_fn=<AddBackward0>)\n",
      "epoch: 9088 loss is tensor([-0.3739], grad_fn=<AddBackward0>)\n",
      "epoch: 9089 loss is tensor([-0.4195], grad_fn=<AddBackward0>)\n",
      "epoch: 9090 loss is tensor([-0.3688], grad_fn=<AddBackward0>)\n",
      "epoch: 9091 loss is tensor([-0.3764], grad_fn=<AddBackward0>)\n",
      "epoch: 9092 loss is tensor([-0.4125], grad_fn=<AddBackward0>)\n",
      "epoch: 9093 loss is tensor([-0.4747], grad_fn=<AddBackward0>)\n",
      "epoch: 9094 loss is tensor([-0.4124], grad_fn=<AddBackward0>)\n",
      "epoch: 9095 loss is tensor([-0.4051], grad_fn=<AddBackward0>)\n",
      "epoch: 9096 loss is tensor([-0.3936], grad_fn=<AddBackward0>)\n",
      "epoch: 9097 loss is tensor([-0.4168], grad_fn=<AddBackward0>)\n",
      "epoch: 9098 loss is tensor([-0.3937], grad_fn=<AddBackward0>)\n",
      "epoch: 9099 loss is tensor([-0.3511], grad_fn=<AddBackward0>)\n",
      "epoch: 9100 loss is tensor([-0.3916], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9101 loss is tensor([-0.3899], grad_fn=<AddBackward0>)\n",
      "epoch: 9102 loss is tensor([-0.3839], grad_fn=<AddBackward0>)\n",
      "epoch: 9103 loss is tensor([-0.3859], grad_fn=<AddBackward0>)\n",
      "epoch: 9104 loss is tensor([-0.3526], grad_fn=<AddBackward0>)\n",
      "epoch: 9105 loss is tensor([-0.4445], grad_fn=<AddBackward0>)\n",
      "epoch: 9106 loss is tensor([-0.3408], grad_fn=<AddBackward0>)\n",
      "epoch: 9107 loss is tensor([-0.4174], grad_fn=<AddBackward0>)\n",
      "epoch: 9108 loss is tensor([-0.4342], grad_fn=<AddBackward0>)\n",
      "epoch: 9109 loss is tensor([-0.4298], grad_fn=<AddBackward0>)\n",
      "epoch: 9110 loss is tensor([-0.3445], grad_fn=<AddBackward0>)\n",
      "epoch: 9111 loss is tensor([-0.3967], grad_fn=<AddBackward0>)\n",
      "epoch: 9112 loss is tensor([-0.4048], grad_fn=<AddBackward0>)\n",
      "epoch: 9113 loss is tensor([-0.4048], grad_fn=<AddBackward0>)\n",
      "epoch: 9114 loss is tensor([-0.4065], grad_fn=<AddBackward0>)\n",
      "epoch: 9115 loss is tensor([-0.4364], grad_fn=<AddBackward0>)\n",
      "epoch: 9116 loss is tensor([-0.3709], grad_fn=<AddBackward0>)\n",
      "epoch: 9117 loss is tensor([-0.3779], grad_fn=<AddBackward0>)\n",
      "epoch: 9118 loss is tensor([-0.4418], grad_fn=<AddBackward0>)\n",
      "epoch: 9119 loss is tensor([-0.4006], grad_fn=<AddBackward0>)\n",
      "epoch: 9120 loss is tensor([-0.4344], grad_fn=<AddBackward0>)\n",
      "epoch: 9121 loss is tensor([-0.3937], grad_fn=<AddBackward0>)\n",
      "epoch: 9122 loss is tensor([-0.3974], grad_fn=<AddBackward0>)\n",
      "epoch: 9123 loss is tensor([-0.3957], grad_fn=<AddBackward0>)\n",
      "epoch: 9124 loss is tensor([-0.3940], grad_fn=<AddBackward0>)\n",
      "epoch: 9125 loss is tensor([-0.3273], grad_fn=<AddBackward0>)\n",
      "epoch: 9126 loss is tensor([-0.3933], grad_fn=<AddBackward0>)\n",
      "epoch: 9127 loss is tensor([-0.4262], grad_fn=<AddBackward0>)\n",
      "epoch: 9128 loss is tensor([-0.4188], grad_fn=<AddBackward0>)\n",
      "epoch: 9129 loss is tensor([-0.3623], grad_fn=<AddBackward0>)\n",
      "epoch: 9130 loss is tensor([-0.4065], grad_fn=<AddBackward0>)\n",
      "epoch: 9131 loss is tensor([-0.3963], grad_fn=<AddBackward0>)\n",
      "epoch: 9132 loss is tensor([-0.3827], grad_fn=<AddBackward0>)\n",
      "epoch: 9133 loss is tensor([-0.2481], grad_fn=<AddBackward0>)\n",
      "epoch: 9134 loss is tensor([-0.3188], grad_fn=<AddBackward0>)\n",
      "epoch: 9135 loss is tensor([-0.3510], grad_fn=<AddBackward0>)\n",
      "epoch: 9136 loss is tensor([-0.3282], grad_fn=<AddBackward0>)\n",
      "epoch: 9137 loss is tensor([-0.3563], grad_fn=<AddBackward0>)\n",
      "epoch: 9138 loss is tensor([-0.3488], grad_fn=<AddBackward0>)\n",
      "epoch: 9139 loss is tensor([-0.3347], grad_fn=<AddBackward0>)\n",
      "epoch: 9140 loss is tensor([-0.4005], grad_fn=<AddBackward0>)\n",
      "epoch: 9141 loss is tensor([-0.4064], grad_fn=<AddBackward0>)\n",
      "epoch: 9142 loss is tensor([-0.3641], grad_fn=<AddBackward0>)\n",
      "epoch: 9143 loss is tensor([-0.3654], grad_fn=<AddBackward0>)\n",
      "epoch: 9144 loss is tensor([-0.3775], grad_fn=<AddBackward0>)\n",
      "epoch: 9145 loss is tensor([-0.4347], grad_fn=<AddBackward0>)\n",
      "epoch: 9146 loss is tensor([-0.3764], grad_fn=<AddBackward0>)\n",
      "epoch: 9147 loss is tensor([-0.3550], grad_fn=<AddBackward0>)\n",
      "epoch: 9148 loss is tensor([-0.3556], grad_fn=<AddBackward0>)\n",
      "epoch: 9149 loss is tensor([-0.3251], grad_fn=<AddBackward0>)\n",
      "epoch: 9150 loss is tensor([-0.3388], grad_fn=<AddBackward0>)\n",
      "epoch: 9151 loss is tensor([-0.2762], grad_fn=<AddBackward0>)\n",
      "epoch: 9152 loss is tensor([-0.3683], grad_fn=<AddBackward0>)\n",
      "epoch: 9153 loss is tensor([-0.3362], grad_fn=<AddBackward0>)\n",
      "epoch: 9154 loss is tensor([-0.2833], grad_fn=<AddBackward0>)\n",
      "epoch: 9155 loss is tensor([-0.2754], grad_fn=<AddBackward0>)\n",
      "epoch: 9156 loss is tensor([-0.2322], grad_fn=<AddBackward0>)\n",
      "epoch: 9157 loss is tensor([-0.3595], grad_fn=<AddBackward0>)\n",
      "epoch: 9158 loss is tensor([-0.3243], grad_fn=<AddBackward0>)\n",
      "epoch: 9159 loss is tensor([-0.2820], grad_fn=<AddBackward0>)\n",
      "epoch: 9160 loss is tensor([-0.3060], grad_fn=<AddBackward0>)\n",
      "epoch: 9161 loss is tensor([-0.3401], grad_fn=<AddBackward0>)\n",
      "epoch: 9162 loss is tensor([-0.3482], grad_fn=<AddBackward0>)\n",
      "epoch: 9163 loss is tensor([-0.3447], grad_fn=<AddBackward0>)\n",
      "epoch: 9164 loss is tensor([-0.3633], grad_fn=<AddBackward0>)\n",
      "epoch: 9165 loss is tensor([-0.3664], grad_fn=<AddBackward0>)\n",
      "epoch: 9166 loss is tensor([-0.3624], grad_fn=<AddBackward0>)\n",
      "epoch: 9167 loss is tensor([-0.3495], grad_fn=<AddBackward0>)\n",
      "epoch: 9168 loss is tensor([-0.3918], grad_fn=<AddBackward0>)\n",
      "epoch: 9169 loss is tensor([-0.3556], grad_fn=<AddBackward0>)\n",
      "epoch: 9170 loss is tensor([-0.3763], grad_fn=<AddBackward0>)\n",
      "epoch: 9171 loss is tensor([-0.4174], grad_fn=<AddBackward0>)\n",
      "epoch: 9172 loss is tensor([-0.3665], grad_fn=<AddBackward0>)\n",
      "epoch: 9173 loss is tensor([-0.3238], grad_fn=<AddBackward0>)\n",
      "epoch: 9174 loss is tensor([-0.4202], grad_fn=<AddBackward0>)\n",
      "epoch: 9175 loss is tensor([-0.3697], grad_fn=<AddBackward0>)\n",
      "epoch: 9176 loss is tensor([-0.3753], grad_fn=<AddBackward0>)\n",
      "epoch: 9177 loss is tensor([-0.4487], grad_fn=<AddBackward0>)\n",
      "epoch: 9178 loss is tensor([-0.4269], grad_fn=<AddBackward0>)\n",
      "epoch: 9179 loss is tensor([-0.4171], grad_fn=<AddBackward0>)\n",
      "epoch: 9180 loss is tensor([-0.3999], grad_fn=<AddBackward0>)\n",
      "epoch: 9181 loss is tensor([-0.3757], grad_fn=<AddBackward0>)\n",
      "epoch: 9182 loss is tensor([-0.4086], grad_fn=<AddBackward0>)\n",
      "epoch: 9183 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 9184 loss is tensor([-0.4038], grad_fn=<AddBackward0>)\n",
      "epoch: 9185 loss is tensor([-0.4223], grad_fn=<AddBackward0>)\n",
      "epoch: 9186 loss is tensor([-0.3872], grad_fn=<AddBackward0>)\n",
      "epoch: 9187 loss is tensor([-0.4169], grad_fn=<AddBackward0>)\n",
      "epoch: 9188 loss is tensor([-0.4531], grad_fn=<AddBackward0>)\n",
      "epoch: 9189 loss is tensor([-0.4198], grad_fn=<AddBackward0>)\n",
      "epoch: 9190 loss is tensor([-0.4173], grad_fn=<AddBackward0>)\n",
      "epoch: 9191 loss is tensor([-0.3801], grad_fn=<AddBackward0>)\n",
      "epoch: 9192 loss is tensor([-0.4328], grad_fn=<AddBackward0>)\n",
      "epoch: 9193 loss is tensor([-0.3682], grad_fn=<AddBackward0>)\n",
      "epoch: 9194 loss is tensor([-0.4005], grad_fn=<AddBackward0>)\n",
      "epoch: 9195 loss is tensor([-0.3939], grad_fn=<AddBackward0>)\n",
      "epoch: 9196 loss is tensor([-0.3842], grad_fn=<AddBackward0>)\n",
      "epoch: 9197 loss is tensor([-0.4323], grad_fn=<AddBackward0>)\n",
      "epoch: 9198 loss is tensor([-0.4229], grad_fn=<AddBackward0>)\n",
      "epoch: 9199 loss is tensor([-0.3992], grad_fn=<AddBackward0>)\n",
      "epoch: 9200 loss is tensor([-0.4468], grad_fn=<AddBackward0>)\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9201 loss is tensor([-0.4226], grad_fn=<AddBackward0>)\n",
      "epoch: 9202 loss is tensor([-0.4233], grad_fn=<AddBackward0>)\n",
      "epoch: 9203 loss is tensor([-0.4592], grad_fn=<AddBackward0>)\n",
      "epoch: 9204 loss is tensor([-0.4537], grad_fn=<AddBackward0>)\n",
      "epoch: 9205 loss is tensor([-0.3811], grad_fn=<AddBackward0>)\n",
      "epoch: 9206 loss is tensor([-0.3876], grad_fn=<AddBackward0>)\n",
      "epoch: 9207 loss is tensor([-0.4314], grad_fn=<AddBackward0>)\n",
      "epoch: 9208 loss is tensor([-0.4454], grad_fn=<AddBackward0>)\n",
      "epoch: 9209 loss is tensor([-0.4238], grad_fn=<AddBackward0>)\n",
      "epoch: 9210 loss is tensor([-0.4289], grad_fn=<AddBackward0>)\n",
      "epoch: 9211 loss is tensor([-0.4447], grad_fn=<AddBackward0>)\n",
      "epoch: 9212 loss is tensor([-0.4089], grad_fn=<AddBackward0>)\n",
      "epoch: 9213 loss is tensor([-0.3733], grad_fn=<AddBackward0>)\n",
      "epoch: 9214 loss is tensor([-0.4119], grad_fn=<AddBackward0>)\n",
      "epoch: 9215 loss is tensor([-0.3572], grad_fn=<AddBackward0>)\n",
      "epoch: 9216 loss is tensor([-0.3945], grad_fn=<AddBackward0>)\n",
      "epoch: 9217 loss is tensor([-0.4282], grad_fn=<AddBackward0>)\n",
      "epoch: 9218 loss is tensor([-0.4352], grad_fn=<AddBackward0>)\n",
      "epoch: 9219 loss is tensor([-0.3399], grad_fn=<AddBackward0>)\n",
      "epoch: 9220 loss is tensor([-0.4352], grad_fn=<AddBackward0>)\n",
      "epoch: 9221 loss is tensor([-0.3994], grad_fn=<AddBackward0>)\n",
      "epoch: 9222 loss is tensor([-0.4544], grad_fn=<AddBackward0>)\n",
      "epoch: 9223 loss is tensor([-0.3998], grad_fn=<AddBackward0>)\n",
      "epoch: 9224 loss is tensor([-0.3606], grad_fn=<AddBackward0>)\n",
      "epoch: 9225 loss is tensor([-0.4507], grad_fn=<AddBackward0>)\n",
      "epoch: 9226 loss is tensor([-0.3502], grad_fn=<AddBackward0>)\n",
      "epoch: 9227 loss is tensor([-0.3702], grad_fn=<AddBackward0>)\n",
      "epoch: 9228 loss is tensor([-0.3582], grad_fn=<AddBackward0>)\n",
      "epoch: 9229 loss is tensor([-0.3821], grad_fn=<AddBackward0>)\n",
      "epoch: 9230 loss is tensor([-0.4041], grad_fn=<AddBackward0>)\n",
      "epoch: 9231 loss is tensor([-0.3750], grad_fn=<AddBackward0>)\n",
      "epoch: 9232 loss is tensor([-0.3754], grad_fn=<AddBackward0>)\n",
      "epoch: 9233 loss is tensor([-0.3980], grad_fn=<AddBackward0>)\n",
      "epoch: 9234 loss is tensor([-0.3925], grad_fn=<AddBackward0>)\n",
      "epoch: 9235 loss is tensor([-0.3723], grad_fn=<AddBackward0>)\n",
      "epoch: 9236 loss is tensor([-0.3739], grad_fn=<AddBackward0>)\n",
      "epoch: 9237 loss is tensor([-0.3249], grad_fn=<AddBackward0>)\n",
      "epoch: 9238 loss is tensor([-0.3855], grad_fn=<AddBackward0>)\n",
      "epoch: 9239 loss is tensor([-0.3961], grad_fn=<AddBackward0>)\n",
      "epoch: 9240 loss is tensor([-0.3815], grad_fn=<AddBackward0>)\n",
      "epoch: 9241 loss is tensor([-0.3779], grad_fn=<AddBackward0>)\n",
      "epoch: 9242 loss is tensor([-0.3575], grad_fn=<AddBackward0>)\n",
      "epoch: 9243 loss is tensor([-0.3589], grad_fn=<AddBackward0>)\n",
      "epoch: 9244 loss is tensor([-0.4062], grad_fn=<AddBackward0>)\n",
      "epoch: 9245 loss is tensor([-0.4147], grad_fn=<AddBackward0>)\n",
      "epoch: 9246 loss is tensor([-0.3655], grad_fn=<AddBackward0>)\n",
      "epoch: 9247 loss is tensor([-0.4570], grad_fn=<AddBackward0>)\n",
      "epoch: 9248 loss is tensor([-0.4195], grad_fn=<AddBackward0>)\n",
      "epoch: 9249 loss is tensor([-0.4069], grad_fn=<AddBackward0>)\n",
      "epoch: 9250 loss is tensor([-0.4635], grad_fn=<AddBackward0>)\n",
      "epoch: 9251 loss is tensor([-0.4505], grad_fn=<AddBackward0>)\n",
      "epoch: 9252 loss is tensor([-0.4258], grad_fn=<AddBackward0>)\n",
      "epoch: 9253 loss is tensor([-0.3890], grad_fn=<AddBackward0>)\n",
      "epoch: 9254 loss is tensor([-0.4168], grad_fn=<AddBackward0>)\n",
      "epoch: 9255 loss is tensor([-0.4170], grad_fn=<AddBackward0>)\n",
      "epoch: 9256 loss is tensor([-0.4502], grad_fn=<AddBackward0>)\n",
      "epoch: 9257 loss is tensor([-0.4351], grad_fn=<AddBackward0>)\n",
      "epoch: 9258 loss is tensor([-0.4290], grad_fn=<AddBackward0>)\n",
      "epoch: 9259 loss is tensor([-0.4296], grad_fn=<AddBackward0>)\n",
      "epoch: 9260 loss is tensor([-0.3911], grad_fn=<AddBackward0>)\n",
      "epoch: 9261 loss is tensor([-0.4580], grad_fn=<AddBackward0>)\n",
      "epoch: 9262 loss is tensor([-0.4426], grad_fn=<AddBackward0>)\n",
      "epoch: 9263 loss is tensor([-0.3995], grad_fn=<AddBackward0>)\n",
      "epoch: 9264 loss is tensor([-0.4325], grad_fn=<AddBackward0>)\n",
      "epoch: 9265 loss is tensor([-0.4272], grad_fn=<AddBackward0>)\n",
      "epoch: 9266 loss is tensor([-0.4287], grad_fn=<AddBackward0>)\n",
      "epoch: 9267 loss is tensor([-0.3951], grad_fn=<AddBackward0>)\n",
      "epoch: 9268 loss is tensor([-0.3921], grad_fn=<AddBackward0>)\n",
      "epoch: 9269 loss is tensor([-0.4041], grad_fn=<AddBackward0>)\n",
      "epoch: 9270 loss is tensor([-0.3834], grad_fn=<AddBackward0>)\n",
      "epoch: 9271 loss is tensor([-0.4276], grad_fn=<AddBackward0>)\n",
      "epoch: 9272 loss is tensor([-0.3682], grad_fn=<AddBackward0>)\n",
      "epoch: 9273 loss is tensor([-0.4298], grad_fn=<AddBackward0>)\n",
      "epoch: 9274 loss is tensor([-0.3862], grad_fn=<AddBackward0>)\n",
      "epoch: 9275 loss is tensor([-0.4067], grad_fn=<AddBackward0>)\n",
      "epoch: 9276 loss is tensor([-0.3813], grad_fn=<AddBackward0>)\n",
      "epoch: 9277 loss is tensor([-0.4202], grad_fn=<AddBackward0>)\n",
      "epoch: 9278 loss is tensor([-0.4545], grad_fn=<AddBackward0>)\n",
      "epoch: 9279 loss is tensor([-0.4786], grad_fn=<AddBackward0>)\n",
      "epoch: 9280 loss is tensor([-0.3859], grad_fn=<AddBackward0>)\n",
      "epoch: 9281 loss is tensor([-0.4296], grad_fn=<AddBackward0>)\n",
      "epoch: 9282 loss is tensor([-0.3816], grad_fn=<AddBackward0>)\n",
      "epoch: 9283 loss is tensor([-0.3809], grad_fn=<AddBackward0>)\n",
      "epoch: 9284 loss is tensor([-0.3738], grad_fn=<AddBackward0>)\n",
      "epoch: 9285 loss is tensor([-0.3948], grad_fn=<AddBackward0>)\n",
      "epoch: 9286 loss is tensor([-0.3185], grad_fn=<AddBackward0>)\n",
      "epoch: 9287 loss is tensor([-0.3966], grad_fn=<AddBackward0>)\n",
      "epoch: 9288 loss is tensor([-0.3355], grad_fn=<AddBackward0>)\n",
      "epoch: 9289 loss is tensor([-0.3945], grad_fn=<AddBackward0>)\n",
      "epoch: 9290 loss is tensor([-0.4153], grad_fn=<AddBackward0>)\n",
      "epoch: 9291 loss is tensor([-0.3918], grad_fn=<AddBackward0>)\n",
      "epoch: 9292 loss is tensor([-0.4418], grad_fn=<AddBackward0>)\n",
      "epoch: 9293 loss is tensor([-0.3822], grad_fn=<AddBackward0>)\n",
      "epoch: 9294 loss is tensor([-0.4400], grad_fn=<AddBackward0>)\n",
      "epoch: 9295 loss is tensor([-0.3943], grad_fn=<AddBackward0>)\n",
      "epoch: 9296 loss is tensor([-0.4187], grad_fn=<AddBackward0>)\n",
      "epoch: 9297 loss is tensor([-0.3695], grad_fn=<AddBackward0>)\n",
      "epoch: 9298 loss is tensor([-0.3165], grad_fn=<AddBackward0>)\n",
      "epoch: 9299 loss is tensor([-0.3663], grad_fn=<AddBackward0>)\n",
      "epoch: 9300 loss is tensor([-0.3870], grad_fn=<AddBackward0>)\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9301 loss is tensor([-0.3588], grad_fn=<AddBackward0>)\n",
      "epoch: 9302 loss is tensor([-0.3638], grad_fn=<AddBackward0>)\n",
      "epoch: 9303 loss is tensor([-0.3939], grad_fn=<AddBackward0>)\n",
      "epoch: 9304 loss is tensor([-0.4164], grad_fn=<AddBackward0>)\n",
      "epoch: 9305 loss is tensor([-0.4152], grad_fn=<AddBackward0>)\n",
      "epoch: 9306 loss is tensor([-0.3707], grad_fn=<AddBackward0>)\n",
      "epoch: 9307 loss is tensor([-0.3841], grad_fn=<AddBackward0>)\n",
      "epoch: 9308 loss is tensor([-0.3778], grad_fn=<AddBackward0>)\n",
      "epoch: 9309 loss is tensor([-0.3597], grad_fn=<AddBackward0>)\n",
      "epoch: 9310 loss is tensor([-0.3708], grad_fn=<AddBackward0>)\n",
      "epoch: 9311 loss is tensor([-0.4198], grad_fn=<AddBackward0>)\n",
      "epoch: 9312 loss is tensor([-0.3574], grad_fn=<AddBackward0>)\n",
      "epoch: 9313 loss is tensor([-0.3940], grad_fn=<AddBackward0>)\n",
      "epoch: 9314 loss is tensor([-0.3962], grad_fn=<AddBackward0>)\n",
      "epoch: 9315 loss is tensor([-0.4103], grad_fn=<AddBackward0>)\n",
      "epoch: 9316 loss is tensor([-0.3366], grad_fn=<AddBackward0>)\n",
      "epoch: 9317 loss is tensor([-0.3550], grad_fn=<AddBackward0>)\n",
      "epoch: 9318 loss is tensor([-0.3465], grad_fn=<AddBackward0>)\n",
      "epoch: 9319 loss is tensor([-0.3737], grad_fn=<AddBackward0>)\n",
      "epoch: 9320 loss is tensor([-0.3877], grad_fn=<AddBackward0>)\n",
      "epoch: 9321 loss is tensor([-0.3909], grad_fn=<AddBackward0>)\n",
      "epoch: 9322 loss is tensor([-0.3772], grad_fn=<AddBackward0>)\n",
      "epoch: 9323 loss is tensor([-0.4155], grad_fn=<AddBackward0>)\n",
      "epoch: 9324 loss is tensor([-0.3726], grad_fn=<AddBackward0>)\n",
      "epoch: 9325 loss is tensor([-0.3976], grad_fn=<AddBackward0>)\n",
      "epoch: 9326 loss is tensor([-0.3909], grad_fn=<AddBackward0>)\n",
      "epoch: 9327 loss is tensor([-0.3851], grad_fn=<AddBackward0>)\n",
      "epoch: 9328 loss is tensor([-0.4519], grad_fn=<AddBackward0>)\n",
      "epoch: 9329 loss is tensor([-0.3778], grad_fn=<AddBackward0>)\n",
      "epoch: 9330 loss is tensor([-0.3689], grad_fn=<AddBackward0>)\n",
      "epoch: 9331 loss is tensor([-0.4488], grad_fn=<AddBackward0>)\n",
      "epoch: 9332 loss is tensor([-0.4102], grad_fn=<AddBackward0>)\n",
      "epoch: 9333 loss is tensor([-0.4101], grad_fn=<AddBackward0>)\n",
      "epoch: 9334 loss is tensor([-0.4132], grad_fn=<AddBackward0>)\n",
      "epoch: 9335 loss is tensor([-0.4385], grad_fn=<AddBackward0>)\n",
      "epoch: 9336 loss is tensor([-0.3447], grad_fn=<AddBackward0>)\n",
      "epoch: 9337 loss is tensor([-0.3632], grad_fn=<AddBackward0>)\n",
      "epoch: 9338 loss is tensor([-0.3280], grad_fn=<AddBackward0>)\n",
      "epoch: 9339 loss is tensor([-0.3507], grad_fn=<AddBackward0>)\n",
      "epoch: 9340 loss is tensor([-0.3150], grad_fn=<AddBackward0>)\n",
      "epoch: 9341 loss is tensor([-0.4096], grad_fn=<AddBackward0>)\n",
      "epoch: 9342 loss is tensor([-0.4028], grad_fn=<AddBackward0>)\n",
      "epoch: 9343 loss is tensor([-0.3358], grad_fn=<AddBackward0>)\n",
      "epoch: 9344 loss is tensor([-0.3677], grad_fn=<AddBackward0>)\n",
      "epoch: 9345 loss is tensor([-0.3407], grad_fn=<AddBackward0>)\n",
      "epoch: 9346 loss is tensor([-0.4050], grad_fn=<AddBackward0>)\n",
      "epoch: 9347 loss is tensor([-0.3870], grad_fn=<AddBackward0>)\n",
      "epoch: 9348 loss is tensor([-0.3442], grad_fn=<AddBackward0>)\n",
      "epoch: 9349 loss is tensor([-0.2511], grad_fn=<AddBackward0>)\n",
      "epoch: 9350 loss is tensor([-0.2495], grad_fn=<AddBackward0>)\n",
      "epoch: 9351 loss is tensor([-0.3139], grad_fn=<AddBackward0>)\n",
      "epoch: 9352 loss is tensor([-0.3218], grad_fn=<AddBackward0>)\n",
      "epoch: 9353 loss is tensor([-0.3036], grad_fn=<AddBackward0>)\n",
      "epoch: 9354 loss is tensor([-0.3283], grad_fn=<AddBackward0>)\n",
      "epoch: 9355 loss is tensor([-0.3328], grad_fn=<AddBackward0>)\n",
      "epoch: 9356 loss is tensor([-0.2828], grad_fn=<AddBackward0>)\n",
      "epoch: 9357 loss is tensor([-0.2922], grad_fn=<AddBackward0>)\n",
      "epoch: 9358 loss is tensor([-0.3380], grad_fn=<AddBackward0>)\n",
      "epoch: 9359 loss is tensor([-0.3486], grad_fn=<AddBackward0>)\n",
      "epoch: 9360 loss is tensor([-0.3695], grad_fn=<AddBackward0>)\n",
      "epoch: 9361 loss is tensor([-0.3116], grad_fn=<AddBackward0>)\n",
      "epoch: 9362 loss is tensor([-0.3896], grad_fn=<AddBackward0>)\n",
      "epoch: 9363 loss is tensor([-0.3598], grad_fn=<AddBackward0>)\n",
      "epoch: 9364 loss is tensor([-0.3686], grad_fn=<AddBackward0>)\n",
      "epoch: 9365 loss is tensor([-0.3871], grad_fn=<AddBackward0>)\n",
      "epoch: 9366 loss is tensor([-0.3804], grad_fn=<AddBackward0>)\n",
      "epoch: 9367 loss is tensor([-0.3424], grad_fn=<AddBackward0>)\n",
      "epoch: 9368 loss is tensor([-0.3443], grad_fn=<AddBackward0>)\n",
      "epoch: 9369 loss is tensor([-0.3585], grad_fn=<AddBackward0>)\n",
      "epoch: 9370 loss is tensor([-0.3888], grad_fn=<AddBackward0>)\n",
      "epoch: 9371 loss is tensor([-0.4317], grad_fn=<AddBackward0>)\n",
      "epoch: 9372 loss is tensor([-0.4356], grad_fn=<AddBackward0>)\n",
      "epoch: 9373 loss is tensor([-0.3513], grad_fn=<AddBackward0>)\n",
      "epoch: 9374 loss is tensor([-0.3880], grad_fn=<AddBackward0>)\n",
      "epoch: 9375 loss is tensor([-0.3814], grad_fn=<AddBackward0>)\n",
      "epoch: 9376 loss is tensor([-0.4105], grad_fn=<AddBackward0>)\n",
      "epoch: 9377 loss is tensor([-0.4072], grad_fn=<AddBackward0>)\n",
      "epoch: 9378 loss is tensor([-0.3764], grad_fn=<AddBackward0>)\n",
      "epoch: 9379 loss is tensor([-0.3721], grad_fn=<AddBackward0>)\n",
      "epoch: 9380 loss is tensor([-0.3587], grad_fn=<AddBackward0>)\n",
      "epoch: 9381 loss is tensor([-0.3453], grad_fn=<AddBackward0>)\n",
      "epoch: 9382 loss is tensor([-0.3723], grad_fn=<AddBackward0>)\n",
      "epoch: 9383 loss is tensor([-0.4306], grad_fn=<AddBackward0>)\n",
      "epoch: 9384 loss is tensor([-0.4192], grad_fn=<AddBackward0>)\n",
      "epoch: 9385 loss is tensor([-0.3972], grad_fn=<AddBackward0>)\n",
      "epoch: 9386 loss is tensor([-0.4319], grad_fn=<AddBackward0>)\n",
      "epoch: 9387 loss is tensor([-0.4745], grad_fn=<AddBackward0>)\n",
      "epoch: 9388 loss is tensor([-0.4589], grad_fn=<AddBackward0>)\n",
      "epoch: 9389 loss is tensor([-0.4127], grad_fn=<AddBackward0>)\n",
      "epoch: 9390 loss is tensor([-0.4418], grad_fn=<AddBackward0>)\n",
      "epoch: 9391 loss is tensor([-0.3947], grad_fn=<AddBackward0>)\n",
      "epoch: 9392 loss is tensor([-0.3991], grad_fn=<AddBackward0>)\n",
      "epoch: 9393 loss is tensor([-0.4349], grad_fn=<AddBackward0>)\n",
      "epoch: 9394 loss is tensor([-0.3695], grad_fn=<AddBackward0>)\n",
      "epoch: 9395 loss is tensor([-0.4268], grad_fn=<AddBackward0>)\n",
      "epoch: 9396 loss is tensor([-0.3050], grad_fn=<AddBackward0>)\n",
      "epoch: 9397 loss is tensor([-0.3417], grad_fn=<AddBackward0>)\n",
      "epoch: 9398 loss is tensor([-0.3441], grad_fn=<AddBackward0>)\n",
      "epoch: 9399 loss is tensor([-0.3365], grad_fn=<AddBackward0>)\n",
      "epoch: 9400 loss is tensor([-0.3456], grad_fn=<AddBackward0>)\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9401 loss is tensor([-0.3168], grad_fn=<AddBackward0>)\n",
      "epoch: 9402 loss is tensor([-0.3875], grad_fn=<AddBackward0>)\n",
      "epoch: 9403 loss is tensor([-0.3100], grad_fn=<AddBackward0>)\n",
      "epoch: 9404 loss is tensor([-0.3671], grad_fn=<AddBackward0>)\n",
      "epoch: 9405 loss is tensor([-0.3846], grad_fn=<AddBackward0>)\n",
      "epoch: 9406 loss is tensor([-0.3924], grad_fn=<AddBackward0>)\n",
      "epoch: 9407 loss is tensor([-0.3820], grad_fn=<AddBackward0>)\n",
      "epoch: 9408 loss is tensor([-0.3635], grad_fn=<AddBackward0>)\n",
      "epoch: 9409 loss is tensor([-0.4054], grad_fn=<AddBackward0>)\n",
      "epoch: 9410 loss is tensor([-0.3818], grad_fn=<AddBackward0>)\n",
      "epoch: 9411 loss is tensor([-0.4046], grad_fn=<AddBackward0>)\n",
      "epoch: 9412 loss is tensor([-0.4081], grad_fn=<AddBackward0>)\n",
      "epoch: 9413 loss is tensor([-0.3496], grad_fn=<AddBackward0>)\n",
      "epoch: 9414 loss is tensor([-0.4134], grad_fn=<AddBackward0>)\n",
      "epoch: 9415 loss is tensor([-0.4219], grad_fn=<AddBackward0>)\n",
      "epoch: 9416 loss is tensor([-0.4227], grad_fn=<AddBackward0>)\n",
      "epoch: 9417 loss is tensor([-0.4086], grad_fn=<AddBackward0>)\n",
      "epoch: 9418 loss is tensor([-0.4553], grad_fn=<AddBackward0>)\n",
      "epoch: 9419 loss is tensor([-0.3963], grad_fn=<AddBackward0>)\n",
      "epoch: 9420 loss is tensor([-0.4318], grad_fn=<AddBackward0>)\n",
      "epoch: 9421 loss is tensor([-0.4185], grad_fn=<AddBackward0>)\n",
      "epoch: 9422 loss is tensor([-0.4333], grad_fn=<AddBackward0>)\n",
      "epoch: 9423 loss is tensor([-0.3739], grad_fn=<AddBackward0>)\n",
      "epoch: 9424 loss is tensor([-0.4482], grad_fn=<AddBackward0>)\n",
      "epoch: 9425 loss is tensor([-0.4052], grad_fn=<AddBackward0>)\n",
      "epoch: 9426 loss is tensor([-0.4673], grad_fn=<AddBackward0>)\n",
      "epoch: 9427 loss is tensor([-0.4245], grad_fn=<AddBackward0>)\n",
      "epoch: 9428 loss is tensor([-0.4173], grad_fn=<AddBackward0>)\n",
      "epoch: 9429 loss is tensor([-0.3708], grad_fn=<AddBackward0>)\n",
      "epoch: 9430 loss is tensor([-0.4164], grad_fn=<AddBackward0>)\n",
      "epoch: 9431 loss is tensor([-0.3770], grad_fn=<AddBackward0>)\n",
      "epoch: 9432 loss is tensor([-0.4301], grad_fn=<AddBackward0>)\n",
      "epoch: 9433 loss is tensor([-0.4566], grad_fn=<AddBackward0>)\n",
      "epoch: 9434 loss is tensor([-0.4105], grad_fn=<AddBackward0>)\n",
      "epoch: 9435 loss is tensor([-0.3974], grad_fn=<AddBackward0>)\n",
      "epoch: 9436 loss is tensor([-0.4213], grad_fn=<AddBackward0>)\n",
      "epoch: 9437 loss is tensor([-0.4352], grad_fn=<AddBackward0>)\n",
      "epoch: 9438 loss is tensor([-0.4121], grad_fn=<AddBackward0>)\n",
      "epoch: 9439 loss is tensor([-0.4248], grad_fn=<AddBackward0>)\n",
      "epoch: 9440 loss is tensor([-0.3816], grad_fn=<AddBackward0>)\n",
      "epoch: 9441 loss is tensor([-0.3352], grad_fn=<AddBackward0>)\n",
      "epoch: 9442 loss is tensor([-0.4605], grad_fn=<AddBackward0>)\n",
      "epoch: 9443 loss is tensor([-0.4081], grad_fn=<AddBackward0>)\n",
      "epoch: 9444 loss is tensor([-0.3618], grad_fn=<AddBackward0>)\n",
      "epoch: 9445 loss is tensor([-0.3907], grad_fn=<AddBackward0>)\n",
      "epoch: 9446 loss is tensor([-0.3734], grad_fn=<AddBackward0>)\n",
      "epoch: 9447 loss is tensor([-0.3606], grad_fn=<AddBackward0>)\n",
      "epoch: 9448 loss is tensor([-0.4109], grad_fn=<AddBackward0>)\n",
      "epoch: 9449 loss is tensor([-0.4248], grad_fn=<AddBackward0>)\n",
      "epoch: 9450 loss is tensor([-0.4510], grad_fn=<AddBackward0>)\n",
      "epoch: 9451 loss is tensor([-0.4508], grad_fn=<AddBackward0>)\n",
      "epoch: 9452 loss is tensor([-0.4188], grad_fn=<AddBackward0>)\n",
      "epoch: 9453 loss is tensor([-0.3814], grad_fn=<AddBackward0>)\n",
      "epoch: 9454 loss is tensor([-0.4248], grad_fn=<AddBackward0>)\n",
      "epoch: 9455 loss is tensor([-0.4253], grad_fn=<AddBackward0>)\n",
      "epoch: 9456 loss is tensor([-0.4564], grad_fn=<AddBackward0>)\n",
      "epoch: 9457 loss is tensor([-0.4076], grad_fn=<AddBackward0>)\n",
      "epoch: 9458 loss is tensor([-0.4080], grad_fn=<AddBackward0>)\n",
      "epoch: 9459 loss is tensor([-0.3913], grad_fn=<AddBackward0>)\n",
      "epoch: 9460 loss is tensor([-0.4582], grad_fn=<AddBackward0>)\n",
      "epoch: 9461 loss is tensor([-0.4616], grad_fn=<AddBackward0>)\n",
      "epoch: 9462 loss is tensor([-0.4135], grad_fn=<AddBackward0>)\n",
      "epoch: 9463 loss is tensor([-0.3787], grad_fn=<AddBackward0>)\n",
      "epoch: 9464 loss is tensor([-0.4294], grad_fn=<AddBackward0>)\n",
      "epoch: 9465 loss is tensor([-0.4174], grad_fn=<AddBackward0>)\n",
      "epoch: 9466 loss is tensor([-0.4304], grad_fn=<AddBackward0>)\n",
      "epoch: 9467 loss is tensor([-0.4702], grad_fn=<AddBackward0>)\n",
      "epoch: 9468 loss is tensor([-0.4551], grad_fn=<AddBackward0>)\n",
      "epoch: 9469 loss is tensor([-0.4398], grad_fn=<AddBackward0>)\n",
      "epoch: 9470 loss is tensor([-0.4274], grad_fn=<AddBackward0>)\n",
      "epoch: 9471 loss is tensor([-0.3697], grad_fn=<AddBackward0>)\n",
      "epoch: 9472 loss is tensor([-0.3945], grad_fn=<AddBackward0>)\n",
      "epoch: 9473 loss is tensor([-0.5177], grad_fn=<AddBackward0>)\n",
      "epoch: 9474 loss is tensor([-0.4454], grad_fn=<AddBackward0>)\n",
      "epoch: 9475 loss is tensor([-0.3940], grad_fn=<AddBackward0>)\n",
      "epoch: 9476 loss is tensor([-0.3926], grad_fn=<AddBackward0>)\n",
      "epoch: 9477 loss is tensor([-0.4723], grad_fn=<AddBackward0>)\n",
      "epoch: 9478 loss is tensor([-0.4367], grad_fn=<AddBackward0>)\n",
      "epoch: 9479 loss is tensor([-0.4476], grad_fn=<AddBackward0>)\n",
      "epoch: 9480 loss is tensor([-0.4203], grad_fn=<AddBackward0>)\n",
      "epoch: 9481 loss is tensor([-0.4457], grad_fn=<AddBackward0>)\n",
      "epoch: 9482 loss is tensor([-0.3824], grad_fn=<AddBackward0>)\n",
      "epoch: 9483 loss is tensor([-0.3992], grad_fn=<AddBackward0>)\n",
      "epoch: 9484 loss is tensor([-0.4113], grad_fn=<AddBackward0>)\n",
      "epoch: 9485 loss is tensor([-0.4054], grad_fn=<AddBackward0>)\n",
      "epoch: 9486 loss is tensor([-0.3726], grad_fn=<AddBackward0>)\n",
      "epoch: 9487 loss is tensor([-0.4158], grad_fn=<AddBackward0>)\n",
      "epoch: 9488 loss is tensor([-0.4094], grad_fn=<AddBackward0>)\n",
      "epoch: 9489 loss is tensor([-0.4290], grad_fn=<AddBackward0>)\n",
      "epoch: 9490 loss is tensor([-0.4862], grad_fn=<AddBackward0>)\n",
      "epoch: 9491 loss is tensor([-0.4136], grad_fn=<AddBackward0>)\n",
      "epoch: 9492 loss is tensor([-0.4219], grad_fn=<AddBackward0>)\n",
      "epoch: 9493 loss is tensor([-0.3790], grad_fn=<AddBackward0>)\n",
      "epoch: 9494 loss is tensor([-0.4026], grad_fn=<AddBackward0>)\n",
      "epoch: 9495 loss is tensor([-0.4196], grad_fn=<AddBackward0>)\n",
      "epoch: 9496 loss is tensor([-0.3963], grad_fn=<AddBackward0>)\n",
      "epoch: 9497 loss is tensor([-0.3974], grad_fn=<AddBackward0>)\n",
      "epoch: 9498 loss is tensor([-0.3743], grad_fn=<AddBackward0>)\n",
      "epoch: 9499 loss is tensor([-0.3821], grad_fn=<AddBackward0>)\n",
      "epoch: 9500 loss is tensor([-0.4175], grad_fn=<AddBackward0>)\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9501 loss is tensor([-0.4155], grad_fn=<AddBackward0>)\n",
      "epoch: 9502 loss is tensor([-0.3622], grad_fn=<AddBackward0>)\n",
      "epoch: 9503 loss is tensor([-0.3528], grad_fn=<AddBackward0>)\n",
      "epoch: 9504 loss is tensor([-0.3605], grad_fn=<AddBackward0>)\n",
      "epoch: 9505 loss is tensor([-0.4103], grad_fn=<AddBackward0>)\n",
      "epoch: 9506 loss is tensor([-0.3857], grad_fn=<AddBackward0>)\n",
      "epoch: 9507 loss is tensor([-0.3592], grad_fn=<AddBackward0>)\n",
      "epoch: 9508 loss is tensor([-0.3591], grad_fn=<AddBackward0>)\n",
      "epoch: 9509 loss is tensor([-0.3398], grad_fn=<AddBackward0>)\n",
      "epoch: 9510 loss is tensor([-0.3286], grad_fn=<AddBackward0>)\n",
      "epoch: 9511 loss is tensor([-0.4085], grad_fn=<AddBackward0>)\n",
      "epoch: 9512 loss is tensor([-0.3577], grad_fn=<AddBackward0>)\n",
      "epoch: 9513 loss is tensor([-0.3133], grad_fn=<AddBackward0>)\n",
      "epoch: 9514 loss is tensor([-0.2655], grad_fn=<AddBackward0>)\n",
      "epoch: 9515 loss is tensor([-0.3461], grad_fn=<AddBackward0>)\n",
      "epoch: 9516 loss is tensor([-0.3567], grad_fn=<AddBackward0>)\n",
      "epoch: 9517 loss is tensor([-0.3820], grad_fn=<AddBackward0>)\n",
      "epoch: 9518 loss is tensor([-0.3961], grad_fn=<AddBackward0>)\n",
      "epoch: 9519 loss is tensor([-0.3378], grad_fn=<AddBackward0>)\n",
      "epoch: 9520 loss is tensor([-0.3353], grad_fn=<AddBackward0>)\n",
      "epoch: 9521 loss is tensor([-0.4003], grad_fn=<AddBackward0>)\n",
      "epoch: 9522 loss is tensor([-0.4053], grad_fn=<AddBackward0>)\n",
      "epoch: 9523 loss is tensor([-0.4209], grad_fn=<AddBackward0>)\n",
      "epoch: 9524 loss is tensor([-0.4633], grad_fn=<AddBackward0>)\n",
      "epoch: 9525 loss is tensor([-0.4094], grad_fn=<AddBackward0>)\n",
      "epoch: 9526 loss is tensor([-0.4281], grad_fn=<AddBackward0>)\n",
      "epoch: 9527 loss is tensor([-0.3807], grad_fn=<AddBackward0>)\n",
      "epoch: 9528 loss is tensor([-0.4099], grad_fn=<AddBackward0>)\n",
      "epoch: 9529 loss is tensor([-0.4114], grad_fn=<AddBackward0>)\n",
      "epoch: 9530 loss is tensor([-0.3863], grad_fn=<AddBackward0>)\n",
      "epoch: 9531 loss is tensor([-0.4275], grad_fn=<AddBackward0>)\n",
      "epoch: 9532 loss is tensor([-0.3874], grad_fn=<AddBackward0>)\n",
      "epoch: 9533 loss is tensor([-0.4552], grad_fn=<AddBackward0>)\n",
      "epoch: 9534 loss is tensor([-0.3734], grad_fn=<AddBackward0>)\n",
      "epoch: 9535 loss is tensor([-0.3908], grad_fn=<AddBackward0>)\n",
      "epoch: 9536 loss is tensor([-0.4353], grad_fn=<AddBackward0>)\n",
      "epoch: 9537 loss is tensor([-0.4338], grad_fn=<AddBackward0>)\n",
      "epoch: 9538 loss is tensor([-0.3771], grad_fn=<AddBackward0>)\n",
      "epoch: 9539 loss is tensor([-0.4579], grad_fn=<AddBackward0>)\n",
      "epoch: 9540 loss is tensor([-0.4656], grad_fn=<AddBackward0>)\n",
      "epoch: 9541 loss is tensor([-0.3878], grad_fn=<AddBackward0>)\n",
      "epoch: 9542 loss is tensor([-0.3651], grad_fn=<AddBackward0>)\n",
      "epoch: 9543 loss is tensor([-0.3768], grad_fn=<AddBackward0>)\n",
      "epoch: 9544 loss is tensor([-0.4532], grad_fn=<AddBackward0>)\n",
      "epoch: 9545 loss is tensor([-0.3826], grad_fn=<AddBackward0>)\n",
      "epoch: 9546 loss is tensor([-0.3928], grad_fn=<AddBackward0>)\n",
      "epoch: 9547 loss is tensor([-0.3971], grad_fn=<AddBackward0>)\n",
      "epoch: 9548 loss is tensor([-0.3999], grad_fn=<AddBackward0>)\n",
      "epoch: 9549 loss is tensor([-0.3834], grad_fn=<AddBackward0>)\n",
      "epoch: 9550 loss is tensor([-0.4575], grad_fn=<AddBackward0>)\n",
      "epoch: 9551 loss is tensor([-0.3713], grad_fn=<AddBackward0>)\n",
      "epoch: 9552 loss is tensor([-0.3708], grad_fn=<AddBackward0>)\n",
      "epoch: 9553 loss is tensor([-0.4555], grad_fn=<AddBackward0>)\n",
      "epoch: 9554 loss is tensor([-0.4677], grad_fn=<AddBackward0>)\n",
      "epoch: 9555 loss is tensor([-0.4038], grad_fn=<AddBackward0>)\n",
      "epoch: 9556 loss is tensor([-0.4225], grad_fn=<AddBackward0>)\n",
      "epoch: 9557 loss is tensor([-0.3907], grad_fn=<AddBackward0>)\n",
      "epoch: 9558 loss is tensor([-0.4341], grad_fn=<AddBackward0>)\n",
      "epoch: 9559 loss is tensor([-0.3768], grad_fn=<AddBackward0>)\n",
      "epoch: 9560 loss is tensor([-0.4226], grad_fn=<AddBackward0>)\n",
      "epoch: 9561 loss is tensor([-0.4177], grad_fn=<AddBackward0>)\n",
      "epoch: 9562 loss is tensor([-0.3924], grad_fn=<AddBackward0>)\n",
      "epoch: 9563 loss is tensor([-0.4295], grad_fn=<AddBackward0>)\n",
      "epoch: 9564 loss is tensor([-0.3883], grad_fn=<AddBackward0>)\n",
      "epoch: 9565 loss is tensor([-0.4385], grad_fn=<AddBackward0>)\n",
      "epoch: 9566 loss is tensor([-0.4356], grad_fn=<AddBackward0>)\n",
      "epoch: 9567 loss is tensor([-0.3740], grad_fn=<AddBackward0>)\n",
      "epoch: 9568 loss is tensor([-0.4418], grad_fn=<AddBackward0>)\n",
      "epoch: 9569 loss is tensor([-0.4404], grad_fn=<AddBackward0>)\n",
      "epoch: 9570 loss is tensor([-0.3814], grad_fn=<AddBackward0>)\n",
      "epoch: 9571 loss is tensor([-0.3750], grad_fn=<AddBackward0>)\n",
      "epoch: 9572 loss is tensor([-0.3866], grad_fn=<AddBackward0>)\n",
      "epoch: 9573 loss is tensor([-0.4346], grad_fn=<AddBackward0>)\n",
      "epoch: 9574 loss is tensor([-0.4175], grad_fn=<AddBackward0>)\n",
      "epoch: 9575 loss is tensor([-0.4292], grad_fn=<AddBackward0>)\n",
      "epoch: 9576 loss is tensor([-0.4415], grad_fn=<AddBackward0>)\n",
      "epoch: 9577 loss is tensor([-0.4573], grad_fn=<AddBackward0>)\n",
      "epoch: 9578 loss is tensor([-0.4465], grad_fn=<AddBackward0>)\n",
      "epoch: 9579 loss is tensor([-0.4991], grad_fn=<AddBackward0>)\n",
      "epoch: 9580 loss is tensor([-0.4209], grad_fn=<AddBackward0>)\n",
      "epoch: 9581 loss is tensor([-0.4820], grad_fn=<AddBackward0>)\n",
      "epoch: 9582 loss is tensor([-0.4467], grad_fn=<AddBackward0>)\n",
      "epoch: 9583 loss is tensor([-0.4402], grad_fn=<AddBackward0>)\n",
      "epoch: 9584 loss is tensor([-0.4432], grad_fn=<AddBackward0>)\n",
      "epoch: 9585 loss is tensor([-0.3537], grad_fn=<AddBackward0>)\n",
      "epoch: 9586 loss is tensor([-0.4218], grad_fn=<AddBackward0>)\n",
      "epoch: 9587 loss is tensor([-0.4354], grad_fn=<AddBackward0>)\n",
      "epoch: 9588 loss is tensor([-0.3819], grad_fn=<AddBackward0>)\n",
      "epoch: 9589 loss is tensor([-0.4513], grad_fn=<AddBackward0>)\n",
      "epoch: 9590 loss is tensor([-0.3159], grad_fn=<AddBackward0>)\n",
      "epoch: 9591 loss is tensor([-0.3897], grad_fn=<AddBackward0>)\n",
      "epoch: 9592 loss is tensor([-0.4189], grad_fn=<AddBackward0>)\n",
      "epoch: 9593 loss is tensor([-0.4116], grad_fn=<AddBackward0>)\n",
      "epoch: 9594 loss is tensor([-0.4168], grad_fn=<AddBackward0>)\n",
      "epoch: 9595 loss is tensor([-0.4092], grad_fn=<AddBackward0>)\n",
      "epoch: 9596 loss is tensor([-0.3816], grad_fn=<AddBackward0>)\n",
      "epoch: 9597 loss is tensor([-0.3974], grad_fn=<AddBackward0>)\n",
      "epoch: 9598 loss is tensor([-0.3986], grad_fn=<AddBackward0>)\n",
      "epoch: 9599 loss is tensor([-0.3578], grad_fn=<AddBackward0>)\n",
      "epoch: 9600 loss is tensor([-0.3726], grad_fn=<AddBackward0>)\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9601 loss is tensor([-0.3761], grad_fn=<AddBackward0>)\n",
      "epoch: 9602 loss is tensor([-0.3870], grad_fn=<AddBackward0>)\n",
      "epoch: 9603 loss is tensor([-0.4188], grad_fn=<AddBackward0>)\n",
      "epoch: 9604 loss is tensor([-0.4327], grad_fn=<AddBackward0>)\n",
      "epoch: 9605 loss is tensor([-0.4480], grad_fn=<AddBackward0>)\n",
      "epoch: 9606 loss is tensor([-0.3437], grad_fn=<AddBackward0>)\n",
      "epoch: 9607 loss is tensor([-0.4426], grad_fn=<AddBackward0>)\n",
      "epoch: 9608 loss is tensor([-0.4317], grad_fn=<AddBackward0>)\n",
      "epoch: 9609 loss is tensor([-0.3959], grad_fn=<AddBackward0>)\n",
      "epoch: 9610 loss is tensor([-0.3818], grad_fn=<AddBackward0>)\n",
      "epoch: 9611 loss is tensor([-0.4036], grad_fn=<AddBackward0>)\n",
      "epoch: 9612 loss is tensor([-0.3932], grad_fn=<AddBackward0>)\n",
      "epoch: 9613 loss is tensor([-0.4435], grad_fn=<AddBackward0>)\n",
      "epoch: 9614 loss is tensor([-0.3917], grad_fn=<AddBackward0>)\n",
      "epoch: 9615 loss is tensor([-0.3597], grad_fn=<AddBackward0>)\n",
      "epoch: 9616 loss is tensor([-0.4185], grad_fn=<AddBackward0>)\n",
      "epoch: 9617 loss is tensor([-0.4166], grad_fn=<AddBackward0>)\n",
      "epoch: 9618 loss is tensor([-0.4440], grad_fn=<AddBackward0>)\n",
      "epoch: 9619 loss is tensor([-0.4256], grad_fn=<AddBackward0>)\n",
      "epoch: 9620 loss is tensor([-0.4066], grad_fn=<AddBackward0>)\n",
      "epoch: 9621 loss is tensor([-0.3966], grad_fn=<AddBackward0>)\n",
      "epoch: 9622 loss is tensor([-0.4484], grad_fn=<AddBackward0>)\n",
      "epoch: 9623 loss is tensor([-0.4135], grad_fn=<AddBackward0>)\n",
      "epoch: 9624 loss is tensor([-0.4375], grad_fn=<AddBackward0>)\n",
      "epoch: 9625 loss is tensor([-0.4043], grad_fn=<AddBackward0>)\n",
      "epoch: 9626 loss is tensor([-0.4563], grad_fn=<AddBackward0>)\n",
      "epoch: 9627 loss is tensor([-0.3890], grad_fn=<AddBackward0>)\n",
      "epoch: 9628 loss is tensor([-0.4692], grad_fn=<AddBackward0>)\n",
      "epoch: 9629 loss is tensor([-0.4405], grad_fn=<AddBackward0>)\n",
      "epoch: 9630 loss is tensor([-0.4488], grad_fn=<AddBackward0>)\n",
      "epoch: 9631 loss is tensor([-0.3920], grad_fn=<AddBackward0>)\n",
      "epoch: 9632 loss is tensor([-0.4012], grad_fn=<AddBackward0>)\n",
      "epoch: 9633 loss is tensor([-0.3891], grad_fn=<AddBackward0>)\n",
      "epoch: 9634 loss is tensor([-0.4322], grad_fn=<AddBackward0>)\n",
      "epoch: 9635 loss is tensor([-0.4672], grad_fn=<AddBackward0>)\n",
      "epoch: 9636 loss is tensor([-0.3764], grad_fn=<AddBackward0>)\n",
      "epoch: 9637 loss is tensor([-0.3003], grad_fn=<AddBackward0>)\n",
      "epoch: 9638 loss is tensor([-0.4003], grad_fn=<AddBackward0>)\n",
      "epoch: 9639 loss is tensor([-0.3906], grad_fn=<AddBackward0>)\n",
      "epoch: 9640 loss is tensor([-0.3892], grad_fn=<AddBackward0>)\n",
      "epoch: 9641 loss is tensor([-0.4374], grad_fn=<AddBackward0>)\n",
      "epoch: 9642 loss is tensor([-0.3920], grad_fn=<AddBackward0>)\n",
      "epoch: 9643 loss is tensor([-0.3930], grad_fn=<AddBackward0>)\n",
      "epoch: 9644 loss is tensor([-0.5098], grad_fn=<AddBackward0>)\n",
      "epoch: 9645 loss is tensor([-0.3855], grad_fn=<AddBackward0>)\n",
      "epoch: 9646 loss is tensor([-0.3907], grad_fn=<AddBackward0>)\n",
      "epoch: 9647 loss is tensor([-0.4508], grad_fn=<AddBackward0>)\n",
      "epoch: 9648 loss is tensor([-0.3907], grad_fn=<AddBackward0>)\n",
      "epoch: 9649 loss is tensor([-0.4022], grad_fn=<AddBackward0>)\n",
      "epoch: 9650 loss is tensor([-0.4535], grad_fn=<AddBackward0>)\n",
      "epoch: 9651 loss is tensor([-0.4147], grad_fn=<AddBackward0>)\n",
      "epoch: 9652 loss is tensor([-0.4241], grad_fn=<AddBackward0>)\n",
      "epoch: 9653 loss is tensor([-0.4629], grad_fn=<AddBackward0>)\n",
      "epoch: 9654 loss is tensor([-0.4455], grad_fn=<AddBackward0>)\n",
      "epoch: 9655 loss is tensor([-0.3283], grad_fn=<AddBackward0>)\n",
      "epoch: 9656 loss is tensor([-0.4022], grad_fn=<AddBackward0>)\n",
      "epoch: 9657 loss is tensor([-0.3793], grad_fn=<AddBackward0>)\n",
      "epoch: 9658 loss is tensor([-0.3389], grad_fn=<AddBackward0>)\n",
      "epoch: 9659 loss is tensor([-0.3165], grad_fn=<AddBackward0>)\n",
      "epoch: 9660 loss is tensor([-0.4324], grad_fn=<AddBackward0>)\n",
      "epoch: 9661 loss is tensor([-0.4749], grad_fn=<AddBackward0>)\n",
      "epoch: 9662 loss is tensor([-0.3862], grad_fn=<AddBackward0>)\n",
      "epoch: 9663 loss is tensor([-0.4007], grad_fn=<AddBackward0>)\n",
      "epoch: 9664 loss is tensor([-0.3983], grad_fn=<AddBackward0>)\n",
      "epoch: 9665 loss is tensor([-0.3406], grad_fn=<AddBackward0>)\n",
      "epoch: 9666 loss is tensor([-0.4014], grad_fn=<AddBackward0>)\n",
      "epoch: 9667 loss is tensor([-0.4081], grad_fn=<AddBackward0>)\n",
      "epoch: 9668 loss is tensor([-0.4201], grad_fn=<AddBackward0>)\n",
      "epoch: 9669 loss is tensor([-0.3570], grad_fn=<AddBackward0>)\n",
      "epoch: 9670 loss is tensor([-0.4274], grad_fn=<AddBackward0>)\n",
      "epoch: 9671 loss is tensor([-0.4043], grad_fn=<AddBackward0>)\n",
      "epoch: 9672 loss is tensor([-0.4399], grad_fn=<AddBackward0>)\n",
      "epoch: 9673 loss is tensor([-0.4289], grad_fn=<AddBackward0>)\n",
      "epoch: 9674 loss is tensor([-0.4121], grad_fn=<AddBackward0>)\n",
      "epoch: 9675 loss is tensor([-0.3419], grad_fn=<AddBackward0>)\n",
      "epoch: 9676 loss is tensor([-0.3550], grad_fn=<AddBackward0>)\n",
      "epoch: 9677 loss is tensor([-0.4211], grad_fn=<AddBackward0>)\n",
      "epoch: 9678 loss is tensor([-0.4052], grad_fn=<AddBackward0>)\n",
      "epoch: 9679 loss is tensor([-0.4140], grad_fn=<AddBackward0>)\n",
      "epoch: 9680 loss is tensor([-0.3793], grad_fn=<AddBackward0>)\n",
      "epoch: 9681 loss is tensor([-0.4747], grad_fn=<AddBackward0>)\n",
      "epoch: 9682 loss is tensor([-0.4410], grad_fn=<AddBackward0>)\n",
      "epoch: 9683 loss is tensor([-0.4078], grad_fn=<AddBackward0>)\n",
      "epoch: 9684 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 9685 loss is tensor([-0.3628], grad_fn=<AddBackward0>)\n",
      "epoch: 9686 loss is tensor([-0.3641], grad_fn=<AddBackward0>)\n",
      "epoch: 9687 loss is tensor([-0.3901], grad_fn=<AddBackward0>)\n",
      "epoch: 9688 loss is tensor([-0.5240], grad_fn=<AddBackward0>)\n",
      "epoch: 9689 loss is tensor([-0.4530], grad_fn=<AddBackward0>)\n",
      "epoch: 9690 loss is tensor([-0.4503], grad_fn=<AddBackward0>)\n",
      "epoch: 9691 loss is tensor([-0.4148], grad_fn=<AddBackward0>)\n",
      "epoch: 9692 loss is tensor([-0.4025], grad_fn=<AddBackward0>)\n",
      "epoch: 9693 loss is tensor([-0.4116], grad_fn=<AddBackward0>)\n",
      "epoch: 9694 loss is tensor([-0.4059], grad_fn=<AddBackward0>)\n",
      "epoch: 9695 loss is tensor([-0.4111], grad_fn=<AddBackward0>)\n",
      "epoch: 9696 loss is tensor([-0.3991], grad_fn=<AddBackward0>)\n",
      "epoch: 9697 loss is tensor([-0.3447], grad_fn=<AddBackward0>)\n",
      "epoch: 9698 loss is tensor([-0.3398], grad_fn=<AddBackward0>)\n",
      "epoch: 9699 loss is tensor([-0.4346], grad_fn=<AddBackward0>)\n",
      "epoch: 9700 loss is tensor([-0.3778], grad_fn=<AddBackward0>)\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9701 loss is tensor([-0.4303], grad_fn=<AddBackward0>)\n",
      "epoch: 9702 loss is tensor([-0.3953], grad_fn=<AddBackward0>)\n",
      "epoch: 9703 loss is tensor([-0.4266], grad_fn=<AddBackward0>)\n",
      "epoch: 9704 loss is tensor([-0.4575], grad_fn=<AddBackward0>)\n",
      "epoch: 9705 loss is tensor([-0.3905], grad_fn=<AddBackward0>)\n",
      "epoch: 9706 loss is tensor([-0.4797], grad_fn=<AddBackward0>)\n",
      "epoch: 9707 loss is tensor([-0.4306], grad_fn=<AddBackward0>)\n",
      "epoch: 9708 loss is tensor([-0.4388], grad_fn=<AddBackward0>)\n",
      "epoch: 9709 loss is tensor([-0.4417], grad_fn=<AddBackward0>)\n",
      "epoch: 9710 loss is tensor([-0.4116], grad_fn=<AddBackward0>)\n",
      "epoch: 9711 loss is tensor([-0.4767], grad_fn=<AddBackward0>)\n",
      "epoch: 9712 loss is tensor([-0.4518], grad_fn=<AddBackward0>)\n",
      "epoch: 9713 loss is tensor([-0.4739], grad_fn=<AddBackward0>)\n",
      "epoch: 9714 loss is tensor([-0.4056], grad_fn=<AddBackward0>)\n",
      "epoch: 9715 loss is tensor([-0.4346], grad_fn=<AddBackward0>)\n",
      "epoch: 9716 loss is tensor([-0.4415], grad_fn=<AddBackward0>)\n",
      "epoch: 9717 loss is tensor([-0.4812], grad_fn=<AddBackward0>)\n",
      "epoch: 9718 loss is tensor([-0.4876], grad_fn=<AddBackward0>)\n",
      "epoch: 9719 loss is tensor([-0.4713], grad_fn=<AddBackward0>)\n",
      "epoch: 9720 loss is tensor([-0.4493], grad_fn=<AddBackward0>)\n",
      "epoch: 9721 loss is tensor([-0.4261], grad_fn=<AddBackward0>)\n",
      "epoch: 9722 loss is tensor([-0.4734], grad_fn=<AddBackward0>)\n",
      "epoch: 9723 loss is tensor([-0.4389], grad_fn=<AddBackward0>)\n",
      "epoch: 9724 loss is tensor([-0.4666], grad_fn=<AddBackward0>)\n",
      "epoch: 9725 loss is tensor([-0.4961], grad_fn=<AddBackward0>)\n",
      "epoch: 9726 loss is tensor([-0.4097], grad_fn=<AddBackward0>)\n",
      "epoch: 9727 loss is tensor([-0.4466], grad_fn=<AddBackward0>)\n",
      "epoch: 9728 loss is tensor([-0.4620], grad_fn=<AddBackward0>)\n",
      "epoch: 9729 loss is tensor([-0.4219], grad_fn=<AddBackward0>)\n",
      "epoch: 9730 loss is tensor([-0.4141], grad_fn=<AddBackward0>)\n",
      "epoch: 9731 loss is tensor([-0.4671], grad_fn=<AddBackward0>)\n",
      "epoch: 9732 loss is tensor([-0.4211], grad_fn=<AddBackward0>)\n",
      "epoch: 9733 loss is tensor([-0.4687], grad_fn=<AddBackward0>)\n",
      "epoch: 9734 loss is tensor([-0.3980], grad_fn=<AddBackward0>)\n",
      "epoch: 9735 loss is tensor([-0.3930], grad_fn=<AddBackward0>)\n",
      "epoch: 9736 loss is tensor([-0.4344], grad_fn=<AddBackward0>)\n",
      "epoch: 9737 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 9738 loss is tensor([-0.3829], grad_fn=<AddBackward0>)\n",
      "epoch: 9739 loss is tensor([-0.3176], grad_fn=<AddBackward0>)\n",
      "epoch: 9740 loss is tensor([-0.3382], grad_fn=<AddBackward0>)\n",
      "epoch: 9741 loss is tensor([-0.4662], grad_fn=<AddBackward0>)\n",
      "epoch: 9742 loss is tensor([-0.4321], grad_fn=<AddBackward0>)\n",
      "epoch: 9743 loss is tensor([-0.4570], grad_fn=<AddBackward0>)\n",
      "epoch: 9744 loss is tensor([-0.4435], grad_fn=<AddBackward0>)\n",
      "epoch: 9745 loss is tensor([-0.3909], grad_fn=<AddBackward0>)\n",
      "epoch: 9746 loss is tensor([-0.4357], grad_fn=<AddBackward0>)\n",
      "epoch: 9747 loss is tensor([-0.4148], grad_fn=<AddBackward0>)\n",
      "epoch: 9748 loss is tensor([-0.3644], grad_fn=<AddBackward0>)\n",
      "epoch: 9749 loss is tensor([-0.4022], grad_fn=<AddBackward0>)\n",
      "epoch: 9750 loss is tensor([-0.3794], grad_fn=<AddBackward0>)\n",
      "epoch: 9751 loss is tensor([-0.4276], grad_fn=<AddBackward0>)\n",
      "epoch: 9752 loss is tensor([-0.4192], grad_fn=<AddBackward0>)\n",
      "epoch: 9753 loss is tensor([-0.4631], grad_fn=<AddBackward0>)\n",
      "epoch: 9754 loss is tensor([-0.4331], grad_fn=<AddBackward0>)\n",
      "epoch: 9755 loss is tensor([-0.4086], grad_fn=<AddBackward0>)\n",
      "epoch: 9756 loss is tensor([-0.4258], grad_fn=<AddBackward0>)\n",
      "epoch: 9757 loss is tensor([-0.4028], grad_fn=<AddBackward0>)\n",
      "epoch: 9758 loss is tensor([-0.4145], grad_fn=<AddBackward0>)\n",
      "epoch: 9759 loss is tensor([-0.4635], grad_fn=<AddBackward0>)\n",
      "epoch: 9760 loss is tensor([-0.4513], grad_fn=<AddBackward0>)\n",
      "epoch: 9761 loss is tensor([-0.4424], grad_fn=<AddBackward0>)\n",
      "epoch: 9762 loss is tensor([-0.4088], grad_fn=<AddBackward0>)\n",
      "epoch: 9763 loss is tensor([-0.4170], grad_fn=<AddBackward0>)\n",
      "epoch: 9764 loss is tensor([-0.3870], grad_fn=<AddBackward0>)\n",
      "epoch: 9765 loss is tensor([-0.4352], grad_fn=<AddBackward0>)\n",
      "epoch: 9766 loss is tensor([-0.4511], grad_fn=<AddBackward0>)\n",
      "epoch: 9767 loss is tensor([-0.4324], grad_fn=<AddBackward0>)\n",
      "epoch: 9768 loss is tensor([-0.3867], grad_fn=<AddBackward0>)\n",
      "epoch: 9769 loss is tensor([-0.4079], grad_fn=<AddBackward0>)\n",
      "epoch: 9770 loss is tensor([-0.4332], grad_fn=<AddBackward0>)\n",
      "epoch: 9771 loss is tensor([-0.4062], grad_fn=<AddBackward0>)\n",
      "epoch: 9772 loss is tensor([-0.3755], grad_fn=<AddBackward0>)\n",
      "epoch: 9773 loss is tensor([-0.4259], grad_fn=<AddBackward0>)\n",
      "epoch: 9774 loss is tensor([-0.4358], grad_fn=<AddBackward0>)\n",
      "epoch: 9775 loss is tensor([-0.4428], grad_fn=<AddBackward0>)\n",
      "epoch: 9776 loss is tensor([-0.4289], grad_fn=<AddBackward0>)\n",
      "epoch: 9777 loss is tensor([-0.4313], grad_fn=<AddBackward0>)\n",
      "epoch: 9778 loss is tensor([-0.3856], grad_fn=<AddBackward0>)\n",
      "epoch: 9779 loss is tensor([-0.4257], grad_fn=<AddBackward0>)\n",
      "epoch: 9780 loss is tensor([-0.3426], grad_fn=<AddBackward0>)\n",
      "epoch: 9781 loss is tensor([-0.3743], grad_fn=<AddBackward0>)\n",
      "epoch: 9782 loss is tensor([-0.4534], grad_fn=<AddBackward0>)\n",
      "epoch: 9783 loss is tensor([-0.4066], grad_fn=<AddBackward0>)\n",
      "epoch: 9784 loss is tensor([-0.4087], grad_fn=<AddBackward0>)\n",
      "epoch: 9785 loss is tensor([-0.3950], grad_fn=<AddBackward0>)\n",
      "epoch: 9786 loss is tensor([-0.4071], grad_fn=<AddBackward0>)\n",
      "epoch: 9787 loss is tensor([-0.3963], grad_fn=<AddBackward0>)\n",
      "epoch: 9788 loss is tensor([-0.4528], grad_fn=<AddBackward0>)\n",
      "epoch: 9789 loss is tensor([-0.4627], grad_fn=<AddBackward0>)\n",
      "epoch: 9790 loss is tensor([-0.4515], grad_fn=<AddBackward0>)\n",
      "epoch: 9791 loss is tensor([-0.4327], grad_fn=<AddBackward0>)\n",
      "epoch: 9792 loss is tensor([-0.4264], grad_fn=<AddBackward0>)\n",
      "epoch: 9793 loss is tensor([-0.4501], grad_fn=<AddBackward0>)\n",
      "epoch: 9794 loss is tensor([-0.4625], grad_fn=<AddBackward0>)\n",
      "epoch: 9795 loss is tensor([-0.4857], grad_fn=<AddBackward0>)\n",
      "epoch: 9796 loss is tensor([-0.4109], grad_fn=<AddBackward0>)\n",
      "epoch: 9797 loss is tensor([-0.4162], grad_fn=<AddBackward0>)\n",
      "epoch: 9798 loss is tensor([-0.4872], grad_fn=<AddBackward0>)\n",
      "epoch: 9799 loss is tensor([-0.4377], grad_fn=<AddBackward0>)\n",
      "epoch: 9800 loss is tensor([-0.4377], grad_fn=<AddBackward0>)\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9801 loss is tensor([-0.4883], grad_fn=<AddBackward0>)\n",
      "epoch: 9802 loss is tensor([-0.4199], grad_fn=<AddBackward0>)\n",
      "epoch: 9803 loss is tensor([-0.4137], grad_fn=<AddBackward0>)\n",
      "epoch: 9804 loss is tensor([-0.4302], grad_fn=<AddBackward0>)\n",
      "epoch: 9805 loss is tensor([-0.4237], grad_fn=<AddBackward0>)\n",
      "epoch: 9806 loss is tensor([-0.4689], grad_fn=<AddBackward0>)\n",
      "epoch: 9807 loss is tensor([-0.4933], grad_fn=<AddBackward0>)\n",
      "epoch: 9808 loss is tensor([-0.4959], grad_fn=<AddBackward0>)\n",
      "epoch: 9809 loss is tensor([-0.4197], grad_fn=<AddBackward0>)\n",
      "epoch: 9810 loss is tensor([-0.4945], grad_fn=<AddBackward0>)\n",
      "epoch: 9811 loss is tensor([-0.4472], grad_fn=<AddBackward0>)\n",
      "epoch: 9812 loss is tensor([-0.4357], grad_fn=<AddBackward0>)\n",
      "epoch: 9813 loss is tensor([-0.4929], grad_fn=<AddBackward0>)\n",
      "epoch: 9814 loss is tensor([-0.4497], grad_fn=<AddBackward0>)\n",
      "epoch: 9815 loss is tensor([-0.4201], grad_fn=<AddBackward0>)\n",
      "epoch: 9816 loss is tensor([-0.3980], grad_fn=<AddBackward0>)\n",
      "epoch: 9817 loss is tensor([-0.4166], grad_fn=<AddBackward0>)\n",
      "epoch: 9818 loss is tensor([-0.4003], grad_fn=<AddBackward0>)\n",
      "epoch: 9819 loss is tensor([-0.3851], grad_fn=<AddBackward0>)\n",
      "epoch: 9820 loss is tensor([-0.3984], grad_fn=<AddBackward0>)\n",
      "epoch: 9821 loss is tensor([-0.4531], grad_fn=<AddBackward0>)\n",
      "epoch: 9822 loss is tensor([-0.4097], grad_fn=<AddBackward0>)\n",
      "epoch: 9823 loss is tensor([-0.4538], grad_fn=<AddBackward0>)\n",
      "epoch: 9824 loss is tensor([-0.4090], grad_fn=<AddBackward0>)\n",
      "epoch: 9825 loss is tensor([-0.4246], grad_fn=<AddBackward0>)\n",
      "epoch: 9826 loss is tensor([-0.4087], grad_fn=<AddBackward0>)\n",
      "epoch: 9827 loss is tensor([-0.4360], grad_fn=<AddBackward0>)\n",
      "epoch: 9828 loss is tensor([-0.4153], grad_fn=<AddBackward0>)\n",
      "epoch: 9829 loss is tensor([-0.4510], grad_fn=<AddBackward0>)\n",
      "epoch: 9830 loss is tensor([-0.3955], grad_fn=<AddBackward0>)\n",
      "epoch: 9831 loss is tensor([-0.4114], grad_fn=<AddBackward0>)\n",
      "epoch: 9832 loss is tensor([-0.4319], grad_fn=<AddBackward0>)\n",
      "epoch: 9833 loss is tensor([-0.4008], grad_fn=<AddBackward0>)\n",
      "epoch: 9834 loss is tensor([-0.4391], grad_fn=<AddBackward0>)\n",
      "epoch: 9835 loss is tensor([-0.4432], grad_fn=<AddBackward0>)\n",
      "epoch: 9836 loss is tensor([-0.4634], grad_fn=<AddBackward0>)\n",
      "epoch: 9837 loss is tensor([-0.4133], grad_fn=<AddBackward0>)\n",
      "epoch: 9838 loss is tensor([-0.4550], grad_fn=<AddBackward0>)\n",
      "epoch: 9839 loss is tensor([-0.3979], grad_fn=<AddBackward0>)\n",
      "epoch: 9840 loss is tensor([-0.4255], grad_fn=<AddBackward0>)\n",
      "epoch: 9841 loss is tensor([-0.3660], grad_fn=<AddBackward0>)\n",
      "epoch: 9842 loss is tensor([-0.4548], grad_fn=<AddBackward0>)\n",
      "epoch: 9843 loss is tensor([-0.3832], grad_fn=<AddBackward0>)\n",
      "epoch: 9844 loss is tensor([-0.2744], grad_fn=<AddBackward0>)\n",
      "epoch: 9845 loss is tensor([-0.3928], grad_fn=<AddBackward0>)\n",
      "epoch: 9846 loss is tensor([-0.4574], grad_fn=<AddBackward0>)\n",
      "epoch: 9847 loss is tensor([-0.3795], grad_fn=<AddBackward0>)\n",
      "epoch: 9848 loss is tensor([-0.2831], grad_fn=<AddBackward0>)\n",
      "epoch: 9849 loss is tensor([-0.3351], grad_fn=<AddBackward0>)\n",
      "epoch: 9850 loss is tensor([-0.3795], grad_fn=<AddBackward0>)\n",
      "epoch: 9851 loss is tensor([-0.3395], grad_fn=<AddBackward0>)\n",
      "epoch: 9852 loss is tensor([-0.3609], grad_fn=<AddBackward0>)\n",
      "epoch: 9853 loss is tensor([-0.3685], grad_fn=<AddBackward0>)\n",
      "epoch: 9854 loss is tensor([-0.4538], grad_fn=<AddBackward0>)\n",
      "epoch: 9855 loss is tensor([-0.3706], grad_fn=<AddBackward0>)\n",
      "epoch: 9856 loss is tensor([-0.3599], grad_fn=<AddBackward0>)\n",
      "epoch: 9857 loss is tensor([-0.3754], grad_fn=<AddBackward0>)\n",
      "epoch: 9858 loss is tensor([-0.4050], grad_fn=<AddBackward0>)\n",
      "epoch: 9859 loss is tensor([-0.4225], grad_fn=<AddBackward0>)\n",
      "epoch: 9860 loss is tensor([-0.3646], grad_fn=<AddBackward0>)\n",
      "epoch: 9861 loss is tensor([-0.4171], grad_fn=<AddBackward0>)\n",
      "epoch: 9862 loss is tensor([-0.4518], grad_fn=<AddBackward0>)\n",
      "epoch: 9863 loss is tensor([-0.4002], grad_fn=<AddBackward0>)\n",
      "epoch: 9864 loss is tensor([-0.4184], grad_fn=<AddBackward0>)\n",
      "epoch: 9865 loss is tensor([-0.3957], grad_fn=<AddBackward0>)\n",
      "epoch: 9866 loss is tensor([-0.4456], grad_fn=<AddBackward0>)\n",
      "epoch: 9867 loss is tensor([-0.4112], grad_fn=<AddBackward0>)\n",
      "epoch: 9868 loss is tensor([-0.4403], grad_fn=<AddBackward0>)\n",
      "epoch: 9869 loss is tensor([-0.3977], grad_fn=<AddBackward0>)\n",
      "epoch: 9870 loss is tensor([-0.4560], grad_fn=<AddBackward0>)\n",
      "epoch: 9871 loss is tensor([-0.4359], grad_fn=<AddBackward0>)\n",
      "epoch: 9872 loss is tensor([-0.4806], grad_fn=<AddBackward0>)\n",
      "epoch: 9873 loss is tensor([-0.3898], grad_fn=<AddBackward0>)\n",
      "epoch: 9874 loss is tensor([-0.3708], grad_fn=<AddBackward0>)\n",
      "epoch: 9875 loss is tensor([-0.3969], grad_fn=<AddBackward0>)\n",
      "epoch: 9876 loss is tensor([-0.4284], grad_fn=<AddBackward0>)\n",
      "epoch: 9877 loss is tensor([-0.4223], grad_fn=<AddBackward0>)\n",
      "epoch: 9878 loss is tensor([-0.3997], grad_fn=<AddBackward0>)\n",
      "epoch: 9879 loss is tensor([-0.3922], grad_fn=<AddBackward0>)\n",
      "epoch: 9880 loss is tensor([-0.3701], grad_fn=<AddBackward0>)\n",
      "epoch: 9881 loss is tensor([-0.4332], grad_fn=<AddBackward0>)\n",
      "epoch: 9882 loss is tensor([-0.4307], grad_fn=<AddBackward0>)\n",
      "epoch: 9883 loss is tensor([-0.4373], grad_fn=<AddBackward0>)\n",
      "epoch: 9884 loss is tensor([-0.4327], grad_fn=<AddBackward0>)\n",
      "epoch: 9885 loss is tensor([-0.4383], grad_fn=<AddBackward0>)\n",
      "epoch: 9886 loss is tensor([-0.3385], grad_fn=<AddBackward0>)\n",
      "epoch: 9887 loss is tensor([-0.3994], grad_fn=<AddBackward0>)\n",
      "epoch: 9888 loss is tensor([-0.4581], grad_fn=<AddBackward0>)\n",
      "epoch: 9889 loss is tensor([-0.3541], grad_fn=<AddBackward0>)\n",
      "epoch: 9890 loss is tensor([-0.3976], grad_fn=<AddBackward0>)\n",
      "epoch: 9891 loss is tensor([-0.3747], grad_fn=<AddBackward0>)\n",
      "epoch: 9892 loss is tensor([-0.4098], grad_fn=<AddBackward0>)\n",
      "epoch: 9893 loss is tensor([-0.3481], grad_fn=<AddBackward0>)\n",
      "epoch: 9894 loss is tensor([-0.4016], grad_fn=<AddBackward0>)\n",
      "epoch: 9895 loss is tensor([-0.4235], grad_fn=<AddBackward0>)\n",
      "epoch: 9896 loss is tensor([-0.4028], grad_fn=<AddBackward0>)\n",
      "epoch: 9897 loss is tensor([-0.4429], grad_fn=<AddBackward0>)\n",
      "epoch: 9898 loss is tensor([-0.4600], grad_fn=<AddBackward0>)\n",
      "epoch: 9899 loss is tensor([-0.4425], grad_fn=<AddBackward0>)\n",
      "epoch: 9900 loss is tensor([-0.4258], grad_fn=<AddBackward0>)\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9901 loss is tensor([-0.3293], grad_fn=<AddBackward0>)\n",
      "epoch: 9902 loss is tensor([-0.3861], grad_fn=<AddBackward0>)\n",
      "epoch: 9903 loss is tensor([-0.3778], grad_fn=<AddBackward0>)\n",
      "epoch: 9904 loss is tensor([-0.3713], grad_fn=<AddBackward0>)\n",
      "epoch: 9905 loss is tensor([-0.4249], grad_fn=<AddBackward0>)\n",
      "epoch: 9906 loss is tensor([-0.3195], grad_fn=<AddBackward0>)\n",
      "epoch: 9907 loss is tensor([-0.4156], grad_fn=<AddBackward0>)\n",
      "epoch: 9908 loss is tensor([-0.4031], grad_fn=<AddBackward0>)\n",
      "epoch: 9909 loss is tensor([-0.3079], grad_fn=<AddBackward0>)\n",
      "epoch: 9910 loss is tensor([-0.3923], grad_fn=<AddBackward0>)\n",
      "epoch: 9911 loss is tensor([-0.3158], grad_fn=<AddBackward0>)\n",
      "epoch: 9912 loss is tensor([-0.4320], grad_fn=<AddBackward0>)\n",
      "epoch: 9913 loss is tensor([-0.3926], grad_fn=<AddBackward0>)\n",
      "epoch: 9914 loss is tensor([-0.4250], grad_fn=<AddBackward0>)\n",
      "epoch: 9915 loss is tensor([-0.3847], grad_fn=<AddBackward0>)\n",
      "epoch: 9916 loss is tensor([-0.4054], grad_fn=<AddBackward0>)\n",
      "epoch: 9917 loss is tensor([-0.4231], grad_fn=<AddBackward0>)\n",
      "epoch: 9918 loss is tensor([-0.4474], grad_fn=<AddBackward0>)\n",
      "epoch: 9919 loss is tensor([-0.3816], grad_fn=<AddBackward0>)\n",
      "epoch: 9920 loss is tensor([-0.3908], grad_fn=<AddBackward0>)\n",
      "epoch: 9921 loss is tensor([-0.4065], grad_fn=<AddBackward0>)\n",
      "epoch: 9922 loss is tensor([-0.3869], grad_fn=<AddBackward0>)\n",
      "epoch: 9923 loss is tensor([-0.3917], grad_fn=<AddBackward0>)\n",
      "epoch: 9924 loss is tensor([-0.3861], grad_fn=<AddBackward0>)\n",
      "epoch: 9925 loss is tensor([-0.3933], grad_fn=<AddBackward0>)\n",
      "epoch: 9926 loss is tensor([-0.3539], grad_fn=<AddBackward0>)\n",
      "epoch: 9927 loss is tensor([-0.3810], grad_fn=<AddBackward0>)\n",
      "epoch: 9928 loss is tensor([-0.4003], grad_fn=<AddBackward0>)\n",
      "epoch: 9929 loss is tensor([-0.4236], grad_fn=<AddBackward0>)\n",
      "epoch: 9930 loss is tensor([-0.4349], grad_fn=<AddBackward0>)\n",
      "epoch: 9931 loss is tensor([-0.3725], grad_fn=<AddBackward0>)\n",
      "epoch: 9932 loss is tensor([-0.4201], grad_fn=<AddBackward0>)\n",
      "epoch: 9933 loss is tensor([-0.4127], grad_fn=<AddBackward0>)\n",
      "epoch: 9934 loss is tensor([-0.4307], grad_fn=<AddBackward0>)\n",
      "epoch: 9935 loss is tensor([-0.4477], grad_fn=<AddBackward0>)\n",
      "epoch: 9936 loss is tensor([-0.4212], grad_fn=<AddBackward0>)\n",
      "epoch: 9937 loss is tensor([-0.4068], grad_fn=<AddBackward0>)\n",
      "epoch: 9938 loss is tensor([-0.3853], grad_fn=<AddBackward0>)\n",
      "epoch: 9939 loss is tensor([-0.3742], grad_fn=<AddBackward0>)\n",
      "epoch: 9940 loss is tensor([-0.4512], grad_fn=<AddBackward0>)\n",
      "epoch: 9941 loss is tensor([-0.3591], grad_fn=<AddBackward0>)\n",
      "epoch: 9942 loss is tensor([-0.4353], grad_fn=<AddBackward0>)\n",
      "epoch: 9943 loss is tensor([-0.4363], grad_fn=<AddBackward0>)\n",
      "epoch: 9944 loss is tensor([-0.4575], grad_fn=<AddBackward0>)\n",
      "epoch: 9945 loss is tensor([-0.3928], grad_fn=<AddBackward0>)\n",
      "epoch: 9946 loss is tensor([-0.4087], grad_fn=<AddBackward0>)\n",
      "epoch: 9947 loss is tensor([-0.3730], grad_fn=<AddBackward0>)\n",
      "epoch: 9948 loss is tensor([-0.4074], grad_fn=<AddBackward0>)\n",
      "epoch: 9949 loss is tensor([-0.4180], grad_fn=<AddBackward0>)\n",
      "epoch: 9950 loss is tensor([-0.4299], grad_fn=<AddBackward0>)\n",
      "epoch: 9951 loss is tensor([-0.4452], grad_fn=<AddBackward0>)\n",
      "epoch: 9952 loss is tensor([-0.3800], grad_fn=<AddBackward0>)\n",
      "epoch: 9953 loss is tensor([-0.4344], grad_fn=<AddBackward0>)\n",
      "epoch: 9954 loss is tensor([-0.3550], grad_fn=<AddBackward0>)\n",
      "epoch: 9955 loss is tensor([-0.2992], grad_fn=<AddBackward0>)\n",
      "epoch: 9956 loss is tensor([-0.3678], grad_fn=<AddBackward0>)\n",
      "epoch: 9957 loss is tensor([-0.3452], grad_fn=<AddBackward0>)\n",
      "epoch: 9958 loss is tensor([-0.3349], grad_fn=<AddBackward0>)\n",
      "epoch: 9959 loss is tensor([-0.3839], grad_fn=<AddBackward0>)\n",
      "epoch: 9960 loss is tensor([-0.3764], grad_fn=<AddBackward0>)\n",
      "epoch: 9961 loss is tensor([-0.3500], grad_fn=<AddBackward0>)\n",
      "epoch: 9962 loss is tensor([-0.4141], grad_fn=<AddBackward0>)\n",
      "epoch: 9963 loss is tensor([-0.3383], grad_fn=<AddBackward0>)\n",
      "epoch: 9964 loss is tensor([-0.3491], grad_fn=<AddBackward0>)\n",
      "epoch: 9965 loss is tensor([-0.3558], grad_fn=<AddBackward0>)\n",
      "epoch: 9966 loss is tensor([-0.3244], grad_fn=<AddBackward0>)\n",
      "epoch: 9967 loss is tensor([-0.3551], grad_fn=<AddBackward0>)\n",
      "epoch: 9968 loss is tensor([-0.4025], grad_fn=<AddBackward0>)\n",
      "epoch: 9969 loss is tensor([-0.4049], grad_fn=<AddBackward0>)\n",
      "epoch: 9970 loss is tensor([-0.4239], grad_fn=<AddBackward0>)\n",
      "epoch: 9971 loss is tensor([-0.4149], grad_fn=<AddBackward0>)\n",
      "epoch: 9972 loss is tensor([-0.4034], grad_fn=<AddBackward0>)\n",
      "epoch: 9973 loss is tensor([-0.3688], grad_fn=<AddBackward0>)\n",
      "epoch: 9974 loss is tensor([-0.4337], grad_fn=<AddBackward0>)\n",
      "epoch: 9975 loss is tensor([-0.4206], grad_fn=<AddBackward0>)\n",
      "epoch: 9976 loss is tensor([-0.3686], grad_fn=<AddBackward0>)\n",
      "epoch: 9977 loss is tensor([-0.4358], grad_fn=<AddBackward0>)\n",
      "epoch: 9978 loss is tensor([-0.4112], grad_fn=<AddBackward0>)\n",
      "epoch: 9979 loss is tensor([-0.4388], grad_fn=<AddBackward0>)\n",
      "epoch: 9980 loss is tensor([-0.4830], grad_fn=<AddBackward0>)\n",
      "epoch: 9981 loss is tensor([-0.4387], grad_fn=<AddBackward0>)\n",
      "epoch: 9982 loss is tensor([-0.4600], grad_fn=<AddBackward0>)\n",
      "epoch: 9983 loss is tensor([-0.3814], grad_fn=<AddBackward0>)\n",
      "epoch: 9984 loss is tensor([-0.4685], grad_fn=<AddBackward0>)\n",
      "epoch: 9985 loss is tensor([-0.4703], grad_fn=<AddBackward0>)\n",
      "epoch: 9986 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 9987 loss is tensor([-0.4540], grad_fn=<AddBackward0>)\n",
      "epoch: 9988 loss is tensor([-0.4691], grad_fn=<AddBackward0>)\n",
      "epoch: 9989 loss is tensor([-0.4132], grad_fn=<AddBackward0>)\n",
      "epoch: 9990 loss is tensor([-0.3958], grad_fn=<AddBackward0>)\n",
      "epoch: 9991 loss is tensor([-0.4380], grad_fn=<AddBackward0>)\n",
      "epoch: 9992 loss is tensor([-0.4369], grad_fn=<AddBackward0>)\n",
      "epoch: 9993 loss is tensor([-0.4555], grad_fn=<AddBackward0>)\n",
      "epoch: 9994 loss is tensor([-0.4350], grad_fn=<AddBackward0>)\n",
      "epoch: 9995 loss is tensor([-0.4663], grad_fn=<AddBackward0>)\n",
      "epoch: 9996 loss is tensor([-0.4369], grad_fn=<AddBackward0>)\n",
      "epoch: 9997 loss is tensor([-0.3813], grad_fn=<AddBackward0>)\n",
      "epoch: 9998 loss is tensor([-0.4746], grad_fn=<AddBackward0>)\n",
      "epoch: 9999 loss is tensor([-0.4528], grad_fn=<AddBackward0>)\n",
      "epoch: 10000 loss is tensor([-0.4542], grad_fn=<AddBackward0>)\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10001 loss is tensor([-0.4110], grad_fn=<AddBackward0>)\n",
      "epoch: 10002 loss is tensor([-0.4059], grad_fn=<AddBackward0>)\n",
      "epoch: 10003 loss is tensor([-0.4880], grad_fn=<AddBackward0>)\n",
      "epoch: 10004 loss is tensor([-0.3917], grad_fn=<AddBackward0>)\n",
      "epoch: 10005 loss is tensor([-0.4902], grad_fn=<AddBackward0>)\n",
      "epoch: 10006 loss is tensor([-0.4168], grad_fn=<AddBackward0>)\n",
      "epoch: 10007 loss is tensor([-0.4301], grad_fn=<AddBackward0>)\n",
      "epoch: 10008 loss is tensor([-0.5265], grad_fn=<AddBackward0>)\n",
      "epoch: 10009 loss is tensor([-0.4891], grad_fn=<AddBackward0>)\n",
      "epoch: 10010 loss is tensor([-0.5126], grad_fn=<AddBackward0>)\n",
      "epoch: 10011 loss is tensor([-0.4585], grad_fn=<AddBackward0>)\n",
      "epoch: 10012 loss is tensor([-0.4100], grad_fn=<AddBackward0>)\n",
      "epoch: 10013 loss is tensor([-0.4744], grad_fn=<AddBackward0>)\n",
      "epoch: 10014 loss is tensor([-0.3836], grad_fn=<AddBackward0>)\n",
      "epoch: 10015 loss is tensor([-0.4792], grad_fn=<AddBackward0>)\n",
      "epoch: 10016 loss is tensor([-0.4071], grad_fn=<AddBackward0>)\n",
      "epoch: 10017 loss is tensor([-0.4446], grad_fn=<AddBackward0>)\n",
      "epoch: 10018 loss is tensor([-0.4572], grad_fn=<AddBackward0>)\n",
      "epoch: 10019 loss is tensor([-0.3490], grad_fn=<AddBackward0>)\n",
      "epoch: 10020 loss is tensor([-0.4354], grad_fn=<AddBackward0>)\n",
      "epoch: 10021 loss is tensor([-0.3875], grad_fn=<AddBackward0>)\n",
      "epoch: 10022 loss is tensor([-0.3894], grad_fn=<AddBackward0>)\n",
      "epoch: 10023 loss is tensor([-0.4353], grad_fn=<AddBackward0>)\n",
      "epoch: 10024 loss is tensor([-0.4271], grad_fn=<AddBackward0>)\n",
      "epoch: 10025 loss is tensor([-0.4054], grad_fn=<AddBackward0>)\n",
      "epoch: 10026 loss is tensor([-0.4371], grad_fn=<AddBackward0>)\n",
      "epoch: 10027 loss is tensor([-0.4579], grad_fn=<AddBackward0>)\n",
      "epoch: 10028 loss is tensor([-0.4864], grad_fn=<AddBackward0>)\n",
      "epoch: 10029 loss is tensor([-0.4772], grad_fn=<AddBackward0>)\n",
      "epoch: 10030 loss is tensor([-0.4135], grad_fn=<AddBackward0>)\n",
      "epoch: 10031 loss is tensor([-0.4036], grad_fn=<AddBackward0>)\n",
      "epoch: 10032 loss is tensor([-0.3787], grad_fn=<AddBackward0>)\n",
      "epoch: 10033 loss is tensor([-0.4351], grad_fn=<AddBackward0>)\n",
      "epoch: 10034 loss is tensor([-0.3267], grad_fn=<AddBackward0>)\n",
      "epoch: 10035 loss is tensor([-0.4112], grad_fn=<AddBackward0>)\n",
      "epoch: 10036 loss is tensor([-0.3890], grad_fn=<AddBackward0>)\n",
      "epoch: 10037 loss is tensor([-0.3568], grad_fn=<AddBackward0>)\n",
      "epoch: 10038 loss is tensor([-0.3838], grad_fn=<AddBackward0>)\n",
      "epoch: 10039 loss is tensor([-0.1983], grad_fn=<AddBackward0>)\n",
      "epoch: 10040 loss is tensor([-0.2712], grad_fn=<AddBackward0>)\n",
      "epoch: 10041 loss is tensor([-0.3939], grad_fn=<AddBackward0>)\n",
      "epoch: 10042 loss is tensor([-0.3869], grad_fn=<AddBackward0>)\n",
      "epoch: 10043 loss is tensor([-0.3277], grad_fn=<AddBackward0>)\n",
      "epoch: 10044 loss is tensor([-0.3941], grad_fn=<AddBackward0>)\n",
      "epoch: 10045 loss is tensor([-0.3496], grad_fn=<AddBackward0>)\n",
      "epoch: 10046 loss is tensor([-0.3945], grad_fn=<AddBackward0>)\n",
      "epoch: 10047 loss is tensor([-0.4051], grad_fn=<AddBackward0>)\n",
      "epoch: 10048 loss is tensor([-0.4319], grad_fn=<AddBackward0>)\n",
      "epoch: 10049 loss is tensor([-0.4376], grad_fn=<AddBackward0>)\n",
      "epoch: 10050 loss is tensor([-0.3893], grad_fn=<AddBackward0>)\n",
      "epoch: 10051 loss is tensor([-0.4375], grad_fn=<AddBackward0>)\n",
      "epoch: 10052 loss is tensor([-0.4400], grad_fn=<AddBackward0>)\n",
      "epoch: 10053 loss is tensor([-0.3886], grad_fn=<AddBackward0>)\n",
      "epoch: 10054 loss is tensor([-0.3626], grad_fn=<AddBackward0>)\n",
      "epoch: 10055 loss is tensor([-0.4367], grad_fn=<AddBackward0>)\n",
      "epoch: 10056 loss is tensor([-0.3603], grad_fn=<AddBackward0>)\n",
      "epoch: 10057 loss is tensor([-0.3957], grad_fn=<AddBackward0>)\n",
      "epoch: 10058 loss is tensor([-0.4209], grad_fn=<AddBackward0>)\n",
      "epoch: 10059 loss is tensor([-0.4001], grad_fn=<AddBackward0>)\n",
      "epoch: 10060 loss is tensor([-0.3681], grad_fn=<AddBackward0>)\n",
      "epoch: 10061 loss is tensor([-0.4118], grad_fn=<AddBackward0>)\n",
      "epoch: 10062 loss is tensor([-0.3748], grad_fn=<AddBackward0>)\n",
      "epoch: 10063 loss is tensor([-0.2352], grad_fn=<AddBackward0>)\n",
      "epoch: 10064 loss is tensor([-0.2056], grad_fn=<AddBackward0>)\n",
      "epoch: 10065 loss is tensor([-0.3087], grad_fn=<AddBackward0>)\n",
      "epoch: 10066 loss is tensor([-0.3649], grad_fn=<AddBackward0>)\n",
      "epoch: 10067 loss is tensor([-0.3403], grad_fn=<AddBackward0>)\n",
      "epoch: 10068 loss is tensor([-0.3506], grad_fn=<AddBackward0>)\n",
      "epoch: 10069 loss is tensor([-0.3452], grad_fn=<AddBackward0>)\n",
      "epoch: 10070 loss is tensor([-0.3913], grad_fn=<AddBackward0>)\n",
      "epoch: 10071 loss is tensor([-0.3591], grad_fn=<AddBackward0>)\n",
      "epoch: 10072 loss is tensor([-0.4381], grad_fn=<AddBackward0>)\n",
      "epoch: 10073 loss is tensor([-0.3547], grad_fn=<AddBackward0>)\n",
      "epoch: 10074 loss is tensor([-0.3985], grad_fn=<AddBackward0>)\n",
      "epoch: 10075 loss is tensor([-0.3971], grad_fn=<AddBackward0>)\n",
      "epoch: 10076 loss is tensor([-0.3731], grad_fn=<AddBackward0>)\n",
      "epoch: 10077 loss is tensor([-0.4275], grad_fn=<AddBackward0>)\n",
      "epoch: 10078 loss is tensor([-0.3631], grad_fn=<AddBackward0>)\n",
      "epoch: 10079 loss is tensor([-0.3913], grad_fn=<AddBackward0>)\n",
      "epoch: 10080 loss is tensor([-0.4363], grad_fn=<AddBackward0>)\n",
      "epoch: 10081 loss is tensor([-0.3898], grad_fn=<AddBackward0>)\n",
      "epoch: 10082 loss is tensor([-0.3447], grad_fn=<AddBackward0>)\n",
      "epoch: 10083 loss is tensor([-0.3761], grad_fn=<AddBackward0>)\n",
      "epoch: 10084 loss is tensor([-0.4075], grad_fn=<AddBackward0>)\n",
      "epoch: 10085 loss is tensor([-0.3698], grad_fn=<AddBackward0>)\n",
      "epoch: 10086 loss is tensor([-0.4546], grad_fn=<AddBackward0>)\n",
      "epoch: 10087 loss is tensor([-0.4038], grad_fn=<AddBackward0>)\n",
      "epoch: 10088 loss is tensor([-0.3931], grad_fn=<AddBackward0>)\n",
      "epoch: 10089 loss is tensor([-0.4235], grad_fn=<AddBackward0>)\n",
      "epoch: 10090 loss is tensor([-0.4548], grad_fn=<AddBackward0>)\n",
      "epoch: 10091 loss is tensor([-0.4069], grad_fn=<AddBackward0>)\n",
      "epoch: 10092 loss is tensor([-0.3972], grad_fn=<AddBackward0>)\n",
      "epoch: 10093 loss is tensor([-0.4454], grad_fn=<AddBackward0>)\n",
      "epoch: 10094 loss is tensor([-0.4368], grad_fn=<AddBackward0>)\n",
      "epoch: 10095 loss is tensor([-0.4286], grad_fn=<AddBackward0>)\n",
      "epoch: 10096 loss is tensor([-0.4478], grad_fn=<AddBackward0>)\n",
      "epoch: 10097 loss is tensor([-0.4239], grad_fn=<AddBackward0>)\n",
      "epoch: 10098 loss is tensor([-0.4013], grad_fn=<AddBackward0>)\n",
      "epoch: 10099 loss is tensor([-0.3423], grad_fn=<AddBackward0>)\n",
      "epoch: 10100 loss is tensor([-0.3710], grad_fn=<AddBackward0>)\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10101 loss is tensor([-0.3805], grad_fn=<AddBackward0>)\n",
      "epoch: 10102 loss is tensor([-0.3538], grad_fn=<AddBackward0>)\n",
      "epoch: 10103 loss is tensor([-0.4587], grad_fn=<AddBackward0>)\n",
      "epoch: 10104 loss is tensor([-0.4228], grad_fn=<AddBackward0>)\n",
      "epoch: 10105 loss is tensor([-0.3843], grad_fn=<AddBackward0>)\n",
      "epoch: 10106 loss is tensor([-0.4138], grad_fn=<AddBackward0>)\n",
      "epoch: 10107 loss is tensor([-0.4447], grad_fn=<AddBackward0>)\n",
      "epoch: 10108 loss is tensor([-0.4507], grad_fn=<AddBackward0>)\n",
      "epoch: 10109 loss is tensor([-0.4080], grad_fn=<AddBackward0>)\n",
      "epoch: 10110 loss is tensor([-0.4416], grad_fn=<AddBackward0>)\n",
      "epoch: 10111 loss is tensor([-0.4083], grad_fn=<AddBackward0>)\n",
      "epoch: 10112 loss is tensor([-0.4068], grad_fn=<AddBackward0>)\n",
      "epoch: 10113 loss is tensor([-0.4405], grad_fn=<AddBackward0>)\n",
      "epoch: 10114 loss is tensor([-0.3991], grad_fn=<AddBackward0>)\n",
      "epoch: 10115 loss is tensor([-0.3740], grad_fn=<AddBackward0>)\n",
      "epoch: 10116 loss is tensor([-0.4369], grad_fn=<AddBackward0>)\n",
      "epoch: 10117 loss is tensor([-0.3871], grad_fn=<AddBackward0>)\n",
      "epoch: 10118 loss is tensor([-0.4309], grad_fn=<AddBackward0>)\n",
      "epoch: 10119 loss is tensor([-0.3539], grad_fn=<AddBackward0>)\n",
      "epoch: 10120 loss is tensor([-0.4005], grad_fn=<AddBackward0>)\n",
      "epoch: 10121 loss is tensor([-0.3768], grad_fn=<AddBackward0>)\n",
      "epoch: 10122 loss is tensor([-0.4060], grad_fn=<AddBackward0>)\n",
      "epoch: 10123 loss is tensor([-0.4333], grad_fn=<AddBackward0>)\n",
      "epoch: 10124 loss is tensor([-0.3506], grad_fn=<AddBackward0>)\n",
      "epoch: 10125 loss is tensor([-0.4062], grad_fn=<AddBackward0>)\n",
      "epoch: 10126 loss is tensor([-0.4037], grad_fn=<AddBackward0>)\n",
      "epoch: 10127 loss is tensor([-0.4164], grad_fn=<AddBackward0>)\n",
      "epoch: 10128 loss is tensor([-0.4610], grad_fn=<AddBackward0>)\n",
      "epoch: 10129 loss is tensor([-0.4561], grad_fn=<AddBackward0>)\n",
      "epoch: 10130 loss is tensor([-0.3871], grad_fn=<AddBackward0>)\n",
      "epoch: 10131 loss is tensor([-0.4335], grad_fn=<AddBackward0>)\n",
      "epoch: 10132 loss is tensor([-0.4377], grad_fn=<AddBackward0>)\n",
      "epoch: 10133 loss is tensor([-0.4714], grad_fn=<AddBackward0>)\n",
      "epoch: 10134 loss is tensor([-0.4352], grad_fn=<AddBackward0>)\n",
      "epoch: 10135 loss is tensor([-0.4370], grad_fn=<AddBackward0>)\n",
      "epoch: 10136 loss is tensor([-0.4206], grad_fn=<AddBackward0>)\n",
      "epoch: 10137 loss is tensor([-0.4374], grad_fn=<AddBackward0>)\n",
      "epoch: 10138 loss is tensor([-0.4659], grad_fn=<AddBackward0>)\n",
      "epoch: 10139 loss is tensor([-0.4390], grad_fn=<AddBackward0>)\n",
      "epoch: 10140 loss is tensor([-0.4776], grad_fn=<AddBackward0>)\n",
      "epoch: 10141 loss is tensor([-0.4136], grad_fn=<AddBackward0>)\n",
      "epoch: 10142 loss is tensor([-0.4465], grad_fn=<AddBackward0>)\n",
      "epoch: 10143 loss is tensor([-0.4754], grad_fn=<AddBackward0>)\n",
      "epoch: 10144 loss is tensor([-0.4536], grad_fn=<AddBackward0>)\n",
      "epoch: 10145 loss is tensor([-0.5129], grad_fn=<AddBackward0>)\n",
      "epoch: 10146 loss is tensor([-0.4543], grad_fn=<AddBackward0>)\n",
      "epoch: 10147 loss is tensor([-0.4439], grad_fn=<AddBackward0>)\n",
      "epoch: 10148 loss is tensor([-0.4364], grad_fn=<AddBackward0>)\n",
      "epoch: 10149 loss is tensor([-0.4453], grad_fn=<AddBackward0>)\n",
      "epoch: 10150 loss is tensor([-0.3913], grad_fn=<AddBackward0>)\n",
      "epoch: 10151 loss is tensor([-0.4523], grad_fn=<AddBackward0>)\n",
      "epoch: 10152 loss is tensor([-0.4250], grad_fn=<AddBackward0>)\n",
      "epoch: 10153 loss is tensor([-0.4531], grad_fn=<AddBackward0>)\n",
      "epoch: 10154 loss is tensor([-0.4928], grad_fn=<AddBackward0>)\n",
      "epoch: 10155 loss is tensor([-0.4466], grad_fn=<AddBackward0>)\n",
      "epoch: 10156 loss is tensor([-0.4711], grad_fn=<AddBackward0>)\n",
      "epoch: 10157 loss is tensor([-0.4531], grad_fn=<AddBackward0>)\n",
      "epoch: 10158 loss is tensor([-0.4467], grad_fn=<AddBackward0>)\n",
      "epoch: 10159 loss is tensor([-0.4440], grad_fn=<AddBackward0>)\n",
      "epoch: 10160 loss is tensor([-0.3995], grad_fn=<AddBackward0>)\n",
      "epoch: 10161 loss is tensor([-0.4058], grad_fn=<AddBackward0>)\n",
      "epoch: 10162 loss is tensor([-0.4092], grad_fn=<AddBackward0>)\n",
      "epoch: 10163 loss is tensor([-0.4121], grad_fn=<AddBackward0>)\n",
      "epoch: 10164 loss is tensor([-0.4572], grad_fn=<AddBackward0>)\n",
      "epoch: 10165 loss is tensor([-0.4281], grad_fn=<AddBackward0>)\n",
      "epoch: 10166 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 10167 loss is tensor([-0.4382], grad_fn=<AddBackward0>)\n",
      "epoch: 10168 loss is tensor([-0.4674], grad_fn=<AddBackward0>)\n",
      "epoch: 10169 loss is tensor([-0.4878], grad_fn=<AddBackward0>)\n",
      "epoch: 10170 loss is tensor([-0.4946], grad_fn=<AddBackward0>)\n",
      "epoch: 10171 loss is tensor([-0.4535], grad_fn=<AddBackward0>)\n",
      "epoch: 10172 loss is tensor([-0.4383], grad_fn=<AddBackward0>)\n",
      "epoch: 10173 loss is tensor([-0.4269], grad_fn=<AddBackward0>)\n",
      "epoch: 10174 loss is tensor([-0.4950], grad_fn=<AddBackward0>)\n",
      "epoch: 10175 loss is tensor([-0.4149], grad_fn=<AddBackward0>)\n",
      "epoch: 10176 loss is tensor([-0.4287], grad_fn=<AddBackward0>)\n",
      "epoch: 10177 loss is tensor([-0.4028], grad_fn=<AddBackward0>)\n",
      "epoch: 10178 loss is tensor([-0.4144], grad_fn=<AddBackward0>)\n",
      "epoch: 10179 loss is tensor([-0.4519], grad_fn=<AddBackward0>)\n",
      "epoch: 10180 loss is tensor([-0.4640], grad_fn=<AddBackward0>)\n",
      "epoch: 10181 loss is tensor([-0.3821], grad_fn=<AddBackward0>)\n",
      "epoch: 10182 loss is tensor([-0.4338], grad_fn=<AddBackward0>)\n",
      "epoch: 10183 loss is tensor([-0.4230], grad_fn=<AddBackward0>)\n",
      "epoch: 10184 loss is tensor([-0.2586], grad_fn=<AddBackward0>)\n",
      "epoch: 10185 loss is tensor([-0.3371], grad_fn=<AddBackward0>)\n",
      "epoch: 10186 loss is tensor([-0.3967], grad_fn=<AddBackward0>)\n",
      "epoch: 10187 loss is tensor([-0.4329], grad_fn=<AddBackward0>)\n",
      "epoch: 10188 loss is tensor([-0.3982], grad_fn=<AddBackward0>)\n",
      "epoch: 10189 loss is tensor([-0.4555], grad_fn=<AddBackward0>)\n",
      "epoch: 10190 loss is tensor([-0.4580], grad_fn=<AddBackward0>)\n",
      "epoch: 10191 loss is tensor([-0.4175], grad_fn=<AddBackward0>)\n",
      "epoch: 10192 loss is tensor([-0.3756], grad_fn=<AddBackward0>)\n",
      "epoch: 10193 loss is tensor([-0.4196], grad_fn=<AddBackward0>)\n",
      "epoch: 10194 loss is tensor([-0.4888], grad_fn=<AddBackward0>)\n",
      "epoch: 10195 loss is tensor([-0.4639], grad_fn=<AddBackward0>)\n",
      "epoch: 10196 loss is tensor([-0.5017], grad_fn=<AddBackward0>)\n",
      "epoch: 10197 loss is tensor([-0.4641], grad_fn=<AddBackward0>)\n",
      "epoch: 10198 loss is tensor([-0.3726], grad_fn=<AddBackward0>)\n",
      "epoch: 10199 loss is tensor([-0.4664], grad_fn=<AddBackward0>)\n",
      "epoch: 10200 loss is tensor([-0.4031], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10201 loss is tensor([-0.4578], grad_fn=<AddBackward0>)\n",
      "epoch: 10202 loss is tensor([-0.4468], grad_fn=<AddBackward0>)\n",
      "epoch: 10203 loss is tensor([-0.4146], grad_fn=<AddBackward0>)\n",
      "epoch: 10204 loss is tensor([-0.4622], grad_fn=<AddBackward0>)\n",
      "epoch: 10205 loss is tensor([-0.4069], grad_fn=<AddBackward0>)\n",
      "epoch: 10206 loss is tensor([-0.4110], grad_fn=<AddBackward0>)\n",
      "epoch: 10207 loss is tensor([-0.4170], grad_fn=<AddBackward0>)\n",
      "epoch: 10208 loss is tensor([-0.4771], grad_fn=<AddBackward0>)\n",
      "epoch: 10209 loss is tensor([-0.4970], grad_fn=<AddBackward0>)\n",
      "epoch: 10210 loss is tensor([-0.4333], grad_fn=<AddBackward0>)\n",
      "epoch: 10211 loss is tensor([-0.4766], grad_fn=<AddBackward0>)\n",
      "epoch: 10212 loss is tensor([-0.4837], grad_fn=<AddBackward0>)\n",
      "epoch: 10213 loss is tensor([-0.4438], grad_fn=<AddBackward0>)\n",
      "epoch: 10214 loss is tensor([-0.4769], grad_fn=<AddBackward0>)\n",
      "epoch: 10215 loss is tensor([-0.4370], grad_fn=<AddBackward0>)\n",
      "epoch: 10216 loss is tensor([-0.4321], grad_fn=<AddBackward0>)\n",
      "epoch: 10217 loss is tensor([-0.4091], grad_fn=<AddBackward0>)\n",
      "epoch: 10218 loss is tensor([-0.4374], grad_fn=<AddBackward0>)\n",
      "epoch: 10219 loss is tensor([-0.4422], grad_fn=<AddBackward0>)\n",
      "epoch: 10220 loss is tensor([-0.4372], grad_fn=<AddBackward0>)\n",
      "epoch: 10221 loss is tensor([-0.4346], grad_fn=<AddBackward0>)\n",
      "epoch: 10222 loss is tensor([-0.4535], grad_fn=<AddBackward0>)\n",
      "epoch: 10223 loss is tensor([-0.4405], grad_fn=<AddBackward0>)\n",
      "epoch: 10224 loss is tensor([-0.4159], grad_fn=<AddBackward0>)\n",
      "epoch: 10225 loss is tensor([-0.4734], grad_fn=<AddBackward0>)\n",
      "epoch: 10226 loss is tensor([-0.4399], grad_fn=<AddBackward0>)\n",
      "epoch: 10227 loss is tensor([-0.4567], grad_fn=<AddBackward0>)\n",
      "epoch: 10228 loss is tensor([-0.4574], grad_fn=<AddBackward0>)\n",
      "epoch: 10229 loss is tensor([-0.4541], grad_fn=<AddBackward0>)\n",
      "epoch: 10230 loss is tensor([-0.4591], grad_fn=<AddBackward0>)\n",
      "epoch: 10231 loss is tensor([-0.4859], grad_fn=<AddBackward0>)\n",
      "epoch: 10232 loss is tensor([-0.4639], grad_fn=<AddBackward0>)\n",
      "epoch: 10233 loss is tensor([-0.3881], grad_fn=<AddBackward0>)\n",
      "epoch: 10234 loss is tensor([-0.3892], grad_fn=<AddBackward0>)\n",
      "epoch: 10235 loss is tensor([-0.4092], grad_fn=<AddBackward0>)\n",
      "epoch: 10236 loss is tensor([-0.4785], grad_fn=<AddBackward0>)\n",
      "epoch: 10237 loss is tensor([-0.4618], grad_fn=<AddBackward0>)\n",
      "epoch: 10238 loss is tensor([-0.4255], grad_fn=<AddBackward0>)\n",
      "epoch: 10239 loss is tensor([-0.4388], grad_fn=<AddBackward0>)\n",
      "epoch: 10240 loss is tensor([-0.4336], grad_fn=<AddBackward0>)\n",
      "epoch: 10241 loss is tensor([-0.3842], grad_fn=<AddBackward0>)\n",
      "epoch: 10242 loss is tensor([-0.3857], grad_fn=<AddBackward0>)\n",
      "epoch: 10243 loss is tensor([-0.3945], grad_fn=<AddBackward0>)\n",
      "epoch: 10244 loss is tensor([-0.4449], grad_fn=<AddBackward0>)\n",
      "epoch: 10245 loss is tensor([-0.4208], grad_fn=<AddBackward0>)\n",
      "epoch: 10246 loss is tensor([-0.4384], grad_fn=<AddBackward0>)\n",
      "epoch: 10247 loss is tensor([-0.4453], grad_fn=<AddBackward0>)\n",
      "epoch: 10248 loss is tensor([-0.4944], grad_fn=<AddBackward0>)\n",
      "epoch: 10249 loss is tensor([-0.4558], grad_fn=<AddBackward0>)\n",
      "epoch: 10250 loss is tensor([-0.4261], grad_fn=<AddBackward0>)\n",
      "epoch: 10251 loss is tensor([-0.4528], grad_fn=<AddBackward0>)\n",
      "epoch: 10252 loss is tensor([-0.4468], grad_fn=<AddBackward0>)\n",
      "epoch: 10253 loss is tensor([-0.4581], grad_fn=<AddBackward0>)\n",
      "epoch: 10254 loss is tensor([-0.4635], grad_fn=<AddBackward0>)\n",
      "epoch: 10255 loss is tensor([-0.4456], grad_fn=<AddBackward0>)\n",
      "epoch: 10256 loss is tensor([-0.4108], grad_fn=<AddBackward0>)\n",
      "epoch: 10257 loss is tensor([-0.4251], grad_fn=<AddBackward0>)\n",
      "epoch: 10258 loss is tensor([-0.4176], grad_fn=<AddBackward0>)\n",
      "epoch: 10259 loss is tensor([-0.4025], grad_fn=<AddBackward0>)\n",
      "epoch: 10260 loss is tensor([-0.4394], grad_fn=<AddBackward0>)\n",
      "epoch: 10261 loss is tensor([-0.4405], grad_fn=<AddBackward0>)\n",
      "epoch: 10262 loss is tensor([-0.4894], grad_fn=<AddBackward0>)\n",
      "epoch: 10263 loss is tensor([-0.4548], grad_fn=<AddBackward0>)\n",
      "epoch: 10264 loss is tensor([-0.4755], grad_fn=<AddBackward0>)\n",
      "epoch: 10265 loss is tensor([-0.4331], grad_fn=<AddBackward0>)\n",
      "epoch: 10266 loss is tensor([-0.4226], grad_fn=<AddBackward0>)\n",
      "epoch: 10267 loss is tensor([-0.4390], grad_fn=<AddBackward0>)\n",
      "epoch: 10268 loss is tensor([-0.3982], grad_fn=<AddBackward0>)\n",
      "epoch: 10269 loss is tensor([-0.4889], grad_fn=<AddBackward0>)\n",
      "epoch: 10270 loss is tensor([-0.4681], grad_fn=<AddBackward0>)\n",
      "epoch: 10271 loss is tensor([-0.4438], grad_fn=<AddBackward0>)\n",
      "epoch: 10272 loss is tensor([-0.4432], grad_fn=<AddBackward0>)\n",
      "epoch: 10273 loss is tensor([-0.4599], grad_fn=<AddBackward0>)\n",
      "epoch: 10274 loss is tensor([-0.4942], grad_fn=<AddBackward0>)\n",
      "epoch: 10275 loss is tensor([-0.4114], grad_fn=<AddBackward0>)\n",
      "epoch: 10276 loss is tensor([-0.4325], grad_fn=<AddBackward0>)\n",
      "epoch: 10277 loss is tensor([-0.4577], grad_fn=<AddBackward0>)\n",
      "epoch: 10278 loss is tensor([-0.4532], grad_fn=<AddBackward0>)\n",
      "epoch: 10279 loss is tensor([-0.4165], grad_fn=<AddBackward0>)\n",
      "epoch: 10280 loss is tensor([-0.4502], grad_fn=<AddBackward0>)\n",
      "epoch: 10281 loss is tensor([-0.3844], grad_fn=<AddBackward0>)\n",
      "epoch: 10282 loss is tensor([-0.3351], grad_fn=<AddBackward0>)\n",
      "epoch: 10283 loss is tensor([-0.4035], grad_fn=<AddBackward0>)\n",
      "epoch: 10284 loss is tensor([-0.3914], grad_fn=<AddBackward0>)\n",
      "epoch: 10285 loss is tensor([-0.3816], grad_fn=<AddBackward0>)\n",
      "epoch: 10286 loss is tensor([-0.3933], grad_fn=<AddBackward0>)\n",
      "epoch: 10287 loss is tensor([-0.3669], grad_fn=<AddBackward0>)\n",
      "epoch: 10288 loss is tensor([-0.4157], grad_fn=<AddBackward0>)\n",
      "epoch: 10289 loss is tensor([-0.4504], grad_fn=<AddBackward0>)\n",
      "epoch: 10290 loss is tensor([-0.4464], grad_fn=<AddBackward0>)\n",
      "epoch: 10291 loss is tensor([-0.4569], grad_fn=<AddBackward0>)\n",
      "epoch: 10292 loss is tensor([-0.4673], grad_fn=<AddBackward0>)\n",
      "epoch: 10293 loss is tensor([-0.4520], grad_fn=<AddBackward0>)\n",
      "epoch: 10294 loss is tensor([-0.3658], grad_fn=<AddBackward0>)\n",
      "epoch: 10295 loss is tensor([-0.4335], grad_fn=<AddBackward0>)\n",
      "epoch: 10296 loss is tensor([-0.3950], grad_fn=<AddBackward0>)\n",
      "epoch: 10297 loss is tensor([-0.4197], grad_fn=<AddBackward0>)\n",
      "epoch: 10298 loss is tensor([-0.4205], grad_fn=<AddBackward0>)\n",
      "epoch: 10299 loss is tensor([-0.3537], grad_fn=<AddBackward0>)\n",
      "epoch: 10300 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10301 loss is tensor([-0.3872], grad_fn=<AddBackward0>)\n",
      "epoch: 10302 loss is tensor([-0.3814], grad_fn=<AddBackward0>)\n",
      "epoch: 10303 loss is tensor([-0.3848], grad_fn=<AddBackward0>)\n",
      "epoch: 10304 loss is tensor([-0.3758], grad_fn=<AddBackward0>)\n",
      "epoch: 10305 loss is tensor([-0.3847], grad_fn=<AddBackward0>)\n",
      "epoch: 10306 loss is tensor([-0.4021], grad_fn=<AddBackward0>)\n",
      "epoch: 10307 loss is tensor([-0.4298], grad_fn=<AddBackward0>)\n",
      "epoch: 10308 loss is tensor([-0.4049], grad_fn=<AddBackward0>)\n",
      "epoch: 10309 loss is tensor([-0.4167], grad_fn=<AddBackward0>)\n",
      "epoch: 10310 loss is tensor([-0.4293], grad_fn=<AddBackward0>)\n",
      "epoch: 10311 loss is tensor([-0.3593], grad_fn=<AddBackward0>)\n",
      "epoch: 10312 loss is tensor([-0.4378], grad_fn=<AddBackward0>)\n",
      "epoch: 10313 loss is tensor([-0.3896], grad_fn=<AddBackward0>)\n",
      "epoch: 10314 loss is tensor([-0.4792], grad_fn=<AddBackward0>)\n",
      "epoch: 10315 loss is tensor([-0.4279], grad_fn=<AddBackward0>)\n",
      "epoch: 10316 loss is tensor([-0.4042], grad_fn=<AddBackward0>)\n",
      "epoch: 10317 loss is tensor([-0.4843], grad_fn=<AddBackward0>)\n",
      "epoch: 10318 loss is tensor([-0.4756], grad_fn=<AddBackward0>)\n",
      "epoch: 10319 loss is tensor([-0.3546], grad_fn=<AddBackward0>)\n",
      "epoch: 10320 loss is tensor([-0.4421], grad_fn=<AddBackward0>)\n",
      "epoch: 10321 loss is tensor([-0.4767], grad_fn=<AddBackward0>)\n",
      "epoch: 10322 loss is tensor([-0.4920], grad_fn=<AddBackward0>)\n",
      "epoch: 10323 loss is tensor([-0.4602], grad_fn=<AddBackward0>)\n",
      "epoch: 10324 loss is tensor([-0.4326], grad_fn=<AddBackward0>)\n",
      "epoch: 10325 loss is tensor([-0.5069], grad_fn=<AddBackward0>)\n",
      "epoch: 10326 loss is tensor([-0.4014], grad_fn=<AddBackward0>)\n",
      "epoch: 10327 loss is tensor([-0.4495], grad_fn=<AddBackward0>)\n",
      "epoch: 10328 loss is tensor([-0.4241], grad_fn=<AddBackward0>)\n",
      "epoch: 10329 loss is tensor([-0.4650], grad_fn=<AddBackward0>)\n",
      "epoch: 10330 loss is tensor([-0.4559], grad_fn=<AddBackward0>)\n",
      "epoch: 10331 loss is tensor([-0.4373], grad_fn=<AddBackward0>)\n",
      "epoch: 10332 loss is tensor([-0.3944], grad_fn=<AddBackward0>)\n",
      "epoch: 10333 loss is tensor([-0.4396], grad_fn=<AddBackward0>)\n",
      "epoch: 10334 loss is tensor([-0.4023], grad_fn=<AddBackward0>)\n",
      "epoch: 10335 loss is tensor([-0.4247], grad_fn=<AddBackward0>)\n",
      "epoch: 10336 loss is tensor([-0.4334], grad_fn=<AddBackward0>)\n",
      "epoch: 10337 loss is tensor([-0.4555], grad_fn=<AddBackward0>)\n",
      "epoch: 10338 loss is tensor([-0.4134], grad_fn=<AddBackward0>)\n",
      "epoch: 10339 loss is tensor([-0.3747], grad_fn=<AddBackward0>)\n",
      "epoch: 10340 loss is tensor([-0.3972], grad_fn=<AddBackward0>)\n",
      "epoch: 10341 loss is tensor([-0.3936], grad_fn=<AddBackward0>)\n",
      "epoch: 10342 loss is tensor([-0.4326], grad_fn=<AddBackward0>)\n",
      "epoch: 10343 loss is tensor([-0.3744], grad_fn=<AddBackward0>)\n",
      "epoch: 10344 loss is tensor([-0.4438], grad_fn=<AddBackward0>)\n",
      "epoch: 10345 loss is tensor([-0.4373], grad_fn=<AddBackward0>)\n",
      "epoch: 10346 loss is tensor([-0.4183], grad_fn=<AddBackward0>)\n",
      "epoch: 10347 loss is tensor([-0.3591], grad_fn=<AddBackward0>)\n",
      "epoch: 10348 loss is tensor([-0.4311], grad_fn=<AddBackward0>)\n",
      "epoch: 10349 loss is tensor([-0.4110], grad_fn=<AddBackward0>)\n",
      "epoch: 10350 loss is tensor([-0.3446], grad_fn=<AddBackward0>)\n",
      "epoch: 10351 loss is tensor([-0.3846], grad_fn=<AddBackward0>)\n",
      "epoch: 10352 loss is tensor([-0.3133], grad_fn=<AddBackward0>)\n",
      "epoch: 10353 loss is tensor([-0.3597], grad_fn=<AddBackward0>)\n",
      "epoch: 10354 loss is tensor([-0.4035], grad_fn=<AddBackward0>)\n",
      "epoch: 10355 loss is tensor([-0.3472], grad_fn=<AddBackward0>)\n",
      "epoch: 10356 loss is tensor([-0.4360], grad_fn=<AddBackward0>)\n",
      "epoch: 10357 loss is tensor([-0.3791], grad_fn=<AddBackward0>)\n",
      "epoch: 10358 loss is tensor([-0.3738], grad_fn=<AddBackward0>)\n",
      "epoch: 10359 loss is tensor([-0.4322], grad_fn=<AddBackward0>)\n",
      "epoch: 10360 loss is tensor([-0.4171], grad_fn=<AddBackward0>)\n",
      "epoch: 10361 loss is tensor([-0.4246], grad_fn=<AddBackward0>)\n",
      "epoch: 10362 loss is tensor([-0.4316], grad_fn=<AddBackward0>)\n",
      "epoch: 10363 loss is tensor([-0.4042], grad_fn=<AddBackward0>)\n",
      "epoch: 10364 loss is tensor([-0.4427], grad_fn=<AddBackward0>)\n",
      "epoch: 10365 loss is tensor([-0.3864], grad_fn=<AddBackward0>)\n",
      "epoch: 10366 loss is tensor([-0.4309], grad_fn=<AddBackward0>)\n",
      "epoch: 10367 loss is tensor([-0.3933], grad_fn=<AddBackward0>)\n",
      "epoch: 10368 loss is tensor([-0.4222], grad_fn=<AddBackward0>)\n",
      "epoch: 10369 loss is tensor([-0.4448], grad_fn=<AddBackward0>)\n",
      "epoch: 10370 loss is tensor([-0.4020], grad_fn=<AddBackward0>)\n",
      "epoch: 10371 loss is tensor([-0.4197], grad_fn=<AddBackward0>)\n",
      "epoch: 10372 loss is tensor([-0.4484], grad_fn=<AddBackward0>)\n",
      "epoch: 10373 loss is tensor([-0.4916], grad_fn=<AddBackward0>)\n",
      "epoch: 10374 loss is tensor([-0.4278], grad_fn=<AddBackward0>)\n",
      "epoch: 10375 loss is tensor([-0.4104], grad_fn=<AddBackward0>)\n",
      "epoch: 10376 loss is tensor([-0.4438], grad_fn=<AddBackward0>)\n",
      "epoch: 10377 loss is tensor([-0.4060], grad_fn=<AddBackward0>)\n",
      "epoch: 10378 loss is tensor([-0.4925], grad_fn=<AddBackward0>)\n",
      "epoch: 10379 loss is tensor([-0.5027], grad_fn=<AddBackward0>)\n",
      "epoch: 10380 loss is tensor([-0.4597], grad_fn=<AddBackward0>)\n",
      "epoch: 10381 loss is tensor([-0.4288], grad_fn=<AddBackward0>)\n",
      "epoch: 10382 loss is tensor([-0.3837], grad_fn=<AddBackward0>)\n",
      "epoch: 10383 loss is tensor([-0.4385], grad_fn=<AddBackward0>)\n",
      "epoch: 10384 loss is tensor([-0.4609], grad_fn=<AddBackward0>)\n",
      "epoch: 10385 loss is tensor([-0.4502], grad_fn=<AddBackward0>)\n",
      "epoch: 10386 loss is tensor([-0.4635], grad_fn=<AddBackward0>)\n",
      "epoch: 10387 loss is tensor([-0.4088], grad_fn=<AddBackward0>)\n",
      "epoch: 10388 loss is tensor([-0.4413], grad_fn=<AddBackward0>)\n",
      "epoch: 10389 loss is tensor([-0.4690], grad_fn=<AddBackward0>)\n",
      "epoch: 10390 loss is tensor([-0.4991], grad_fn=<AddBackward0>)\n",
      "epoch: 10391 loss is tensor([-0.4363], grad_fn=<AddBackward0>)\n",
      "epoch: 10392 loss is tensor([-0.4781], grad_fn=<AddBackward0>)\n",
      "epoch: 10393 loss is tensor([-0.4625], grad_fn=<AddBackward0>)\n",
      "epoch: 10394 loss is tensor([-0.4724], grad_fn=<AddBackward0>)\n",
      "epoch: 10395 loss is tensor([-0.4615], grad_fn=<AddBackward0>)\n",
      "epoch: 10396 loss is tensor([-0.4512], grad_fn=<AddBackward0>)\n",
      "epoch: 10397 loss is tensor([-0.4431], grad_fn=<AddBackward0>)\n",
      "epoch: 10398 loss is tensor([-0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 10399 loss is tensor([-0.5045], grad_fn=<AddBackward0>)\n",
      "epoch: 10400 loss is tensor([-0.4532], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10401 loss is tensor([-0.4459], grad_fn=<AddBackward0>)\n",
      "epoch: 10402 loss is tensor([-0.4351], grad_fn=<AddBackward0>)\n",
      "epoch: 10403 loss is tensor([-0.4829], grad_fn=<AddBackward0>)\n",
      "epoch: 10404 loss is tensor([-0.4725], grad_fn=<AddBackward0>)\n",
      "epoch: 10405 loss is tensor([-0.4691], grad_fn=<AddBackward0>)\n",
      "epoch: 10406 loss is tensor([-0.4678], grad_fn=<AddBackward0>)\n",
      "epoch: 10407 loss is tensor([-0.5070], grad_fn=<AddBackward0>)\n",
      "epoch: 10408 loss is tensor([-0.4857], grad_fn=<AddBackward0>)\n",
      "epoch: 10409 loss is tensor([-0.4664], grad_fn=<AddBackward0>)\n",
      "epoch: 10410 loss is tensor([-0.4908], grad_fn=<AddBackward0>)\n",
      "epoch: 10411 loss is tensor([-0.4482], grad_fn=<AddBackward0>)\n",
      "epoch: 10412 loss is tensor([-0.4494], grad_fn=<AddBackward0>)\n",
      "epoch: 10413 loss is tensor([-0.4608], grad_fn=<AddBackward0>)\n",
      "epoch: 10414 loss is tensor([-0.4931], grad_fn=<AddBackward0>)\n",
      "epoch: 10415 loss is tensor([-0.4290], grad_fn=<AddBackward0>)\n",
      "epoch: 10416 loss is tensor([-0.4828], grad_fn=<AddBackward0>)\n",
      "epoch: 10417 loss is tensor([-0.4236], grad_fn=<AddBackward0>)\n",
      "epoch: 10418 loss is tensor([-0.4906], grad_fn=<AddBackward0>)\n",
      "epoch: 10419 loss is tensor([-0.4988], grad_fn=<AddBackward0>)\n",
      "epoch: 10420 loss is tensor([-0.4725], grad_fn=<AddBackward0>)\n",
      "epoch: 10421 loss is tensor([-0.4872], grad_fn=<AddBackward0>)\n",
      "epoch: 10422 loss is tensor([-0.4134], grad_fn=<AddBackward0>)\n",
      "epoch: 10423 loss is tensor([-0.4577], grad_fn=<AddBackward0>)\n",
      "epoch: 10424 loss is tensor([-0.4685], grad_fn=<AddBackward0>)\n",
      "epoch: 10425 loss is tensor([-0.5250], grad_fn=<AddBackward0>)\n",
      "epoch: 10426 loss is tensor([-0.4690], grad_fn=<AddBackward0>)\n",
      "epoch: 10427 loss is tensor([-0.4397], grad_fn=<AddBackward0>)\n",
      "epoch: 10428 loss is tensor([-0.4377], grad_fn=<AddBackward0>)\n",
      "epoch: 10429 loss is tensor([-0.4823], grad_fn=<AddBackward0>)\n",
      "epoch: 10430 loss is tensor([-0.4481], grad_fn=<AddBackward0>)\n",
      "epoch: 10431 loss is tensor([-0.4625], grad_fn=<AddBackward0>)\n",
      "epoch: 10432 loss is tensor([-0.4944], grad_fn=<AddBackward0>)\n",
      "epoch: 10433 loss is tensor([-0.4728], grad_fn=<AddBackward0>)\n",
      "epoch: 10434 loss is tensor([-0.4552], grad_fn=<AddBackward0>)\n",
      "epoch: 10435 loss is tensor([-0.4837], grad_fn=<AddBackward0>)\n",
      "epoch: 10436 loss is tensor([-0.4697], grad_fn=<AddBackward0>)\n",
      "epoch: 10437 loss is tensor([-0.4524], grad_fn=<AddBackward0>)\n",
      "epoch: 10438 loss is tensor([-0.5370], grad_fn=<AddBackward0>)\n",
      "epoch: 10439 loss is tensor([-0.4798], grad_fn=<AddBackward0>)\n",
      "epoch: 10440 loss is tensor([-0.4430], grad_fn=<AddBackward0>)\n",
      "epoch: 10441 loss is tensor([-0.4571], grad_fn=<AddBackward0>)\n",
      "epoch: 10442 loss is tensor([-0.5270], grad_fn=<AddBackward0>)\n",
      "epoch: 10443 loss is tensor([-0.4308], grad_fn=<AddBackward0>)\n",
      "epoch: 10444 loss is tensor([-0.4592], grad_fn=<AddBackward0>)\n",
      "epoch: 10445 loss is tensor([-0.4639], grad_fn=<AddBackward0>)\n",
      "epoch: 10446 loss is tensor([-0.5026], grad_fn=<AddBackward0>)\n",
      "epoch: 10447 loss is tensor([-0.5192], grad_fn=<AddBackward0>)\n",
      "epoch: 10448 loss is tensor([-0.4872], grad_fn=<AddBackward0>)\n",
      "epoch: 10449 loss is tensor([-0.4456], grad_fn=<AddBackward0>)\n",
      "epoch: 10450 loss is tensor([-0.4901], grad_fn=<AddBackward0>)\n",
      "epoch: 10451 loss is tensor([-0.4853], grad_fn=<AddBackward0>)\n",
      "epoch: 10452 loss is tensor([-0.5023], grad_fn=<AddBackward0>)\n",
      "epoch: 10453 loss is tensor([-0.4474], grad_fn=<AddBackward0>)\n",
      "epoch: 10454 loss is tensor([-0.5110], grad_fn=<AddBackward0>)\n",
      "epoch: 10455 loss is tensor([-0.4181], grad_fn=<AddBackward0>)\n",
      "epoch: 10456 loss is tensor([-0.4543], grad_fn=<AddBackward0>)\n",
      "epoch: 10457 loss is tensor([-0.4826], grad_fn=<AddBackward0>)\n",
      "epoch: 10458 loss is tensor([-0.5206], grad_fn=<AddBackward0>)\n",
      "epoch: 10459 loss is tensor([-0.4120], grad_fn=<AddBackward0>)\n",
      "epoch: 10460 loss is tensor([-0.3820], grad_fn=<AddBackward0>)\n",
      "epoch: 10461 loss is tensor([-0.4239], grad_fn=<AddBackward0>)\n",
      "epoch: 10462 loss is tensor([-0.4631], grad_fn=<AddBackward0>)\n",
      "epoch: 10463 loss is tensor([-0.3819], grad_fn=<AddBackward0>)\n",
      "epoch: 10464 loss is tensor([-0.4319], grad_fn=<AddBackward0>)\n",
      "epoch: 10465 loss is tensor([-0.4102], grad_fn=<AddBackward0>)\n",
      "epoch: 10466 loss is tensor([-0.4538], grad_fn=<AddBackward0>)\n",
      "epoch: 10467 loss is tensor([-0.4595], grad_fn=<AddBackward0>)\n",
      "epoch: 10468 loss is tensor([-0.4975], grad_fn=<AddBackward0>)\n",
      "epoch: 10469 loss is tensor([-0.4583], grad_fn=<AddBackward0>)\n",
      "epoch: 10470 loss is tensor([-0.4505], grad_fn=<AddBackward0>)\n",
      "epoch: 10471 loss is tensor([-0.4638], grad_fn=<AddBackward0>)\n",
      "epoch: 10472 loss is tensor([-0.5169], grad_fn=<AddBackward0>)\n",
      "epoch: 10473 loss is tensor([-0.5080], grad_fn=<AddBackward0>)\n",
      "epoch: 10474 loss is tensor([-0.4996], grad_fn=<AddBackward0>)\n",
      "epoch: 10475 loss is tensor([-0.4625], grad_fn=<AddBackward0>)\n",
      "epoch: 10476 loss is tensor([-0.4754], grad_fn=<AddBackward0>)\n",
      "epoch: 10477 loss is tensor([-0.4679], grad_fn=<AddBackward0>)\n",
      "epoch: 10478 loss is tensor([-0.5037], grad_fn=<AddBackward0>)\n",
      "epoch: 10479 loss is tensor([-0.4582], grad_fn=<AddBackward0>)\n",
      "epoch: 10480 loss is tensor([-0.5134], grad_fn=<AddBackward0>)\n",
      "epoch: 10481 loss is tensor([-0.4475], grad_fn=<AddBackward0>)\n",
      "epoch: 10482 loss is tensor([-0.4955], grad_fn=<AddBackward0>)\n",
      "epoch: 10483 loss is tensor([-0.4961], grad_fn=<AddBackward0>)\n",
      "epoch: 10484 loss is tensor([-0.4750], grad_fn=<AddBackward0>)\n",
      "epoch: 10485 loss is tensor([-0.4987], grad_fn=<AddBackward0>)\n",
      "epoch: 10486 loss is tensor([-0.4858], grad_fn=<AddBackward0>)\n",
      "epoch: 10487 loss is tensor([-0.4428], grad_fn=<AddBackward0>)\n",
      "epoch: 10488 loss is tensor([-0.4191], grad_fn=<AddBackward0>)\n",
      "epoch: 10489 loss is tensor([-0.5296], grad_fn=<AddBackward0>)\n",
      "epoch: 10490 loss is tensor([-0.4503], grad_fn=<AddBackward0>)\n",
      "epoch: 10491 loss is tensor([-0.4774], grad_fn=<AddBackward0>)\n",
      "epoch: 10492 loss is tensor([-0.4036], grad_fn=<AddBackward0>)\n",
      "epoch: 10493 loss is tensor([-0.4461], grad_fn=<AddBackward0>)\n",
      "epoch: 10494 loss is tensor([-0.3904], grad_fn=<AddBackward0>)\n",
      "epoch: 10495 loss is tensor([-0.4164], grad_fn=<AddBackward0>)\n",
      "epoch: 10496 loss is tensor([-0.4448], grad_fn=<AddBackward0>)\n",
      "epoch: 10497 loss is tensor([-0.4468], grad_fn=<AddBackward0>)\n",
      "epoch: 10498 loss is tensor([-0.4057], grad_fn=<AddBackward0>)\n",
      "epoch: 10499 loss is tensor([-0.4412], grad_fn=<AddBackward0>)\n",
      "epoch: 10500 loss is tensor([-0.4759], grad_fn=<AddBackward0>)\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10501 loss is tensor([-0.4558], grad_fn=<AddBackward0>)\n",
      "epoch: 10502 loss is tensor([-0.4418], grad_fn=<AddBackward0>)\n",
      "epoch: 10503 loss is tensor([-0.4630], grad_fn=<AddBackward0>)\n",
      "epoch: 10504 loss is tensor([-0.4421], grad_fn=<AddBackward0>)\n",
      "epoch: 10505 loss is tensor([-0.4395], grad_fn=<AddBackward0>)\n",
      "epoch: 10506 loss is tensor([-0.4362], grad_fn=<AddBackward0>)\n",
      "epoch: 10507 loss is tensor([-0.4876], grad_fn=<AddBackward0>)\n",
      "epoch: 10508 loss is tensor([-0.4592], grad_fn=<AddBackward0>)\n",
      "epoch: 10509 loss is tensor([-0.4185], grad_fn=<AddBackward0>)\n",
      "epoch: 10510 loss is tensor([-0.4078], grad_fn=<AddBackward0>)\n",
      "epoch: 10511 loss is tensor([-0.3876], grad_fn=<AddBackward0>)\n",
      "epoch: 10512 loss is tensor([-0.3779], grad_fn=<AddBackward0>)\n",
      "epoch: 10513 loss is tensor([-0.4783], grad_fn=<AddBackward0>)\n",
      "epoch: 10514 loss is tensor([-0.4034], grad_fn=<AddBackward0>)\n",
      "epoch: 10515 loss is tensor([-0.4172], grad_fn=<AddBackward0>)\n",
      "epoch: 10516 loss is tensor([-0.3962], grad_fn=<AddBackward0>)\n",
      "epoch: 10517 loss is tensor([-0.4218], grad_fn=<AddBackward0>)\n",
      "epoch: 10518 loss is tensor([-0.4698], grad_fn=<AddBackward0>)\n",
      "epoch: 10519 loss is tensor([-0.4543], grad_fn=<AddBackward0>)\n",
      "epoch: 10520 loss is tensor([-0.4205], grad_fn=<AddBackward0>)\n",
      "epoch: 10521 loss is tensor([-0.4333], grad_fn=<AddBackward0>)\n",
      "epoch: 10522 loss is tensor([-0.4957], grad_fn=<AddBackward0>)\n",
      "epoch: 10523 loss is tensor([-0.5309], grad_fn=<AddBackward0>)\n",
      "epoch: 10524 loss is tensor([-0.4904], grad_fn=<AddBackward0>)\n",
      "epoch: 10525 loss is tensor([-0.4961], grad_fn=<AddBackward0>)\n",
      "epoch: 10526 loss is tensor([-0.4916], grad_fn=<AddBackward0>)\n",
      "epoch: 10527 loss is tensor([-0.4852], grad_fn=<AddBackward0>)\n",
      "epoch: 10528 loss is tensor([-0.5007], grad_fn=<AddBackward0>)\n",
      "epoch: 10529 loss is tensor([-0.4729], grad_fn=<AddBackward0>)\n",
      "epoch: 10530 loss is tensor([-0.4768], grad_fn=<AddBackward0>)\n",
      "epoch: 10531 loss is tensor([-0.4993], grad_fn=<AddBackward0>)\n",
      "epoch: 10532 loss is tensor([-0.5032], grad_fn=<AddBackward0>)\n",
      "epoch: 10533 loss is tensor([-0.5061], grad_fn=<AddBackward0>)\n",
      "epoch: 10534 loss is tensor([-0.4879], grad_fn=<AddBackward0>)\n",
      "epoch: 10535 loss is tensor([-0.4972], grad_fn=<AddBackward0>)\n",
      "epoch: 10536 loss is tensor([-0.4957], grad_fn=<AddBackward0>)\n",
      "epoch: 10537 loss is tensor([-0.3703], grad_fn=<AddBackward0>)\n",
      "epoch: 10538 loss is tensor([-0.4604], grad_fn=<AddBackward0>)\n",
      "epoch: 10539 loss is tensor([-0.4896], grad_fn=<AddBackward0>)\n",
      "epoch: 10540 loss is tensor([-0.5005], grad_fn=<AddBackward0>)\n",
      "epoch: 10541 loss is tensor([-0.4851], grad_fn=<AddBackward0>)\n",
      "epoch: 10542 loss is tensor([-0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 10543 loss is tensor([-0.4095], grad_fn=<AddBackward0>)\n",
      "epoch: 10544 loss is tensor([-0.4503], grad_fn=<AddBackward0>)\n",
      "epoch: 10545 loss is tensor([-0.4457], grad_fn=<AddBackward0>)\n",
      "epoch: 10546 loss is tensor([-0.4545], grad_fn=<AddBackward0>)\n",
      "epoch: 10547 loss is tensor([-0.4054], grad_fn=<AddBackward0>)\n",
      "epoch: 10548 loss is tensor([-0.4463], grad_fn=<AddBackward0>)\n",
      "epoch: 10549 loss is tensor([-0.4809], grad_fn=<AddBackward0>)\n",
      "epoch: 10550 loss is tensor([-0.4279], grad_fn=<AddBackward0>)\n",
      "epoch: 10551 loss is tensor([-0.4296], grad_fn=<AddBackward0>)\n",
      "epoch: 10552 loss is tensor([-0.4622], grad_fn=<AddBackward0>)\n",
      "epoch: 10553 loss is tensor([-0.4310], grad_fn=<AddBackward0>)\n",
      "epoch: 10554 loss is tensor([-0.4442], grad_fn=<AddBackward0>)\n",
      "epoch: 10555 loss is tensor([-0.4509], grad_fn=<AddBackward0>)\n",
      "epoch: 10556 loss is tensor([-0.4663], grad_fn=<AddBackward0>)\n",
      "epoch: 10557 loss is tensor([-0.4893], grad_fn=<AddBackward0>)\n",
      "epoch: 10558 loss is tensor([-0.4039], grad_fn=<AddBackward0>)\n",
      "epoch: 10559 loss is tensor([-0.4610], grad_fn=<AddBackward0>)\n",
      "epoch: 10560 loss is tensor([-0.4868], grad_fn=<AddBackward0>)\n",
      "epoch: 10561 loss is tensor([-0.4934], grad_fn=<AddBackward0>)\n",
      "epoch: 10562 loss is tensor([-0.4527], grad_fn=<AddBackward0>)\n",
      "epoch: 10563 loss is tensor([-0.4557], grad_fn=<AddBackward0>)\n",
      "epoch: 10564 loss is tensor([-0.4621], grad_fn=<AddBackward0>)\n",
      "epoch: 10565 loss is tensor([-0.4806], grad_fn=<AddBackward0>)\n",
      "epoch: 10566 loss is tensor([-0.4466], grad_fn=<AddBackward0>)\n",
      "epoch: 10567 loss is tensor([-0.4703], grad_fn=<AddBackward0>)\n",
      "epoch: 10568 loss is tensor([-0.4841], grad_fn=<AddBackward0>)\n",
      "epoch: 10569 loss is tensor([-0.5141], grad_fn=<AddBackward0>)\n",
      "epoch: 10570 loss is tensor([-0.5044], grad_fn=<AddBackward0>)\n",
      "epoch: 10571 loss is tensor([-0.4779], grad_fn=<AddBackward0>)\n",
      "epoch: 10572 loss is tensor([-0.4065], grad_fn=<AddBackward0>)\n",
      "epoch: 10573 loss is tensor([-0.5353], grad_fn=<AddBackward0>)\n",
      "epoch: 10574 loss is tensor([-0.4717], grad_fn=<AddBackward0>)\n",
      "epoch: 10575 loss is tensor([-0.4783], grad_fn=<AddBackward0>)\n",
      "epoch: 10576 loss is tensor([-0.5200], grad_fn=<AddBackward0>)\n",
      "epoch: 10577 loss is tensor([-0.5074], grad_fn=<AddBackward0>)\n",
      "epoch: 10578 loss is tensor([-0.5133], grad_fn=<AddBackward0>)\n",
      "epoch: 10579 loss is tensor([-0.5238], grad_fn=<AddBackward0>)\n",
      "epoch: 10580 loss is tensor([-0.5208], grad_fn=<AddBackward0>)\n",
      "epoch: 10581 loss is tensor([-0.4480], grad_fn=<AddBackward0>)\n",
      "epoch: 10582 loss is tensor([-0.4372], grad_fn=<AddBackward0>)\n",
      "epoch: 10583 loss is tensor([-0.4789], grad_fn=<AddBackward0>)\n",
      "epoch: 10584 loss is tensor([-0.5265], grad_fn=<AddBackward0>)\n",
      "epoch: 10585 loss is tensor([-0.4953], grad_fn=<AddBackward0>)\n",
      "epoch: 10586 loss is tensor([-0.5232], grad_fn=<AddBackward0>)\n",
      "epoch: 10587 loss is tensor([-0.4653], grad_fn=<AddBackward0>)\n",
      "epoch: 10588 loss is tensor([-0.4522], grad_fn=<AddBackward0>)\n",
      "epoch: 10589 loss is tensor([-0.4602], grad_fn=<AddBackward0>)\n",
      "epoch: 10590 loss is tensor([-0.4652], grad_fn=<AddBackward0>)\n",
      "epoch: 10591 loss is tensor([-0.5192], grad_fn=<AddBackward0>)\n",
      "epoch: 10592 loss is tensor([-0.4244], grad_fn=<AddBackward0>)\n",
      "epoch: 10593 loss is tensor([-0.4969], grad_fn=<AddBackward0>)\n",
      "epoch: 10594 loss is tensor([-0.4534], grad_fn=<AddBackward0>)\n",
      "epoch: 10595 loss is tensor([-0.4691], grad_fn=<AddBackward0>)\n",
      "epoch: 10596 loss is tensor([-0.4600], grad_fn=<AddBackward0>)\n",
      "epoch: 10597 loss is tensor([-0.3994], grad_fn=<AddBackward0>)\n",
      "epoch: 10598 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 10599 loss is tensor([-0.4160], grad_fn=<AddBackward0>)\n",
      "epoch: 10600 loss is tensor([-0.4371], grad_fn=<AddBackward0>)\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10601 loss is tensor([-0.4134], grad_fn=<AddBackward0>)\n",
      "epoch: 10602 loss is tensor([-0.4852], grad_fn=<AddBackward0>)\n",
      "epoch: 10603 loss is tensor([-0.4561], grad_fn=<AddBackward0>)\n",
      "epoch: 10604 loss is tensor([-0.4996], grad_fn=<AddBackward0>)\n",
      "epoch: 10605 loss is tensor([-0.4668], grad_fn=<AddBackward0>)\n",
      "epoch: 10606 loss is tensor([-0.4381], grad_fn=<AddBackward0>)\n",
      "epoch: 10607 loss is tensor([-0.4900], grad_fn=<AddBackward0>)\n",
      "epoch: 10608 loss is tensor([-0.4904], grad_fn=<AddBackward0>)\n",
      "epoch: 10609 loss is tensor([-0.4425], grad_fn=<AddBackward0>)\n",
      "epoch: 10610 loss is tensor([-0.4952], grad_fn=<AddBackward0>)\n",
      "epoch: 10611 loss is tensor([-0.4811], grad_fn=<AddBackward0>)\n",
      "epoch: 10612 loss is tensor([-0.4283], grad_fn=<AddBackward0>)\n",
      "epoch: 10613 loss is tensor([-0.4291], grad_fn=<AddBackward0>)\n",
      "epoch: 10614 loss is tensor([-0.3835], grad_fn=<AddBackward0>)\n",
      "epoch: 10615 loss is tensor([-0.4708], grad_fn=<AddBackward0>)\n",
      "epoch: 10616 loss is tensor([-0.4682], grad_fn=<AddBackward0>)\n",
      "epoch: 10617 loss is tensor([-0.4221], grad_fn=<AddBackward0>)\n",
      "epoch: 10618 loss is tensor([-0.4864], grad_fn=<AddBackward0>)\n",
      "epoch: 10619 loss is tensor([-0.4795], grad_fn=<AddBackward0>)\n",
      "epoch: 10620 loss is tensor([-0.3947], grad_fn=<AddBackward0>)\n",
      "epoch: 10621 loss is tensor([-0.4559], grad_fn=<AddBackward0>)\n",
      "epoch: 10622 loss is tensor([-0.4579], grad_fn=<AddBackward0>)\n",
      "epoch: 10623 loss is tensor([-0.4431], grad_fn=<AddBackward0>)\n",
      "epoch: 10624 loss is tensor([-0.4677], grad_fn=<AddBackward0>)\n",
      "epoch: 10625 loss is tensor([-0.4523], grad_fn=<AddBackward0>)\n",
      "epoch: 10626 loss is tensor([-0.4276], grad_fn=<AddBackward0>)\n",
      "epoch: 10627 loss is tensor([-0.3891], grad_fn=<AddBackward0>)\n",
      "epoch: 10628 loss is tensor([-0.4820], grad_fn=<AddBackward0>)\n",
      "epoch: 10629 loss is tensor([-0.4661], grad_fn=<AddBackward0>)\n",
      "epoch: 10630 loss is tensor([-0.4110], grad_fn=<AddBackward0>)\n",
      "epoch: 10631 loss is tensor([-0.4476], grad_fn=<AddBackward0>)\n",
      "epoch: 10632 loss is tensor([-0.4177], grad_fn=<AddBackward0>)\n",
      "epoch: 10633 loss is tensor([-0.4403], grad_fn=<AddBackward0>)\n",
      "epoch: 10634 loss is tensor([-0.4397], grad_fn=<AddBackward0>)\n",
      "epoch: 10635 loss is tensor([-0.4156], grad_fn=<AddBackward0>)\n",
      "epoch: 10636 loss is tensor([-0.4455], grad_fn=<AddBackward0>)\n",
      "epoch: 10637 loss is tensor([-0.4559], grad_fn=<AddBackward0>)\n",
      "epoch: 10638 loss is tensor([-0.4204], grad_fn=<AddBackward0>)\n",
      "epoch: 10639 loss is tensor([-0.4736], grad_fn=<AddBackward0>)\n",
      "epoch: 10640 loss is tensor([-0.4650], grad_fn=<AddBackward0>)\n",
      "epoch: 10641 loss is tensor([-0.4194], grad_fn=<AddBackward0>)\n",
      "epoch: 10642 loss is tensor([-0.4194], grad_fn=<AddBackward0>)\n",
      "epoch: 10643 loss is tensor([-0.4608], grad_fn=<AddBackward0>)\n",
      "epoch: 10644 loss is tensor([-0.4199], grad_fn=<AddBackward0>)\n",
      "epoch: 10645 loss is tensor([-0.4212], grad_fn=<AddBackward0>)\n",
      "epoch: 10646 loss is tensor([-0.4737], grad_fn=<AddBackward0>)\n",
      "epoch: 10647 loss is tensor([-0.4201], grad_fn=<AddBackward0>)\n",
      "epoch: 10648 loss is tensor([-0.4501], grad_fn=<AddBackward0>)\n",
      "epoch: 10649 loss is tensor([-0.4559], grad_fn=<AddBackward0>)\n",
      "epoch: 10650 loss is tensor([-0.4820], grad_fn=<AddBackward0>)\n",
      "epoch: 10651 loss is tensor([-0.4524], grad_fn=<AddBackward0>)\n",
      "epoch: 10652 loss is tensor([-0.4979], grad_fn=<AddBackward0>)\n",
      "epoch: 10653 loss is tensor([-0.4415], grad_fn=<AddBackward0>)\n",
      "epoch: 10654 loss is tensor([-0.4748], grad_fn=<AddBackward0>)\n",
      "epoch: 10655 loss is tensor([-0.4975], grad_fn=<AddBackward0>)\n",
      "epoch: 10656 loss is tensor([-0.4809], grad_fn=<AddBackward0>)\n",
      "epoch: 10657 loss is tensor([-0.4701], grad_fn=<AddBackward0>)\n",
      "epoch: 10658 loss is tensor([-0.5091], grad_fn=<AddBackward0>)\n",
      "epoch: 10659 loss is tensor([-0.4523], grad_fn=<AddBackward0>)\n",
      "epoch: 10660 loss is tensor([-0.4787], grad_fn=<AddBackward0>)\n",
      "epoch: 10661 loss is tensor([-0.4874], grad_fn=<AddBackward0>)\n",
      "epoch: 10662 loss is tensor([-0.5109], grad_fn=<AddBackward0>)\n",
      "epoch: 10663 loss is tensor([-0.4421], grad_fn=<AddBackward0>)\n",
      "epoch: 10664 loss is tensor([-0.4940], grad_fn=<AddBackward0>)\n",
      "epoch: 10665 loss is tensor([-0.4620], grad_fn=<AddBackward0>)\n",
      "epoch: 10666 loss is tensor([-0.4908], grad_fn=<AddBackward0>)\n",
      "epoch: 10667 loss is tensor([-0.4582], grad_fn=<AddBackward0>)\n",
      "epoch: 10668 loss is tensor([-0.4834], grad_fn=<AddBackward0>)\n",
      "epoch: 10669 loss is tensor([-0.4967], grad_fn=<AddBackward0>)\n",
      "epoch: 10670 loss is tensor([-0.5387], grad_fn=<AddBackward0>)\n",
      "epoch: 10671 loss is tensor([-0.4956], grad_fn=<AddBackward0>)\n",
      "epoch: 10672 loss is tensor([-0.5057], grad_fn=<AddBackward0>)\n",
      "epoch: 10673 loss is tensor([-0.5445], grad_fn=<AddBackward0>)\n",
      "epoch: 10674 loss is tensor([-0.4730], grad_fn=<AddBackward0>)\n",
      "epoch: 10675 loss is tensor([-0.5148], grad_fn=<AddBackward0>)\n",
      "epoch: 10676 loss is tensor([-0.4483], grad_fn=<AddBackward0>)\n",
      "epoch: 10677 loss is tensor([-0.4613], grad_fn=<AddBackward0>)\n",
      "epoch: 10678 loss is tensor([-0.4750], grad_fn=<AddBackward0>)\n",
      "epoch: 10679 loss is tensor([-0.4497], grad_fn=<AddBackward0>)\n",
      "epoch: 10680 loss is tensor([-0.5303], grad_fn=<AddBackward0>)\n",
      "epoch: 10681 loss is tensor([-0.4534], grad_fn=<AddBackward0>)\n",
      "epoch: 10682 loss is tensor([-0.4493], grad_fn=<AddBackward0>)\n",
      "epoch: 10683 loss is tensor([-0.4965], grad_fn=<AddBackward0>)\n",
      "epoch: 10684 loss is tensor([-0.4692], grad_fn=<AddBackward0>)\n",
      "epoch: 10685 loss is tensor([-0.4848], grad_fn=<AddBackward0>)\n",
      "epoch: 10686 loss is tensor([-0.4881], grad_fn=<AddBackward0>)\n",
      "epoch: 10687 loss is tensor([-0.4428], grad_fn=<AddBackward0>)\n",
      "epoch: 10688 loss is tensor([-0.4654], grad_fn=<AddBackward0>)\n",
      "epoch: 10689 loss is tensor([-0.5248], grad_fn=<AddBackward0>)\n",
      "epoch: 10690 loss is tensor([-0.5105], grad_fn=<AddBackward0>)\n",
      "epoch: 10691 loss is tensor([-0.5215], grad_fn=<AddBackward0>)\n",
      "epoch: 10692 loss is tensor([-0.5139], grad_fn=<AddBackward0>)\n",
      "epoch: 10693 loss is tensor([-0.4951], grad_fn=<AddBackward0>)\n",
      "epoch: 10694 loss is tensor([-0.4410], grad_fn=<AddBackward0>)\n",
      "epoch: 10695 loss is tensor([-0.4777], grad_fn=<AddBackward0>)\n",
      "epoch: 10696 loss is tensor([-0.4544], grad_fn=<AddBackward0>)\n",
      "epoch: 10697 loss is tensor([-0.4568], grad_fn=<AddBackward0>)\n",
      "epoch: 10698 loss is tensor([-0.4111], grad_fn=<AddBackward0>)\n",
      "epoch: 10699 loss is tensor([-0.4986], grad_fn=<AddBackward0>)\n",
      "epoch: 10700 loss is tensor([-0.4145], grad_fn=<AddBackward0>)\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10701 loss is tensor([-0.5406], grad_fn=<AddBackward0>)\n",
      "epoch: 10702 loss is tensor([-0.4682], grad_fn=<AddBackward0>)\n",
      "epoch: 10703 loss is tensor([-0.3439], grad_fn=<AddBackward0>)\n",
      "epoch: 10704 loss is tensor([-0.3683], grad_fn=<AddBackward0>)\n",
      "epoch: 10705 loss is tensor([-0.4790], grad_fn=<AddBackward0>)\n",
      "epoch: 10706 loss is tensor([-0.4898], grad_fn=<AddBackward0>)\n",
      "epoch: 10707 loss is tensor([-0.4417], grad_fn=<AddBackward0>)\n",
      "epoch: 10708 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 10709 loss is tensor([-0.3937], grad_fn=<AddBackward0>)\n",
      "epoch: 10710 loss is tensor([-0.4420], grad_fn=<AddBackward0>)\n",
      "epoch: 10711 loss is tensor([-0.4738], grad_fn=<AddBackward0>)\n",
      "epoch: 10712 loss is tensor([-0.4238], grad_fn=<AddBackward0>)\n",
      "epoch: 10713 loss is tensor([-0.4339], grad_fn=<AddBackward0>)\n",
      "epoch: 10714 loss is tensor([-0.4521], grad_fn=<AddBackward0>)\n",
      "epoch: 10715 loss is tensor([-0.4446], grad_fn=<AddBackward0>)\n",
      "epoch: 10716 loss is tensor([-0.4071], grad_fn=<AddBackward0>)\n",
      "epoch: 10717 loss is tensor([-0.4459], grad_fn=<AddBackward0>)\n",
      "epoch: 10718 loss is tensor([-0.4982], grad_fn=<AddBackward0>)\n",
      "epoch: 10719 loss is tensor([-0.4766], grad_fn=<AddBackward0>)\n",
      "epoch: 10720 loss is tensor([-0.4787], grad_fn=<AddBackward0>)\n",
      "epoch: 10721 loss is tensor([-0.5017], grad_fn=<AddBackward0>)\n",
      "epoch: 10722 loss is tensor([-0.4254], grad_fn=<AddBackward0>)\n",
      "epoch: 10723 loss is tensor([-0.4538], grad_fn=<AddBackward0>)\n",
      "epoch: 10724 loss is tensor([-0.5079], grad_fn=<AddBackward0>)\n",
      "epoch: 10725 loss is tensor([-0.4273], grad_fn=<AddBackward0>)\n",
      "epoch: 10726 loss is tensor([-0.4819], grad_fn=<AddBackward0>)\n",
      "epoch: 10727 loss is tensor([-0.4954], grad_fn=<AddBackward0>)\n",
      "epoch: 10728 loss is tensor([-0.4755], grad_fn=<AddBackward0>)\n",
      "epoch: 10729 loss is tensor([-0.3815], grad_fn=<AddBackward0>)\n",
      "epoch: 10730 loss is tensor([-0.4451], grad_fn=<AddBackward0>)\n",
      "epoch: 10731 loss is tensor([-0.5097], grad_fn=<AddBackward0>)\n",
      "epoch: 10732 loss is tensor([-0.4765], grad_fn=<AddBackward0>)\n",
      "epoch: 10733 loss is tensor([-0.4412], grad_fn=<AddBackward0>)\n",
      "epoch: 10734 loss is tensor([-0.4598], grad_fn=<AddBackward0>)\n",
      "epoch: 10735 loss is tensor([-0.4562], grad_fn=<AddBackward0>)\n",
      "epoch: 10736 loss is tensor([-0.4467], grad_fn=<AddBackward0>)\n",
      "epoch: 10737 loss is tensor([-0.4317], grad_fn=<AddBackward0>)\n",
      "epoch: 10738 loss is tensor([-0.4478], grad_fn=<AddBackward0>)\n",
      "epoch: 10739 loss is tensor([-0.4704], grad_fn=<AddBackward0>)\n",
      "epoch: 10740 loss is tensor([-0.4568], grad_fn=<AddBackward0>)\n",
      "epoch: 10741 loss is tensor([-0.4420], grad_fn=<AddBackward0>)\n",
      "epoch: 10742 loss is tensor([-0.4086], grad_fn=<AddBackward0>)\n",
      "epoch: 10743 loss is tensor([-0.4804], grad_fn=<AddBackward0>)\n",
      "epoch: 10744 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 10745 loss is tensor([-0.4653], grad_fn=<AddBackward0>)\n",
      "epoch: 10746 loss is tensor([-0.4024], grad_fn=<AddBackward0>)\n",
      "epoch: 10747 loss is tensor([-0.4434], grad_fn=<AddBackward0>)\n",
      "epoch: 10748 loss is tensor([-0.4790], grad_fn=<AddBackward0>)\n",
      "epoch: 10749 loss is tensor([-0.4872], grad_fn=<AddBackward0>)\n",
      "epoch: 10750 loss is tensor([-0.4689], grad_fn=<AddBackward0>)\n",
      "epoch: 10751 loss is tensor([-0.4407], grad_fn=<AddBackward0>)\n",
      "epoch: 10752 loss is tensor([-0.4732], grad_fn=<AddBackward0>)\n",
      "epoch: 10753 loss is tensor([-0.4335], grad_fn=<AddBackward0>)\n",
      "epoch: 10754 loss is tensor([-0.4818], grad_fn=<AddBackward0>)\n",
      "epoch: 10755 loss is tensor([-0.4297], grad_fn=<AddBackward0>)\n",
      "epoch: 10756 loss is tensor([-0.5052], grad_fn=<AddBackward0>)\n",
      "epoch: 10757 loss is tensor([-0.4071], grad_fn=<AddBackward0>)\n",
      "epoch: 10758 loss is tensor([-0.4215], grad_fn=<AddBackward0>)\n",
      "epoch: 10759 loss is tensor([-0.3807], grad_fn=<AddBackward0>)\n",
      "epoch: 10760 loss is tensor([-0.4294], grad_fn=<AddBackward0>)\n",
      "epoch: 10761 loss is tensor([-0.4116], grad_fn=<AddBackward0>)\n",
      "epoch: 10762 loss is tensor([-0.3962], grad_fn=<AddBackward0>)\n",
      "epoch: 10763 loss is tensor([-0.4471], grad_fn=<AddBackward0>)\n",
      "epoch: 10764 loss is tensor([-0.4248], grad_fn=<AddBackward0>)\n",
      "epoch: 10765 loss is tensor([-0.4510], grad_fn=<AddBackward0>)\n",
      "epoch: 10766 loss is tensor([-0.4020], grad_fn=<AddBackward0>)\n",
      "epoch: 10767 loss is tensor([-0.4698], grad_fn=<AddBackward0>)\n",
      "epoch: 10768 loss is tensor([-0.4498], grad_fn=<AddBackward0>)\n",
      "epoch: 10769 loss is tensor([-0.4740], grad_fn=<AddBackward0>)\n",
      "epoch: 10770 loss is tensor([-0.4693], grad_fn=<AddBackward0>)\n",
      "epoch: 10771 loss is tensor([-0.4641], grad_fn=<AddBackward0>)\n",
      "epoch: 10772 loss is tensor([-0.4096], grad_fn=<AddBackward0>)\n",
      "epoch: 10773 loss is tensor([-0.4749], grad_fn=<AddBackward0>)\n",
      "epoch: 10774 loss is tensor([-0.4503], grad_fn=<AddBackward0>)\n",
      "epoch: 10775 loss is tensor([-0.4398], grad_fn=<AddBackward0>)\n",
      "epoch: 10776 loss is tensor([-0.4718], grad_fn=<AddBackward0>)\n",
      "epoch: 10777 loss is tensor([-0.5108], grad_fn=<AddBackward0>)\n",
      "epoch: 10778 loss is tensor([-0.4833], grad_fn=<AddBackward0>)\n",
      "epoch: 10779 loss is tensor([-0.5186], grad_fn=<AddBackward0>)\n",
      "epoch: 10780 loss is tensor([-0.4919], grad_fn=<AddBackward0>)\n",
      "epoch: 10781 loss is tensor([-0.5010], grad_fn=<AddBackward0>)\n",
      "epoch: 10782 loss is tensor([-0.4891], grad_fn=<AddBackward0>)\n",
      "epoch: 10783 loss is tensor([-0.4460], grad_fn=<AddBackward0>)\n",
      "epoch: 10784 loss is tensor([-0.4751], grad_fn=<AddBackward0>)\n",
      "epoch: 10785 loss is tensor([-0.4738], grad_fn=<AddBackward0>)\n",
      "epoch: 10786 loss is tensor([-0.5246], grad_fn=<AddBackward0>)\n",
      "epoch: 10787 loss is tensor([-0.4703], grad_fn=<AddBackward0>)\n",
      "epoch: 10788 loss is tensor([-0.5009], grad_fn=<AddBackward0>)\n",
      "epoch: 10789 loss is tensor([-0.5356], grad_fn=<AddBackward0>)\n",
      "epoch: 10790 loss is tensor([-0.5303], grad_fn=<AddBackward0>)\n",
      "epoch: 10791 loss is tensor([-0.5328], grad_fn=<AddBackward0>)\n",
      "epoch: 10792 loss is tensor([-0.4882], grad_fn=<AddBackward0>)\n",
      "epoch: 10793 loss is tensor([-0.4572], grad_fn=<AddBackward0>)\n",
      "epoch: 10794 loss is tensor([-0.4339], grad_fn=<AddBackward0>)\n",
      "epoch: 10795 loss is tensor([-0.4950], grad_fn=<AddBackward0>)\n",
      "epoch: 10796 loss is tensor([-0.4546], grad_fn=<AddBackward0>)\n",
      "epoch: 10797 loss is tensor([-0.5170], grad_fn=<AddBackward0>)\n",
      "epoch: 10798 loss is tensor([-0.4145], grad_fn=<AddBackward0>)\n",
      "epoch: 10799 loss is tensor([-0.4764], grad_fn=<AddBackward0>)\n",
      "epoch: 10800 loss is tensor([-0.5082], grad_fn=<AddBackward0>)\n",
      "52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10801 loss is tensor([-0.4579], grad_fn=<AddBackward0>)\n",
      "epoch: 10802 loss is tensor([-0.4439], grad_fn=<AddBackward0>)\n",
      "epoch: 10803 loss is tensor([-0.4131], grad_fn=<AddBackward0>)\n",
      "epoch: 10804 loss is tensor([-0.5041], grad_fn=<AddBackward0>)\n",
      "epoch: 10805 loss is tensor([-0.4777], grad_fn=<AddBackward0>)\n",
      "epoch: 10806 loss is tensor([-0.4107], grad_fn=<AddBackward0>)\n",
      "epoch: 10807 loss is tensor([-0.4611], grad_fn=<AddBackward0>)\n",
      "epoch: 10808 loss is tensor([-0.4592], grad_fn=<AddBackward0>)\n",
      "epoch: 10809 loss is tensor([-0.4213], grad_fn=<AddBackward0>)\n",
      "epoch: 10810 loss is tensor([-0.4367], grad_fn=<AddBackward0>)\n",
      "epoch: 10811 loss is tensor([-0.4732], grad_fn=<AddBackward0>)\n",
      "epoch: 10812 loss is tensor([-0.4516], grad_fn=<AddBackward0>)\n",
      "epoch: 10813 loss is tensor([-0.4506], grad_fn=<AddBackward0>)\n",
      "epoch: 10814 loss is tensor([-0.4431], grad_fn=<AddBackward0>)\n",
      "epoch: 10815 loss is tensor([-0.4599], grad_fn=<AddBackward0>)\n",
      "epoch: 10816 loss is tensor([-0.4109], grad_fn=<AddBackward0>)\n",
      "epoch: 10817 loss is tensor([-0.4626], grad_fn=<AddBackward0>)\n",
      "epoch: 10818 loss is tensor([-0.4317], grad_fn=<AddBackward0>)\n",
      "epoch: 10819 loss is tensor([-0.3853], grad_fn=<AddBackward0>)\n",
      "epoch: 10820 loss is tensor([-0.3558], grad_fn=<AddBackward0>)\n",
      "epoch: 10821 loss is tensor([-0.4613], grad_fn=<AddBackward0>)\n",
      "epoch: 10822 loss is tensor([-0.4539], grad_fn=<AddBackward0>)\n",
      "epoch: 10823 loss is tensor([-0.4080], grad_fn=<AddBackward0>)\n",
      "epoch: 10824 loss is tensor([-0.4402], grad_fn=<AddBackward0>)\n",
      "epoch: 10825 loss is tensor([-0.4734], grad_fn=<AddBackward0>)\n",
      "epoch: 10826 loss is tensor([-0.4339], grad_fn=<AddBackward0>)\n",
      "epoch: 10827 loss is tensor([-0.4685], grad_fn=<AddBackward0>)\n",
      "epoch: 10828 loss is tensor([-0.4045], grad_fn=<AddBackward0>)\n",
      "epoch: 10829 loss is tensor([-0.4719], grad_fn=<AddBackward0>)\n",
      "epoch: 10830 loss is tensor([-0.4308], grad_fn=<AddBackward0>)\n",
      "epoch: 10831 loss is tensor([-0.3953], grad_fn=<AddBackward0>)\n",
      "epoch: 10832 loss is tensor([-0.4279], grad_fn=<AddBackward0>)\n",
      "epoch: 10833 loss is tensor([-0.4275], grad_fn=<AddBackward0>)\n",
      "epoch: 10834 loss is tensor([-0.4370], grad_fn=<AddBackward0>)\n",
      "epoch: 10835 loss is tensor([-0.4005], grad_fn=<AddBackward0>)\n",
      "epoch: 10836 loss is tensor([-0.3977], grad_fn=<AddBackward0>)\n",
      "epoch: 10837 loss is tensor([-0.3858], grad_fn=<AddBackward0>)\n",
      "epoch: 10838 loss is tensor([-0.3785], grad_fn=<AddBackward0>)\n",
      "epoch: 10839 loss is tensor([-0.4178], grad_fn=<AddBackward0>)\n",
      "epoch: 10840 loss is tensor([-0.4505], grad_fn=<AddBackward0>)\n",
      "epoch: 10841 loss is tensor([-0.4884], grad_fn=<AddBackward0>)\n",
      "epoch: 10842 loss is tensor([-0.4169], grad_fn=<AddBackward0>)\n",
      "epoch: 10843 loss is tensor([-0.4877], grad_fn=<AddBackward0>)\n",
      "epoch: 10844 loss is tensor([-0.4673], grad_fn=<AddBackward0>)\n",
      "epoch: 10845 loss is tensor([-0.4996], grad_fn=<AddBackward0>)\n",
      "epoch: 10846 loss is tensor([-0.4367], grad_fn=<AddBackward0>)\n",
      "epoch: 10847 loss is tensor([-0.4677], grad_fn=<AddBackward0>)\n",
      "epoch: 10848 loss is tensor([-0.3840], grad_fn=<AddBackward0>)\n",
      "epoch: 10849 loss is tensor([-0.4405], grad_fn=<AddBackward0>)\n",
      "epoch: 10850 loss is tensor([-0.4201], grad_fn=<AddBackward0>)\n",
      "epoch: 10851 loss is tensor([-0.4176], grad_fn=<AddBackward0>)\n",
      "epoch: 10852 loss is tensor([-0.4485], grad_fn=<AddBackward0>)\n",
      "epoch: 10853 loss is tensor([-0.4382], grad_fn=<AddBackward0>)\n",
      "epoch: 10854 loss is tensor([-0.4078], grad_fn=<AddBackward0>)\n",
      "epoch: 10855 loss is tensor([-0.4365], grad_fn=<AddBackward0>)\n",
      "epoch: 10856 loss is tensor([-0.4604], grad_fn=<AddBackward0>)\n",
      "epoch: 10857 loss is tensor([-0.4868], grad_fn=<AddBackward0>)\n",
      "epoch: 10858 loss is tensor([-0.4990], grad_fn=<AddBackward0>)\n",
      "epoch: 10859 loss is tensor([-0.5037], grad_fn=<AddBackward0>)\n",
      "epoch: 10860 loss is tensor([-0.4441], grad_fn=<AddBackward0>)\n",
      "epoch: 10861 loss is tensor([-0.4495], grad_fn=<AddBackward0>)\n",
      "epoch: 10862 loss is tensor([-0.4991], grad_fn=<AddBackward0>)\n",
      "epoch: 10863 loss is tensor([-0.4728], grad_fn=<AddBackward0>)\n",
      "epoch: 10864 loss is tensor([-0.4792], grad_fn=<AddBackward0>)\n",
      "epoch: 10865 loss is tensor([-0.5198], grad_fn=<AddBackward0>)\n",
      "epoch: 10866 loss is tensor([-0.4072], grad_fn=<AddBackward0>)\n",
      "epoch: 10867 loss is tensor([-0.4619], grad_fn=<AddBackward0>)\n",
      "epoch: 10868 loss is tensor([-0.4940], grad_fn=<AddBackward0>)\n",
      "epoch: 10869 loss is tensor([-0.4575], grad_fn=<AddBackward0>)\n",
      "epoch: 10870 loss is tensor([-0.4639], grad_fn=<AddBackward0>)\n",
      "epoch: 10871 loss is tensor([-0.5058], grad_fn=<AddBackward0>)\n",
      "epoch: 10872 loss is tensor([-0.4614], grad_fn=<AddBackward0>)\n",
      "epoch: 10873 loss is tensor([-0.4992], grad_fn=<AddBackward0>)\n",
      "epoch: 10874 loss is tensor([-0.4825], grad_fn=<AddBackward0>)\n",
      "epoch: 10875 loss is tensor([-0.5120], grad_fn=<AddBackward0>)\n",
      "epoch: 10876 loss is tensor([-0.4802], grad_fn=<AddBackward0>)\n",
      "epoch: 10877 loss is tensor([-0.4620], grad_fn=<AddBackward0>)\n",
      "epoch: 10878 loss is tensor([-0.4282], grad_fn=<AddBackward0>)\n",
      "epoch: 10879 loss is tensor([-0.4802], grad_fn=<AddBackward0>)\n",
      "epoch: 10880 loss is tensor([-0.4807], grad_fn=<AddBackward0>)\n",
      "epoch: 10881 loss is tensor([-0.4817], grad_fn=<AddBackward0>)\n",
      "epoch: 10882 loss is tensor([-0.4401], grad_fn=<AddBackward0>)\n",
      "epoch: 10883 loss is tensor([-0.4847], grad_fn=<AddBackward0>)\n",
      "epoch: 10884 loss is tensor([-0.4997], grad_fn=<AddBackward0>)\n",
      "epoch: 10885 loss is tensor([-0.5351], grad_fn=<AddBackward0>)\n",
      "epoch: 10886 loss is tensor([-0.5000], grad_fn=<AddBackward0>)\n",
      "epoch: 10887 loss is tensor([-0.4309], grad_fn=<AddBackward0>)\n",
      "epoch: 10888 loss is tensor([-0.5238], grad_fn=<AddBackward0>)\n",
      "epoch: 10889 loss is tensor([-0.5033], grad_fn=<AddBackward0>)\n",
      "epoch: 10890 loss is tensor([-0.5248], grad_fn=<AddBackward0>)\n",
      "epoch: 10891 loss is tensor([-0.4860], grad_fn=<AddBackward0>)\n",
      "epoch: 10892 loss is tensor([-0.4876], grad_fn=<AddBackward0>)\n",
      "epoch: 10893 loss is tensor([-0.4948], grad_fn=<AddBackward0>)\n",
      "epoch: 10894 loss is tensor([-0.4388], grad_fn=<AddBackward0>)\n",
      "epoch: 10895 loss is tensor([-0.4509], grad_fn=<AddBackward0>)\n",
      "epoch: 10896 loss is tensor([-0.4794], grad_fn=<AddBackward0>)\n",
      "epoch: 10897 loss is tensor([-0.5224], grad_fn=<AddBackward0>)\n",
      "epoch: 10898 loss is tensor([-0.4984], grad_fn=<AddBackward0>)\n",
      "epoch: 10899 loss is tensor([-0.5113], grad_fn=<AddBackward0>)\n",
      "epoch: 10900 loss is tensor([-0.5150], grad_fn=<AddBackward0>)\n",
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10901 loss is tensor([-0.4760], grad_fn=<AddBackward0>)\n",
      "epoch: 10902 loss is tensor([-0.4510], grad_fn=<AddBackward0>)\n",
      "epoch: 10903 loss is tensor([-0.5036], grad_fn=<AddBackward0>)\n",
      "epoch: 10904 loss is tensor([-0.4698], grad_fn=<AddBackward0>)\n",
      "epoch: 10905 loss is tensor([-0.4822], grad_fn=<AddBackward0>)\n",
      "epoch: 10906 loss is tensor([-0.5087], grad_fn=<AddBackward0>)\n",
      "epoch: 10907 loss is tensor([-0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 10908 loss is tensor([-0.5131], grad_fn=<AddBackward0>)\n",
      "epoch: 10909 loss is tensor([-0.4751], grad_fn=<AddBackward0>)\n",
      "epoch: 10910 loss is tensor([-0.5178], grad_fn=<AddBackward0>)\n",
      "epoch: 10911 loss is tensor([-0.5282], grad_fn=<AddBackward0>)\n",
      "epoch: 10912 loss is tensor([-0.4656], grad_fn=<AddBackward0>)\n",
      "epoch: 10913 loss is tensor([-0.4950], grad_fn=<AddBackward0>)\n",
      "epoch: 10914 loss is tensor([-0.5032], grad_fn=<AddBackward0>)\n",
      "epoch: 10915 loss is tensor([-0.4538], grad_fn=<AddBackward0>)\n",
      "epoch: 10916 loss is tensor([-0.4301], grad_fn=<AddBackward0>)\n",
      "epoch: 10917 loss is tensor([-0.5574], grad_fn=<AddBackward0>)\n",
      "epoch: 10918 loss is tensor([-0.5070], grad_fn=<AddBackward0>)\n",
      "epoch: 10919 loss is tensor([-0.4711], grad_fn=<AddBackward0>)\n",
      "epoch: 10920 loss is tensor([-0.4942], grad_fn=<AddBackward0>)\n",
      "epoch: 10921 loss is tensor([-0.5018], grad_fn=<AddBackward0>)\n",
      "epoch: 10922 loss is tensor([-0.5405], grad_fn=<AddBackward0>)\n",
      "epoch: 10923 loss is tensor([-0.5532], grad_fn=<AddBackward0>)\n",
      "epoch: 10924 loss is tensor([-0.4947], grad_fn=<AddBackward0>)\n",
      "epoch: 10925 loss is tensor([-0.4032], grad_fn=<AddBackward0>)\n",
      "epoch: 10926 loss is tensor([-0.4538], grad_fn=<AddBackward0>)\n",
      "epoch: 10927 loss is tensor([-0.4234], grad_fn=<AddBackward0>)\n",
      "epoch: 10928 loss is tensor([-0.3708], grad_fn=<AddBackward0>)\n",
      "epoch: 10929 loss is tensor([-0.3730], grad_fn=<AddBackward0>)\n",
      "epoch: 10930 loss is tensor([-0.4987], grad_fn=<AddBackward0>)\n",
      "epoch: 10931 loss is tensor([-0.4754], grad_fn=<AddBackward0>)\n",
      "epoch: 10932 loss is tensor([-0.4105], grad_fn=<AddBackward0>)\n",
      "epoch: 10933 loss is tensor([-0.4099], grad_fn=<AddBackward0>)\n",
      "epoch: 10934 loss is tensor([-0.4401], grad_fn=<AddBackward0>)\n",
      "epoch: 10935 loss is tensor([-0.3978], grad_fn=<AddBackward0>)\n",
      "epoch: 10936 loss is tensor([-0.4542], grad_fn=<AddBackward0>)\n",
      "epoch: 10937 loss is tensor([-0.5132], grad_fn=<AddBackward0>)\n",
      "epoch: 10938 loss is tensor([-0.4277], grad_fn=<AddBackward0>)\n",
      "epoch: 10939 loss is tensor([-0.4542], grad_fn=<AddBackward0>)\n",
      "epoch: 10940 loss is tensor([-0.4700], grad_fn=<AddBackward0>)\n",
      "epoch: 10941 loss is tensor([-0.4273], grad_fn=<AddBackward0>)\n",
      "epoch: 10942 loss is tensor([-0.4625], grad_fn=<AddBackward0>)\n",
      "epoch: 10943 loss is tensor([-0.4181], grad_fn=<AddBackward0>)\n",
      "epoch: 10944 loss is tensor([-0.4933], grad_fn=<AddBackward0>)\n",
      "epoch: 10945 loss is tensor([-0.4627], grad_fn=<AddBackward0>)\n",
      "epoch: 10946 loss is tensor([-0.5117], grad_fn=<AddBackward0>)\n",
      "epoch: 10947 loss is tensor([-0.4571], grad_fn=<AddBackward0>)\n",
      "epoch: 10948 loss is tensor([-0.5282], grad_fn=<AddBackward0>)\n",
      "epoch: 10949 loss is tensor([-0.5166], grad_fn=<AddBackward0>)\n",
      "epoch: 10950 loss is tensor([-0.4935], grad_fn=<AddBackward0>)\n",
      "epoch: 10951 loss is tensor([-0.4845], grad_fn=<AddBackward0>)\n",
      "epoch: 10952 loss is tensor([-0.4758], grad_fn=<AddBackward0>)\n",
      "epoch: 10953 loss is tensor([-0.5370], grad_fn=<AddBackward0>)\n",
      "epoch: 10954 loss is tensor([-0.4482], grad_fn=<AddBackward0>)\n",
      "epoch: 10955 loss is tensor([-0.5027], grad_fn=<AddBackward0>)\n",
      "epoch: 10956 loss is tensor([-0.4616], grad_fn=<AddBackward0>)\n",
      "epoch: 10957 loss is tensor([-0.4489], grad_fn=<AddBackward0>)\n",
      "epoch: 10958 loss is tensor([-0.4855], grad_fn=<AddBackward0>)\n",
      "epoch: 10959 loss is tensor([-0.4726], grad_fn=<AddBackward0>)\n",
      "epoch: 10960 loss is tensor([-0.4053], grad_fn=<AddBackward0>)\n",
      "epoch: 10961 loss is tensor([-0.4999], grad_fn=<AddBackward0>)\n",
      "epoch: 10962 loss is tensor([-0.4681], grad_fn=<AddBackward0>)\n",
      "epoch: 10963 loss is tensor([-0.5176], grad_fn=<AddBackward0>)\n",
      "epoch: 10964 loss is tensor([-0.4516], grad_fn=<AddBackward0>)\n",
      "epoch: 10965 loss is tensor([-0.4953], grad_fn=<AddBackward0>)\n",
      "epoch: 10966 loss is tensor([-0.4857], grad_fn=<AddBackward0>)\n",
      "epoch: 10967 loss is tensor([-0.4821], grad_fn=<AddBackward0>)\n",
      "epoch: 10968 loss is tensor([-0.4868], grad_fn=<AddBackward0>)\n",
      "epoch: 10969 loss is tensor([-0.4628], grad_fn=<AddBackward0>)\n",
      "epoch: 10970 loss is tensor([-0.5168], grad_fn=<AddBackward0>)\n",
      "epoch: 10971 loss is tensor([-0.4851], grad_fn=<AddBackward0>)\n",
      "epoch: 10972 loss is tensor([-0.4833], grad_fn=<AddBackward0>)\n",
      "epoch: 10973 loss is tensor([-0.5539], grad_fn=<AddBackward0>)\n",
      "epoch: 10974 loss is tensor([-0.4831], grad_fn=<AddBackward0>)\n",
      "epoch: 10975 loss is tensor([-0.4840], grad_fn=<AddBackward0>)\n",
      "epoch: 10976 loss is tensor([-0.4626], grad_fn=<AddBackward0>)\n",
      "epoch: 10977 loss is tensor([-0.4945], grad_fn=<AddBackward0>)\n",
      "epoch: 10978 loss is tensor([-0.4553], grad_fn=<AddBackward0>)\n",
      "epoch: 10979 loss is tensor([-0.5373], grad_fn=<AddBackward0>)\n",
      "epoch: 10980 loss is tensor([-0.4358], grad_fn=<AddBackward0>)\n",
      "epoch: 10981 loss is tensor([-0.3948], grad_fn=<AddBackward0>)\n",
      "epoch: 10982 loss is tensor([-0.4668], grad_fn=<AddBackward0>)\n",
      "epoch: 10983 loss is tensor([-0.5058], grad_fn=<AddBackward0>)\n",
      "epoch: 10984 loss is tensor([-0.4395], grad_fn=<AddBackward0>)\n",
      "epoch: 10985 loss is tensor([-0.5055], grad_fn=<AddBackward0>)\n",
      "epoch: 10986 loss is tensor([-0.4739], grad_fn=<AddBackward0>)\n",
      "epoch: 10987 loss is tensor([-0.5013], grad_fn=<AddBackward0>)\n",
      "epoch: 10988 loss is tensor([-0.5387], grad_fn=<AddBackward0>)\n",
      "epoch: 10989 loss is tensor([-0.5055], grad_fn=<AddBackward0>)\n",
      "epoch: 10990 loss is tensor([-0.4816], grad_fn=<AddBackward0>)\n",
      "epoch: 10991 loss is tensor([-0.4835], grad_fn=<AddBackward0>)\n",
      "epoch: 10992 loss is tensor([-0.4682], grad_fn=<AddBackward0>)\n",
      "epoch: 10993 loss is tensor([-0.5371], grad_fn=<AddBackward0>)\n",
      "epoch: 10994 loss is tensor([-0.4924], grad_fn=<AddBackward0>)\n",
      "epoch: 10995 loss is tensor([-0.4785], grad_fn=<AddBackward0>)\n",
      "epoch: 10996 loss is tensor([-0.4722], grad_fn=<AddBackward0>)\n",
      "epoch: 10997 loss is tensor([-0.5252], grad_fn=<AddBackward0>)\n",
      "epoch: 10998 loss is tensor([-0.5040], grad_fn=<AddBackward0>)\n",
      "epoch: 10999 loss is tensor([-0.5400], grad_fn=<AddBackward0>)\n",
      "epoch: 11000 loss is tensor([-0.5092], grad_fn=<AddBackward0>)\n",
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11001 loss is tensor([-0.4597], grad_fn=<AddBackward0>)\n",
      "epoch: 11002 loss is tensor([-0.5146], grad_fn=<AddBackward0>)\n",
      "epoch: 11003 loss is tensor([-0.4938], grad_fn=<AddBackward0>)\n",
      "epoch: 11004 loss is tensor([-0.5193], grad_fn=<AddBackward0>)\n",
      "epoch: 11005 loss is tensor([-0.4728], grad_fn=<AddBackward0>)\n",
      "epoch: 11006 loss is tensor([-0.5082], grad_fn=<AddBackward0>)\n",
      "epoch: 11007 loss is tensor([-0.5183], grad_fn=<AddBackward0>)\n",
      "epoch: 11008 loss is tensor([-0.4469], grad_fn=<AddBackward0>)\n",
      "epoch: 11009 loss is tensor([-0.4672], grad_fn=<AddBackward0>)\n",
      "epoch: 11010 loss is tensor([-0.5382], grad_fn=<AddBackward0>)\n",
      "epoch: 11011 loss is tensor([-0.5124], grad_fn=<AddBackward0>)\n",
      "epoch: 11012 loss is tensor([-0.5030], grad_fn=<AddBackward0>)\n",
      "epoch: 11013 loss is tensor([-0.5155], grad_fn=<AddBackward0>)\n",
      "epoch: 11014 loss is tensor([-0.5142], grad_fn=<AddBackward0>)\n",
      "epoch: 11015 loss is tensor([-0.5237], grad_fn=<AddBackward0>)\n",
      "epoch: 11016 loss is tensor([-0.4964], grad_fn=<AddBackward0>)\n",
      "epoch: 11017 loss is tensor([-0.5101], grad_fn=<AddBackward0>)\n",
      "epoch: 11018 loss is tensor([-0.5146], grad_fn=<AddBackward0>)\n",
      "epoch: 11019 loss is tensor([-0.5436], grad_fn=<AddBackward0>)\n",
      "epoch: 11020 loss is tensor([-0.5011], grad_fn=<AddBackward0>)\n",
      "epoch: 11021 loss is tensor([-0.4574], grad_fn=<AddBackward0>)\n",
      "epoch: 11022 loss is tensor([-0.5181], grad_fn=<AddBackward0>)\n",
      "epoch: 11023 loss is tensor([-0.4826], grad_fn=<AddBackward0>)\n",
      "epoch: 11024 loss is tensor([-0.4960], grad_fn=<AddBackward0>)\n",
      "epoch: 11025 loss is tensor([-0.4988], grad_fn=<AddBackward0>)\n",
      "epoch: 11026 loss is tensor([-0.5618], grad_fn=<AddBackward0>)\n",
      "epoch: 11027 loss is tensor([-0.4959], grad_fn=<AddBackward0>)\n",
      "epoch: 11028 loss is tensor([-0.4904], grad_fn=<AddBackward0>)\n",
      "epoch: 11029 loss is tensor([-0.4788], grad_fn=<AddBackward0>)\n",
      "epoch: 11030 loss is tensor([-0.4568], grad_fn=<AddBackward0>)\n",
      "epoch: 11031 loss is tensor([-0.4228], grad_fn=<AddBackward0>)\n",
      "epoch: 11032 loss is tensor([-0.4572], grad_fn=<AddBackward0>)\n",
      "epoch: 11033 loss is tensor([-0.5105], grad_fn=<AddBackward0>)\n",
      "epoch: 11034 loss is tensor([-0.4940], grad_fn=<AddBackward0>)\n",
      "epoch: 11035 loss is tensor([-0.4859], grad_fn=<AddBackward0>)\n",
      "epoch: 11036 loss is tensor([-0.3888], grad_fn=<AddBackward0>)\n",
      "epoch: 11037 loss is tensor([-0.4612], grad_fn=<AddBackward0>)\n",
      "epoch: 11038 loss is tensor([-0.4445], grad_fn=<AddBackward0>)\n",
      "epoch: 11039 loss is tensor([-0.4800], grad_fn=<AddBackward0>)\n",
      "epoch: 11040 loss is tensor([-0.5017], grad_fn=<AddBackward0>)\n",
      "epoch: 11041 loss is tensor([-0.4853], grad_fn=<AddBackward0>)\n",
      "epoch: 11042 loss is tensor([-0.5322], grad_fn=<AddBackward0>)\n",
      "epoch: 11043 loss is tensor([-0.4558], grad_fn=<AddBackward0>)\n",
      "epoch: 11044 loss is tensor([-0.4508], grad_fn=<AddBackward0>)\n",
      "epoch: 11045 loss is tensor([-0.4841], grad_fn=<AddBackward0>)\n",
      "epoch: 11046 loss is tensor([-0.5020], grad_fn=<AddBackward0>)\n",
      "epoch: 11047 loss is tensor([-0.3940], grad_fn=<AddBackward0>)\n",
      "epoch: 11048 loss is tensor([-0.4648], grad_fn=<AddBackward0>)\n",
      "epoch: 11049 loss is tensor([-0.4495], grad_fn=<AddBackward0>)\n",
      "epoch: 11050 loss is tensor([-0.4843], grad_fn=<AddBackward0>)\n",
      "epoch: 11051 loss is tensor([-0.4843], grad_fn=<AddBackward0>)\n",
      "epoch: 11052 loss is tensor([-0.4431], grad_fn=<AddBackward0>)\n",
      "epoch: 11053 loss is tensor([-0.4967], grad_fn=<AddBackward0>)\n",
      "epoch: 11054 loss is tensor([-0.4496], grad_fn=<AddBackward0>)\n",
      "epoch: 11055 loss is tensor([-0.5329], grad_fn=<AddBackward0>)\n",
      "epoch: 11056 loss is tensor([-0.4901], grad_fn=<AddBackward0>)\n",
      "epoch: 11057 loss is tensor([-0.5070], grad_fn=<AddBackward0>)\n",
      "epoch: 11058 loss is tensor([-0.5226], grad_fn=<AddBackward0>)\n",
      "epoch: 11059 loss is tensor([-0.4956], grad_fn=<AddBackward0>)\n",
      "epoch: 11060 loss is tensor([-0.4070], grad_fn=<AddBackward0>)\n",
      "epoch: 11061 loss is tensor([-0.5428], grad_fn=<AddBackward0>)\n",
      "epoch: 11062 loss is tensor([-0.4347], grad_fn=<AddBackward0>)\n",
      "epoch: 11063 loss is tensor([-0.5458], grad_fn=<AddBackward0>)\n",
      "epoch: 11064 loss is tensor([-0.4480], grad_fn=<AddBackward0>)\n",
      "epoch: 11065 loss is tensor([-0.4368], grad_fn=<AddBackward0>)\n",
      "epoch: 11066 loss is tensor([-0.4867], grad_fn=<AddBackward0>)\n",
      "epoch: 11067 loss is tensor([-0.4929], grad_fn=<AddBackward0>)\n",
      "epoch: 11068 loss is tensor([-0.5466], grad_fn=<AddBackward0>)\n",
      "epoch: 11069 loss is tensor([-0.4920], grad_fn=<AddBackward0>)\n",
      "epoch: 11070 loss is tensor([-0.4756], grad_fn=<AddBackward0>)\n",
      "epoch: 11071 loss is tensor([-0.4766], grad_fn=<AddBackward0>)\n",
      "epoch: 11072 loss is tensor([-0.4931], grad_fn=<AddBackward0>)\n",
      "epoch: 11073 loss is tensor([-0.4980], grad_fn=<AddBackward0>)\n",
      "epoch: 11074 loss is tensor([-0.4833], grad_fn=<AddBackward0>)\n",
      "epoch: 11075 loss is tensor([-0.5030], grad_fn=<AddBackward0>)\n",
      "epoch: 11076 loss is tensor([-0.4754], grad_fn=<AddBackward0>)\n",
      "epoch: 11077 loss is tensor([-0.5013], grad_fn=<AddBackward0>)\n",
      "epoch: 11078 loss is tensor([-0.5120], grad_fn=<AddBackward0>)\n",
      "epoch: 11079 loss is tensor([-0.5089], grad_fn=<AddBackward0>)\n",
      "epoch: 11080 loss is tensor([-0.5027], grad_fn=<AddBackward0>)\n",
      "epoch: 11081 loss is tensor([-0.5208], grad_fn=<AddBackward0>)\n",
      "epoch: 11082 loss is tensor([-0.4307], grad_fn=<AddBackward0>)\n",
      "epoch: 11083 loss is tensor([-0.5256], grad_fn=<AddBackward0>)\n",
      "epoch: 11084 loss is tensor([-0.4932], grad_fn=<AddBackward0>)\n",
      "epoch: 11085 loss is tensor([-0.5180], grad_fn=<AddBackward0>)\n",
      "epoch: 11086 loss is tensor([-0.5347], grad_fn=<AddBackward0>)\n",
      "epoch: 11087 loss is tensor([-0.4967], grad_fn=<AddBackward0>)\n",
      "epoch: 11088 loss is tensor([-0.5627], grad_fn=<AddBackward0>)\n",
      "epoch: 11089 loss is tensor([-0.5346], grad_fn=<AddBackward0>)\n",
      "epoch: 11090 loss is tensor([-0.4684], grad_fn=<AddBackward0>)\n",
      "epoch: 11091 loss is tensor([-0.4727], grad_fn=<AddBackward0>)\n",
      "epoch: 11092 loss is tensor([-0.4854], grad_fn=<AddBackward0>)\n",
      "epoch: 11093 loss is tensor([-0.4850], grad_fn=<AddBackward0>)\n",
      "epoch: 11094 loss is tensor([-0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 11095 loss is tensor([-0.4126], grad_fn=<AddBackward0>)\n",
      "epoch: 11096 loss is tensor([-0.4696], grad_fn=<AddBackward0>)\n",
      "epoch: 11097 loss is tensor([-0.3720], grad_fn=<AddBackward0>)\n",
      "epoch: 11098 loss is tensor([-0.4001], grad_fn=<AddBackward0>)\n",
      "epoch: 11099 loss is tensor([-0.4281], grad_fn=<AddBackward0>)\n",
      "epoch: 11100 loss is tensor([-0.4432], grad_fn=<AddBackward0>)\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11101 loss is tensor([-0.3790], grad_fn=<AddBackward0>)\n",
      "epoch: 11102 loss is tensor([-0.4717], grad_fn=<AddBackward0>)\n",
      "epoch: 11103 loss is tensor([-0.4417], grad_fn=<AddBackward0>)\n",
      "epoch: 11104 loss is tensor([-0.4482], grad_fn=<AddBackward0>)\n",
      "epoch: 11105 loss is tensor([-0.4205], grad_fn=<AddBackward0>)\n",
      "epoch: 11106 loss is tensor([-0.4701], grad_fn=<AddBackward0>)\n",
      "epoch: 11107 loss is tensor([-0.4840], grad_fn=<AddBackward0>)\n",
      "epoch: 11108 loss is tensor([-0.4705], grad_fn=<AddBackward0>)\n",
      "epoch: 11109 loss is tensor([-0.4397], grad_fn=<AddBackward0>)\n",
      "epoch: 11110 loss is tensor([-0.5065], grad_fn=<AddBackward0>)\n",
      "epoch: 11111 loss is tensor([-0.4511], grad_fn=<AddBackward0>)\n",
      "epoch: 11112 loss is tensor([-0.4367], grad_fn=<AddBackward0>)\n",
      "epoch: 11113 loss is tensor([-0.4562], grad_fn=<AddBackward0>)\n",
      "epoch: 11114 loss is tensor([-0.5426], grad_fn=<AddBackward0>)\n",
      "epoch: 11115 loss is tensor([-0.4933], grad_fn=<AddBackward0>)\n",
      "epoch: 11116 loss is tensor([-0.4730], grad_fn=<AddBackward0>)\n",
      "epoch: 11117 loss is tensor([-0.5085], grad_fn=<AddBackward0>)\n",
      "epoch: 11118 loss is tensor([-0.4703], grad_fn=<AddBackward0>)\n",
      "epoch: 11119 loss is tensor([-0.4195], grad_fn=<AddBackward0>)\n",
      "epoch: 11120 loss is tensor([-0.4906], grad_fn=<AddBackward0>)\n",
      "epoch: 11121 loss is tensor([-0.4635], grad_fn=<AddBackward0>)\n",
      "epoch: 11122 loss is tensor([-0.5273], grad_fn=<AddBackward0>)\n",
      "epoch: 11123 loss is tensor([-0.5008], grad_fn=<AddBackward0>)\n",
      "epoch: 11124 loss is tensor([-0.5094], grad_fn=<AddBackward0>)\n",
      "epoch: 11125 loss is tensor([-0.4503], grad_fn=<AddBackward0>)\n",
      "epoch: 11126 loss is tensor([-0.4595], grad_fn=<AddBackward0>)\n",
      "epoch: 11127 loss is tensor([-0.5013], grad_fn=<AddBackward0>)\n",
      "epoch: 11128 loss is tensor([-0.4939], grad_fn=<AddBackward0>)\n",
      "epoch: 11129 loss is tensor([-0.5207], grad_fn=<AddBackward0>)\n",
      "epoch: 11130 loss is tensor([-0.5392], grad_fn=<AddBackward0>)\n",
      "epoch: 11131 loss is tensor([-0.4823], grad_fn=<AddBackward0>)\n",
      "epoch: 11132 loss is tensor([-0.4759], grad_fn=<AddBackward0>)\n",
      "epoch: 11133 loss is tensor([-0.4650], grad_fn=<AddBackward0>)\n",
      "epoch: 11134 loss is tensor([-0.5128], grad_fn=<AddBackward0>)\n",
      "epoch: 11135 loss is tensor([-0.5204], grad_fn=<AddBackward0>)\n",
      "epoch: 11136 loss is tensor([-0.5045], grad_fn=<AddBackward0>)\n",
      "epoch: 11137 loss is tensor([-0.5225], grad_fn=<AddBackward0>)\n",
      "epoch: 11138 loss is tensor([-0.4800], grad_fn=<AddBackward0>)\n",
      "epoch: 11139 loss is tensor([-0.5394], grad_fn=<AddBackward0>)\n",
      "epoch: 11140 loss is tensor([-0.4924], grad_fn=<AddBackward0>)\n",
      "epoch: 11141 loss is tensor([-0.5128], grad_fn=<AddBackward0>)\n",
      "epoch: 11142 loss is tensor([-0.4804], grad_fn=<AddBackward0>)\n",
      "epoch: 11143 loss is tensor([-0.5150], grad_fn=<AddBackward0>)\n",
      "epoch: 11144 loss is tensor([-0.5463], grad_fn=<AddBackward0>)\n",
      "epoch: 11145 loss is tensor([-0.5149], grad_fn=<AddBackward0>)\n",
      "epoch: 11146 loss is tensor([-0.4991], grad_fn=<AddBackward0>)\n",
      "epoch: 11147 loss is tensor([-0.5302], grad_fn=<AddBackward0>)\n",
      "epoch: 11148 loss is tensor([-0.4970], grad_fn=<AddBackward0>)\n",
      "epoch: 11149 loss is tensor([-0.5393], grad_fn=<AddBackward0>)\n",
      "epoch: 11150 loss is tensor([-0.5030], grad_fn=<AddBackward0>)\n",
      "epoch: 11151 loss is tensor([-0.4349], grad_fn=<AddBackward0>)\n",
      "epoch: 11152 loss is tensor([-0.5223], grad_fn=<AddBackward0>)\n",
      "epoch: 11153 loss is tensor([-0.4479], grad_fn=<AddBackward0>)\n",
      "epoch: 11154 loss is tensor([-0.5256], grad_fn=<AddBackward0>)\n",
      "epoch: 11155 loss is tensor([-0.5134], grad_fn=<AddBackward0>)\n",
      "epoch: 11156 loss is tensor([-0.5296], grad_fn=<AddBackward0>)\n",
      "epoch: 11157 loss is tensor([-0.5241], grad_fn=<AddBackward0>)\n",
      "epoch: 11158 loss is tensor([-0.4792], grad_fn=<AddBackward0>)\n",
      "epoch: 11159 loss is tensor([-0.5491], grad_fn=<AddBackward0>)\n",
      "epoch: 11160 loss is tensor([-0.4806], grad_fn=<AddBackward0>)\n",
      "epoch: 11161 loss is tensor([-0.4420], grad_fn=<AddBackward0>)\n",
      "epoch: 11162 loss is tensor([-0.5340], grad_fn=<AddBackward0>)\n",
      "epoch: 11163 loss is tensor([-0.5049], grad_fn=<AddBackward0>)\n",
      "epoch: 11164 loss is tensor([-0.5450], grad_fn=<AddBackward0>)\n",
      "epoch: 11165 loss is tensor([-0.5446], grad_fn=<AddBackward0>)\n",
      "epoch: 11166 loss is tensor([-0.5066], grad_fn=<AddBackward0>)\n",
      "epoch: 11167 loss is tensor([-0.4460], grad_fn=<AddBackward0>)\n",
      "epoch: 11168 loss is tensor([-0.4437], grad_fn=<AddBackward0>)\n",
      "epoch: 11169 loss is tensor([-0.4869], grad_fn=<AddBackward0>)\n",
      "epoch: 11170 loss is tensor([-0.4727], grad_fn=<AddBackward0>)\n",
      "epoch: 11171 loss is tensor([-0.4240], grad_fn=<AddBackward0>)\n",
      "epoch: 11172 loss is tensor([-0.4668], grad_fn=<AddBackward0>)\n",
      "epoch: 11173 loss is tensor([-0.4773], grad_fn=<AddBackward0>)\n",
      "epoch: 11174 loss is tensor([-0.4237], grad_fn=<AddBackward0>)\n",
      "epoch: 11175 loss is tensor([-0.4857], grad_fn=<AddBackward0>)\n",
      "epoch: 11176 loss is tensor([-0.4647], grad_fn=<AddBackward0>)\n",
      "epoch: 11177 loss is tensor([-0.3982], grad_fn=<AddBackward0>)\n",
      "epoch: 11178 loss is tensor([-0.4057], grad_fn=<AddBackward0>)\n",
      "epoch: 11179 loss is tensor([-0.4933], grad_fn=<AddBackward0>)\n",
      "epoch: 11180 loss is tensor([-0.4986], grad_fn=<AddBackward0>)\n",
      "epoch: 11181 loss is tensor([-0.4534], grad_fn=<AddBackward0>)\n",
      "epoch: 11182 loss is tensor([-0.4693], grad_fn=<AddBackward0>)\n",
      "epoch: 11183 loss is tensor([-0.5210], grad_fn=<AddBackward0>)\n",
      "epoch: 11184 loss is tensor([-0.5127], grad_fn=<AddBackward0>)\n",
      "epoch: 11185 loss is tensor([-0.4455], grad_fn=<AddBackward0>)\n",
      "epoch: 11186 loss is tensor([-0.5005], grad_fn=<AddBackward0>)\n",
      "epoch: 11187 loss is tensor([-0.4989], grad_fn=<AddBackward0>)\n",
      "epoch: 11188 loss is tensor([-0.4812], grad_fn=<AddBackward0>)\n",
      "epoch: 11189 loss is tensor([-0.5467], grad_fn=<AddBackward0>)\n",
      "epoch: 11190 loss is tensor([-0.4658], grad_fn=<AddBackward0>)\n",
      "epoch: 11191 loss is tensor([-0.4551], grad_fn=<AddBackward0>)\n",
      "epoch: 11192 loss is tensor([-0.4672], grad_fn=<AddBackward0>)\n",
      "epoch: 11193 loss is tensor([-0.4751], grad_fn=<AddBackward0>)\n",
      "epoch: 11194 loss is tensor([-0.4121], grad_fn=<AddBackward0>)\n",
      "epoch: 11195 loss is tensor([-0.4280], grad_fn=<AddBackward0>)\n",
      "epoch: 11196 loss is tensor([-0.4773], grad_fn=<AddBackward0>)\n",
      "epoch: 11197 loss is tensor([-0.4237], grad_fn=<AddBackward0>)\n",
      "epoch: 11198 loss is tensor([-0.5283], grad_fn=<AddBackward0>)\n",
      "epoch: 11199 loss is tensor([-0.5166], grad_fn=<AddBackward0>)\n",
      "epoch: 11200 loss is tensor([-0.4642], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11201 loss is tensor([-0.4575], grad_fn=<AddBackward0>)\n",
      "epoch: 11202 loss is tensor([-0.5073], grad_fn=<AddBackward0>)\n",
      "epoch: 11203 loss is tensor([-0.4757], grad_fn=<AddBackward0>)\n",
      "epoch: 11204 loss is tensor([-0.4945], grad_fn=<AddBackward0>)\n",
      "epoch: 11205 loss is tensor([-0.4754], grad_fn=<AddBackward0>)\n",
      "epoch: 11206 loss is tensor([-0.4914], grad_fn=<AddBackward0>)\n",
      "epoch: 11207 loss is tensor([-0.5043], grad_fn=<AddBackward0>)\n",
      "epoch: 11208 loss is tensor([-0.5083], grad_fn=<AddBackward0>)\n",
      "epoch: 11209 loss is tensor([-0.4976], grad_fn=<AddBackward0>)\n",
      "epoch: 11210 loss is tensor([-0.4810], grad_fn=<AddBackward0>)\n",
      "epoch: 11211 loss is tensor([-0.5539], grad_fn=<AddBackward0>)\n",
      "epoch: 11212 loss is tensor([-0.5229], grad_fn=<AddBackward0>)\n",
      "epoch: 11213 loss is tensor([-0.4707], grad_fn=<AddBackward0>)\n",
      "epoch: 11214 loss is tensor([-0.4709], grad_fn=<AddBackward0>)\n",
      "epoch: 11215 loss is tensor([-0.4841], grad_fn=<AddBackward0>)\n",
      "epoch: 11216 loss is tensor([-0.5304], grad_fn=<AddBackward0>)\n",
      "epoch: 11217 loss is tensor([-0.5164], grad_fn=<AddBackward0>)\n",
      "epoch: 11218 loss is tensor([-0.5322], grad_fn=<AddBackward0>)\n",
      "epoch: 11219 loss is tensor([-0.5333], grad_fn=<AddBackward0>)\n",
      "epoch: 11220 loss is tensor([-0.5208], grad_fn=<AddBackward0>)\n",
      "epoch: 11221 loss is tensor([-0.4633], grad_fn=<AddBackward0>)\n",
      "epoch: 11222 loss is tensor([-0.4511], grad_fn=<AddBackward0>)\n",
      "epoch: 11223 loss is tensor([-0.5221], grad_fn=<AddBackward0>)\n",
      "epoch: 11224 loss is tensor([-0.4908], grad_fn=<AddBackward0>)\n",
      "epoch: 11225 loss is tensor([-0.5337], grad_fn=<AddBackward0>)\n",
      "epoch: 11226 loss is tensor([-0.5008], grad_fn=<AddBackward0>)\n",
      "epoch: 11227 loss is tensor([-0.5386], grad_fn=<AddBackward0>)\n",
      "epoch: 11228 loss is tensor([-0.4660], grad_fn=<AddBackward0>)\n",
      "epoch: 11229 loss is tensor([-0.4682], grad_fn=<AddBackward0>)\n",
      "epoch: 11230 loss is tensor([-0.4653], grad_fn=<AddBackward0>)\n",
      "epoch: 11231 loss is tensor([-0.4571], grad_fn=<AddBackward0>)\n",
      "epoch: 11232 loss is tensor([-0.4580], grad_fn=<AddBackward0>)\n",
      "epoch: 11233 loss is tensor([-0.4846], grad_fn=<AddBackward0>)\n",
      "epoch: 11234 loss is tensor([-0.3939], grad_fn=<AddBackward0>)\n",
      "epoch: 11235 loss is tensor([-0.4942], grad_fn=<AddBackward0>)\n",
      "epoch: 11236 loss is tensor([-0.5198], grad_fn=<AddBackward0>)\n",
      "epoch: 11237 loss is tensor([-0.4913], grad_fn=<AddBackward0>)\n",
      "epoch: 11238 loss is tensor([-0.5114], grad_fn=<AddBackward0>)\n",
      "epoch: 11239 loss is tensor([-0.5243], grad_fn=<AddBackward0>)\n",
      "epoch: 11240 loss is tensor([-0.5182], grad_fn=<AddBackward0>)\n",
      "epoch: 11241 loss is tensor([-0.4636], grad_fn=<AddBackward0>)\n",
      "epoch: 11242 loss is tensor([-0.5136], grad_fn=<AddBackward0>)\n",
      "epoch: 11243 loss is tensor([-0.4954], grad_fn=<AddBackward0>)\n",
      "epoch: 11244 loss is tensor([-0.5108], grad_fn=<AddBackward0>)\n",
      "epoch: 11245 loss is tensor([-0.5130], grad_fn=<AddBackward0>)\n",
      "epoch: 11246 loss is tensor([-0.4806], grad_fn=<AddBackward0>)\n",
      "epoch: 11247 loss is tensor([-0.5084], grad_fn=<AddBackward0>)\n",
      "epoch: 11248 loss is tensor([-0.5593], grad_fn=<AddBackward0>)\n",
      "epoch: 11249 loss is tensor([-0.5191], grad_fn=<AddBackward0>)\n",
      "epoch: 11250 loss is tensor([-0.4853], grad_fn=<AddBackward0>)\n",
      "epoch: 11251 loss is tensor([-0.4736], grad_fn=<AddBackward0>)\n",
      "epoch: 11252 loss is tensor([-0.5415], grad_fn=<AddBackward0>)\n",
      "epoch: 11253 loss is tensor([-0.5344], grad_fn=<AddBackward0>)\n",
      "epoch: 11254 loss is tensor([-0.4863], grad_fn=<AddBackward0>)\n",
      "epoch: 11255 loss is tensor([-0.4522], grad_fn=<AddBackward0>)\n",
      "epoch: 11256 loss is tensor([-0.4161], grad_fn=<AddBackward0>)\n",
      "epoch: 11257 loss is tensor([-0.3944], grad_fn=<AddBackward0>)\n",
      "epoch: 11258 loss is tensor([-0.4803], grad_fn=<AddBackward0>)\n",
      "epoch: 11259 loss is tensor([-0.4594], grad_fn=<AddBackward0>)\n",
      "epoch: 11260 loss is tensor([-0.4443], grad_fn=<AddBackward0>)\n",
      "epoch: 11261 loss is tensor([-0.3951], grad_fn=<AddBackward0>)\n",
      "epoch: 11262 loss is tensor([-0.4265], grad_fn=<AddBackward0>)\n",
      "epoch: 11263 loss is tensor([-0.3810], grad_fn=<AddBackward0>)\n",
      "epoch: 11264 loss is tensor([-0.5068], grad_fn=<AddBackward0>)\n",
      "epoch: 11265 loss is tensor([-0.4763], grad_fn=<AddBackward0>)\n",
      "epoch: 11266 loss is tensor([-0.4373], grad_fn=<AddBackward0>)\n",
      "epoch: 11267 loss is tensor([-0.4695], grad_fn=<AddBackward0>)\n",
      "epoch: 11268 loss is tensor([-0.3993], grad_fn=<AddBackward0>)\n",
      "epoch: 11269 loss is tensor([-0.3746], grad_fn=<AddBackward0>)\n",
      "epoch: 11270 loss is tensor([-0.4570], grad_fn=<AddBackward0>)\n",
      "epoch: 11271 loss is tensor([-0.4192], grad_fn=<AddBackward0>)\n",
      "epoch: 11272 loss is tensor([-0.4133], grad_fn=<AddBackward0>)\n",
      "epoch: 11273 loss is tensor([-0.4524], grad_fn=<AddBackward0>)\n",
      "epoch: 11274 loss is tensor([-0.4138], grad_fn=<AddBackward0>)\n",
      "epoch: 11275 loss is tensor([-0.4410], grad_fn=<AddBackward0>)\n",
      "epoch: 11276 loss is tensor([-0.4650], grad_fn=<AddBackward0>)\n",
      "epoch: 11277 loss is tensor([-0.4608], grad_fn=<AddBackward0>)\n",
      "epoch: 11278 loss is tensor([-0.5170], grad_fn=<AddBackward0>)\n",
      "epoch: 11279 loss is tensor([-0.3968], grad_fn=<AddBackward0>)\n",
      "epoch: 11280 loss is tensor([-0.4585], grad_fn=<AddBackward0>)\n",
      "epoch: 11281 loss is tensor([-0.4397], grad_fn=<AddBackward0>)\n",
      "epoch: 11282 loss is tensor([-0.4863], grad_fn=<AddBackward0>)\n",
      "epoch: 11283 loss is tensor([-0.4680], grad_fn=<AddBackward0>)\n",
      "epoch: 11284 loss is tensor([-0.5436], grad_fn=<AddBackward0>)\n",
      "epoch: 11285 loss is tensor([-0.5081], grad_fn=<AddBackward0>)\n",
      "epoch: 11286 loss is tensor([-0.4814], grad_fn=<AddBackward0>)\n",
      "epoch: 11287 loss is tensor([-0.4967], grad_fn=<AddBackward0>)\n",
      "epoch: 11288 loss is tensor([-0.4914], grad_fn=<AddBackward0>)\n",
      "epoch: 11289 loss is tensor([-0.4551], grad_fn=<AddBackward0>)\n",
      "epoch: 11290 loss is tensor([-0.4613], grad_fn=<AddBackward0>)\n",
      "epoch: 11291 loss is tensor([-0.4523], grad_fn=<AddBackward0>)\n",
      "epoch: 11292 loss is tensor([-0.5404], grad_fn=<AddBackward0>)\n",
      "epoch: 11293 loss is tensor([-0.4680], grad_fn=<AddBackward0>)\n",
      "epoch: 11294 loss is tensor([-0.5128], grad_fn=<AddBackward0>)\n",
      "epoch: 11295 loss is tensor([-0.4956], grad_fn=<AddBackward0>)\n",
      "epoch: 11296 loss is tensor([-0.5172], grad_fn=<AddBackward0>)\n",
      "epoch: 11297 loss is tensor([-0.4830], grad_fn=<AddBackward0>)\n",
      "epoch: 11298 loss is tensor([-0.5251], grad_fn=<AddBackward0>)\n",
      "epoch: 11299 loss is tensor([-0.4412], grad_fn=<AddBackward0>)\n",
      "epoch: 11300 loss is tensor([-0.4966], grad_fn=<AddBackward0>)\n",
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11301 loss is tensor([-0.5119], grad_fn=<AddBackward0>)\n",
      "epoch: 11302 loss is tensor([-0.4994], grad_fn=<AddBackward0>)\n",
      "epoch: 11303 loss is tensor([-0.5077], grad_fn=<AddBackward0>)\n",
      "epoch: 11304 loss is tensor([-0.4983], grad_fn=<AddBackward0>)\n",
      "epoch: 11305 loss is tensor([-0.4291], grad_fn=<AddBackward0>)\n",
      "epoch: 11306 loss is tensor([-0.4962], grad_fn=<AddBackward0>)\n",
      "epoch: 11307 loss is tensor([-0.4904], grad_fn=<AddBackward0>)\n",
      "epoch: 11308 loss is tensor([-0.5160], grad_fn=<AddBackward0>)\n",
      "epoch: 11309 loss is tensor([-0.4953], grad_fn=<AddBackward0>)\n",
      "epoch: 11310 loss is tensor([-0.5189], grad_fn=<AddBackward0>)\n",
      "epoch: 11311 loss is tensor([-0.4960], grad_fn=<AddBackward0>)\n",
      "epoch: 11312 loss is tensor([-0.5165], grad_fn=<AddBackward0>)\n",
      "epoch: 11313 loss is tensor([-0.4632], grad_fn=<AddBackward0>)\n",
      "epoch: 11314 loss is tensor([-0.5083], grad_fn=<AddBackward0>)\n",
      "epoch: 11315 loss is tensor([-0.4965], grad_fn=<AddBackward0>)\n",
      "epoch: 11316 loss is tensor([-0.4911], grad_fn=<AddBackward0>)\n",
      "epoch: 11317 loss is tensor([-0.5196], grad_fn=<AddBackward0>)\n",
      "epoch: 11318 loss is tensor([-0.4325], grad_fn=<AddBackward0>)\n",
      "epoch: 11319 loss is tensor([-0.5051], grad_fn=<AddBackward0>)\n",
      "epoch: 11320 loss is tensor([-0.4329], grad_fn=<AddBackward0>)\n",
      "epoch: 11321 loss is tensor([-0.4510], grad_fn=<AddBackward0>)\n",
      "epoch: 11322 loss is tensor([-0.4862], grad_fn=<AddBackward0>)\n",
      "epoch: 11323 loss is tensor([-0.4998], grad_fn=<AddBackward0>)\n",
      "epoch: 11324 loss is tensor([-0.3890], grad_fn=<AddBackward0>)\n",
      "epoch: 11325 loss is tensor([-0.4813], grad_fn=<AddBackward0>)\n",
      "epoch: 11326 loss is tensor([-0.5123], grad_fn=<AddBackward0>)\n",
      "epoch: 11327 loss is tensor([-0.5257], grad_fn=<AddBackward0>)\n",
      "epoch: 11328 loss is tensor([-0.4930], grad_fn=<AddBackward0>)\n",
      "epoch: 11329 loss is tensor([-0.4483], grad_fn=<AddBackward0>)\n",
      "epoch: 11330 loss is tensor([-0.5020], grad_fn=<AddBackward0>)\n",
      "epoch: 11331 loss is tensor([-0.5747], grad_fn=<AddBackward0>)\n",
      "epoch: 11332 loss is tensor([-0.5030], grad_fn=<AddBackward0>)\n",
      "epoch: 11333 loss is tensor([-0.5303], grad_fn=<AddBackward0>)\n",
      "epoch: 11334 loss is tensor([-0.4638], grad_fn=<AddBackward0>)\n",
      "epoch: 11335 loss is tensor([-0.4653], grad_fn=<AddBackward0>)\n",
      "epoch: 11336 loss is tensor([-0.4015], grad_fn=<AddBackward0>)\n",
      "epoch: 11337 loss is tensor([-0.4619], grad_fn=<AddBackward0>)\n",
      "epoch: 11338 loss is tensor([-0.4732], grad_fn=<AddBackward0>)\n",
      "epoch: 11339 loss is tensor([-0.4929], grad_fn=<AddBackward0>)\n",
      "epoch: 11340 loss is tensor([-0.4987], grad_fn=<AddBackward0>)\n",
      "epoch: 11341 loss is tensor([-0.5088], grad_fn=<AddBackward0>)\n",
      "epoch: 11342 loss is tensor([-0.5199], grad_fn=<AddBackward0>)\n",
      "epoch: 11343 loss is tensor([-0.4543], grad_fn=<AddBackward0>)\n",
      "epoch: 11344 loss is tensor([-0.4177], grad_fn=<AddBackward0>)\n",
      "epoch: 11345 loss is tensor([-0.4773], grad_fn=<AddBackward0>)\n",
      "epoch: 11346 loss is tensor([-0.4837], grad_fn=<AddBackward0>)\n",
      "epoch: 11347 loss is tensor([-0.4300], grad_fn=<AddBackward0>)\n",
      "epoch: 11348 loss is tensor([-0.5058], grad_fn=<AddBackward0>)\n",
      "epoch: 11349 loss is tensor([-0.4756], grad_fn=<AddBackward0>)\n",
      "epoch: 11350 loss is tensor([-0.4999], grad_fn=<AddBackward0>)\n",
      "epoch: 11351 loss is tensor([-0.4982], grad_fn=<AddBackward0>)\n",
      "epoch: 11352 loss is tensor([-0.4857], grad_fn=<AddBackward0>)\n",
      "epoch: 11353 loss is tensor([-0.4376], grad_fn=<AddBackward0>)\n",
      "epoch: 11354 loss is tensor([-0.4604], grad_fn=<AddBackward0>)\n",
      "epoch: 11355 loss is tensor([-0.5062], grad_fn=<AddBackward0>)\n",
      "epoch: 11356 loss is tensor([-0.5178], grad_fn=<AddBackward0>)\n",
      "epoch: 11357 loss is tensor([-0.5132], grad_fn=<AddBackward0>)\n",
      "epoch: 11358 loss is tensor([-0.5112], grad_fn=<AddBackward0>)\n",
      "epoch: 11359 loss is tensor([-0.4135], grad_fn=<AddBackward0>)\n",
      "epoch: 11360 loss is tensor([-0.5092], grad_fn=<AddBackward0>)\n",
      "epoch: 11361 loss is tensor([-0.4563], grad_fn=<AddBackward0>)\n",
      "epoch: 11362 loss is tensor([-0.4992], grad_fn=<AddBackward0>)\n",
      "epoch: 11363 loss is tensor([-0.4422], grad_fn=<AddBackward0>)\n",
      "epoch: 11364 loss is tensor([-0.4995], grad_fn=<AddBackward0>)\n",
      "epoch: 11365 loss is tensor([-0.4663], grad_fn=<AddBackward0>)\n",
      "epoch: 11366 loss is tensor([-0.4313], grad_fn=<AddBackward0>)\n",
      "epoch: 11367 loss is tensor([-0.5316], grad_fn=<AddBackward0>)\n",
      "epoch: 11368 loss is tensor([-0.4829], grad_fn=<AddBackward0>)\n",
      "epoch: 11369 loss is tensor([-0.5071], grad_fn=<AddBackward0>)\n",
      "epoch: 11370 loss is tensor([-0.4835], grad_fn=<AddBackward0>)\n",
      "epoch: 11371 loss is tensor([-0.5207], grad_fn=<AddBackward0>)\n",
      "epoch: 11372 loss is tensor([-0.4779], grad_fn=<AddBackward0>)\n",
      "epoch: 11373 loss is tensor([-0.4745], grad_fn=<AddBackward0>)\n",
      "epoch: 11374 loss is tensor([-0.4891], grad_fn=<AddBackward0>)\n",
      "epoch: 11375 loss is tensor([-0.4576], grad_fn=<AddBackward0>)\n",
      "epoch: 11376 loss is tensor([-0.4594], grad_fn=<AddBackward0>)\n",
      "epoch: 11377 loss is tensor([-0.5223], grad_fn=<AddBackward0>)\n",
      "epoch: 11378 loss is tensor([-0.4840], grad_fn=<AddBackward0>)\n",
      "epoch: 11379 loss is tensor([-0.4744], grad_fn=<AddBackward0>)\n",
      "epoch: 11380 loss is tensor([-0.5220], grad_fn=<AddBackward0>)\n",
      "epoch: 11381 loss is tensor([-0.4702], grad_fn=<AddBackward0>)\n",
      "epoch: 11382 loss is tensor([-0.5128], grad_fn=<AddBackward0>)\n",
      "epoch: 11383 loss is tensor([-0.5363], grad_fn=<AddBackward0>)\n",
      "epoch: 11384 loss is tensor([-0.5403], grad_fn=<AddBackward0>)\n",
      "epoch: 11385 loss is tensor([-0.5165], grad_fn=<AddBackward0>)\n",
      "epoch: 11386 loss is tensor([-0.4398], grad_fn=<AddBackward0>)\n",
      "epoch: 11387 loss is tensor([-0.4332], grad_fn=<AddBackward0>)\n",
      "epoch: 11388 loss is tensor([-0.4163], grad_fn=<AddBackward0>)\n",
      "epoch: 11389 loss is tensor([-0.4207], grad_fn=<AddBackward0>)\n",
      "epoch: 11390 loss is tensor([-0.4513], grad_fn=<AddBackward0>)\n",
      "epoch: 11391 loss is tensor([-0.4969], grad_fn=<AddBackward0>)\n",
      "epoch: 11392 loss is tensor([-0.4695], grad_fn=<AddBackward0>)\n",
      "epoch: 11393 loss is tensor([-0.5018], grad_fn=<AddBackward0>)\n",
      "epoch: 11394 loss is tensor([-0.5328], grad_fn=<AddBackward0>)\n",
      "epoch: 11395 loss is tensor([-0.4759], grad_fn=<AddBackward0>)\n",
      "epoch: 11396 loss is tensor([-0.5003], grad_fn=<AddBackward0>)\n",
      "epoch: 11397 loss is tensor([-0.4831], grad_fn=<AddBackward0>)\n",
      "epoch: 11398 loss is tensor([-0.4818], grad_fn=<AddBackward0>)\n",
      "epoch: 11399 loss is tensor([-0.5038], grad_fn=<AddBackward0>)\n",
      "epoch: 11400 loss is tensor([-0.5233], grad_fn=<AddBackward0>)\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11401 loss is tensor([-0.5224], grad_fn=<AddBackward0>)\n",
      "epoch: 11402 loss is tensor([-0.4695], grad_fn=<AddBackward0>)\n",
      "epoch: 11403 loss is tensor([-0.5092], grad_fn=<AddBackward0>)\n",
      "epoch: 11404 loss is tensor([-0.4756], grad_fn=<AddBackward0>)\n",
      "epoch: 11405 loss is tensor([-0.5041], grad_fn=<AddBackward0>)\n",
      "epoch: 11406 loss is tensor([-0.4679], grad_fn=<AddBackward0>)\n",
      "epoch: 11407 loss is tensor([-0.5217], grad_fn=<AddBackward0>)\n",
      "epoch: 11408 loss is tensor([-0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 11409 loss is tensor([-0.5094], grad_fn=<AddBackward0>)\n",
      "epoch: 11410 loss is tensor([-0.4877], grad_fn=<AddBackward0>)\n",
      "epoch: 11411 loss is tensor([-0.5538], grad_fn=<AddBackward0>)\n",
      "epoch: 11412 loss is tensor([-0.4681], grad_fn=<AddBackward0>)\n",
      "epoch: 11413 loss is tensor([-0.5190], grad_fn=<AddBackward0>)\n",
      "epoch: 11414 loss is tensor([-0.4999], grad_fn=<AddBackward0>)\n",
      "epoch: 11415 loss is tensor([-0.5237], grad_fn=<AddBackward0>)\n",
      "epoch: 11416 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 11417 loss is tensor([-0.5387], grad_fn=<AddBackward0>)\n",
      "epoch: 11418 loss is tensor([-0.4780], grad_fn=<AddBackward0>)\n",
      "epoch: 11419 loss is tensor([-0.5328], grad_fn=<AddBackward0>)\n",
      "epoch: 11420 loss is tensor([-0.5168], grad_fn=<AddBackward0>)\n",
      "epoch: 11421 loss is tensor([-0.4952], grad_fn=<AddBackward0>)\n",
      "epoch: 11422 loss is tensor([-0.5364], grad_fn=<AddBackward0>)\n",
      "epoch: 11423 loss is tensor([-0.5142], grad_fn=<AddBackward0>)\n",
      "epoch: 11424 loss is tensor([-0.5363], grad_fn=<AddBackward0>)\n",
      "epoch: 11425 loss is tensor([-0.5287], grad_fn=<AddBackward0>)\n",
      "epoch: 11426 loss is tensor([-0.5272], grad_fn=<AddBackward0>)\n",
      "epoch: 11427 loss is tensor([-0.5349], grad_fn=<AddBackward0>)\n",
      "epoch: 11428 loss is tensor([-0.5025], grad_fn=<AddBackward0>)\n",
      "epoch: 11429 loss is tensor([-0.4944], grad_fn=<AddBackward0>)\n",
      "epoch: 11430 loss is tensor([-0.4691], grad_fn=<AddBackward0>)\n",
      "epoch: 11431 loss is tensor([-0.5547], grad_fn=<AddBackward0>)\n",
      "epoch: 11432 loss is tensor([-0.4672], grad_fn=<AddBackward0>)\n",
      "epoch: 11433 loss is tensor([-0.5663], grad_fn=<AddBackward0>)\n",
      "epoch: 11434 loss is tensor([-0.5124], grad_fn=<AddBackward0>)\n",
      "epoch: 11435 loss is tensor([-0.5007], grad_fn=<AddBackward0>)\n",
      "epoch: 11436 loss is tensor([-0.5593], grad_fn=<AddBackward0>)\n",
      "epoch: 11437 loss is tensor([-0.5012], grad_fn=<AddBackward0>)\n",
      "epoch: 11438 loss is tensor([-0.4487], grad_fn=<AddBackward0>)\n",
      "epoch: 11439 loss is tensor([-0.5461], grad_fn=<AddBackward0>)\n",
      "epoch: 11440 loss is tensor([-0.4677], grad_fn=<AddBackward0>)\n",
      "epoch: 11441 loss is tensor([-0.4926], grad_fn=<AddBackward0>)\n",
      "epoch: 11442 loss is tensor([-0.5253], grad_fn=<AddBackward0>)\n",
      "epoch: 11443 loss is tensor([-0.4546], grad_fn=<AddBackward0>)\n",
      "epoch: 11444 loss is tensor([-0.4044], grad_fn=<AddBackward0>)\n",
      "epoch: 11445 loss is tensor([-0.4607], grad_fn=<AddBackward0>)\n",
      "epoch: 11446 loss is tensor([-0.4763], grad_fn=<AddBackward0>)\n",
      "epoch: 11447 loss is tensor([-0.4886], grad_fn=<AddBackward0>)\n",
      "epoch: 11448 loss is tensor([-0.4563], grad_fn=<AddBackward0>)\n",
      "epoch: 11449 loss is tensor([-0.5042], grad_fn=<AddBackward0>)\n",
      "epoch: 11450 loss is tensor([-0.4887], grad_fn=<AddBackward0>)\n",
      "epoch: 11451 loss is tensor([-0.4707], grad_fn=<AddBackward0>)\n",
      "epoch: 11452 loss is tensor([-0.4623], grad_fn=<AddBackward0>)\n",
      "epoch: 11453 loss is tensor([-0.5518], grad_fn=<AddBackward0>)\n",
      "epoch: 11454 loss is tensor([-0.4774], grad_fn=<AddBackward0>)\n",
      "epoch: 11455 loss is tensor([-0.5004], grad_fn=<AddBackward0>)\n",
      "epoch: 11456 loss is tensor([-0.4831], grad_fn=<AddBackward0>)\n",
      "epoch: 11457 loss is tensor([-0.5191], grad_fn=<AddBackward0>)\n",
      "epoch: 11458 loss is tensor([-0.4350], grad_fn=<AddBackward0>)\n",
      "epoch: 11459 loss is tensor([-0.5268], grad_fn=<AddBackward0>)\n",
      "epoch: 11460 loss is tensor([-0.5234], grad_fn=<AddBackward0>)\n",
      "epoch: 11461 loss is tensor([-0.5531], grad_fn=<AddBackward0>)\n",
      "epoch: 11462 loss is tensor([-0.5513], grad_fn=<AddBackward0>)\n",
      "epoch: 11463 loss is tensor([-0.5067], grad_fn=<AddBackward0>)\n",
      "epoch: 11464 loss is tensor([-0.4625], grad_fn=<AddBackward0>)\n",
      "epoch: 11465 loss is tensor([-0.4335], grad_fn=<AddBackward0>)\n",
      "epoch: 11466 loss is tensor([-0.4559], grad_fn=<AddBackward0>)\n",
      "epoch: 11467 loss is tensor([-0.4779], grad_fn=<AddBackward0>)\n",
      "epoch: 11468 loss is tensor([-0.4950], grad_fn=<AddBackward0>)\n",
      "epoch: 11469 loss is tensor([-0.4207], grad_fn=<AddBackward0>)\n",
      "epoch: 11470 loss is tensor([-0.5255], grad_fn=<AddBackward0>)\n",
      "epoch: 11471 loss is tensor([-0.4313], grad_fn=<AddBackward0>)\n",
      "epoch: 11472 loss is tensor([-0.4262], grad_fn=<AddBackward0>)\n",
      "epoch: 11473 loss is tensor([-0.4485], grad_fn=<AddBackward0>)\n",
      "epoch: 11474 loss is tensor([-0.4966], grad_fn=<AddBackward0>)\n",
      "epoch: 11475 loss is tensor([-0.4656], grad_fn=<AddBackward0>)\n",
      "epoch: 11476 loss is tensor([-0.5286], grad_fn=<AddBackward0>)\n",
      "epoch: 11477 loss is tensor([-0.5602], grad_fn=<AddBackward0>)\n",
      "epoch: 11478 loss is tensor([-0.4765], grad_fn=<AddBackward0>)\n",
      "epoch: 11479 loss is tensor([-0.4242], grad_fn=<AddBackward0>)\n",
      "epoch: 11480 loss is tensor([-0.4367], grad_fn=<AddBackward0>)\n",
      "epoch: 11481 loss is tensor([-0.4836], grad_fn=<AddBackward0>)\n",
      "epoch: 11482 loss is tensor([-0.4605], grad_fn=<AddBackward0>)\n",
      "epoch: 11483 loss is tensor([-0.4971], grad_fn=<AddBackward0>)\n",
      "epoch: 11484 loss is tensor([-0.4827], grad_fn=<AddBackward0>)\n",
      "epoch: 11485 loss is tensor([-0.5178], grad_fn=<AddBackward0>)\n",
      "epoch: 11486 loss is tensor([-0.4328], grad_fn=<AddBackward0>)\n",
      "epoch: 11487 loss is tensor([-0.4267], grad_fn=<AddBackward0>)\n",
      "epoch: 11488 loss is tensor([-0.4775], grad_fn=<AddBackward0>)\n",
      "epoch: 11489 loss is tensor([-0.4935], grad_fn=<AddBackward0>)\n",
      "epoch: 11490 loss is tensor([-0.4346], grad_fn=<AddBackward0>)\n",
      "epoch: 11491 loss is tensor([-0.4383], grad_fn=<AddBackward0>)\n",
      "epoch: 11492 loss is tensor([-0.4695], grad_fn=<AddBackward0>)\n",
      "epoch: 11493 loss is tensor([-0.3899], grad_fn=<AddBackward0>)\n",
      "epoch: 11494 loss is tensor([-0.4942], grad_fn=<AddBackward0>)\n",
      "epoch: 11495 loss is tensor([-0.4414], grad_fn=<AddBackward0>)\n",
      "epoch: 11496 loss is tensor([-0.4661], grad_fn=<AddBackward0>)\n",
      "epoch: 11497 loss is tensor([-0.4641], grad_fn=<AddBackward0>)\n",
      "epoch: 11498 loss is tensor([-0.5324], grad_fn=<AddBackward0>)\n",
      "epoch: 11499 loss is tensor([-0.4705], grad_fn=<AddBackward0>)\n",
      "epoch: 11500 loss is tensor([-0.3982], grad_fn=<AddBackward0>)\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11501 loss is tensor([-0.4820], grad_fn=<AddBackward0>)\n",
      "epoch: 11502 loss is tensor([-0.4848], grad_fn=<AddBackward0>)\n",
      "epoch: 11503 loss is tensor([-0.4511], grad_fn=<AddBackward0>)\n",
      "epoch: 11504 loss is tensor([-0.5043], grad_fn=<AddBackward0>)\n",
      "epoch: 11505 loss is tensor([-0.5336], grad_fn=<AddBackward0>)\n",
      "epoch: 11506 loss is tensor([-0.4619], grad_fn=<AddBackward0>)\n",
      "epoch: 11507 loss is tensor([-0.4726], grad_fn=<AddBackward0>)\n",
      "epoch: 11508 loss is tensor([-0.4722], grad_fn=<AddBackward0>)\n",
      "epoch: 11509 loss is tensor([-0.4925], grad_fn=<AddBackward0>)\n",
      "epoch: 11510 loss is tensor([-0.4847], grad_fn=<AddBackward0>)\n",
      "epoch: 11511 loss is tensor([-0.4787], grad_fn=<AddBackward0>)\n",
      "epoch: 11512 loss is tensor([-0.4858], grad_fn=<AddBackward0>)\n",
      "epoch: 11513 loss is tensor([-0.5187], grad_fn=<AddBackward0>)\n",
      "epoch: 11514 loss is tensor([-0.5131], grad_fn=<AddBackward0>)\n",
      "epoch: 11515 loss is tensor([-0.4256], grad_fn=<AddBackward0>)\n",
      "epoch: 11516 loss is tensor([-0.5205], grad_fn=<AddBackward0>)\n",
      "epoch: 11517 loss is tensor([-0.4697], grad_fn=<AddBackward0>)\n",
      "epoch: 11518 loss is tensor([-0.4584], grad_fn=<AddBackward0>)\n",
      "epoch: 11519 loss is tensor([-0.4672], grad_fn=<AddBackward0>)\n",
      "epoch: 11520 loss is tensor([-0.4605], grad_fn=<AddBackward0>)\n",
      "epoch: 11521 loss is tensor([-0.5191], grad_fn=<AddBackward0>)\n",
      "epoch: 11522 loss is tensor([-0.5104], grad_fn=<AddBackward0>)\n",
      "epoch: 11523 loss is tensor([-0.5321], grad_fn=<AddBackward0>)\n",
      "epoch: 11524 loss is tensor([-0.4657], grad_fn=<AddBackward0>)\n",
      "epoch: 11525 loss is tensor([-0.5378], grad_fn=<AddBackward0>)\n",
      "epoch: 11526 loss is tensor([-0.4633], grad_fn=<AddBackward0>)\n",
      "epoch: 11527 loss is tensor([-0.4593], grad_fn=<AddBackward0>)\n",
      "epoch: 11528 loss is tensor([-0.4792], grad_fn=<AddBackward0>)\n",
      "epoch: 11529 loss is tensor([-0.4639], grad_fn=<AddBackward0>)\n",
      "epoch: 11530 loss is tensor([-0.5038], grad_fn=<AddBackward0>)\n",
      "epoch: 11531 loss is tensor([-0.5363], grad_fn=<AddBackward0>)\n",
      "epoch: 11532 loss is tensor([-0.5643], grad_fn=<AddBackward0>)\n",
      "epoch: 11533 loss is tensor([-0.4582], grad_fn=<AddBackward0>)\n",
      "epoch: 11534 loss is tensor([-0.5019], grad_fn=<AddBackward0>)\n",
      "epoch: 11535 loss is tensor([-0.4994], grad_fn=<AddBackward0>)\n",
      "epoch: 11536 loss is tensor([-0.5496], grad_fn=<AddBackward0>)\n",
      "epoch: 11537 loss is tensor([-0.5352], grad_fn=<AddBackward0>)\n",
      "epoch: 11538 loss is tensor([-0.5460], grad_fn=<AddBackward0>)\n",
      "epoch: 11539 loss is tensor([-0.5212], grad_fn=<AddBackward0>)\n",
      "epoch: 11540 loss is tensor([-0.4949], grad_fn=<AddBackward0>)\n",
      "epoch: 11541 loss is tensor([-0.5147], grad_fn=<AddBackward0>)\n",
      "epoch: 11542 loss is tensor([-0.5205], grad_fn=<AddBackward0>)\n",
      "epoch: 11543 loss is tensor([-0.4575], grad_fn=<AddBackward0>)\n",
      "epoch: 11544 loss is tensor([-0.4896], grad_fn=<AddBackward0>)\n",
      "epoch: 11545 loss is tensor([-0.4721], grad_fn=<AddBackward0>)\n",
      "epoch: 11546 loss is tensor([-0.5101], grad_fn=<AddBackward0>)\n",
      "epoch: 11547 loss is tensor([-0.5120], grad_fn=<AddBackward0>)\n",
      "epoch: 11548 loss is tensor([-0.4886], grad_fn=<AddBackward0>)\n",
      "epoch: 11549 loss is tensor([-0.4323], grad_fn=<AddBackward0>)\n",
      "epoch: 11550 loss is tensor([-0.4779], grad_fn=<AddBackward0>)\n",
      "epoch: 11551 loss is tensor([-0.4830], grad_fn=<AddBackward0>)\n",
      "epoch: 11552 loss is tensor([-0.4501], grad_fn=<AddBackward0>)\n",
      "epoch: 11553 loss is tensor([-0.4778], grad_fn=<AddBackward0>)\n",
      "epoch: 11554 loss is tensor([-0.4370], grad_fn=<AddBackward0>)\n",
      "epoch: 11555 loss is tensor([-0.5099], grad_fn=<AddBackward0>)\n",
      "epoch: 11556 loss is tensor([-0.4937], grad_fn=<AddBackward0>)\n",
      "epoch: 11557 loss is tensor([-0.5146], grad_fn=<AddBackward0>)\n",
      "epoch: 11558 loss is tensor([-0.4864], grad_fn=<AddBackward0>)\n",
      "epoch: 11559 loss is tensor([-0.5143], grad_fn=<AddBackward0>)\n",
      "epoch: 11560 loss is tensor([-0.4715], grad_fn=<AddBackward0>)\n",
      "epoch: 11561 loss is tensor([-0.5037], grad_fn=<AddBackward0>)\n",
      "epoch: 11562 loss is tensor([-0.5022], grad_fn=<AddBackward0>)\n",
      "epoch: 11563 loss is tensor([-0.4360], grad_fn=<AddBackward0>)\n",
      "epoch: 11564 loss is tensor([-0.4525], grad_fn=<AddBackward0>)\n",
      "epoch: 11565 loss is tensor([-0.5061], grad_fn=<AddBackward0>)\n",
      "epoch: 11566 loss is tensor([-0.4961], grad_fn=<AddBackward0>)\n",
      "epoch: 11567 loss is tensor([-0.5320], grad_fn=<AddBackward0>)\n",
      "epoch: 11568 loss is tensor([-0.4708], grad_fn=<AddBackward0>)\n",
      "epoch: 11569 loss is tensor([-0.4957], grad_fn=<AddBackward0>)\n",
      "epoch: 11570 loss is tensor([-0.5428], grad_fn=<AddBackward0>)\n",
      "epoch: 11571 loss is tensor([-0.4718], grad_fn=<AddBackward0>)\n",
      "epoch: 11572 loss is tensor([-0.5069], grad_fn=<AddBackward0>)\n",
      "epoch: 11573 loss is tensor([-0.5558], grad_fn=<AddBackward0>)\n",
      "epoch: 11574 loss is tensor([-0.5237], grad_fn=<AddBackward0>)\n",
      "epoch: 11575 loss is tensor([-0.4820], grad_fn=<AddBackward0>)\n",
      "epoch: 11576 loss is tensor([-0.4996], grad_fn=<AddBackward0>)\n",
      "epoch: 11577 loss is tensor([-0.4704], grad_fn=<AddBackward0>)\n",
      "epoch: 11578 loss is tensor([-0.4416], grad_fn=<AddBackward0>)\n",
      "epoch: 11579 loss is tensor([-0.4384], grad_fn=<AddBackward0>)\n",
      "epoch: 11580 loss is tensor([-0.5097], grad_fn=<AddBackward0>)\n",
      "epoch: 11581 loss is tensor([-0.4397], grad_fn=<AddBackward0>)\n",
      "epoch: 11582 loss is tensor([-0.4914], grad_fn=<AddBackward0>)\n",
      "epoch: 11583 loss is tensor([-0.5044], grad_fn=<AddBackward0>)\n",
      "epoch: 11584 loss is tensor([-0.4494], grad_fn=<AddBackward0>)\n",
      "epoch: 11585 loss is tensor([-0.4777], grad_fn=<AddBackward0>)\n",
      "epoch: 11586 loss is tensor([-0.5370], grad_fn=<AddBackward0>)\n",
      "epoch: 11587 loss is tensor([-0.4787], grad_fn=<AddBackward0>)\n",
      "epoch: 11588 loss is tensor([-0.4799], grad_fn=<AddBackward0>)\n",
      "epoch: 11589 loss is tensor([-0.4876], grad_fn=<AddBackward0>)\n",
      "epoch: 11590 loss is tensor([-0.4846], grad_fn=<AddBackward0>)\n",
      "epoch: 11591 loss is tensor([-0.4783], grad_fn=<AddBackward0>)\n",
      "epoch: 11592 loss is tensor([-0.4953], grad_fn=<AddBackward0>)\n",
      "epoch: 11593 loss is tensor([-0.4937], grad_fn=<AddBackward0>)\n",
      "epoch: 11594 loss is tensor([-0.5405], grad_fn=<AddBackward0>)\n",
      "epoch: 11595 loss is tensor([-0.4822], grad_fn=<AddBackward0>)\n",
      "epoch: 11596 loss is tensor([-0.5043], grad_fn=<AddBackward0>)\n",
      "epoch: 11597 loss is tensor([-0.4466], grad_fn=<AddBackward0>)\n",
      "epoch: 11598 loss is tensor([-0.5365], grad_fn=<AddBackward0>)\n",
      "epoch: 11599 loss is tensor([-0.5478], grad_fn=<AddBackward0>)\n",
      "epoch: 11600 loss is tensor([-0.4385], grad_fn=<AddBackward0>)\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11601 loss is tensor([-0.4918], grad_fn=<AddBackward0>)\n",
      "epoch: 11602 loss is tensor([-0.4964], grad_fn=<AddBackward0>)\n",
      "epoch: 11603 loss is tensor([-0.5019], grad_fn=<AddBackward0>)\n",
      "epoch: 11604 loss is tensor([-0.4675], grad_fn=<AddBackward0>)\n",
      "epoch: 11605 loss is tensor([-0.5202], grad_fn=<AddBackward0>)\n",
      "epoch: 11606 loss is tensor([-0.4635], grad_fn=<AddBackward0>)\n",
      "epoch: 11607 loss is tensor([-0.4880], grad_fn=<AddBackward0>)\n",
      "epoch: 11608 loss is tensor([-0.5432], grad_fn=<AddBackward0>)\n",
      "epoch: 11609 loss is tensor([-0.4644], grad_fn=<AddBackward0>)\n",
      "epoch: 11610 loss is tensor([-0.5660], grad_fn=<AddBackward0>)\n",
      "epoch: 11611 loss is tensor([-0.5128], grad_fn=<AddBackward0>)\n",
      "epoch: 11612 loss is tensor([-0.5452], grad_fn=<AddBackward0>)\n",
      "epoch: 11613 loss is tensor([-0.5224], grad_fn=<AddBackward0>)\n",
      "epoch: 11614 loss is tensor([-0.5670], grad_fn=<AddBackward0>)\n",
      "epoch: 11615 loss is tensor([-0.4791], grad_fn=<AddBackward0>)\n",
      "epoch: 11616 loss is tensor([-0.5488], grad_fn=<AddBackward0>)\n",
      "epoch: 11617 loss is tensor([-0.5489], grad_fn=<AddBackward0>)\n",
      "epoch: 11618 loss is tensor([-0.5266], grad_fn=<AddBackward0>)\n",
      "epoch: 11619 loss is tensor([-0.5322], grad_fn=<AddBackward0>)\n",
      "epoch: 11620 loss is tensor([-0.5022], grad_fn=<AddBackward0>)\n",
      "epoch: 11621 loss is tensor([-0.4973], grad_fn=<AddBackward0>)\n",
      "epoch: 11622 loss is tensor([-0.5797], grad_fn=<AddBackward0>)\n",
      "epoch: 11623 loss is tensor([-0.5100], grad_fn=<AddBackward0>)\n",
      "epoch: 11624 loss is tensor([-0.4955], grad_fn=<AddBackward0>)\n",
      "epoch: 11625 loss is tensor([-0.4694], grad_fn=<AddBackward0>)\n",
      "epoch: 11626 loss is tensor([-0.3890], grad_fn=<AddBackward0>)\n",
      "epoch: 11627 loss is tensor([-0.3993], grad_fn=<AddBackward0>)\n",
      "epoch: 11628 loss is tensor([-0.4691], grad_fn=<AddBackward0>)\n",
      "epoch: 11629 loss is tensor([-0.4532], grad_fn=<AddBackward0>)\n",
      "epoch: 11630 loss is tensor([-0.4043], grad_fn=<AddBackward0>)\n",
      "epoch: 11631 loss is tensor([-0.4441], grad_fn=<AddBackward0>)\n",
      "epoch: 11632 loss is tensor([-0.3341], grad_fn=<AddBackward0>)\n",
      "epoch: 11633 loss is tensor([-0.3767], grad_fn=<AddBackward0>)\n",
      "epoch: 11634 loss is tensor([-0.4037], grad_fn=<AddBackward0>)\n",
      "epoch: 11635 loss is tensor([-0.4431], grad_fn=<AddBackward0>)\n",
      "epoch: 11636 loss is tensor([-0.4281], grad_fn=<AddBackward0>)\n",
      "epoch: 11637 loss is tensor([-0.4114], grad_fn=<AddBackward0>)\n",
      "epoch: 11638 loss is tensor([-0.4638], grad_fn=<AddBackward0>)\n",
      "epoch: 11639 loss is tensor([-0.4171], grad_fn=<AddBackward0>)\n",
      "epoch: 11640 loss is tensor([-0.4457], grad_fn=<AddBackward0>)\n",
      "epoch: 11641 loss is tensor([-0.3850], grad_fn=<AddBackward0>)\n",
      "epoch: 11642 loss is tensor([-0.4380], grad_fn=<AddBackward0>)\n",
      "epoch: 11643 loss is tensor([-0.4536], grad_fn=<AddBackward0>)\n",
      "epoch: 11644 loss is tensor([-0.3886], grad_fn=<AddBackward0>)\n",
      "epoch: 11645 loss is tensor([-0.3703], grad_fn=<AddBackward0>)\n",
      "epoch: 11646 loss is tensor([-0.4382], grad_fn=<AddBackward0>)\n",
      "epoch: 11647 loss is tensor([-0.4198], grad_fn=<AddBackward0>)\n",
      "epoch: 11648 loss is tensor([-0.4453], grad_fn=<AddBackward0>)\n",
      "epoch: 11649 loss is tensor([-0.5140], grad_fn=<AddBackward0>)\n",
      "epoch: 11650 loss is tensor([-0.4431], grad_fn=<AddBackward0>)\n",
      "epoch: 11651 loss is tensor([-0.4310], grad_fn=<AddBackward0>)\n",
      "epoch: 11652 loss is tensor([-0.4367], grad_fn=<AddBackward0>)\n",
      "epoch: 11653 loss is tensor([-0.4896], grad_fn=<AddBackward0>)\n",
      "epoch: 11654 loss is tensor([-0.4828], grad_fn=<AddBackward0>)\n",
      "epoch: 11655 loss is tensor([-0.4691], grad_fn=<AddBackward0>)\n",
      "epoch: 11656 loss is tensor([-0.4446], grad_fn=<AddBackward0>)\n",
      "epoch: 11657 loss is tensor([-0.4834], grad_fn=<AddBackward0>)\n",
      "epoch: 11658 loss is tensor([-0.4121], grad_fn=<AddBackward0>)\n",
      "epoch: 11659 loss is tensor([-0.4854], grad_fn=<AddBackward0>)\n",
      "epoch: 11660 loss is tensor([-0.4949], grad_fn=<AddBackward0>)\n",
      "epoch: 11661 loss is tensor([-0.4740], grad_fn=<AddBackward0>)\n",
      "epoch: 11662 loss is tensor([-0.4521], grad_fn=<AddBackward0>)\n",
      "epoch: 11663 loss is tensor([-0.5078], grad_fn=<AddBackward0>)\n",
      "epoch: 11664 loss is tensor([-0.5217], grad_fn=<AddBackward0>)\n",
      "epoch: 11665 loss is tensor([-0.4938], grad_fn=<AddBackward0>)\n",
      "epoch: 11666 loss is tensor([-0.5114], grad_fn=<AddBackward0>)\n",
      "epoch: 11667 loss is tensor([-0.5064], grad_fn=<AddBackward0>)\n",
      "epoch: 11668 loss is tensor([-0.5160], grad_fn=<AddBackward0>)\n",
      "epoch: 11669 loss is tensor([-0.5497], grad_fn=<AddBackward0>)\n",
      "epoch: 11670 loss is tensor([-0.4852], grad_fn=<AddBackward0>)\n",
      "epoch: 11671 loss is tensor([-0.5366], grad_fn=<AddBackward0>)\n",
      "epoch: 11672 loss is tensor([-0.5166], grad_fn=<AddBackward0>)\n",
      "epoch: 11673 loss is tensor([-0.4630], grad_fn=<AddBackward0>)\n",
      "epoch: 11674 loss is tensor([-0.5298], grad_fn=<AddBackward0>)\n",
      "epoch: 11675 loss is tensor([-0.4713], grad_fn=<AddBackward0>)\n",
      "epoch: 11676 loss is tensor([-0.4529], grad_fn=<AddBackward0>)\n",
      "epoch: 11677 loss is tensor([-0.5090], grad_fn=<AddBackward0>)\n",
      "epoch: 11678 loss is tensor([-0.4847], grad_fn=<AddBackward0>)\n",
      "epoch: 11679 loss is tensor([-0.5600], grad_fn=<AddBackward0>)\n",
      "epoch: 11680 loss is tensor([-0.4744], grad_fn=<AddBackward0>)\n",
      "epoch: 11681 loss is tensor([-0.5192], grad_fn=<AddBackward0>)\n",
      "epoch: 11682 loss is tensor([-0.4631], grad_fn=<AddBackward0>)\n",
      "epoch: 11683 loss is tensor([-0.4878], grad_fn=<AddBackward0>)\n",
      "epoch: 11684 loss is tensor([-0.4589], grad_fn=<AddBackward0>)\n",
      "epoch: 11685 loss is tensor([-0.4989], grad_fn=<AddBackward0>)\n",
      "epoch: 11686 loss is tensor([-0.4940], grad_fn=<AddBackward0>)\n",
      "epoch: 11687 loss is tensor([-0.4543], grad_fn=<AddBackward0>)\n",
      "epoch: 11688 loss is tensor([-0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 11689 loss is tensor([-0.4725], grad_fn=<AddBackward0>)\n",
      "epoch: 11690 loss is tensor([-0.4920], grad_fn=<AddBackward0>)\n",
      "epoch: 11691 loss is tensor([-0.5322], grad_fn=<AddBackward0>)\n",
      "epoch: 11692 loss is tensor([-0.4885], grad_fn=<AddBackward0>)\n",
      "epoch: 11693 loss is tensor([-0.5296], grad_fn=<AddBackward0>)\n",
      "epoch: 11694 loss is tensor([-0.4885], grad_fn=<AddBackward0>)\n",
      "epoch: 11695 loss is tensor([-0.5031], grad_fn=<AddBackward0>)\n",
      "epoch: 11696 loss is tensor([-0.5204], grad_fn=<AddBackward0>)\n",
      "epoch: 11697 loss is tensor([-0.4651], grad_fn=<AddBackward0>)\n",
      "epoch: 11698 loss is tensor([-0.4413], grad_fn=<AddBackward0>)\n",
      "epoch: 11699 loss is tensor([-0.5167], grad_fn=<AddBackward0>)\n",
      "epoch: 11700 loss is tensor([-0.4264], grad_fn=<AddBackward0>)\n",
      "32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11701 loss is tensor([-0.4201], grad_fn=<AddBackward0>)\n",
      "epoch: 11702 loss is tensor([-0.4792], grad_fn=<AddBackward0>)\n",
      "epoch: 11703 loss is tensor([-0.5041], grad_fn=<AddBackward0>)\n",
      "epoch: 11704 loss is tensor([-0.4784], grad_fn=<AddBackward0>)\n",
      "epoch: 11705 loss is tensor([-0.4944], grad_fn=<AddBackward0>)\n",
      "epoch: 11706 loss is tensor([-0.4309], grad_fn=<AddBackward0>)\n",
      "epoch: 11707 loss is tensor([-0.4843], grad_fn=<AddBackward0>)\n",
      "epoch: 11708 loss is tensor([-0.4976], grad_fn=<AddBackward0>)\n",
      "epoch: 11709 loss is tensor([-0.5221], grad_fn=<AddBackward0>)\n",
      "epoch: 11710 loss is tensor([-0.4873], grad_fn=<AddBackward0>)\n",
      "epoch: 11711 loss is tensor([-0.4619], grad_fn=<AddBackward0>)\n",
      "epoch: 11712 loss is tensor([-0.5197], grad_fn=<AddBackward0>)\n",
      "epoch: 11713 loss is tensor([-0.5008], grad_fn=<AddBackward0>)\n",
      "epoch: 11714 loss is tensor([-0.5198], grad_fn=<AddBackward0>)\n",
      "epoch: 11715 loss is tensor([-0.4328], grad_fn=<AddBackward0>)\n",
      "epoch: 11716 loss is tensor([-0.4463], grad_fn=<AddBackward0>)\n",
      "epoch: 11717 loss is tensor([-0.4756], grad_fn=<AddBackward0>)\n",
      "epoch: 11718 loss is tensor([-0.4853], grad_fn=<AddBackward0>)\n",
      "epoch: 11719 loss is tensor([-0.4613], grad_fn=<AddBackward0>)\n",
      "epoch: 11720 loss is tensor([-0.4723], grad_fn=<AddBackward0>)\n",
      "epoch: 11721 loss is tensor([-0.4679], grad_fn=<AddBackward0>)\n",
      "epoch: 11722 loss is tensor([-0.5133], grad_fn=<AddBackward0>)\n",
      "epoch: 11723 loss is tensor([-0.4995], grad_fn=<AddBackward0>)\n",
      "epoch: 11724 loss is tensor([-0.4337], grad_fn=<AddBackward0>)\n",
      "epoch: 11725 loss is tensor([-0.4721], grad_fn=<AddBackward0>)\n",
      "epoch: 11726 loss is tensor([-0.4954], grad_fn=<AddBackward0>)\n",
      "epoch: 11727 loss is tensor([-0.5341], grad_fn=<AddBackward0>)\n",
      "epoch: 11728 loss is tensor([-0.5092], grad_fn=<AddBackward0>)\n",
      "epoch: 11729 loss is tensor([-0.5207], grad_fn=<AddBackward0>)\n",
      "epoch: 11730 loss is tensor([-0.4800], grad_fn=<AddBackward0>)\n",
      "epoch: 11731 loss is tensor([-0.5129], grad_fn=<AddBackward0>)\n",
      "epoch: 11732 loss is tensor([-0.5166], grad_fn=<AddBackward0>)\n",
      "epoch: 11733 loss is tensor([-0.4947], grad_fn=<AddBackward0>)\n",
      "epoch: 11734 loss is tensor([-0.4541], grad_fn=<AddBackward0>)\n",
      "epoch: 11735 loss is tensor([-0.4844], grad_fn=<AddBackward0>)\n",
      "epoch: 11736 loss is tensor([-0.5254], grad_fn=<AddBackward0>)\n",
      "epoch: 11737 loss is tensor([-0.4475], grad_fn=<AddBackward0>)\n",
      "epoch: 11738 loss is tensor([-0.5152], grad_fn=<AddBackward0>)\n",
      "epoch: 11739 loss is tensor([-0.5097], grad_fn=<AddBackward0>)\n",
      "epoch: 11740 loss is tensor([-0.5342], grad_fn=<AddBackward0>)\n",
      "epoch: 11741 loss is tensor([-0.4847], grad_fn=<AddBackward0>)\n",
      "epoch: 11742 loss is tensor([-0.5336], grad_fn=<AddBackward0>)\n",
      "epoch: 11743 loss is tensor([-0.5332], grad_fn=<AddBackward0>)\n",
      "epoch: 11744 loss is tensor([-0.5403], grad_fn=<AddBackward0>)\n",
      "epoch: 11745 loss is tensor([-0.5376], grad_fn=<AddBackward0>)\n",
      "epoch: 11746 loss is tensor([-0.5566], grad_fn=<AddBackward0>)\n",
      "epoch: 11747 loss is tensor([-0.4961], grad_fn=<AddBackward0>)\n",
      "epoch: 11748 loss is tensor([-0.5545], grad_fn=<AddBackward0>)\n",
      "epoch: 11749 loss is tensor([-0.5100], grad_fn=<AddBackward0>)\n",
      "epoch: 11750 loss is tensor([-0.4645], grad_fn=<AddBackward0>)\n",
      "epoch: 11751 loss is tensor([-0.5215], grad_fn=<AddBackward0>)\n",
      "epoch: 11752 loss is tensor([-0.5466], grad_fn=<AddBackward0>)\n",
      "epoch: 11753 loss is tensor([-0.5070], grad_fn=<AddBackward0>)\n",
      "epoch: 11754 loss is tensor([-0.5178], grad_fn=<AddBackward0>)\n",
      "epoch: 11755 loss is tensor([-0.4691], grad_fn=<AddBackward0>)\n",
      "epoch: 11756 loss is tensor([-0.4229], grad_fn=<AddBackward0>)\n",
      "epoch: 11757 loss is tensor([-0.4822], grad_fn=<AddBackward0>)\n",
      "epoch: 11758 loss is tensor([-0.4878], grad_fn=<AddBackward0>)\n",
      "epoch: 11759 loss is tensor([-0.5018], grad_fn=<AddBackward0>)\n",
      "epoch: 11760 loss is tensor([-0.4839], grad_fn=<AddBackward0>)\n",
      "epoch: 11761 loss is tensor([-0.4854], grad_fn=<AddBackward0>)\n",
      "epoch: 11762 loss is tensor([-0.5178], grad_fn=<AddBackward0>)\n",
      "epoch: 11763 loss is tensor([-0.5086], grad_fn=<AddBackward0>)\n",
      "epoch: 11764 loss is tensor([-0.5155], grad_fn=<AddBackward0>)\n",
      "epoch: 11765 loss is tensor([-0.5133], grad_fn=<AddBackward0>)\n",
      "epoch: 11766 loss is tensor([-0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 11767 loss is tensor([-0.4630], grad_fn=<AddBackward0>)\n",
      "epoch: 11768 loss is tensor([-0.4901], grad_fn=<AddBackward0>)\n",
      "epoch: 11769 loss is tensor([-0.5054], grad_fn=<AddBackward0>)\n",
      "epoch: 11770 loss is tensor([-0.5457], grad_fn=<AddBackward0>)\n",
      "epoch: 11771 loss is tensor([-0.4915], grad_fn=<AddBackward0>)\n",
      "epoch: 11772 loss is tensor([-0.5204], grad_fn=<AddBackward0>)\n",
      "epoch: 11773 loss is tensor([-0.5281], grad_fn=<AddBackward0>)\n",
      "epoch: 11774 loss is tensor([-0.5231], grad_fn=<AddBackward0>)\n",
      "epoch: 11775 loss is tensor([-0.5499], grad_fn=<AddBackward0>)\n",
      "epoch: 11776 loss is tensor([-0.4811], grad_fn=<AddBackward0>)\n",
      "epoch: 11777 loss is tensor([-0.4468], grad_fn=<AddBackward0>)\n",
      "epoch: 11778 loss is tensor([-0.4976], grad_fn=<AddBackward0>)\n",
      "epoch: 11779 loss is tensor([-0.4954], grad_fn=<AddBackward0>)\n",
      "epoch: 11780 loss is tensor([-0.5087], grad_fn=<AddBackward0>)\n",
      "epoch: 11781 loss is tensor([-0.4642], grad_fn=<AddBackward0>)\n",
      "epoch: 11782 loss is tensor([-0.5116], grad_fn=<AddBackward0>)\n",
      "epoch: 11783 loss is tensor([-0.4669], grad_fn=<AddBackward0>)\n",
      "epoch: 11784 loss is tensor([-0.4273], grad_fn=<AddBackward0>)\n",
      "epoch: 11785 loss is tensor([-0.5026], grad_fn=<AddBackward0>)\n",
      "epoch: 11786 loss is tensor([-0.5460], grad_fn=<AddBackward0>)\n",
      "epoch: 11787 loss is tensor([-0.4936], grad_fn=<AddBackward0>)\n",
      "epoch: 11788 loss is tensor([-0.5380], grad_fn=<AddBackward0>)\n",
      "epoch: 11789 loss is tensor([-0.4842], grad_fn=<AddBackward0>)\n",
      "epoch: 11790 loss is tensor([-0.5253], grad_fn=<AddBackward0>)\n",
      "epoch: 11791 loss is tensor([-0.5313], grad_fn=<AddBackward0>)\n",
      "epoch: 11792 loss is tensor([-0.5567], grad_fn=<AddBackward0>)\n",
      "epoch: 11793 loss is tensor([-0.4476], grad_fn=<AddBackward0>)\n",
      "epoch: 11794 loss is tensor([-0.5063], grad_fn=<AddBackward0>)\n",
      "epoch: 11795 loss is tensor([-0.4400], grad_fn=<AddBackward0>)\n",
      "epoch: 11796 loss is tensor([-0.5178], grad_fn=<AddBackward0>)\n",
      "epoch: 11797 loss is tensor([-0.5094], grad_fn=<AddBackward0>)\n",
      "epoch: 11798 loss is tensor([-0.4578], grad_fn=<AddBackward0>)\n",
      "epoch: 11799 loss is tensor([-0.4626], grad_fn=<AddBackward0>)\n",
      "epoch: 11800 loss is tensor([-0.4741], grad_fn=<AddBackward0>)\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11801 loss is tensor([-0.5284], grad_fn=<AddBackward0>)\n",
      "epoch: 11802 loss is tensor([-0.5133], grad_fn=<AddBackward0>)\n",
      "epoch: 11803 loss is tensor([-0.5184], grad_fn=<AddBackward0>)\n",
      "epoch: 11804 loss is tensor([-0.5423], grad_fn=<AddBackward0>)\n",
      "epoch: 11805 loss is tensor([-0.5501], grad_fn=<AddBackward0>)\n",
      "epoch: 11806 loss is tensor([-0.4700], grad_fn=<AddBackward0>)\n",
      "epoch: 11807 loss is tensor([-0.5076], grad_fn=<AddBackward0>)\n",
      "epoch: 11808 loss is tensor([-0.4815], grad_fn=<AddBackward0>)\n",
      "epoch: 11809 loss is tensor([-0.4862], grad_fn=<AddBackward0>)\n",
      "epoch: 11810 loss is tensor([-0.5618], grad_fn=<AddBackward0>)\n",
      "epoch: 11811 loss is tensor([-0.4361], grad_fn=<AddBackward0>)\n",
      "epoch: 11812 loss is tensor([-0.4510], grad_fn=<AddBackward0>)\n",
      "epoch: 11813 loss is tensor([-0.5508], grad_fn=<AddBackward0>)\n",
      "epoch: 11814 loss is tensor([-0.4598], grad_fn=<AddBackward0>)\n",
      "epoch: 11815 loss is tensor([-0.4670], grad_fn=<AddBackward0>)\n",
      "epoch: 11816 loss is tensor([-0.4331], grad_fn=<AddBackward0>)\n",
      "epoch: 11817 loss is tensor([-0.5304], grad_fn=<AddBackward0>)\n",
      "epoch: 11818 loss is tensor([-0.4469], grad_fn=<AddBackward0>)\n",
      "epoch: 11819 loss is tensor([-0.4746], grad_fn=<AddBackward0>)\n",
      "epoch: 11820 loss is tensor([-0.5326], grad_fn=<AddBackward0>)\n",
      "epoch: 11821 loss is tensor([-0.4614], grad_fn=<AddBackward0>)\n",
      "epoch: 11822 loss is tensor([-0.4893], grad_fn=<AddBackward0>)\n",
      "epoch: 11823 loss is tensor([-0.4516], grad_fn=<AddBackward0>)\n",
      "epoch: 11824 loss is tensor([-0.4417], grad_fn=<AddBackward0>)\n",
      "epoch: 11825 loss is tensor([-0.4574], grad_fn=<AddBackward0>)\n",
      "epoch: 11826 loss is tensor([-0.4280], grad_fn=<AddBackward0>)\n",
      "epoch: 11827 loss is tensor([-0.5010], grad_fn=<AddBackward0>)\n",
      "epoch: 11828 loss is tensor([-0.4727], grad_fn=<AddBackward0>)\n",
      "epoch: 11829 loss is tensor([-0.4802], grad_fn=<AddBackward0>)\n",
      "epoch: 11830 loss is tensor([-0.4263], grad_fn=<AddBackward0>)\n",
      "epoch: 11831 loss is tensor([-0.4658], grad_fn=<AddBackward0>)\n",
      "epoch: 11832 loss is tensor([-0.5077], grad_fn=<AddBackward0>)\n",
      "epoch: 11833 loss is tensor([-0.4827], grad_fn=<AddBackward0>)\n",
      "epoch: 11834 loss is tensor([-0.4601], grad_fn=<AddBackward0>)\n",
      "epoch: 11835 loss is tensor([-0.5144], grad_fn=<AddBackward0>)\n",
      "epoch: 11836 loss is tensor([-0.4619], grad_fn=<AddBackward0>)\n",
      "epoch: 11837 loss is tensor([-0.4975], grad_fn=<AddBackward0>)\n",
      "epoch: 11838 loss is tensor([-0.5228], grad_fn=<AddBackward0>)\n",
      "epoch: 11839 loss is tensor([-0.4922], grad_fn=<AddBackward0>)\n",
      "epoch: 11840 loss is tensor([-0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 11841 loss is tensor([-0.5505], grad_fn=<AddBackward0>)\n",
      "epoch: 11842 loss is tensor([-0.5215], grad_fn=<AddBackward0>)\n",
      "epoch: 11843 loss is tensor([-0.4786], grad_fn=<AddBackward0>)\n",
      "epoch: 11844 loss is tensor([-0.5393], grad_fn=<AddBackward0>)\n",
      "epoch: 11845 loss is tensor([-0.5433], grad_fn=<AddBackward0>)\n",
      "epoch: 11846 loss is tensor([-0.5525], grad_fn=<AddBackward0>)\n",
      "epoch: 11847 loss is tensor([-0.5997], grad_fn=<AddBackward0>)\n",
      "epoch: 11848 loss is tensor([-0.4550], grad_fn=<AddBackward0>)\n",
      "epoch: 11849 loss is tensor([-0.5256], grad_fn=<AddBackward0>)\n",
      "epoch: 11850 loss is tensor([-0.5035], grad_fn=<AddBackward0>)\n",
      "epoch: 11851 loss is tensor([-0.5141], grad_fn=<AddBackward0>)\n",
      "epoch: 11852 loss is tensor([-0.5383], grad_fn=<AddBackward0>)\n",
      "epoch: 11853 loss is tensor([-0.5203], grad_fn=<AddBackward0>)\n",
      "epoch: 11854 loss is tensor([-0.5430], grad_fn=<AddBackward0>)\n",
      "epoch: 11855 loss is tensor([-0.5471], grad_fn=<AddBackward0>)\n",
      "epoch: 11856 loss is tensor([-0.5076], grad_fn=<AddBackward0>)\n",
      "epoch: 11857 loss is tensor([-0.5142], grad_fn=<AddBackward0>)\n",
      "epoch: 11858 loss is tensor([-0.5345], grad_fn=<AddBackward0>)\n",
      "epoch: 11859 loss is tensor([-0.5460], grad_fn=<AddBackward0>)\n",
      "epoch: 11860 loss is tensor([-0.5111], grad_fn=<AddBackward0>)\n",
      "epoch: 11861 loss is tensor([-0.4975], grad_fn=<AddBackward0>)\n",
      "epoch: 11862 loss is tensor([-0.4979], grad_fn=<AddBackward0>)\n",
      "epoch: 11863 loss is tensor([-0.4898], grad_fn=<AddBackward0>)\n",
      "epoch: 11864 loss is tensor([-0.5355], grad_fn=<AddBackward0>)\n",
      "epoch: 11865 loss is tensor([-0.5155], grad_fn=<AddBackward0>)\n",
      "epoch: 11866 loss is tensor([-0.4469], grad_fn=<AddBackward0>)\n",
      "epoch: 11867 loss is tensor([-0.3943], grad_fn=<AddBackward0>)\n",
      "epoch: 11868 loss is tensor([-0.4900], grad_fn=<AddBackward0>)\n",
      "epoch: 11869 loss is tensor([-0.4251], grad_fn=<AddBackward0>)\n",
      "epoch: 11870 loss is tensor([-0.4937], grad_fn=<AddBackward0>)\n",
      "epoch: 11871 loss is tensor([-0.4492], grad_fn=<AddBackward0>)\n",
      "epoch: 11872 loss is tensor([-0.4664], grad_fn=<AddBackward0>)\n",
      "epoch: 11873 loss is tensor([-0.5013], grad_fn=<AddBackward0>)\n",
      "epoch: 11874 loss is tensor([-0.5066], grad_fn=<AddBackward0>)\n",
      "epoch: 11875 loss is tensor([-0.5067], grad_fn=<AddBackward0>)\n",
      "epoch: 11876 loss is tensor([-0.4288], grad_fn=<AddBackward0>)\n",
      "epoch: 11877 loss is tensor([-0.4885], grad_fn=<AddBackward0>)\n",
      "epoch: 11878 loss is tensor([-0.4390], grad_fn=<AddBackward0>)\n",
      "epoch: 11879 loss is tensor([-0.5176], grad_fn=<AddBackward0>)\n",
      "epoch: 11880 loss is tensor([-0.4830], grad_fn=<AddBackward0>)\n",
      "epoch: 11881 loss is tensor([-0.4845], grad_fn=<AddBackward0>)\n",
      "epoch: 11882 loss is tensor([-0.5016], grad_fn=<AddBackward0>)\n",
      "epoch: 11883 loss is tensor([-0.4686], grad_fn=<AddBackward0>)\n",
      "epoch: 11884 loss is tensor([-0.4517], grad_fn=<AddBackward0>)\n",
      "epoch: 11885 loss is tensor([-0.5267], grad_fn=<AddBackward0>)\n",
      "epoch: 11886 loss is tensor([-0.4680], grad_fn=<AddBackward0>)\n",
      "epoch: 11887 loss is tensor([-0.5168], grad_fn=<AddBackward0>)\n",
      "epoch: 11888 loss is tensor([-0.4500], grad_fn=<AddBackward0>)\n",
      "epoch: 11889 loss is tensor([-0.4816], grad_fn=<AddBackward0>)\n",
      "epoch: 11890 loss is tensor([-0.5537], grad_fn=<AddBackward0>)\n",
      "epoch: 11891 loss is tensor([-0.5324], grad_fn=<AddBackward0>)\n",
      "epoch: 11892 loss is tensor([-0.5059], grad_fn=<AddBackward0>)\n",
      "epoch: 11893 loss is tensor([-0.4878], grad_fn=<AddBackward0>)\n",
      "epoch: 11894 loss is tensor([-0.4439], grad_fn=<AddBackward0>)\n",
      "epoch: 11895 loss is tensor([-0.5170], grad_fn=<AddBackward0>)\n",
      "epoch: 11896 loss is tensor([-0.5059], grad_fn=<AddBackward0>)\n",
      "epoch: 11897 loss is tensor([-0.5152], grad_fn=<AddBackward0>)\n",
      "epoch: 11898 loss is tensor([-0.4845], grad_fn=<AddBackward0>)\n",
      "epoch: 11899 loss is tensor([-0.5415], grad_fn=<AddBackward0>)\n",
      "epoch: 11900 loss is tensor([-0.4805], grad_fn=<AddBackward0>)\n",
      "41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 11901 loss is tensor([-0.5198], grad_fn=<AddBackward0>)\n",
      "epoch: 11902 loss is tensor([-0.5441], grad_fn=<AddBackward0>)\n",
      "epoch: 11903 loss is tensor([-0.5556], grad_fn=<AddBackward0>)\n",
      "epoch: 11904 loss is tensor([-0.5326], grad_fn=<AddBackward0>)\n",
      "epoch: 11905 loss is tensor([-0.5623], grad_fn=<AddBackward0>)\n",
      "epoch: 11906 loss is tensor([-0.5679], grad_fn=<AddBackward0>)\n",
      "epoch: 11907 loss is tensor([-0.5425], grad_fn=<AddBackward0>)\n",
      "epoch: 11908 loss is tensor([-0.5176], grad_fn=<AddBackward0>)\n",
      "epoch: 11909 loss is tensor([-0.5185], grad_fn=<AddBackward0>)\n",
      "epoch: 11910 loss is tensor([-0.5064], grad_fn=<AddBackward0>)\n",
      "epoch: 11911 loss is tensor([-0.5421], grad_fn=<AddBackward0>)\n",
      "epoch: 11912 loss is tensor([-0.5016], grad_fn=<AddBackward0>)\n",
      "epoch: 11913 loss is tensor([-0.5238], grad_fn=<AddBackward0>)\n",
      "epoch: 11914 loss is tensor([-0.5656], grad_fn=<AddBackward0>)\n",
      "epoch: 11915 loss is tensor([-0.5620], grad_fn=<AddBackward0>)\n",
      "epoch: 11916 loss is tensor([-0.5101], grad_fn=<AddBackward0>)\n",
      "epoch: 11917 loss is tensor([-0.5335], grad_fn=<AddBackward0>)\n",
      "epoch: 11918 loss is tensor([-0.4700], grad_fn=<AddBackward0>)\n",
      "epoch: 11919 loss is tensor([-0.5417], grad_fn=<AddBackward0>)\n",
      "epoch: 11920 loss is tensor([-0.5422], grad_fn=<AddBackward0>)\n",
      "epoch: 11921 loss is tensor([-0.5424], grad_fn=<AddBackward0>)\n",
      "epoch: 11922 loss is tensor([-0.5362], grad_fn=<AddBackward0>)\n",
      "epoch: 11923 loss is tensor([-0.5292], grad_fn=<AddBackward0>)\n",
      "epoch: 11924 loss is tensor([-0.5688], grad_fn=<AddBackward0>)\n",
      "epoch: 11925 loss is tensor([-0.5358], grad_fn=<AddBackward0>)\n",
      "epoch: 11926 loss is tensor([-0.5536], grad_fn=<AddBackward0>)\n",
      "epoch: 11927 loss is tensor([-0.5475], grad_fn=<AddBackward0>)\n",
      "epoch: 11928 loss is tensor([-0.5320], grad_fn=<AddBackward0>)\n",
      "epoch: 11929 loss is tensor([-0.4750], grad_fn=<AddBackward0>)\n",
      "epoch: 11930 loss is tensor([-0.4675], grad_fn=<AddBackward0>)\n",
      "epoch: 11931 loss is tensor([-0.4584], grad_fn=<AddBackward0>)\n",
      "epoch: 11932 loss is tensor([-0.4888], grad_fn=<AddBackward0>)\n",
      "epoch: 11933 loss is tensor([-0.4905], grad_fn=<AddBackward0>)\n",
      "epoch: 11934 loss is tensor([-0.4822], grad_fn=<AddBackward0>)\n",
      "epoch: 11935 loss is tensor([-0.4967], grad_fn=<AddBackward0>)\n",
      "epoch: 11936 loss is tensor([-0.4903], grad_fn=<AddBackward0>)\n",
      "epoch: 11937 loss is tensor([-0.4841], grad_fn=<AddBackward0>)\n",
      "epoch: 11938 loss is tensor([-0.4570], grad_fn=<AddBackward0>)\n",
      "epoch: 11939 loss is tensor([-0.5062], grad_fn=<AddBackward0>)\n",
      "epoch: 11940 loss is tensor([-0.4169], grad_fn=<AddBackward0>)\n",
      "epoch: 11941 loss is tensor([-0.4988], grad_fn=<AddBackward0>)\n",
      "epoch: 11942 loss is tensor([-0.5103], grad_fn=<AddBackward0>)\n",
      "epoch: 11943 loss is tensor([-0.4915], grad_fn=<AddBackward0>)\n",
      "epoch: 11944 loss is tensor([-0.5385], grad_fn=<AddBackward0>)\n",
      "epoch: 11945 loss is tensor([-0.5009], grad_fn=<AddBackward0>)\n",
      "epoch: 11946 loss is tensor([-0.5123], grad_fn=<AddBackward0>)\n",
      "epoch: 11947 loss is tensor([-0.5225], grad_fn=<AddBackward0>)\n",
      "epoch: 11948 loss is tensor([-0.5018], grad_fn=<AddBackward0>)\n",
      "epoch: 11949 loss is tensor([-0.5533], grad_fn=<AddBackward0>)\n",
      "epoch: 11950 loss is tensor([-0.4864], grad_fn=<AddBackward0>)\n",
      "epoch: 11951 loss is tensor([-0.4694], grad_fn=<AddBackward0>)\n",
      "epoch: 11952 loss is tensor([-0.4960], grad_fn=<AddBackward0>)\n",
      "epoch: 11953 loss is tensor([-0.4980], grad_fn=<AddBackward0>)\n",
      "epoch: 11954 loss is tensor([-0.5490], grad_fn=<AddBackward0>)\n",
      "epoch: 11955 loss is tensor([-0.5334], grad_fn=<AddBackward0>)\n",
      "epoch: 11956 loss is tensor([-0.5191], grad_fn=<AddBackward0>)\n",
      "epoch: 11957 loss is tensor([-0.5016], grad_fn=<AddBackward0>)\n",
      "epoch: 11958 loss is tensor([-0.5273], grad_fn=<AddBackward0>)\n",
      "epoch: 11959 loss is tensor([-0.4459], grad_fn=<AddBackward0>)\n",
      "epoch: 11960 loss is tensor([-0.5614], grad_fn=<AddBackward0>)\n",
      "epoch: 11961 loss is tensor([-0.5554], grad_fn=<AddBackward0>)\n",
      "epoch: 11962 loss is tensor([-0.4831], grad_fn=<AddBackward0>)\n",
      "epoch: 11963 loss is tensor([-0.4637], grad_fn=<AddBackward0>)\n",
      "epoch: 11964 loss is tensor([-0.5330], grad_fn=<AddBackward0>)\n",
      "epoch: 11965 loss is tensor([-0.5411], grad_fn=<AddBackward0>)\n",
      "epoch: 11966 loss is tensor([-0.5375], grad_fn=<AddBackward0>)\n",
      "epoch: 11967 loss is tensor([-0.5171], grad_fn=<AddBackward0>)\n",
      "epoch: 11968 loss is tensor([-0.5566], grad_fn=<AddBackward0>)\n",
      "epoch: 11969 loss is tensor([-0.5429], grad_fn=<AddBackward0>)\n",
      "epoch: 11970 loss is tensor([-0.4584], grad_fn=<AddBackward0>)\n",
      "epoch: 11971 loss is tensor([-0.5206], grad_fn=<AddBackward0>)\n",
      "epoch: 11972 loss is tensor([-0.5122], grad_fn=<AddBackward0>)\n",
      "epoch: 11973 loss is tensor([-0.5845], grad_fn=<AddBackward0>)\n",
      "epoch: 11974 loss is tensor([-0.5217], grad_fn=<AddBackward0>)\n",
      "epoch: 11975 loss is tensor([-0.5108], grad_fn=<AddBackward0>)\n",
      "epoch: 11976 loss is tensor([-0.5103], grad_fn=<AddBackward0>)\n",
      "epoch: 11977 loss is tensor([-0.5753], grad_fn=<AddBackward0>)\n",
      "epoch: 11978 loss is tensor([-0.4944], grad_fn=<AddBackward0>)\n",
      "epoch: 11979 loss is tensor([-0.4761], grad_fn=<AddBackward0>)\n",
      "epoch: 11980 loss is tensor([-0.4936], grad_fn=<AddBackward0>)\n",
      "epoch: 11981 loss is tensor([-0.4766], grad_fn=<AddBackward0>)\n",
      "epoch: 11982 loss is tensor([-0.5204], grad_fn=<AddBackward0>)\n",
      "epoch: 11983 loss is tensor([-0.4804], grad_fn=<AddBackward0>)\n",
      "epoch: 11984 loss is tensor([-0.4992], grad_fn=<AddBackward0>)\n",
      "epoch: 11985 loss is tensor([-0.5201], grad_fn=<AddBackward0>)\n",
      "epoch: 11986 loss is tensor([-0.5024], grad_fn=<AddBackward0>)\n",
      "epoch: 11987 loss is tensor([-0.4842], grad_fn=<AddBackward0>)\n",
      "epoch: 11988 loss is tensor([-0.4859], grad_fn=<AddBackward0>)\n",
      "epoch: 11989 loss is tensor([-0.5163], grad_fn=<AddBackward0>)\n",
      "epoch: 11990 loss is tensor([-0.5320], grad_fn=<AddBackward0>)\n",
      "epoch: 11991 loss is tensor([-0.5276], grad_fn=<AddBackward0>)\n",
      "epoch: 11992 loss is tensor([-0.4964], grad_fn=<AddBackward0>)\n",
      "epoch: 11993 loss is tensor([-0.5344], grad_fn=<AddBackward0>)\n",
      "epoch: 11994 loss is tensor([-0.5276], grad_fn=<AddBackward0>)\n",
      "epoch: 11995 loss is tensor([-0.5063], grad_fn=<AddBackward0>)\n",
      "epoch: 11996 loss is tensor([-0.5229], grad_fn=<AddBackward0>)\n",
      "epoch: 11997 loss is tensor([-0.5196], grad_fn=<AddBackward0>)\n",
      "epoch: 11998 loss is tensor([-0.5147], grad_fn=<AddBackward0>)\n",
      "epoch: 11999 loss is tensor([-0.4860], grad_fn=<AddBackward0>)\n",
      "epoch: 12000 loss is tensor([-0.4971], grad_fn=<AddBackward0>)\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12001 loss is tensor([-0.4069], grad_fn=<AddBackward0>)\n",
      "epoch: 12002 loss is tensor([-0.4396], grad_fn=<AddBackward0>)\n",
      "epoch: 12003 loss is tensor([-0.4638], grad_fn=<AddBackward0>)\n",
      "epoch: 12004 loss is tensor([-0.4888], grad_fn=<AddBackward0>)\n",
      "epoch: 12005 loss is tensor([-0.4555], grad_fn=<AddBackward0>)\n",
      "epoch: 12006 loss is tensor([-0.4801], grad_fn=<AddBackward0>)\n",
      "epoch: 12007 loss is tensor([-0.4641], grad_fn=<AddBackward0>)\n",
      "epoch: 12008 loss is tensor([-0.5135], grad_fn=<AddBackward0>)\n",
      "epoch: 12009 loss is tensor([-0.4778], grad_fn=<AddBackward0>)\n",
      "epoch: 12010 loss is tensor([-0.4800], grad_fn=<AddBackward0>)\n",
      "epoch: 12011 loss is tensor([-0.4500], grad_fn=<AddBackward0>)\n",
      "epoch: 12012 loss is tensor([-0.4928], grad_fn=<AddBackward0>)\n",
      "epoch: 12013 loss is tensor([-0.4982], grad_fn=<AddBackward0>)\n",
      "epoch: 12014 loss is tensor([-0.5347], grad_fn=<AddBackward0>)\n",
      "epoch: 12015 loss is tensor([-0.5630], grad_fn=<AddBackward0>)\n",
      "epoch: 12016 loss is tensor([-0.5410], grad_fn=<AddBackward0>)\n",
      "epoch: 12017 loss is tensor([-0.5131], grad_fn=<AddBackward0>)\n",
      "epoch: 12018 loss is tensor([-0.5203], grad_fn=<AddBackward0>)\n",
      "epoch: 12019 loss is tensor([-0.5004], grad_fn=<AddBackward0>)\n",
      "epoch: 12020 loss is tensor([-0.5488], grad_fn=<AddBackward0>)\n",
      "epoch: 12021 loss is tensor([-0.4754], grad_fn=<AddBackward0>)\n",
      "epoch: 12022 loss is tensor([-0.5328], grad_fn=<AddBackward0>)\n",
      "epoch: 12023 loss is tensor([-0.5433], grad_fn=<AddBackward0>)\n",
      "epoch: 12024 loss is tensor([-0.5144], grad_fn=<AddBackward0>)\n",
      "epoch: 12025 loss is tensor([-0.5139], grad_fn=<AddBackward0>)\n",
      "epoch: 12026 loss is tensor([-0.5040], grad_fn=<AddBackward0>)\n",
      "epoch: 12027 loss is tensor([-0.4439], grad_fn=<AddBackward0>)\n",
      "epoch: 12028 loss is tensor([-0.5559], grad_fn=<AddBackward0>)\n",
      "epoch: 12029 loss is tensor([-0.5211], grad_fn=<AddBackward0>)\n",
      "epoch: 12030 loss is tensor([-0.5277], grad_fn=<AddBackward0>)\n",
      "epoch: 12031 loss is tensor([-0.4405], grad_fn=<AddBackward0>)\n",
      "epoch: 12032 loss is tensor([-0.4669], grad_fn=<AddBackward0>)\n",
      "epoch: 12033 loss is tensor([-0.5058], grad_fn=<AddBackward0>)\n",
      "epoch: 12034 loss is tensor([-0.4847], grad_fn=<AddBackward0>)\n",
      "epoch: 12035 loss is tensor([-0.4864], grad_fn=<AddBackward0>)\n",
      "epoch: 12036 loss is tensor([-0.5151], grad_fn=<AddBackward0>)\n",
      "epoch: 12037 loss is tensor([-0.5160], grad_fn=<AddBackward0>)\n",
      "epoch: 12038 loss is tensor([-0.5098], grad_fn=<AddBackward0>)\n",
      "epoch: 12039 loss is tensor([-0.5102], grad_fn=<AddBackward0>)\n",
      "epoch: 12040 loss is tensor([-0.4926], grad_fn=<AddBackward0>)\n",
      "epoch: 12041 loss is tensor([-0.4100], grad_fn=<AddBackward0>)\n",
      "epoch: 12042 loss is tensor([-0.4835], grad_fn=<AddBackward0>)\n",
      "epoch: 12043 loss is tensor([-0.4785], grad_fn=<AddBackward0>)\n",
      "epoch: 12044 loss is tensor([-0.4727], grad_fn=<AddBackward0>)\n",
      "epoch: 12045 loss is tensor([-0.5255], grad_fn=<AddBackward0>)\n",
      "epoch: 12046 loss is tensor([-0.5502], grad_fn=<AddBackward0>)\n",
      "epoch: 12047 loss is tensor([-0.5021], grad_fn=<AddBackward0>)\n",
      "epoch: 12048 loss is tensor([-0.4974], grad_fn=<AddBackward0>)\n",
      "epoch: 12049 loss is tensor([-0.5008], grad_fn=<AddBackward0>)\n",
      "epoch: 12050 loss is tensor([-0.4587], grad_fn=<AddBackward0>)\n",
      "epoch: 12051 loss is tensor([-0.4690], grad_fn=<AddBackward0>)\n",
      "epoch: 12052 loss is tensor([-0.5084], grad_fn=<AddBackward0>)\n",
      "epoch: 12053 loss is tensor([-0.4978], grad_fn=<AddBackward0>)\n",
      "epoch: 12054 loss is tensor([-0.4742], grad_fn=<AddBackward0>)\n",
      "epoch: 12055 loss is tensor([-0.3535], grad_fn=<AddBackward0>)\n",
      "epoch: 12056 loss is tensor([-0.3482], grad_fn=<AddBackward0>)\n",
      "epoch: 12057 loss is tensor([-0.4567], grad_fn=<AddBackward0>)\n",
      "epoch: 12058 loss is tensor([-0.4353], grad_fn=<AddBackward0>)\n",
      "epoch: 12059 loss is tensor([-0.4243], grad_fn=<AddBackward0>)\n",
      "epoch: 12060 loss is tensor([-0.3748], grad_fn=<AddBackward0>)\n",
      "epoch: 12061 loss is tensor([-0.4364], grad_fn=<AddBackward0>)\n",
      "epoch: 12062 loss is tensor([-0.4280], grad_fn=<AddBackward0>)\n",
      "epoch: 12063 loss is tensor([-0.4351], grad_fn=<AddBackward0>)\n",
      "epoch: 12064 loss is tensor([-0.4644], grad_fn=<AddBackward0>)\n",
      "epoch: 12065 loss is tensor([-0.5177], grad_fn=<AddBackward0>)\n",
      "epoch: 12066 loss is tensor([-0.5036], grad_fn=<AddBackward0>)\n",
      "epoch: 12067 loss is tensor([-0.4714], grad_fn=<AddBackward0>)\n",
      "epoch: 12068 loss is tensor([-0.4785], grad_fn=<AddBackward0>)\n",
      "epoch: 12069 loss is tensor([-0.5229], grad_fn=<AddBackward0>)\n",
      "epoch: 12070 loss is tensor([-0.4742], grad_fn=<AddBackward0>)\n",
      "epoch: 12071 loss is tensor([-0.4142], grad_fn=<AddBackward0>)\n",
      "epoch: 12072 loss is tensor([-0.4817], grad_fn=<AddBackward0>)\n",
      "epoch: 12073 loss is tensor([-0.4868], grad_fn=<AddBackward0>)\n",
      "epoch: 12074 loss is tensor([-0.4933], grad_fn=<AddBackward0>)\n",
      "epoch: 12075 loss is tensor([-0.5245], grad_fn=<AddBackward0>)\n",
      "epoch: 12076 loss is tensor([-0.5391], grad_fn=<AddBackward0>)\n",
      "epoch: 12077 loss is tensor([-0.4958], grad_fn=<AddBackward0>)\n",
      "epoch: 12078 loss is tensor([-0.4852], grad_fn=<AddBackward0>)\n",
      "epoch: 12079 loss is tensor([-0.4850], grad_fn=<AddBackward0>)\n",
      "epoch: 12080 loss is tensor([-0.4591], grad_fn=<AddBackward0>)\n",
      "epoch: 12081 loss is tensor([-0.4940], grad_fn=<AddBackward0>)\n",
      "epoch: 12082 loss is tensor([-0.5025], grad_fn=<AddBackward0>)\n",
      "epoch: 12083 loss is tensor([-0.5194], grad_fn=<AddBackward0>)\n",
      "epoch: 12084 loss is tensor([-0.4720], grad_fn=<AddBackward0>)\n",
      "epoch: 12085 loss is tensor([-0.4845], grad_fn=<AddBackward0>)\n",
      "epoch: 12086 loss is tensor([-0.5149], grad_fn=<AddBackward0>)\n",
      "epoch: 12087 loss is tensor([-0.4755], grad_fn=<AddBackward0>)\n",
      "epoch: 12088 loss is tensor([-0.5488], grad_fn=<AddBackward0>)\n",
      "epoch: 12089 loss is tensor([-0.5306], grad_fn=<AddBackward0>)\n",
      "epoch: 12090 loss is tensor([-0.5107], grad_fn=<AddBackward0>)\n",
      "epoch: 12091 loss is tensor([-0.5181], grad_fn=<AddBackward0>)\n",
      "epoch: 12092 loss is tensor([-0.5242], grad_fn=<AddBackward0>)\n",
      "epoch: 12093 loss is tensor([-0.4740], grad_fn=<AddBackward0>)\n",
      "epoch: 12094 loss is tensor([-0.4766], grad_fn=<AddBackward0>)\n",
      "epoch: 12095 loss is tensor([-0.5259], grad_fn=<AddBackward0>)\n",
      "epoch: 12096 loss is tensor([-0.4699], grad_fn=<AddBackward0>)\n",
      "epoch: 12097 loss is tensor([-0.5085], grad_fn=<AddBackward0>)\n",
      "epoch: 12098 loss is tensor([-0.4738], grad_fn=<AddBackward0>)\n",
      "epoch: 12099 loss is tensor([-0.4503], grad_fn=<AddBackward0>)\n",
      "epoch: 12100 loss is tensor([-0.5395], grad_fn=<AddBackward0>)\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12101 loss is tensor([-0.4676], grad_fn=<AddBackward0>)\n",
      "epoch: 12102 loss is tensor([-0.5218], grad_fn=<AddBackward0>)\n",
      "epoch: 12103 loss is tensor([-0.5606], grad_fn=<AddBackward0>)\n",
      "epoch: 12104 loss is tensor([-0.5002], grad_fn=<AddBackward0>)\n",
      "epoch: 12105 loss is tensor([-0.4477], grad_fn=<AddBackward0>)\n",
      "epoch: 12106 loss is tensor([-0.4960], grad_fn=<AddBackward0>)\n",
      "epoch: 12107 loss is tensor([-0.3000], grad_fn=<AddBackward0>)\n",
      "epoch: 12108 loss is tensor([-0.4730], grad_fn=<AddBackward0>)\n",
      "epoch: 12109 loss is tensor([-0.4089], grad_fn=<AddBackward0>)\n",
      "epoch: 12110 loss is tensor([-0.4082], grad_fn=<AddBackward0>)\n",
      "epoch: 12111 loss is tensor([-0.4512], grad_fn=<AddBackward0>)\n",
      "epoch: 12112 loss is tensor([-0.3983], grad_fn=<AddBackward0>)\n",
      "epoch: 12113 loss is tensor([-0.4559], grad_fn=<AddBackward0>)\n",
      "epoch: 12114 loss is tensor([-0.4103], grad_fn=<AddBackward0>)\n",
      "epoch: 12115 loss is tensor([-0.4790], grad_fn=<AddBackward0>)\n",
      "epoch: 12116 loss is tensor([-0.4513], grad_fn=<AddBackward0>)\n",
      "epoch: 12117 loss is tensor([-0.4276], grad_fn=<AddBackward0>)\n",
      "epoch: 12118 loss is tensor([-0.4968], grad_fn=<AddBackward0>)\n",
      "epoch: 12119 loss is tensor([-0.4658], grad_fn=<AddBackward0>)\n",
      "epoch: 12120 loss is tensor([-0.4168], grad_fn=<AddBackward0>)\n",
      "epoch: 12121 loss is tensor([-0.4819], grad_fn=<AddBackward0>)\n",
      "epoch: 12122 loss is tensor([-0.4802], grad_fn=<AddBackward0>)\n",
      "epoch: 12123 loss is tensor([-0.5005], grad_fn=<AddBackward0>)\n",
      "epoch: 12124 loss is tensor([-0.4602], grad_fn=<AddBackward0>)\n",
      "epoch: 12125 loss is tensor([-0.4372], grad_fn=<AddBackward0>)\n",
      "epoch: 12126 loss is tensor([-0.4230], grad_fn=<AddBackward0>)\n",
      "epoch: 12127 loss is tensor([-0.5288], grad_fn=<AddBackward0>)\n",
      "epoch: 12128 loss is tensor([-0.5349], grad_fn=<AddBackward0>)\n",
      "epoch: 12129 loss is tensor([-0.5282], grad_fn=<AddBackward0>)\n",
      "epoch: 12130 loss is tensor([-0.5476], grad_fn=<AddBackward0>)\n",
      "epoch: 12131 loss is tensor([-0.4972], grad_fn=<AddBackward0>)\n",
      "epoch: 12132 loss is tensor([-0.4904], grad_fn=<AddBackward0>)\n",
      "epoch: 12133 loss is tensor([-0.4883], grad_fn=<AddBackward0>)\n",
      "epoch: 12134 loss is tensor([-0.4937], grad_fn=<AddBackward0>)\n",
      "epoch: 12135 loss is tensor([-0.5085], grad_fn=<AddBackward0>)\n",
      "epoch: 12136 loss is tensor([-0.5403], grad_fn=<AddBackward0>)\n",
      "epoch: 12137 loss is tensor([-0.5197], grad_fn=<AddBackward0>)\n",
      "epoch: 12138 loss is tensor([-0.5451], grad_fn=<AddBackward0>)\n",
      "epoch: 12139 loss is tensor([-0.5660], grad_fn=<AddBackward0>)\n",
      "epoch: 12140 loss is tensor([-0.5453], grad_fn=<AddBackward0>)\n",
      "epoch: 12141 loss is tensor([-0.5086], grad_fn=<AddBackward0>)\n",
      "epoch: 12142 loss is tensor([-0.5718], grad_fn=<AddBackward0>)\n",
      "epoch: 12143 loss is tensor([-0.4696], grad_fn=<AddBackward0>)\n",
      "epoch: 12144 loss is tensor([-0.4735], grad_fn=<AddBackward0>)\n",
      "epoch: 12145 loss is tensor([-0.4966], grad_fn=<AddBackward0>)\n",
      "epoch: 12146 loss is tensor([-0.4903], grad_fn=<AddBackward0>)\n",
      "epoch: 12147 loss is tensor([-0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 12148 loss is tensor([-0.4982], grad_fn=<AddBackward0>)\n",
      "epoch: 12149 loss is tensor([-0.4867], grad_fn=<AddBackward0>)\n",
      "epoch: 12150 loss is tensor([-0.5228], grad_fn=<AddBackward0>)\n",
      "epoch: 12151 loss is tensor([-0.5088], grad_fn=<AddBackward0>)\n",
      "epoch: 12152 loss is tensor([-0.4974], grad_fn=<AddBackward0>)\n",
      "epoch: 12153 loss is tensor([-0.5423], grad_fn=<AddBackward0>)\n",
      "epoch: 12154 loss is tensor([-0.4952], grad_fn=<AddBackward0>)\n",
      "epoch: 12155 loss is tensor([-0.5442], grad_fn=<AddBackward0>)\n",
      "epoch: 12156 loss is tensor([-0.4796], grad_fn=<AddBackward0>)\n",
      "epoch: 12157 loss is tensor([-0.5671], grad_fn=<AddBackward0>)\n",
      "epoch: 12158 loss is tensor([-0.4690], grad_fn=<AddBackward0>)\n",
      "epoch: 12159 loss is tensor([-0.5085], grad_fn=<AddBackward0>)\n",
      "epoch: 12160 loss is tensor([-0.5155], grad_fn=<AddBackward0>)\n",
      "epoch: 12161 loss is tensor([-0.4835], grad_fn=<AddBackward0>)\n",
      "epoch: 12162 loss is tensor([-0.5420], grad_fn=<AddBackward0>)\n",
      "epoch: 12163 loss is tensor([-0.5468], grad_fn=<AddBackward0>)\n",
      "epoch: 12164 loss is tensor([-0.4829], grad_fn=<AddBackward0>)\n",
      "epoch: 12165 loss is tensor([-0.5391], grad_fn=<AddBackward0>)\n",
      "epoch: 12166 loss is tensor([-0.5229], grad_fn=<AddBackward0>)\n",
      "epoch: 12167 loss is tensor([-0.5132], grad_fn=<AddBackward0>)\n",
      "epoch: 12168 loss is tensor([-0.4960], grad_fn=<AddBackward0>)\n",
      "epoch: 12169 loss is tensor([-0.5478], grad_fn=<AddBackward0>)\n",
      "epoch: 12170 loss is tensor([-0.5879], grad_fn=<AddBackward0>)\n",
      "epoch: 12171 loss is tensor([-0.5219], grad_fn=<AddBackward0>)\n",
      "epoch: 12172 loss is tensor([-0.4417], grad_fn=<AddBackward0>)\n",
      "epoch: 12173 loss is tensor([-0.4957], grad_fn=<AddBackward0>)\n",
      "epoch: 12174 loss is tensor([-0.4946], grad_fn=<AddBackward0>)\n",
      "epoch: 12175 loss is tensor([-0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 12176 loss is tensor([-0.4775], grad_fn=<AddBackward0>)\n",
      "epoch: 12177 loss is tensor([-0.4899], grad_fn=<AddBackward0>)\n",
      "epoch: 12178 loss is tensor([-0.5563], grad_fn=<AddBackward0>)\n",
      "epoch: 12179 loss is tensor([-0.5839], grad_fn=<AddBackward0>)\n",
      "epoch: 12180 loss is tensor([-0.5295], grad_fn=<AddBackward0>)\n",
      "epoch: 12181 loss is tensor([-0.5427], grad_fn=<AddBackward0>)\n",
      "epoch: 12182 loss is tensor([-0.5351], grad_fn=<AddBackward0>)\n",
      "epoch: 12183 loss is tensor([-0.5976], grad_fn=<AddBackward0>)\n",
      "epoch: 12184 loss is tensor([-0.5394], grad_fn=<AddBackward0>)\n",
      "epoch: 12185 loss is tensor([-0.5765], grad_fn=<AddBackward0>)\n",
      "epoch: 12186 loss is tensor([-0.5569], grad_fn=<AddBackward0>)\n",
      "epoch: 12187 loss is tensor([-0.5363], grad_fn=<AddBackward0>)\n",
      "epoch: 12188 loss is tensor([-0.5331], grad_fn=<AddBackward0>)\n",
      "epoch: 12189 loss is tensor([-0.6032], grad_fn=<AddBackward0>)\n",
      "epoch: 12190 loss is tensor([-0.5319], grad_fn=<AddBackward0>)\n",
      "epoch: 12191 loss is tensor([-0.5210], grad_fn=<AddBackward0>)\n",
      "epoch: 12192 loss is tensor([-0.5365], grad_fn=<AddBackward0>)\n",
      "epoch: 12193 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 12194 loss is tensor([-0.5436], grad_fn=<AddBackward0>)\n",
      "epoch: 12195 loss is tensor([-0.5452], grad_fn=<AddBackward0>)\n",
      "epoch: 12196 loss is tensor([-0.5517], grad_fn=<AddBackward0>)\n",
      "epoch: 12197 loss is tensor([-0.5724], grad_fn=<AddBackward0>)\n",
      "epoch: 12198 loss is tensor([-0.5440], grad_fn=<AddBackward0>)\n",
      "epoch: 12199 loss is tensor([-0.5149], grad_fn=<AddBackward0>)\n",
      "epoch: 12200 loss is tensor([-0.5586], grad_fn=<AddBackward0>)\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12201 loss is tensor([-0.5368], grad_fn=<AddBackward0>)\n",
      "epoch: 12202 loss is tensor([-0.5439], grad_fn=<AddBackward0>)\n",
      "epoch: 12203 loss is tensor([-0.5558], grad_fn=<AddBackward0>)\n",
      "epoch: 12204 loss is tensor([-0.5370], grad_fn=<AddBackward0>)\n",
      "epoch: 12205 loss is tensor([-0.6376], grad_fn=<AddBackward0>)\n",
      "epoch: 12206 loss is tensor([-0.5355], grad_fn=<AddBackward0>)\n",
      "epoch: 12207 loss is tensor([-0.5191], grad_fn=<AddBackward0>)\n",
      "epoch: 12208 loss is tensor([-0.5466], grad_fn=<AddBackward0>)\n",
      "epoch: 12209 loss is tensor([-0.5706], grad_fn=<AddBackward0>)\n",
      "epoch: 12210 loss is tensor([-0.4967], grad_fn=<AddBackward0>)\n",
      "epoch: 12211 loss is tensor([-0.5491], grad_fn=<AddBackward0>)\n",
      "epoch: 12212 loss is tensor([-0.5147], grad_fn=<AddBackward0>)\n",
      "epoch: 12213 loss is tensor([-0.5020], grad_fn=<AddBackward0>)\n",
      "epoch: 12214 loss is tensor([-0.4760], grad_fn=<AddBackward0>)\n",
      "epoch: 12215 loss is tensor([-0.5787], grad_fn=<AddBackward0>)\n",
      "epoch: 12216 loss is tensor([-0.5023], grad_fn=<AddBackward0>)\n",
      "epoch: 12217 loss is tensor([-0.5434], grad_fn=<AddBackward0>)\n",
      "epoch: 12218 loss is tensor([-0.5642], grad_fn=<AddBackward0>)\n",
      "epoch: 12219 loss is tensor([-0.4691], grad_fn=<AddBackward0>)\n",
      "epoch: 12220 loss is tensor([-0.5206], grad_fn=<AddBackward0>)\n",
      "epoch: 12221 loss is tensor([-0.5089], grad_fn=<AddBackward0>)\n",
      "epoch: 12222 loss is tensor([-0.5232], grad_fn=<AddBackward0>)\n",
      "epoch: 12223 loss is tensor([-0.4854], grad_fn=<AddBackward0>)\n",
      "epoch: 12224 loss is tensor([-0.5340], grad_fn=<AddBackward0>)\n",
      "epoch: 12225 loss is tensor([-0.5206], grad_fn=<AddBackward0>)\n",
      "epoch: 12226 loss is tensor([-0.4662], grad_fn=<AddBackward0>)\n",
      "epoch: 12227 loss is tensor([-0.4889], grad_fn=<AddBackward0>)\n",
      "epoch: 12228 loss is tensor([-0.5348], grad_fn=<AddBackward0>)\n",
      "epoch: 12229 loss is tensor([-0.5646], grad_fn=<AddBackward0>)\n",
      "epoch: 12230 loss is tensor([-0.5409], grad_fn=<AddBackward0>)\n",
      "epoch: 12231 loss is tensor([-0.5271], grad_fn=<AddBackward0>)\n",
      "epoch: 12232 loss is tensor([-0.4930], grad_fn=<AddBackward0>)\n",
      "epoch: 12233 loss is tensor([-0.5417], grad_fn=<AddBackward0>)\n",
      "epoch: 12234 loss is tensor([-0.5369], grad_fn=<AddBackward0>)\n",
      "epoch: 12235 loss is tensor([-0.5536], grad_fn=<AddBackward0>)\n",
      "epoch: 12236 loss is tensor([-0.4914], grad_fn=<AddBackward0>)\n",
      "epoch: 12237 loss is tensor([-0.4817], grad_fn=<AddBackward0>)\n",
      "epoch: 12238 loss is tensor([-0.5140], grad_fn=<AddBackward0>)\n",
      "epoch: 12239 loss is tensor([-0.5120], grad_fn=<AddBackward0>)\n",
      "epoch: 12240 loss is tensor([-0.4583], grad_fn=<AddBackward0>)\n",
      "epoch: 12241 loss is tensor([-0.5060], grad_fn=<AddBackward0>)\n",
      "epoch: 12242 loss is tensor([-0.4923], grad_fn=<AddBackward0>)\n",
      "epoch: 12243 loss is tensor([-0.5061], grad_fn=<AddBackward0>)\n",
      "epoch: 12244 loss is tensor([-0.4808], grad_fn=<AddBackward0>)\n",
      "epoch: 12245 loss is tensor([-0.4488], grad_fn=<AddBackward0>)\n",
      "epoch: 12246 loss is tensor([-0.4951], grad_fn=<AddBackward0>)\n",
      "epoch: 12247 loss is tensor([-0.4755], grad_fn=<AddBackward0>)\n",
      "epoch: 12248 loss is tensor([-0.5683], grad_fn=<AddBackward0>)\n",
      "epoch: 12249 loss is tensor([-0.4808], grad_fn=<AddBackward0>)\n",
      "epoch: 12250 loss is tensor([-0.4868], grad_fn=<AddBackward0>)\n",
      "epoch: 12251 loss is tensor([-0.4975], grad_fn=<AddBackward0>)\n",
      "epoch: 12252 loss is tensor([-0.5341], grad_fn=<AddBackward0>)\n",
      "epoch: 12253 loss is tensor([-0.5605], grad_fn=<AddBackward0>)\n",
      "epoch: 12254 loss is tensor([-0.5129], grad_fn=<AddBackward0>)\n",
      "epoch: 12255 loss is tensor([-0.5461], grad_fn=<AddBackward0>)\n",
      "epoch: 12256 loss is tensor([-0.5322], grad_fn=<AddBackward0>)\n",
      "epoch: 12257 loss is tensor([-0.4585], grad_fn=<AddBackward0>)\n",
      "epoch: 12258 loss is tensor([-0.5203], grad_fn=<AddBackward0>)\n",
      "epoch: 12259 loss is tensor([-0.5061], grad_fn=<AddBackward0>)\n",
      "epoch: 12260 loss is tensor([-0.5002], grad_fn=<AddBackward0>)\n",
      "epoch: 12261 loss is tensor([-0.5430], grad_fn=<AddBackward0>)\n",
      "epoch: 12262 loss is tensor([-0.5339], grad_fn=<AddBackward0>)\n",
      "epoch: 12263 loss is tensor([-0.5196], grad_fn=<AddBackward0>)\n",
      "epoch: 12264 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 12265 loss is tensor([-0.5123], grad_fn=<AddBackward0>)\n",
      "epoch: 12266 loss is tensor([-0.5641], grad_fn=<AddBackward0>)\n",
      "epoch: 12267 loss is tensor([-0.4962], grad_fn=<AddBackward0>)\n",
      "epoch: 12268 loss is tensor([-0.5186], grad_fn=<AddBackward0>)\n",
      "epoch: 12269 loss is tensor([-0.4837], grad_fn=<AddBackward0>)\n",
      "epoch: 12270 loss is tensor([-0.5023], grad_fn=<AddBackward0>)\n",
      "epoch: 12271 loss is tensor([-0.4700], grad_fn=<AddBackward0>)\n",
      "epoch: 12272 loss is tensor([-0.4827], grad_fn=<AddBackward0>)\n",
      "epoch: 12273 loss is tensor([-0.4698], grad_fn=<AddBackward0>)\n",
      "epoch: 12274 loss is tensor([-0.4360], grad_fn=<AddBackward0>)\n",
      "epoch: 12275 loss is tensor([-0.4324], grad_fn=<AddBackward0>)\n",
      "epoch: 12276 loss is tensor([-0.5045], grad_fn=<AddBackward0>)\n",
      "epoch: 12277 loss is tensor([-0.4670], grad_fn=<AddBackward0>)\n",
      "epoch: 12278 loss is tensor([-0.4258], grad_fn=<AddBackward0>)\n",
      "epoch: 12279 loss is tensor([-0.3940], grad_fn=<AddBackward0>)\n",
      "epoch: 12280 loss is tensor([-0.3914], grad_fn=<AddBackward0>)\n",
      "epoch: 12281 loss is tensor([-0.3606], grad_fn=<AddBackward0>)\n",
      "epoch: 12282 loss is tensor([-0.5053], grad_fn=<AddBackward0>)\n",
      "epoch: 12283 loss is tensor([-0.4620], grad_fn=<AddBackward0>)\n",
      "epoch: 12284 loss is tensor([-0.4651], grad_fn=<AddBackward0>)\n",
      "epoch: 12285 loss is tensor([-0.4853], grad_fn=<AddBackward0>)\n",
      "epoch: 12286 loss is tensor([-0.4303], grad_fn=<AddBackward0>)\n",
      "epoch: 12287 loss is tensor([-0.5271], grad_fn=<AddBackward0>)\n",
      "epoch: 12288 loss is tensor([-0.5128], grad_fn=<AddBackward0>)\n",
      "epoch: 12289 loss is tensor([-0.4941], grad_fn=<AddBackward0>)\n",
      "epoch: 12290 loss is tensor([-0.5057], grad_fn=<AddBackward0>)\n",
      "epoch: 12291 loss is tensor([-0.5035], grad_fn=<AddBackward0>)\n",
      "epoch: 12292 loss is tensor([-0.4786], grad_fn=<AddBackward0>)\n",
      "epoch: 12293 loss is tensor([-0.4949], grad_fn=<AddBackward0>)\n",
      "epoch: 12294 loss is tensor([-0.4782], grad_fn=<AddBackward0>)\n",
      "epoch: 12295 loss is tensor([-0.5764], grad_fn=<AddBackward0>)\n",
      "epoch: 12296 loss is tensor([-0.4825], grad_fn=<AddBackward0>)\n",
      "epoch: 12297 loss is tensor([-0.5239], grad_fn=<AddBackward0>)\n",
      "epoch: 12298 loss is tensor([-0.5251], grad_fn=<AddBackward0>)\n",
      "epoch: 12299 loss is tensor([-0.5152], grad_fn=<AddBackward0>)\n",
      "epoch: 12300 loss is tensor([-0.4967], grad_fn=<AddBackward0>)\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12301 loss is tensor([-0.4432], grad_fn=<AddBackward0>)\n",
      "epoch: 12302 loss is tensor([-0.5045], grad_fn=<AddBackward0>)\n",
      "epoch: 12303 loss is tensor([-0.4967], grad_fn=<AddBackward0>)\n",
      "epoch: 12304 loss is tensor([-0.4881], grad_fn=<AddBackward0>)\n",
      "epoch: 12305 loss is tensor([-0.4433], grad_fn=<AddBackward0>)\n",
      "epoch: 12306 loss is tensor([-0.4554], grad_fn=<AddBackward0>)\n",
      "epoch: 12307 loss is tensor([-0.5805], grad_fn=<AddBackward0>)\n",
      "epoch: 12308 loss is tensor([-0.4987], grad_fn=<AddBackward0>)\n",
      "epoch: 12309 loss is tensor([-0.4439], grad_fn=<AddBackward0>)\n",
      "epoch: 12310 loss is tensor([-0.5239], grad_fn=<AddBackward0>)\n",
      "epoch: 12311 loss is tensor([-0.4737], grad_fn=<AddBackward0>)\n",
      "epoch: 12312 loss is tensor([-0.3944], grad_fn=<AddBackward0>)\n",
      "epoch: 12313 loss is tensor([-0.4383], grad_fn=<AddBackward0>)\n",
      "epoch: 12314 loss is tensor([-0.5050], grad_fn=<AddBackward0>)\n",
      "epoch: 12315 loss is tensor([-0.4705], grad_fn=<AddBackward0>)\n",
      "epoch: 12316 loss is tensor([-0.4604], grad_fn=<AddBackward0>)\n",
      "epoch: 12317 loss is tensor([-0.5004], grad_fn=<AddBackward0>)\n",
      "epoch: 12318 loss is tensor([-0.5152], grad_fn=<AddBackward0>)\n",
      "epoch: 12319 loss is tensor([-0.4069], grad_fn=<AddBackward0>)\n",
      "epoch: 12320 loss is tensor([-0.4947], grad_fn=<AddBackward0>)\n",
      "epoch: 12321 loss is tensor([-0.3996], grad_fn=<AddBackward0>)\n",
      "epoch: 12322 loss is tensor([-0.4841], grad_fn=<AddBackward0>)\n",
      "epoch: 12323 loss is tensor([-0.5765], grad_fn=<AddBackward0>)\n",
      "epoch: 12324 loss is tensor([-0.5175], grad_fn=<AddBackward0>)\n",
      "epoch: 12325 loss is tensor([-0.4781], grad_fn=<AddBackward0>)\n",
      "epoch: 12326 loss is tensor([-0.4944], grad_fn=<AddBackward0>)\n",
      "epoch: 12327 loss is tensor([-0.5088], grad_fn=<AddBackward0>)\n",
      "epoch: 12328 loss is tensor([-0.4759], grad_fn=<AddBackward0>)\n",
      "epoch: 12329 loss is tensor([-0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 12330 loss is tensor([-0.5504], grad_fn=<AddBackward0>)\n",
      "epoch: 12331 loss is tensor([-0.5229], grad_fn=<AddBackward0>)\n",
      "epoch: 12332 loss is tensor([-0.5046], grad_fn=<AddBackward0>)\n",
      "epoch: 12333 loss is tensor([-0.5243], grad_fn=<AddBackward0>)\n",
      "epoch: 12334 loss is tensor([-0.4962], grad_fn=<AddBackward0>)\n",
      "epoch: 12335 loss is tensor([-0.4545], grad_fn=<AddBackward0>)\n",
      "epoch: 12336 loss is tensor([-0.4877], grad_fn=<AddBackward0>)\n",
      "epoch: 12337 loss is tensor([-0.5639], grad_fn=<AddBackward0>)\n",
      "epoch: 12338 loss is tensor([-0.4182], grad_fn=<AddBackward0>)\n",
      "epoch: 12339 loss is tensor([-0.4662], grad_fn=<AddBackward0>)\n",
      "epoch: 12340 loss is tensor([-0.4792], grad_fn=<AddBackward0>)\n",
      "epoch: 12341 loss is tensor([-0.5104], grad_fn=<AddBackward0>)\n",
      "epoch: 12342 loss is tensor([-0.5500], grad_fn=<AddBackward0>)\n",
      "epoch: 12343 loss is tensor([-0.5486], grad_fn=<AddBackward0>)\n",
      "epoch: 12344 loss is tensor([-0.4883], grad_fn=<AddBackward0>)\n",
      "epoch: 12345 loss is tensor([-0.5386], grad_fn=<AddBackward0>)\n",
      "epoch: 12346 loss is tensor([-0.4951], grad_fn=<AddBackward0>)\n",
      "epoch: 12347 loss is tensor([-0.5953], grad_fn=<AddBackward0>)\n",
      "epoch: 12348 loss is tensor([-0.5543], grad_fn=<AddBackward0>)\n",
      "epoch: 12349 loss is tensor([-0.5054], grad_fn=<AddBackward0>)\n",
      "epoch: 12350 loss is tensor([-0.5161], grad_fn=<AddBackward0>)\n",
      "epoch: 12351 loss is tensor([-0.4774], grad_fn=<AddBackward0>)\n",
      "epoch: 12352 loss is tensor([-0.4892], grad_fn=<AddBackward0>)\n",
      "epoch: 12353 loss is tensor([-0.4530], grad_fn=<AddBackward0>)\n",
      "epoch: 12354 loss is tensor([-0.4673], grad_fn=<AddBackward0>)\n",
      "epoch: 12355 loss is tensor([-0.4883], grad_fn=<AddBackward0>)\n",
      "epoch: 12356 loss is tensor([-0.4915], grad_fn=<AddBackward0>)\n",
      "epoch: 12357 loss is tensor([-0.4837], grad_fn=<AddBackward0>)\n",
      "epoch: 12358 loss is tensor([-0.5026], grad_fn=<AddBackward0>)\n",
      "epoch: 12359 loss is tensor([-0.5674], grad_fn=<AddBackward0>)\n",
      "epoch: 12360 loss is tensor([-0.5488], grad_fn=<AddBackward0>)\n",
      "epoch: 12361 loss is tensor([-0.4597], grad_fn=<AddBackward0>)\n",
      "epoch: 12362 loss is tensor([-0.5386], grad_fn=<AddBackward0>)\n",
      "epoch: 12363 loss is tensor([-0.4867], grad_fn=<AddBackward0>)\n",
      "epoch: 12364 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 12365 loss is tensor([-0.4951], grad_fn=<AddBackward0>)\n",
      "epoch: 12366 loss is tensor([-0.4747], grad_fn=<AddBackward0>)\n",
      "epoch: 12367 loss is tensor([-0.5373], grad_fn=<AddBackward0>)\n",
      "epoch: 12368 loss is tensor([-0.5097], grad_fn=<AddBackward0>)\n",
      "epoch: 12369 loss is tensor([-0.4749], grad_fn=<AddBackward0>)\n",
      "epoch: 12370 loss is tensor([-0.5126], grad_fn=<AddBackward0>)\n",
      "epoch: 12371 loss is tensor([-0.5011], grad_fn=<AddBackward0>)\n",
      "epoch: 12372 loss is tensor([-0.5078], grad_fn=<AddBackward0>)\n",
      "epoch: 12373 loss is tensor([-0.5586], grad_fn=<AddBackward0>)\n",
      "epoch: 12374 loss is tensor([-0.5434], grad_fn=<AddBackward0>)\n",
      "epoch: 12375 loss is tensor([-0.5285], grad_fn=<AddBackward0>)\n",
      "epoch: 12376 loss is tensor([-0.5068], grad_fn=<AddBackward0>)\n",
      "epoch: 12377 loss is tensor([-0.6163], grad_fn=<AddBackward0>)\n",
      "epoch: 12378 loss is tensor([-0.4620], grad_fn=<AddBackward0>)\n",
      "epoch: 12379 loss is tensor([-0.4814], grad_fn=<AddBackward0>)\n",
      "epoch: 12380 loss is tensor([-0.4998], grad_fn=<AddBackward0>)\n",
      "epoch: 12381 loss is tensor([-0.5042], grad_fn=<AddBackward0>)\n",
      "epoch: 12382 loss is tensor([-0.4498], grad_fn=<AddBackward0>)\n",
      "epoch: 12383 loss is tensor([-0.4569], grad_fn=<AddBackward0>)\n",
      "epoch: 12384 loss is tensor([-0.5139], grad_fn=<AddBackward0>)\n",
      "epoch: 12385 loss is tensor([-0.5325], grad_fn=<AddBackward0>)\n",
      "epoch: 12386 loss is tensor([-0.4435], grad_fn=<AddBackward0>)\n",
      "epoch: 12387 loss is tensor([-0.4636], grad_fn=<AddBackward0>)\n",
      "epoch: 12388 loss is tensor([-0.5325], grad_fn=<AddBackward0>)\n",
      "epoch: 12389 loss is tensor([-0.4892], grad_fn=<AddBackward0>)\n",
      "epoch: 12390 loss is tensor([-0.4487], grad_fn=<AddBackward0>)\n",
      "epoch: 12391 loss is tensor([-0.5372], grad_fn=<AddBackward0>)\n",
      "epoch: 12392 loss is tensor([-0.5351], grad_fn=<AddBackward0>)\n",
      "epoch: 12393 loss is tensor([-0.5236], grad_fn=<AddBackward0>)\n",
      "epoch: 12394 loss is tensor([-0.5666], grad_fn=<AddBackward0>)\n",
      "epoch: 12395 loss is tensor([-0.4733], grad_fn=<AddBackward0>)\n",
      "epoch: 12396 loss is tensor([-0.5107], grad_fn=<AddBackward0>)\n",
      "epoch: 12397 loss is tensor([-0.4985], grad_fn=<AddBackward0>)\n",
      "epoch: 12398 loss is tensor([-0.4657], grad_fn=<AddBackward0>)\n",
      "epoch: 12399 loss is tensor([-0.5375], grad_fn=<AddBackward0>)\n",
      "epoch: 12400 loss is tensor([-0.5319], grad_fn=<AddBackward0>)\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12401 loss is tensor([-0.5151], grad_fn=<AddBackward0>)\n",
      "epoch: 12402 loss is tensor([-0.5241], grad_fn=<AddBackward0>)\n",
      "epoch: 12403 loss is tensor([-0.5348], grad_fn=<AddBackward0>)\n",
      "epoch: 12404 loss is tensor([-0.5496], grad_fn=<AddBackward0>)\n",
      "epoch: 12405 loss is tensor([-0.4746], grad_fn=<AddBackward0>)\n",
      "epoch: 12406 loss is tensor([-0.5205], grad_fn=<AddBackward0>)\n",
      "epoch: 12407 loss is tensor([-0.5200], grad_fn=<AddBackward0>)\n",
      "epoch: 12408 loss is tensor([-0.5217], grad_fn=<AddBackward0>)\n",
      "epoch: 12409 loss is tensor([-0.4819], grad_fn=<AddBackward0>)\n",
      "epoch: 12410 loss is tensor([-0.5828], grad_fn=<AddBackward0>)\n",
      "epoch: 12411 loss is tensor([-0.5410], grad_fn=<AddBackward0>)\n",
      "epoch: 12412 loss is tensor([-0.5508], grad_fn=<AddBackward0>)\n",
      "epoch: 12413 loss is tensor([-0.5720], grad_fn=<AddBackward0>)\n",
      "epoch: 12414 loss is tensor([-0.5738], grad_fn=<AddBackward0>)\n",
      "epoch: 12415 loss is tensor([-0.5016], grad_fn=<AddBackward0>)\n",
      "epoch: 12416 loss is tensor([-0.5224], grad_fn=<AddBackward0>)\n",
      "epoch: 12417 loss is tensor([-0.5545], grad_fn=<AddBackward0>)\n",
      "epoch: 12418 loss is tensor([-0.5351], grad_fn=<AddBackward0>)\n",
      "epoch: 12419 loss is tensor([-0.5298], grad_fn=<AddBackward0>)\n",
      "epoch: 12420 loss is tensor([-0.5518], grad_fn=<AddBackward0>)\n",
      "epoch: 12421 loss is tensor([-0.5704], grad_fn=<AddBackward0>)\n",
      "epoch: 12422 loss is tensor([-0.5909], grad_fn=<AddBackward0>)\n",
      "epoch: 12423 loss is tensor([-0.5384], grad_fn=<AddBackward0>)\n",
      "epoch: 12424 loss is tensor([-0.5230], grad_fn=<AddBackward0>)\n",
      "epoch: 12425 loss is tensor([-0.5273], grad_fn=<AddBackward0>)\n",
      "epoch: 12426 loss is tensor([-0.5338], grad_fn=<AddBackward0>)\n",
      "epoch: 12427 loss is tensor([-0.5488], grad_fn=<AddBackward0>)\n",
      "epoch: 12428 loss is tensor([-0.5666], grad_fn=<AddBackward0>)\n",
      "epoch: 12429 loss is tensor([-0.5169], grad_fn=<AddBackward0>)\n",
      "epoch: 12430 loss is tensor([-0.5463], grad_fn=<AddBackward0>)\n",
      "epoch: 12431 loss is tensor([-0.5676], grad_fn=<AddBackward0>)\n",
      "epoch: 12432 loss is tensor([-0.6274], grad_fn=<AddBackward0>)\n",
      "epoch: 12433 loss is tensor([-0.5665], grad_fn=<AddBackward0>)\n",
      "epoch: 12434 loss is tensor([-0.5470], grad_fn=<AddBackward0>)\n",
      "epoch: 12435 loss is tensor([-0.5787], grad_fn=<AddBackward0>)\n",
      "epoch: 12436 loss is tensor([-0.5734], grad_fn=<AddBackward0>)\n",
      "epoch: 12437 loss is tensor([-0.5471], grad_fn=<AddBackward0>)\n",
      "epoch: 12438 loss is tensor([-0.5650], grad_fn=<AddBackward0>)\n",
      "epoch: 12439 loss is tensor([-0.5222], grad_fn=<AddBackward0>)\n",
      "epoch: 12440 loss is tensor([-0.5973], grad_fn=<AddBackward0>)\n",
      "epoch: 12441 loss is tensor([-0.5296], grad_fn=<AddBackward0>)\n",
      "epoch: 12442 loss is tensor([-0.5645], grad_fn=<AddBackward0>)\n",
      "epoch: 12443 loss is tensor([-0.5427], grad_fn=<AddBackward0>)\n",
      "epoch: 12444 loss is tensor([-0.5585], grad_fn=<AddBackward0>)\n",
      "epoch: 12445 loss is tensor([-0.4862], grad_fn=<AddBackward0>)\n",
      "epoch: 12446 loss is tensor([-0.5241], grad_fn=<AddBackward0>)\n",
      "epoch: 12447 loss is tensor([-0.5354], grad_fn=<AddBackward0>)\n",
      "epoch: 12448 loss is tensor([-0.4982], grad_fn=<AddBackward0>)\n",
      "epoch: 12449 loss is tensor([-0.4937], grad_fn=<AddBackward0>)\n",
      "epoch: 12450 loss is tensor([-0.5179], grad_fn=<AddBackward0>)\n",
      "epoch: 12451 loss is tensor([-0.5455], grad_fn=<AddBackward0>)\n",
      "epoch: 12452 loss is tensor([-0.5647], grad_fn=<AddBackward0>)\n",
      "epoch: 12453 loss is tensor([-0.5780], grad_fn=<AddBackward0>)\n",
      "epoch: 12454 loss is tensor([-0.4907], grad_fn=<AddBackward0>)\n",
      "epoch: 12455 loss is tensor([-0.4820], grad_fn=<AddBackward0>)\n",
      "epoch: 12456 loss is tensor([-0.5003], grad_fn=<AddBackward0>)\n",
      "epoch: 12457 loss is tensor([-0.5423], grad_fn=<AddBackward0>)\n",
      "epoch: 12458 loss is tensor([-0.5850], grad_fn=<AddBackward0>)\n",
      "epoch: 12459 loss is tensor([-0.5028], grad_fn=<AddBackward0>)\n",
      "epoch: 12460 loss is tensor([-0.5591], grad_fn=<AddBackward0>)\n",
      "epoch: 12461 loss is tensor([-0.5245], grad_fn=<AddBackward0>)\n",
      "epoch: 12462 loss is tensor([-0.5243], grad_fn=<AddBackward0>)\n",
      "epoch: 12463 loss is tensor([-0.5351], grad_fn=<AddBackward0>)\n",
      "epoch: 12464 loss is tensor([-0.5174], grad_fn=<AddBackward0>)\n",
      "epoch: 12465 loss is tensor([-0.5427], grad_fn=<AddBackward0>)\n",
      "epoch: 12466 loss is tensor([-0.5467], grad_fn=<AddBackward0>)\n",
      "epoch: 12467 loss is tensor([-0.5382], grad_fn=<AddBackward0>)\n",
      "epoch: 12468 loss is tensor([-0.4784], grad_fn=<AddBackward0>)\n",
      "epoch: 12469 loss is tensor([-0.5490], grad_fn=<AddBackward0>)\n",
      "epoch: 12470 loss is tensor([-0.5629], grad_fn=<AddBackward0>)\n",
      "epoch: 12471 loss is tensor([-0.5025], grad_fn=<AddBackward0>)\n",
      "epoch: 12472 loss is tensor([-0.4913], grad_fn=<AddBackward0>)\n",
      "epoch: 12473 loss is tensor([-0.5378], grad_fn=<AddBackward0>)\n",
      "epoch: 12474 loss is tensor([-0.5274], grad_fn=<AddBackward0>)\n",
      "epoch: 12475 loss is tensor([-0.4997], grad_fn=<AddBackward0>)\n",
      "epoch: 12476 loss is tensor([-0.5094], grad_fn=<AddBackward0>)\n",
      "epoch: 12477 loss is tensor([-0.5727], grad_fn=<AddBackward0>)\n",
      "epoch: 12478 loss is tensor([-0.5187], grad_fn=<AddBackward0>)\n",
      "epoch: 12479 loss is tensor([-0.5525], grad_fn=<AddBackward0>)\n",
      "epoch: 12480 loss is tensor([-0.5187], grad_fn=<AddBackward0>)\n",
      "epoch: 12481 loss is tensor([-0.5339], grad_fn=<AddBackward0>)\n",
      "epoch: 12482 loss is tensor([-0.5783], grad_fn=<AddBackward0>)\n",
      "epoch: 12483 loss is tensor([-0.5132], grad_fn=<AddBackward0>)\n",
      "epoch: 12484 loss is tensor([-0.4928], grad_fn=<AddBackward0>)\n",
      "epoch: 12485 loss is tensor([-0.5419], grad_fn=<AddBackward0>)\n",
      "epoch: 12486 loss is tensor([-0.5161], grad_fn=<AddBackward0>)\n",
      "epoch: 12487 loss is tensor([-0.5167], grad_fn=<AddBackward0>)\n",
      "epoch: 12488 loss is tensor([-0.5024], grad_fn=<AddBackward0>)\n",
      "epoch: 12489 loss is tensor([-0.5060], grad_fn=<AddBackward0>)\n",
      "epoch: 12490 loss is tensor([-0.5590], grad_fn=<AddBackward0>)\n",
      "epoch: 12491 loss is tensor([-0.5156], grad_fn=<AddBackward0>)\n",
      "epoch: 12492 loss is tensor([-0.5309], grad_fn=<AddBackward0>)\n",
      "epoch: 12493 loss is tensor([-0.5274], grad_fn=<AddBackward0>)\n",
      "epoch: 12494 loss is tensor([-0.5880], grad_fn=<AddBackward0>)\n",
      "epoch: 12495 loss is tensor([-0.5275], grad_fn=<AddBackward0>)\n",
      "epoch: 12496 loss is tensor([-0.5342], grad_fn=<AddBackward0>)\n",
      "epoch: 12497 loss is tensor([-0.5851], grad_fn=<AddBackward0>)\n",
      "epoch: 12498 loss is tensor([-0.5728], grad_fn=<AddBackward0>)\n",
      "epoch: 12499 loss is tensor([-0.5956], grad_fn=<AddBackward0>)\n",
      "epoch: 12500 loss is tensor([-0.5639], grad_fn=<AddBackward0>)\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12501 loss is tensor([-0.5463], grad_fn=<AddBackward0>)\n",
      "epoch: 12502 loss is tensor([-0.6338], grad_fn=<AddBackward0>)\n",
      "epoch: 12503 loss is tensor([-0.6362], grad_fn=<AddBackward0>)\n",
      "epoch: 12504 loss is tensor([-0.5883], grad_fn=<AddBackward0>)\n",
      "epoch: 12505 loss is tensor([-0.5587], grad_fn=<AddBackward0>)\n",
      "epoch: 12506 loss is tensor([-0.5434], grad_fn=<AddBackward0>)\n",
      "epoch: 12507 loss is tensor([-0.5632], grad_fn=<AddBackward0>)\n",
      "epoch: 12508 loss is tensor([-0.5380], grad_fn=<AddBackward0>)\n",
      "epoch: 12509 loss is tensor([-0.5193], grad_fn=<AddBackward0>)\n",
      "epoch: 12510 loss is tensor([-0.5561], grad_fn=<AddBackward0>)\n",
      "epoch: 12511 loss is tensor([-0.5354], grad_fn=<AddBackward0>)\n",
      "epoch: 12512 loss is tensor([-0.5545], grad_fn=<AddBackward0>)\n",
      "epoch: 12513 loss is tensor([-0.5221], grad_fn=<AddBackward0>)\n",
      "epoch: 12514 loss is tensor([-0.5157], grad_fn=<AddBackward0>)\n",
      "epoch: 12515 loss is tensor([-0.6022], grad_fn=<AddBackward0>)\n",
      "epoch: 12516 loss is tensor([-0.5897], grad_fn=<AddBackward0>)\n",
      "epoch: 12517 loss is tensor([-0.5033], grad_fn=<AddBackward0>)\n",
      "epoch: 12518 loss is tensor([-0.5415], grad_fn=<AddBackward0>)\n",
      "epoch: 12519 loss is tensor([-0.5642], grad_fn=<AddBackward0>)\n",
      "epoch: 12520 loss is tensor([-0.5570], grad_fn=<AddBackward0>)\n",
      "epoch: 12521 loss is tensor([-0.5727], grad_fn=<AddBackward0>)\n",
      "epoch: 12522 loss is tensor([-0.5291], grad_fn=<AddBackward0>)\n",
      "epoch: 12523 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 12524 loss is tensor([-0.5653], grad_fn=<AddBackward0>)\n",
      "epoch: 12525 loss is tensor([-0.5537], grad_fn=<AddBackward0>)\n",
      "epoch: 12526 loss is tensor([-0.5820], grad_fn=<AddBackward0>)\n",
      "epoch: 12527 loss is tensor([-0.6087], grad_fn=<AddBackward0>)\n",
      "epoch: 12528 loss is tensor([-0.5515], grad_fn=<AddBackward0>)\n",
      "epoch: 12529 loss is tensor([-0.5970], grad_fn=<AddBackward0>)\n",
      "epoch: 12530 loss is tensor([-0.5885], grad_fn=<AddBackward0>)\n",
      "epoch: 12531 loss is tensor([-0.5570], grad_fn=<AddBackward0>)\n",
      "epoch: 12532 loss is tensor([-0.5427], grad_fn=<AddBackward0>)\n",
      "epoch: 12533 loss is tensor([-0.5703], grad_fn=<AddBackward0>)\n",
      "epoch: 12534 loss is tensor([-0.5374], grad_fn=<AddBackward0>)\n",
      "epoch: 12535 loss is tensor([-0.5851], grad_fn=<AddBackward0>)\n",
      "epoch: 12536 loss is tensor([-0.5373], grad_fn=<AddBackward0>)\n",
      "epoch: 12537 loss is tensor([-0.5629], grad_fn=<AddBackward0>)\n",
      "epoch: 12538 loss is tensor([-0.4992], grad_fn=<AddBackward0>)\n",
      "epoch: 12539 loss is tensor([-0.5629], grad_fn=<AddBackward0>)\n",
      "epoch: 12540 loss is tensor([-0.5301], grad_fn=<AddBackward0>)\n",
      "epoch: 12541 loss is tensor([-0.5664], grad_fn=<AddBackward0>)\n",
      "epoch: 12542 loss is tensor([-0.5776], grad_fn=<AddBackward0>)\n",
      "epoch: 12543 loss is tensor([-0.5379], grad_fn=<AddBackward0>)\n",
      "epoch: 12544 loss is tensor([-0.5638], grad_fn=<AddBackward0>)\n",
      "epoch: 12545 loss is tensor([-0.5780], grad_fn=<AddBackward0>)\n",
      "epoch: 12546 loss is tensor([-0.5477], grad_fn=<AddBackward0>)\n",
      "epoch: 12547 loss is tensor([-0.5183], grad_fn=<AddBackward0>)\n",
      "epoch: 12548 loss is tensor([-0.4951], grad_fn=<AddBackward0>)\n",
      "epoch: 12549 loss is tensor([-0.5602], grad_fn=<AddBackward0>)\n",
      "epoch: 12550 loss is tensor([-0.5832], grad_fn=<AddBackward0>)\n",
      "epoch: 12551 loss is tensor([-0.5256], grad_fn=<AddBackward0>)\n",
      "epoch: 12552 loss is tensor([-0.5033], grad_fn=<AddBackward0>)\n",
      "epoch: 12553 loss is tensor([-0.6021], grad_fn=<AddBackward0>)\n",
      "epoch: 12554 loss is tensor([-0.6057], grad_fn=<AddBackward0>)\n",
      "epoch: 12555 loss is tensor([-0.5174], grad_fn=<AddBackward0>)\n",
      "epoch: 12556 loss is tensor([-0.5291], grad_fn=<AddBackward0>)\n",
      "epoch: 12557 loss is tensor([-0.5767], grad_fn=<AddBackward0>)\n",
      "epoch: 12558 loss is tensor([-0.5839], grad_fn=<AddBackward0>)\n",
      "epoch: 12559 loss is tensor([-0.5623], grad_fn=<AddBackward0>)\n",
      "epoch: 12560 loss is tensor([-0.5557], grad_fn=<AddBackward0>)\n",
      "epoch: 12561 loss is tensor([-0.5031], grad_fn=<AddBackward0>)\n",
      "epoch: 12562 loss is tensor([-0.5145], grad_fn=<AddBackward0>)\n",
      "epoch: 12563 loss is tensor([-0.5041], grad_fn=<AddBackward0>)\n",
      "epoch: 12564 loss is tensor([-0.4872], grad_fn=<AddBackward0>)\n",
      "epoch: 12565 loss is tensor([-0.5550], grad_fn=<AddBackward0>)\n",
      "epoch: 12566 loss is tensor([-0.5234], grad_fn=<AddBackward0>)\n",
      "epoch: 12567 loss is tensor([-0.5642], grad_fn=<AddBackward0>)\n",
      "epoch: 12568 loss is tensor([-0.5416], grad_fn=<AddBackward0>)\n",
      "epoch: 12569 loss is tensor([-0.5249], grad_fn=<AddBackward0>)\n",
      "epoch: 12570 loss is tensor([-0.5594], grad_fn=<AddBackward0>)\n",
      "epoch: 12571 loss is tensor([-0.5644], grad_fn=<AddBackward0>)\n",
      "epoch: 12572 loss is tensor([-0.4662], grad_fn=<AddBackward0>)\n",
      "epoch: 12573 loss is tensor([-0.5919], grad_fn=<AddBackward0>)\n",
      "epoch: 12574 loss is tensor([-0.5753], grad_fn=<AddBackward0>)\n",
      "epoch: 12575 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 12576 loss is tensor([-0.5349], grad_fn=<AddBackward0>)\n",
      "epoch: 12577 loss is tensor([-0.5423], grad_fn=<AddBackward0>)\n",
      "epoch: 12578 loss is tensor([-0.5280], grad_fn=<AddBackward0>)\n",
      "epoch: 12579 loss is tensor([-0.5384], grad_fn=<AddBackward0>)\n",
      "epoch: 12580 loss is tensor([-0.5071], grad_fn=<AddBackward0>)\n",
      "epoch: 12581 loss is tensor([-0.5122], grad_fn=<AddBackward0>)\n",
      "epoch: 12582 loss is tensor([-0.4773], grad_fn=<AddBackward0>)\n",
      "epoch: 12583 loss is tensor([-0.4831], grad_fn=<AddBackward0>)\n",
      "epoch: 12584 loss is tensor([-0.4623], grad_fn=<AddBackward0>)\n",
      "epoch: 12585 loss is tensor([-0.4581], grad_fn=<AddBackward0>)\n",
      "epoch: 12586 loss is tensor([-0.4512], grad_fn=<AddBackward0>)\n",
      "epoch: 12587 loss is tensor([-0.5121], grad_fn=<AddBackward0>)\n",
      "epoch: 12588 loss is tensor([-0.5538], grad_fn=<AddBackward0>)\n",
      "epoch: 12589 loss is tensor([-0.4849], grad_fn=<AddBackward0>)\n",
      "epoch: 12590 loss is tensor([-0.5267], grad_fn=<AddBackward0>)\n",
      "epoch: 12591 loss is tensor([-0.4932], grad_fn=<AddBackward0>)\n",
      "epoch: 12592 loss is tensor([-0.4976], grad_fn=<AddBackward0>)\n",
      "epoch: 12593 loss is tensor([-0.5310], grad_fn=<AddBackward0>)\n",
      "epoch: 12594 loss is tensor([-0.5351], grad_fn=<AddBackward0>)\n",
      "epoch: 12595 loss is tensor([-0.5697], grad_fn=<AddBackward0>)\n",
      "epoch: 12596 loss is tensor([-0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 12597 loss is tensor([-0.5719], grad_fn=<AddBackward0>)\n",
      "epoch: 12598 loss is tensor([-0.5302], grad_fn=<AddBackward0>)\n",
      "epoch: 12599 loss is tensor([-0.5238], grad_fn=<AddBackward0>)\n",
      "epoch: 12600 loss is tensor([-0.5380], grad_fn=<AddBackward0>)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12601 loss is tensor([-0.5700], grad_fn=<AddBackward0>)\n",
      "epoch: 12602 loss is tensor([-0.5165], grad_fn=<AddBackward0>)\n",
      "epoch: 12603 loss is tensor([-0.5455], grad_fn=<AddBackward0>)\n",
      "epoch: 12604 loss is tensor([-0.5977], grad_fn=<AddBackward0>)\n",
      "epoch: 12605 loss is tensor([-0.5681], grad_fn=<AddBackward0>)\n",
      "epoch: 12606 loss is tensor([-0.5722], grad_fn=<AddBackward0>)\n",
      "epoch: 12607 loss is tensor([-0.5946], grad_fn=<AddBackward0>)\n",
      "epoch: 12608 loss is tensor([-0.4917], grad_fn=<AddBackward0>)\n",
      "epoch: 12609 loss is tensor([-0.5580], grad_fn=<AddBackward0>)\n",
      "epoch: 12610 loss is tensor([-0.5299], grad_fn=<AddBackward0>)\n",
      "epoch: 12611 loss is tensor([-0.5727], grad_fn=<AddBackward0>)\n",
      "epoch: 12612 loss is tensor([-0.5646], grad_fn=<AddBackward0>)\n",
      "epoch: 12613 loss is tensor([-0.5056], grad_fn=<AddBackward0>)\n",
      "epoch: 12614 loss is tensor([-0.6130], grad_fn=<AddBackward0>)\n",
      "epoch: 12615 loss is tensor([-0.5563], grad_fn=<AddBackward0>)\n",
      "epoch: 12616 loss is tensor([-0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 12617 loss is tensor([-0.5837], grad_fn=<AddBackward0>)\n",
      "epoch: 12618 loss is tensor([-0.5059], grad_fn=<AddBackward0>)\n",
      "epoch: 12619 loss is tensor([-0.5848], grad_fn=<AddBackward0>)\n",
      "epoch: 12620 loss is tensor([-0.5690], grad_fn=<AddBackward0>)\n",
      "epoch: 12621 loss is tensor([-0.5557], grad_fn=<AddBackward0>)\n",
      "epoch: 12622 loss is tensor([-0.5674], grad_fn=<AddBackward0>)\n",
      "epoch: 12623 loss is tensor([-0.6116], grad_fn=<AddBackward0>)\n",
      "epoch: 12624 loss is tensor([-0.5474], grad_fn=<AddBackward0>)\n",
      "epoch: 12625 loss is tensor([-0.6197], grad_fn=<AddBackward0>)\n",
      "epoch: 12626 loss is tensor([-0.6016], grad_fn=<AddBackward0>)\n",
      "epoch: 12627 loss is tensor([-0.5427], grad_fn=<AddBackward0>)\n",
      "epoch: 12628 loss is tensor([-0.5898], grad_fn=<AddBackward0>)\n",
      "epoch: 12629 loss is tensor([-0.5208], grad_fn=<AddBackward0>)\n",
      "epoch: 12630 loss is tensor([-0.5794], grad_fn=<AddBackward0>)\n",
      "epoch: 12631 loss is tensor([-0.5542], grad_fn=<AddBackward0>)\n",
      "epoch: 12632 loss is tensor([-0.5999], grad_fn=<AddBackward0>)\n",
      "epoch: 12633 loss is tensor([-0.5953], grad_fn=<AddBackward0>)\n",
      "epoch: 12634 loss is tensor([-0.5616], grad_fn=<AddBackward0>)\n",
      "epoch: 12635 loss is tensor([-0.6123], grad_fn=<AddBackward0>)\n",
      "epoch: 12636 loss is tensor([-0.5740], grad_fn=<AddBackward0>)\n",
      "epoch: 12637 loss is tensor([-0.5538], grad_fn=<AddBackward0>)\n",
      "epoch: 12638 loss is tensor([-0.5255], grad_fn=<AddBackward0>)\n",
      "epoch: 12639 loss is tensor([-0.4489], grad_fn=<AddBackward0>)\n",
      "epoch: 12640 loss is tensor([-0.5097], grad_fn=<AddBackward0>)\n",
      "epoch: 12641 loss is tensor([-0.4205], grad_fn=<AddBackward0>)\n",
      "epoch: 12642 loss is tensor([-0.4671], grad_fn=<AddBackward0>)\n",
      "epoch: 12643 loss is tensor([-0.5091], grad_fn=<AddBackward0>)\n",
      "epoch: 12644 loss is tensor([-0.4859], grad_fn=<AddBackward0>)\n",
      "epoch: 12645 loss is tensor([-0.4937], grad_fn=<AddBackward0>)\n",
      "epoch: 12646 loss is tensor([-0.4830], grad_fn=<AddBackward0>)\n",
      "epoch: 12647 loss is tensor([-0.2363], grad_fn=<AddBackward0>)\n",
      "epoch: 12648 loss is tensor([-0.3030], grad_fn=<AddBackward0>)\n",
      "epoch: 12649 loss is tensor([-0.4334], grad_fn=<AddBackward0>)\n",
      "epoch: 12650 loss is tensor([-0.4249], grad_fn=<AddBackward0>)\n",
      "epoch: 12651 loss is tensor([-0.3903], grad_fn=<AddBackward0>)\n",
      "epoch: 12652 loss is tensor([-0.4379], grad_fn=<AddBackward0>)\n",
      "epoch: 12653 loss is tensor([-0.4714], grad_fn=<AddBackward0>)\n",
      "epoch: 12654 loss is tensor([-0.4403], grad_fn=<AddBackward0>)\n",
      "epoch: 12655 loss is tensor([-0.4751], grad_fn=<AddBackward0>)\n",
      "epoch: 12656 loss is tensor([-0.4474], grad_fn=<AddBackward0>)\n",
      "epoch: 12657 loss is tensor([-0.4568], grad_fn=<AddBackward0>)\n",
      "epoch: 12658 loss is tensor([-0.5073], grad_fn=<AddBackward0>)\n",
      "epoch: 12659 loss is tensor([-0.5387], grad_fn=<AddBackward0>)\n",
      "epoch: 12660 loss is tensor([-0.5173], grad_fn=<AddBackward0>)\n",
      "epoch: 12661 loss is tensor([-0.4989], grad_fn=<AddBackward0>)\n",
      "epoch: 12662 loss is tensor([-0.4988], grad_fn=<AddBackward0>)\n",
      "epoch: 12663 loss is tensor([-0.4699], grad_fn=<AddBackward0>)\n",
      "epoch: 12664 loss is tensor([-0.4812], grad_fn=<AddBackward0>)\n",
      "epoch: 12665 loss is tensor([-0.4864], grad_fn=<AddBackward0>)\n",
      "epoch: 12666 loss is tensor([-0.5109], grad_fn=<AddBackward0>)\n",
      "epoch: 12667 loss is tensor([-0.3901], grad_fn=<AddBackward0>)\n",
      "epoch: 12668 loss is tensor([-0.4436], grad_fn=<AddBackward0>)\n",
      "epoch: 12669 loss is tensor([-0.4877], grad_fn=<AddBackward0>)\n",
      "epoch: 12670 loss is tensor([-0.4406], grad_fn=<AddBackward0>)\n",
      "epoch: 12671 loss is tensor([-0.3952], grad_fn=<AddBackward0>)\n",
      "epoch: 12672 loss is tensor([-0.4743], grad_fn=<AddBackward0>)\n",
      "epoch: 12673 loss is tensor([-0.4255], grad_fn=<AddBackward0>)\n",
      "epoch: 12674 loss is tensor([-0.4453], grad_fn=<AddBackward0>)\n",
      "epoch: 12675 loss is tensor([-0.5032], grad_fn=<AddBackward0>)\n",
      "epoch: 12676 loss is tensor([-0.4673], grad_fn=<AddBackward0>)\n",
      "epoch: 12677 loss is tensor([-0.4622], grad_fn=<AddBackward0>)\n",
      "epoch: 12678 loss is tensor([-0.5372], grad_fn=<AddBackward0>)\n",
      "epoch: 12679 loss is tensor([-0.5746], grad_fn=<AddBackward0>)\n",
      "epoch: 12680 loss is tensor([-0.4813], grad_fn=<AddBackward0>)\n",
      "epoch: 12681 loss is tensor([-0.4551], grad_fn=<AddBackward0>)\n",
      "epoch: 12682 loss is tensor([-0.4712], grad_fn=<AddBackward0>)\n",
      "epoch: 12683 loss is tensor([-0.4744], grad_fn=<AddBackward0>)\n",
      "epoch: 12684 loss is tensor([-0.5051], grad_fn=<AddBackward0>)\n",
      "epoch: 12685 loss is tensor([-0.4916], grad_fn=<AddBackward0>)\n",
      "epoch: 12686 loss is tensor([-0.5349], grad_fn=<AddBackward0>)\n",
      "epoch: 12687 loss is tensor([-0.4963], grad_fn=<AddBackward0>)\n",
      "epoch: 12688 loss is tensor([-0.4803], grad_fn=<AddBackward0>)\n",
      "epoch: 12689 loss is tensor([-0.5710], grad_fn=<AddBackward0>)\n",
      "epoch: 12690 loss is tensor([-0.5100], grad_fn=<AddBackward0>)\n",
      "epoch: 12691 loss is tensor([-0.5421], grad_fn=<AddBackward0>)\n",
      "epoch: 12692 loss is tensor([-0.4850], grad_fn=<AddBackward0>)\n",
      "epoch: 12693 loss is tensor([-0.5506], grad_fn=<AddBackward0>)\n",
      "epoch: 12694 loss is tensor([-0.5432], grad_fn=<AddBackward0>)\n",
      "epoch: 12695 loss is tensor([-0.5171], grad_fn=<AddBackward0>)\n",
      "epoch: 12696 loss is tensor([-0.4403], grad_fn=<AddBackward0>)\n",
      "epoch: 12697 loss is tensor([-0.4102], grad_fn=<AddBackward0>)\n",
      "epoch: 12698 loss is tensor([-0.4746], grad_fn=<AddBackward0>)\n",
      "epoch: 12699 loss is tensor([-0.3261], grad_fn=<AddBackward0>)\n",
      "epoch: 12700 loss is tensor([-0.3487], grad_fn=<AddBackward0>)\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12701 loss is tensor([-0.4329], grad_fn=<AddBackward0>)\n",
      "epoch: 12702 loss is tensor([-0.3814], grad_fn=<AddBackward0>)\n",
      "epoch: 12703 loss is tensor([-0.3770], grad_fn=<AddBackward0>)\n",
      "epoch: 12704 loss is tensor([-0.4552], grad_fn=<AddBackward0>)\n",
      "epoch: 12705 loss is tensor([-0.4335], grad_fn=<AddBackward0>)\n",
      "epoch: 12706 loss is tensor([-0.4317], grad_fn=<AddBackward0>)\n",
      "epoch: 12707 loss is tensor([-0.4376], grad_fn=<AddBackward0>)\n",
      "epoch: 12708 loss is tensor([-0.4678], grad_fn=<AddBackward0>)\n",
      "epoch: 12709 loss is tensor([-0.4490], grad_fn=<AddBackward0>)\n",
      "epoch: 12710 loss is tensor([-0.5281], grad_fn=<AddBackward0>)\n",
      "epoch: 12711 loss is tensor([-0.4932], grad_fn=<AddBackward0>)\n",
      "epoch: 12712 loss is tensor([-0.5022], grad_fn=<AddBackward0>)\n",
      "epoch: 12713 loss is tensor([-0.5635], grad_fn=<AddBackward0>)\n",
      "epoch: 12714 loss is tensor([-0.5303], grad_fn=<AddBackward0>)\n",
      "epoch: 12715 loss is tensor([-0.5110], grad_fn=<AddBackward0>)\n",
      "epoch: 12716 loss is tensor([-0.5211], grad_fn=<AddBackward0>)\n",
      "epoch: 12717 loss is tensor([-0.4838], grad_fn=<AddBackward0>)\n",
      "epoch: 12718 loss is tensor([-0.5051], grad_fn=<AddBackward0>)\n",
      "epoch: 12719 loss is tensor([-0.4890], grad_fn=<AddBackward0>)\n",
      "epoch: 12720 loss is tensor([-0.5414], grad_fn=<AddBackward0>)\n",
      "epoch: 12721 loss is tensor([-0.4702], grad_fn=<AddBackward0>)\n",
      "epoch: 12722 loss is tensor([-0.4811], grad_fn=<AddBackward0>)\n",
      "epoch: 12723 loss is tensor([-0.4949], grad_fn=<AddBackward0>)\n",
      "epoch: 12724 loss is tensor([-0.4670], grad_fn=<AddBackward0>)\n",
      "epoch: 12725 loss is tensor([-0.4608], grad_fn=<AddBackward0>)\n",
      "epoch: 12726 loss is tensor([-0.5240], grad_fn=<AddBackward0>)\n",
      "epoch: 12727 loss is tensor([-0.5254], grad_fn=<AddBackward0>)\n",
      "epoch: 12728 loss is tensor([-0.5067], grad_fn=<AddBackward0>)\n",
      "epoch: 12729 loss is tensor([-0.5069], grad_fn=<AddBackward0>)\n",
      "epoch: 12730 loss is tensor([-0.4772], grad_fn=<AddBackward0>)\n",
      "epoch: 12731 loss is tensor([-0.4311], grad_fn=<AddBackward0>)\n",
      "epoch: 12732 loss is tensor([-0.5278], grad_fn=<AddBackward0>)\n",
      "epoch: 12733 loss is tensor([-0.5080], grad_fn=<AddBackward0>)\n",
      "epoch: 12734 loss is tensor([-0.5126], grad_fn=<AddBackward0>)\n",
      "epoch: 12735 loss is tensor([-0.5408], grad_fn=<AddBackward0>)\n",
      "epoch: 12736 loss is tensor([-0.4872], grad_fn=<AddBackward0>)\n",
      "epoch: 12737 loss is tensor([-0.5411], grad_fn=<AddBackward0>)\n",
      "epoch: 12738 loss is tensor([-0.4494], grad_fn=<AddBackward0>)\n",
      "epoch: 12739 loss is tensor([-0.5140], grad_fn=<AddBackward0>)\n",
      "epoch: 12740 loss is tensor([-0.5445], grad_fn=<AddBackward0>)\n",
      "epoch: 12741 loss is tensor([-0.4706], grad_fn=<AddBackward0>)\n",
      "epoch: 12742 loss is tensor([-0.4961], grad_fn=<AddBackward0>)\n",
      "epoch: 12743 loss is tensor([-0.4911], grad_fn=<AddBackward0>)\n",
      "epoch: 12744 loss is tensor([-0.5818], grad_fn=<AddBackward0>)\n",
      "epoch: 12745 loss is tensor([-0.5507], grad_fn=<AddBackward0>)\n",
      "epoch: 12746 loss is tensor([-0.5428], grad_fn=<AddBackward0>)\n",
      "epoch: 12747 loss is tensor([-0.5862], grad_fn=<AddBackward0>)\n",
      "epoch: 12748 loss is tensor([-0.5617], grad_fn=<AddBackward0>)\n",
      "epoch: 12749 loss is tensor([-0.5258], grad_fn=<AddBackward0>)\n",
      "epoch: 12750 loss is tensor([-0.5692], grad_fn=<AddBackward0>)\n",
      "epoch: 12751 loss is tensor([-0.5554], grad_fn=<AddBackward0>)\n",
      "epoch: 12752 loss is tensor([-0.5791], grad_fn=<AddBackward0>)\n",
      "epoch: 12753 loss is tensor([-0.5585], grad_fn=<AddBackward0>)\n",
      "epoch: 12754 loss is tensor([-0.5318], grad_fn=<AddBackward0>)\n",
      "epoch: 12755 loss is tensor([-0.5768], grad_fn=<AddBackward0>)\n",
      "epoch: 12756 loss is tensor([-0.5954], grad_fn=<AddBackward0>)\n",
      "epoch: 12757 loss is tensor([-0.5759], grad_fn=<AddBackward0>)\n",
      "epoch: 12758 loss is tensor([-0.5849], grad_fn=<AddBackward0>)\n",
      "epoch: 12759 loss is tensor([-0.5461], grad_fn=<AddBackward0>)\n",
      "epoch: 12760 loss is tensor([-0.5024], grad_fn=<AddBackward0>)\n",
      "epoch: 12761 loss is tensor([-0.5442], grad_fn=<AddBackward0>)\n",
      "epoch: 12762 loss is tensor([-0.5418], grad_fn=<AddBackward0>)\n",
      "epoch: 12763 loss is tensor([-0.5443], grad_fn=<AddBackward0>)\n",
      "epoch: 12764 loss is tensor([-0.4860], grad_fn=<AddBackward0>)\n",
      "epoch: 12765 loss is tensor([-0.5528], grad_fn=<AddBackward0>)\n",
      "epoch: 12766 loss is tensor([-0.5517], grad_fn=<AddBackward0>)\n",
      "epoch: 12767 loss is tensor([-0.5195], grad_fn=<AddBackward0>)\n",
      "epoch: 12768 loss is tensor([-0.5216], grad_fn=<AddBackward0>)\n",
      "epoch: 12769 loss is tensor([-0.4686], grad_fn=<AddBackward0>)\n",
      "epoch: 12770 loss is tensor([-0.5517], grad_fn=<AddBackward0>)\n",
      "epoch: 12771 loss is tensor([-0.5438], grad_fn=<AddBackward0>)\n",
      "epoch: 12772 loss is tensor([-0.6229], grad_fn=<AddBackward0>)\n",
      "epoch: 12773 loss is tensor([-0.5323], grad_fn=<AddBackward0>)\n",
      "epoch: 12774 loss is tensor([-0.5374], grad_fn=<AddBackward0>)\n",
      "epoch: 12775 loss is tensor([-0.5780], grad_fn=<AddBackward0>)\n",
      "epoch: 12776 loss is tensor([-0.5329], grad_fn=<AddBackward0>)\n",
      "epoch: 12777 loss is tensor([-0.5657], grad_fn=<AddBackward0>)\n",
      "epoch: 12778 loss is tensor([-0.5107], grad_fn=<AddBackward0>)\n",
      "epoch: 12779 loss is tensor([-0.5534], grad_fn=<AddBackward0>)\n",
      "epoch: 12780 loss is tensor([-0.5176], grad_fn=<AddBackward0>)\n",
      "epoch: 12781 loss is tensor([-0.5378], grad_fn=<AddBackward0>)\n",
      "epoch: 12782 loss is tensor([-0.5006], grad_fn=<AddBackward0>)\n",
      "epoch: 12783 loss is tensor([-0.4999], grad_fn=<AddBackward0>)\n",
      "epoch: 12784 loss is tensor([-0.5777], grad_fn=<AddBackward0>)\n",
      "epoch: 12785 loss is tensor([-0.5564], grad_fn=<AddBackward0>)\n",
      "epoch: 12786 loss is tensor([-0.5745], grad_fn=<AddBackward0>)\n",
      "epoch: 12787 loss is tensor([-0.5223], grad_fn=<AddBackward0>)\n",
      "epoch: 12788 loss is tensor([-0.5520], grad_fn=<AddBackward0>)\n",
      "epoch: 12789 loss is tensor([-0.5166], grad_fn=<AddBackward0>)\n",
      "epoch: 12790 loss is tensor([-0.6044], grad_fn=<AddBackward0>)\n",
      "epoch: 12791 loss is tensor([-0.5899], grad_fn=<AddBackward0>)\n",
      "epoch: 12792 loss is tensor([-0.5940], grad_fn=<AddBackward0>)\n",
      "epoch: 12793 loss is tensor([-0.5467], grad_fn=<AddBackward0>)\n",
      "epoch: 12794 loss is tensor([-0.5649], grad_fn=<AddBackward0>)\n",
      "epoch: 12795 loss is tensor([-0.5644], grad_fn=<AddBackward0>)\n",
      "epoch: 12796 loss is tensor([-0.5166], grad_fn=<AddBackward0>)\n",
      "epoch: 12797 loss is tensor([-0.5614], grad_fn=<AddBackward0>)\n",
      "epoch: 12798 loss is tensor([-0.5506], grad_fn=<AddBackward0>)\n",
      "epoch: 12799 loss is tensor([-0.5049], grad_fn=<AddBackward0>)\n",
      "epoch: 12800 loss is tensor([-0.4934], grad_fn=<AddBackward0>)\n",
      "37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12801 loss is tensor([-0.5227], grad_fn=<AddBackward0>)\n",
      "epoch: 12802 loss is tensor([-0.4735], grad_fn=<AddBackward0>)\n",
      "epoch: 12803 loss is tensor([-0.3765], grad_fn=<AddBackward0>)\n",
      "epoch: 12804 loss is tensor([-0.4294], grad_fn=<AddBackward0>)\n",
      "epoch: 12805 loss is tensor([-0.5025], grad_fn=<AddBackward0>)\n",
      "epoch: 12806 loss is tensor([-0.4074], grad_fn=<AddBackward0>)\n",
      "epoch: 12807 loss is tensor([-0.4482], grad_fn=<AddBackward0>)\n",
      "epoch: 12808 loss is tensor([-0.4697], grad_fn=<AddBackward0>)\n",
      "epoch: 12809 loss is tensor([-0.3819], grad_fn=<AddBackward0>)\n",
      "epoch: 12810 loss is tensor([-0.4966], grad_fn=<AddBackward0>)\n",
      "epoch: 12811 loss is tensor([-0.4961], grad_fn=<AddBackward0>)\n",
      "epoch: 12812 loss is tensor([-0.4842], grad_fn=<AddBackward0>)\n",
      "epoch: 12813 loss is tensor([-0.4313], grad_fn=<AddBackward0>)\n",
      "epoch: 12814 loss is tensor([-0.4188], grad_fn=<AddBackward0>)\n",
      "epoch: 12815 loss is tensor([-0.4679], grad_fn=<AddBackward0>)\n",
      "epoch: 12816 loss is tensor([-0.4515], grad_fn=<AddBackward0>)\n",
      "epoch: 12817 loss is tensor([-0.4958], grad_fn=<AddBackward0>)\n",
      "epoch: 12818 loss is tensor([-0.5061], grad_fn=<AddBackward0>)\n",
      "epoch: 12819 loss is tensor([-0.5184], grad_fn=<AddBackward0>)\n",
      "epoch: 12820 loss is tensor([-0.5248], grad_fn=<AddBackward0>)\n",
      "epoch: 12821 loss is tensor([-0.5455], grad_fn=<AddBackward0>)\n",
      "epoch: 12822 loss is tensor([-0.5102], grad_fn=<AddBackward0>)\n",
      "epoch: 12823 loss is tensor([-0.5181], grad_fn=<AddBackward0>)\n",
      "epoch: 12824 loss is tensor([-0.4480], grad_fn=<AddBackward0>)\n",
      "epoch: 12825 loss is tensor([-0.5294], grad_fn=<AddBackward0>)\n",
      "epoch: 12826 loss is tensor([-0.4941], grad_fn=<AddBackward0>)\n",
      "epoch: 12827 loss is tensor([-0.5033], grad_fn=<AddBackward0>)\n",
      "epoch: 12828 loss is tensor([-0.5146], grad_fn=<AddBackward0>)\n",
      "epoch: 12829 loss is tensor([-0.5353], grad_fn=<AddBackward0>)\n",
      "epoch: 12830 loss is tensor([-0.5451], grad_fn=<AddBackward0>)\n",
      "epoch: 12831 loss is tensor([-0.4897], grad_fn=<AddBackward0>)\n",
      "epoch: 12832 loss is tensor([-0.5291], grad_fn=<AddBackward0>)\n",
      "epoch: 12833 loss is tensor([-0.4377], grad_fn=<AddBackward0>)\n",
      "epoch: 12834 loss is tensor([-0.4237], grad_fn=<AddBackward0>)\n",
      "epoch: 12835 loss is tensor([-0.4858], grad_fn=<AddBackward0>)\n",
      "epoch: 12836 loss is tensor([-0.5183], grad_fn=<AddBackward0>)\n",
      "epoch: 12837 loss is tensor([-0.5669], grad_fn=<AddBackward0>)\n",
      "epoch: 12838 loss is tensor([-0.4999], grad_fn=<AddBackward0>)\n",
      "epoch: 12839 loss is tensor([-0.5200], grad_fn=<AddBackward0>)\n",
      "epoch: 12840 loss is tensor([-0.5395], grad_fn=<AddBackward0>)\n",
      "epoch: 12841 loss is tensor([-0.5144], grad_fn=<AddBackward0>)\n",
      "epoch: 12842 loss is tensor([-0.5243], grad_fn=<AddBackward0>)\n",
      "epoch: 12843 loss is tensor([-0.5082], grad_fn=<AddBackward0>)\n",
      "epoch: 12844 loss is tensor([-0.5055], grad_fn=<AddBackward0>)\n",
      "epoch: 12845 loss is tensor([-0.4305], grad_fn=<AddBackward0>)\n",
      "epoch: 12846 loss is tensor([-0.3334], grad_fn=<AddBackward0>)\n",
      "epoch: 12847 loss is tensor([-0.4898], grad_fn=<AddBackward0>)\n",
      "epoch: 12848 loss is tensor([-0.3370], grad_fn=<AddBackward0>)\n",
      "epoch: 12849 loss is tensor([-0.3505], grad_fn=<AddBackward0>)\n",
      "epoch: 12850 loss is tensor([-0.4823], grad_fn=<AddBackward0>)\n",
      "epoch: 12851 loss is tensor([-0.4652], grad_fn=<AddBackward0>)\n",
      "epoch: 12852 loss is tensor([-0.4514], grad_fn=<AddBackward0>)\n",
      "epoch: 12853 loss is tensor([-0.4624], grad_fn=<AddBackward0>)\n",
      "epoch: 12854 loss is tensor([-0.4563], grad_fn=<AddBackward0>)\n",
      "epoch: 12855 loss is tensor([-0.5031], grad_fn=<AddBackward0>)\n",
      "epoch: 12856 loss is tensor([-0.4610], grad_fn=<AddBackward0>)\n",
      "epoch: 12857 loss is tensor([-0.4542], grad_fn=<AddBackward0>)\n",
      "epoch: 12858 loss is tensor([-0.4668], grad_fn=<AddBackward0>)\n",
      "epoch: 12859 loss is tensor([-0.5184], grad_fn=<AddBackward0>)\n",
      "epoch: 12860 loss is tensor([-0.5158], grad_fn=<AddBackward0>)\n",
      "epoch: 12861 loss is tensor([-0.4191], grad_fn=<AddBackward0>)\n",
      "epoch: 12862 loss is tensor([-0.4951], grad_fn=<AddBackward0>)\n",
      "epoch: 12863 loss is tensor([-0.4903], grad_fn=<AddBackward0>)\n",
      "epoch: 12864 loss is tensor([-0.5366], grad_fn=<AddBackward0>)\n",
      "epoch: 12865 loss is tensor([-0.4662], grad_fn=<AddBackward0>)\n",
      "epoch: 12866 loss is tensor([-0.5526], grad_fn=<AddBackward0>)\n",
      "epoch: 12867 loss is tensor([-0.5270], grad_fn=<AddBackward0>)\n",
      "epoch: 12868 loss is tensor([-0.5164], grad_fn=<AddBackward0>)\n",
      "epoch: 12869 loss is tensor([-0.5152], grad_fn=<AddBackward0>)\n",
      "epoch: 12870 loss is tensor([-0.5394], grad_fn=<AddBackward0>)\n",
      "epoch: 12871 loss is tensor([-0.6130], grad_fn=<AddBackward0>)\n",
      "epoch: 12872 loss is tensor([-0.5395], grad_fn=<AddBackward0>)\n",
      "epoch: 12873 loss is tensor([-0.5404], grad_fn=<AddBackward0>)\n",
      "epoch: 12874 loss is tensor([-0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 12875 loss is tensor([-0.5735], grad_fn=<AddBackward0>)\n",
      "epoch: 12876 loss is tensor([-0.5445], grad_fn=<AddBackward0>)\n",
      "epoch: 12877 loss is tensor([-0.5397], grad_fn=<AddBackward0>)\n",
      "epoch: 12878 loss is tensor([-0.5805], grad_fn=<AddBackward0>)\n",
      "epoch: 12879 loss is tensor([-0.5139], grad_fn=<AddBackward0>)\n",
      "epoch: 12880 loss is tensor([-0.5487], grad_fn=<AddBackward0>)\n",
      "epoch: 12881 loss is tensor([-0.4752], grad_fn=<AddBackward0>)\n",
      "epoch: 12882 loss is tensor([-0.5122], grad_fn=<AddBackward0>)\n",
      "epoch: 12883 loss is tensor([-0.5099], grad_fn=<AddBackward0>)\n",
      "epoch: 12884 loss is tensor([-0.5219], grad_fn=<AddBackward0>)\n",
      "epoch: 12885 loss is tensor([-0.5022], grad_fn=<AddBackward0>)\n",
      "epoch: 12886 loss is tensor([-0.5469], grad_fn=<AddBackward0>)\n",
      "epoch: 12887 loss is tensor([-0.5537], grad_fn=<AddBackward0>)\n",
      "epoch: 12888 loss is tensor([-0.5202], grad_fn=<AddBackward0>)\n",
      "epoch: 12889 loss is tensor([-0.5698], grad_fn=<AddBackward0>)\n",
      "epoch: 12890 loss is tensor([-0.5013], grad_fn=<AddBackward0>)\n",
      "epoch: 12891 loss is tensor([-0.5522], grad_fn=<AddBackward0>)\n",
      "epoch: 12892 loss is tensor([-0.5173], grad_fn=<AddBackward0>)\n",
      "epoch: 12893 loss is tensor([-0.5177], grad_fn=<AddBackward0>)\n",
      "epoch: 12894 loss is tensor([-0.5373], grad_fn=<AddBackward0>)\n",
      "epoch: 12895 loss is tensor([-0.5450], grad_fn=<AddBackward0>)\n",
      "epoch: 12896 loss is tensor([-0.6087], grad_fn=<AddBackward0>)\n",
      "epoch: 12897 loss is tensor([-0.5489], grad_fn=<AddBackward0>)\n",
      "epoch: 12898 loss is tensor([-0.5879], grad_fn=<AddBackward0>)\n",
      "epoch: 12899 loss is tensor([-0.5802], grad_fn=<AddBackward0>)\n",
      "epoch: 12900 loss is tensor([-0.5817], grad_fn=<AddBackward0>)\n",
      "27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 12901 loss is tensor([-0.5659], grad_fn=<AddBackward0>)\n",
      "epoch: 12902 loss is tensor([-0.5471], grad_fn=<AddBackward0>)\n",
      "epoch: 12903 loss is tensor([-0.5521], grad_fn=<AddBackward0>)\n",
      "epoch: 12904 loss is tensor([-0.5428], grad_fn=<AddBackward0>)\n",
      "epoch: 12905 loss is tensor([-0.5154], grad_fn=<AddBackward0>)\n",
      "epoch: 12906 loss is tensor([-0.5235], grad_fn=<AddBackward0>)\n",
      "epoch: 12907 loss is tensor([-0.5198], grad_fn=<AddBackward0>)\n",
      "epoch: 12908 loss is tensor([-0.5481], grad_fn=<AddBackward0>)\n",
      "epoch: 12909 loss is tensor([-0.4939], grad_fn=<AddBackward0>)\n",
      "epoch: 12910 loss is tensor([-0.4944], grad_fn=<AddBackward0>)\n",
      "epoch: 12911 loss is tensor([-0.5959], grad_fn=<AddBackward0>)\n",
      "epoch: 12912 loss is tensor([-0.5636], grad_fn=<AddBackward0>)\n",
      "epoch: 12913 loss is tensor([-0.6111], grad_fn=<AddBackward0>)\n",
      "epoch: 12914 loss is tensor([-0.5536], grad_fn=<AddBackward0>)\n",
      "epoch: 12915 loss is tensor([-0.5903], grad_fn=<AddBackward0>)\n",
      "epoch: 12916 loss is tensor([-0.5032], grad_fn=<AddBackward0>)\n",
      "epoch: 12917 loss is tensor([-0.4975], grad_fn=<AddBackward0>)\n",
      "epoch: 12918 loss is tensor([-0.5808], grad_fn=<AddBackward0>)\n",
      "epoch: 12919 loss is tensor([-0.5897], grad_fn=<AddBackward0>)\n",
      "epoch: 12920 loss is tensor([-0.5551], grad_fn=<AddBackward0>)\n",
      "epoch: 12921 loss is tensor([-0.5963], grad_fn=<AddBackward0>)\n",
      "epoch: 12922 loss is tensor([-0.5008], grad_fn=<AddBackward0>)\n",
      "epoch: 12923 loss is tensor([-0.5625], grad_fn=<AddBackward0>)\n",
      "epoch: 12924 loss is tensor([-0.5992], grad_fn=<AddBackward0>)\n",
      "epoch: 12925 loss is tensor([-0.5222], grad_fn=<AddBackward0>)\n",
      "epoch: 12926 loss is tensor([-0.5187], grad_fn=<AddBackward0>)\n",
      "epoch: 12927 loss is tensor([-0.5858], grad_fn=<AddBackward0>)\n",
      "epoch: 12928 loss is tensor([-0.5548], grad_fn=<AddBackward0>)\n",
      "epoch: 12929 loss is tensor([-0.5489], grad_fn=<AddBackward0>)\n",
      "epoch: 12930 loss is tensor([-0.5480], grad_fn=<AddBackward0>)\n",
      "epoch: 12931 loss is tensor([-0.5217], grad_fn=<AddBackward0>)\n",
      "epoch: 12932 loss is tensor([-0.5968], grad_fn=<AddBackward0>)\n",
      "epoch: 12933 loss is tensor([-0.4992], grad_fn=<AddBackward0>)\n",
      "epoch: 12934 loss is tensor([-0.5289], grad_fn=<AddBackward0>)\n",
      "epoch: 12935 loss is tensor([-0.6046], grad_fn=<AddBackward0>)\n",
      "epoch: 12936 loss is tensor([-0.5494], grad_fn=<AddBackward0>)\n",
      "epoch: 12937 loss is tensor([-0.5936], grad_fn=<AddBackward0>)\n",
      "epoch: 12938 loss is tensor([-0.5570], grad_fn=<AddBackward0>)\n",
      "epoch: 12939 loss is tensor([-0.5474], grad_fn=<AddBackward0>)\n",
      "epoch: 12940 loss is tensor([-0.5531], grad_fn=<AddBackward0>)\n",
      "epoch: 12941 loss is tensor([-0.5425], grad_fn=<AddBackward0>)\n",
      "epoch: 12942 loss is tensor([-0.5588], grad_fn=<AddBackward0>)\n",
      "epoch: 12943 loss is tensor([-0.5618], grad_fn=<AddBackward0>)\n",
      "epoch: 12944 loss is tensor([-0.5476], grad_fn=<AddBackward0>)\n",
      "epoch: 12945 loss is tensor([-0.5713], grad_fn=<AddBackward0>)\n",
      "epoch: 12946 loss is tensor([-0.4741], grad_fn=<AddBackward0>)\n",
      "epoch: 12947 loss is tensor([-0.5315], grad_fn=<AddBackward0>)\n",
      "epoch: 12948 loss is tensor([-0.5451], grad_fn=<AddBackward0>)\n",
      "epoch: 12949 loss is tensor([-0.5042], grad_fn=<AddBackward0>)\n",
      "epoch: 12950 loss is tensor([-0.6130], grad_fn=<AddBackward0>)\n",
      "epoch: 12951 loss is tensor([-0.5253], grad_fn=<AddBackward0>)\n",
      "epoch: 12952 loss is tensor([-0.5274], grad_fn=<AddBackward0>)\n",
      "epoch: 12953 loss is tensor([-0.5723], grad_fn=<AddBackward0>)\n",
      "epoch: 12954 loss is tensor([-0.5615], grad_fn=<AddBackward0>)\n",
      "epoch: 12955 loss is tensor([-0.5740], grad_fn=<AddBackward0>)\n",
      "epoch: 12956 loss is tensor([-0.5107], grad_fn=<AddBackward0>)\n",
      "epoch: 12957 loss is tensor([-0.5429], grad_fn=<AddBackward0>)\n",
      "epoch: 12958 loss is tensor([-0.5643], grad_fn=<AddBackward0>)\n",
      "epoch: 12959 loss is tensor([-0.5689], grad_fn=<AddBackward0>)\n",
      "epoch: 12960 loss is tensor([-0.5459], grad_fn=<AddBackward0>)\n",
      "epoch: 12961 loss is tensor([-0.5574], grad_fn=<AddBackward0>)\n",
      "epoch: 12962 loss is tensor([-0.5671], grad_fn=<AddBackward0>)\n",
      "epoch: 12963 loss is tensor([-0.5483], grad_fn=<AddBackward0>)\n",
      "epoch: 12964 loss is tensor([-0.5509], grad_fn=<AddBackward0>)\n",
      "epoch: 12965 loss is tensor([-0.5427], grad_fn=<AddBackward0>)\n",
      "epoch: 12966 loss is tensor([-0.5554], grad_fn=<AddBackward0>)\n",
      "epoch: 12967 loss is tensor([-0.5869], grad_fn=<AddBackward0>)\n",
      "epoch: 12968 loss is tensor([-0.5365], grad_fn=<AddBackward0>)\n",
      "epoch: 12969 loss is tensor([-0.5444], grad_fn=<AddBackward0>)\n",
      "epoch: 12970 loss is tensor([-0.5772], grad_fn=<AddBackward0>)\n",
      "epoch: 12971 loss is tensor([-0.5408], grad_fn=<AddBackward0>)\n",
      "epoch: 12972 loss is tensor([-0.5658], grad_fn=<AddBackward0>)\n",
      "epoch: 12973 loss is tensor([-0.5256], grad_fn=<AddBackward0>)\n",
      "epoch: 12974 loss is tensor([-0.5539], grad_fn=<AddBackward0>)\n",
      "epoch: 12975 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 12976 loss is tensor([-0.5687], grad_fn=<AddBackward0>)\n",
      "epoch: 12977 loss is tensor([-0.5271], grad_fn=<AddBackward0>)\n",
      "epoch: 12978 loss is tensor([-0.5580], grad_fn=<AddBackward0>)\n",
      "epoch: 12979 loss is tensor([-0.5296], grad_fn=<AddBackward0>)\n",
      "epoch: 12980 loss is tensor([-0.5463], grad_fn=<AddBackward0>)\n",
      "epoch: 12981 loss is tensor([-0.4845], grad_fn=<AddBackward0>)\n",
      "epoch: 12982 loss is tensor([-0.5375], grad_fn=<AddBackward0>)\n",
      "epoch: 12983 loss is tensor([-0.5817], grad_fn=<AddBackward0>)\n",
      "epoch: 12984 loss is tensor([-0.5934], grad_fn=<AddBackward0>)\n",
      "epoch: 12985 loss is tensor([-0.5503], grad_fn=<AddBackward0>)\n",
      "epoch: 12986 loss is tensor([-0.5297], grad_fn=<AddBackward0>)\n",
      "epoch: 12987 loss is tensor([-0.5303], grad_fn=<AddBackward0>)\n",
      "epoch: 12988 loss is tensor([-0.4972], grad_fn=<AddBackward0>)\n",
      "epoch: 12989 loss is tensor([-0.6166], grad_fn=<AddBackward0>)\n",
      "epoch: 12990 loss is tensor([-0.5793], grad_fn=<AddBackward0>)\n",
      "epoch: 12991 loss is tensor([-0.5681], grad_fn=<AddBackward0>)\n",
      "epoch: 12992 loss is tensor([-0.5669], grad_fn=<AddBackward0>)\n",
      "epoch: 12993 loss is tensor([-0.5883], grad_fn=<AddBackward0>)\n",
      "epoch: 12994 loss is tensor([-0.5554], grad_fn=<AddBackward0>)\n",
      "epoch: 12995 loss is tensor([-0.5937], grad_fn=<AddBackward0>)\n",
      "epoch: 12996 loss is tensor([-0.5603], grad_fn=<AddBackward0>)\n",
      "epoch: 12997 loss is tensor([-0.6156], grad_fn=<AddBackward0>)\n",
      "epoch: 12998 loss is tensor([-0.5944], grad_fn=<AddBackward0>)\n",
      "epoch: 12999 loss is tensor([-0.5771], grad_fn=<AddBackward0>)\n",
      "epoch: 13000 loss is tensor([-0.5729], grad_fn=<AddBackward0>)\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13001 loss is tensor([-0.5279], grad_fn=<AddBackward0>)\n",
      "epoch: 13002 loss is tensor([-0.5788], grad_fn=<AddBackward0>)\n",
      "epoch: 13003 loss is tensor([-0.6216], grad_fn=<AddBackward0>)\n",
      "epoch: 13004 loss is tensor([-0.6072], grad_fn=<AddBackward0>)\n",
      "epoch: 13005 loss is tensor([-0.6037], grad_fn=<AddBackward0>)\n",
      "epoch: 13006 loss is tensor([-0.5547], grad_fn=<AddBackward0>)\n",
      "epoch: 13007 loss is tensor([-0.5713], grad_fn=<AddBackward0>)\n",
      "epoch: 13008 loss is tensor([-0.5481], grad_fn=<AddBackward0>)\n",
      "epoch: 13009 loss is tensor([-0.5692], grad_fn=<AddBackward0>)\n",
      "epoch: 13010 loss is tensor([-0.6014], grad_fn=<AddBackward0>)\n",
      "epoch: 13011 loss is tensor([-0.5431], grad_fn=<AddBackward0>)\n",
      "epoch: 13012 loss is tensor([-0.5569], grad_fn=<AddBackward0>)\n",
      "epoch: 13013 loss is tensor([-0.5835], grad_fn=<AddBackward0>)\n",
      "epoch: 13014 loss is tensor([-0.5862], grad_fn=<AddBackward0>)\n",
      "epoch: 13015 loss is tensor([-0.6157], grad_fn=<AddBackward0>)\n",
      "epoch: 13016 loss is tensor([-0.5876], grad_fn=<AddBackward0>)\n",
      "epoch: 13017 loss is tensor([-0.5010], grad_fn=<AddBackward0>)\n",
      "epoch: 13018 loss is tensor([-0.5690], grad_fn=<AddBackward0>)\n",
      "epoch: 13019 loss is tensor([-0.5633], grad_fn=<AddBackward0>)\n",
      "epoch: 13020 loss is tensor([-0.5588], grad_fn=<AddBackward0>)\n",
      "epoch: 13021 loss is tensor([-0.5329], grad_fn=<AddBackward0>)\n",
      "epoch: 13022 loss is tensor([-0.5968], grad_fn=<AddBackward0>)\n",
      "epoch: 13023 loss is tensor([-0.5938], grad_fn=<AddBackward0>)\n",
      "epoch: 13024 loss is tensor([-0.5566], grad_fn=<AddBackward0>)\n",
      "epoch: 13025 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 13026 loss is tensor([-0.5638], grad_fn=<AddBackward0>)\n",
      "epoch: 13027 loss is tensor([-0.5556], grad_fn=<AddBackward0>)\n",
      "epoch: 13028 loss is tensor([-0.4447], grad_fn=<AddBackward0>)\n",
      "epoch: 13029 loss is tensor([-0.5305], grad_fn=<AddBackward0>)\n",
      "epoch: 13030 loss is tensor([-0.5230], grad_fn=<AddBackward0>)\n",
      "epoch: 13031 loss is tensor([-0.5475], grad_fn=<AddBackward0>)\n",
      "epoch: 13032 loss is tensor([-0.5503], grad_fn=<AddBackward0>)\n",
      "epoch: 13033 loss is tensor([-0.5400], grad_fn=<AddBackward0>)\n",
      "epoch: 13034 loss is tensor([-0.5223], grad_fn=<AddBackward0>)\n",
      "epoch: 13035 loss is tensor([-0.5166], grad_fn=<AddBackward0>)\n",
      "epoch: 13036 loss is tensor([-0.5438], grad_fn=<AddBackward0>)\n",
      "epoch: 13037 loss is tensor([-0.4704], grad_fn=<AddBackward0>)\n",
      "epoch: 13038 loss is tensor([-0.4870], grad_fn=<AddBackward0>)\n",
      "epoch: 13039 loss is tensor([-0.4849], grad_fn=<AddBackward0>)\n",
      "epoch: 13040 loss is tensor([-0.5250], grad_fn=<AddBackward0>)\n",
      "epoch: 13041 loss is tensor([-0.5372], grad_fn=<AddBackward0>)\n",
      "epoch: 13042 loss is tensor([-0.5484], grad_fn=<AddBackward0>)\n",
      "epoch: 13043 loss is tensor([-0.5066], grad_fn=<AddBackward0>)\n",
      "epoch: 13044 loss is tensor([-0.4460], grad_fn=<AddBackward0>)\n",
      "epoch: 13045 loss is tensor([-0.4512], grad_fn=<AddBackward0>)\n",
      "epoch: 13046 loss is tensor([-0.4897], grad_fn=<AddBackward0>)\n",
      "epoch: 13047 loss is tensor([-0.4882], grad_fn=<AddBackward0>)\n",
      "epoch: 13048 loss is tensor([-0.5062], grad_fn=<AddBackward0>)\n",
      "epoch: 13049 loss is tensor([-0.4850], grad_fn=<AddBackward0>)\n",
      "epoch: 13050 loss is tensor([-0.4480], grad_fn=<AddBackward0>)\n",
      "epoch: 13051 loss is tensor([-0.5112], grad_fn=<AddBackward0>)\n",
      "epoch: 13052 loss is tensor([-0.5517], grad_fn=<AddBackward0>)\n",
      "epoch: 13053 loss is tensor([-0.4896], grad_fn=<AddBackward0>)\n",
      "epoch: 13054 loss is tensor([-0.5990], grad_fn=<AddBackward0>)\n",
      "epoch: 13055 loss is tensor([-0.5281], grad_fn=<AddBackward0>)\n",
      "epoch: 13056 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 13057 loss is tensor([-0.5181], grad_fn=<AddBackward0>)\n",
      "epoch: 13058 loss is tensor([-0.5545], grad_fn=<AddBackward0>)\n",
      "epoch: 13059 loss is tensor([-0.6142], grad_fn=<AddBackward0>)\n",
      "epoch: 13060 loss is tensor([-0.5471], grad_fn=<AddBackward0>)\n",
      "epoch: 13061 loss is tensor([-0.5609], grad_fn=<AddBackward0>)\n",
      "epoch: 13062 loss is tensor([-0.5708], grad_fn=<AddBackward0>)\n",
      "epoch: 13063 loss is tensor([-0.5835], grad_fn=<AddBackward0>)\n",
      "epoch: 13064 loss is tensor([-0.5465], grad_fn=<AddBackward0>)\n",
      "epoch: 13065 loss is tensor([-0.5872], grad_fn=<AddBackward0>)\n",
      "epoch: 13066 loss is tensor([-0.5027], grad_fn=<AddBackward0>)\n",
      "epoch: 13067 loss is tensor([-0.5582], grad_fn=<AddBackward0>)\n",
      "epoch: 13068 loss is tensor([-0.5638], grad_fn=<AddBackward0>)\n",
      "epoch: 13069 loss is tensor([-0.5524], grad_fn=<AddBackward0>)\n",
      "epoch: 13070 loss is tensor([-0.5642], grad_fn=<AddBackward0>)\n",
      "epoch: 13071 loss is tensor([-0.5245], grad_fn=<AddBackward0>)\n",
      "epoch: 13072 loss is tensor([-0.5824], grad_fn=<AddBackward0>)\n",
      "epoch: 13073 loss is tensor([-0.5576], grad_fn=<AddBackward0>)\n",
      "epoch: 13074 loss is tensor([-0.5601], grad_fn=<AddBackward0>)\n",
      "epoch: 13075 loss is tensor([-0.5116], grad_fn=<AddBackward0>)\n",
      "epoch: 13076 loss is tensor([-0.5696], grad_fn=<AddBackward0>)\n",
      "epoch: 13077 loss is tensor([-0.5127], grad_fn=<AddBackward0>)\n",
      "epoch: 13078 loss is tensor([-0.5224], grad_fn=<AddBackward0>)\n",
      "epoch: 13079 loss is tensor([-0.5527], grad_fn=<AddBackward0>)\n",
      "epoch: 13080 loss is tensor([-0.5290], grad_fn=<AddBackward0>)\n",
      "epoch: 13081 loss is tensor([-0.4660], grad_fn=<AddBackward0>)\n",
      "epoch: 13082 loss is tensor([-0.5762], grad_fn=<AddBackward0>)\n",
      "epoch: 13083 loss is tensor([-0.5428], grad_fn=<AddBackward0>)\n",
      "epoch: 13084 loss is tensor([-0.5385], grad_fn=<AddBackward0>)\n",
      "epoch: 13085 loss is tensor([-0.5548], grad_fn=<AddBackward0>)\n",
      "epoch: 13086 loss is tensor([-0.4537], grad_fn=<AddBackward0>)\n",
      "epoch: 13087 loss is tensor([-0.5530], grad_fn=<AddBackward0>)\n",
      "epoch: 13088 loss is tensor([-0.5081], grad_fn=<AddBackward0>)\n",
      "epoch: 13089 loss is tensor([-0.5327], grad_fn=<AddBackward0>)\n",
      "epoch: 13090 loss is tensor([-0.5507], grad_fn=<AddBackward0>)\n",
      "epoch: 13091 loss is tensor([-0.4526], grad_fn=<AddBackward0>)\n",
      "epoch: 13092 loss is tensor([-0.5370], grad_fn=<AddBackward0>)\n",
      "epoch: 13093 loss is tensor([-0.5242], grad_fn=<AddBackward0>)\n",
      "epoch: 13094 loss is tensor([-0.4345], grad_fn=<AddBackward0>)\n",
      "epoch: 13095 loss is tensor([-0.4245], grad_fn=<AddBackward0>)\n",
      "epoch: 13096 loss is tensor([-0.4366], grad_fn=<AddBackward0>)\n",
      "epoch: 13097 loss is tensor([-0.4614], grad_fn=<AddBackward0>)\n",
      "epoch: 13098 loss is tensor([-0.5223], grad_fn=<AddBackward0>)\n",
      "epoch: 13099 loss is tensor([-0.4680], grad_fn=<AddBackward0>)\n",
      "epoch: 13100 loss is tensor([-0.4614], grad_fn=<AddBackward0>)\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13101 loss is tensor([-0.5091], grad_fn=<AddBackward0>)\n",
      "epoch: 13102 loss is tensor([-0.4873], grad_fn=<AddBackward0>)\n",
      "epoch: 13103 loss is tensor([-0.5058], grad_fn=<AddBackward0>)\n",
      "epoch: 13104 loss is tensor([-0.5158], grad_fn=<AddBackward0>)\n",
      "epoch: 13105 loss is tensor([-0.5125], grad_fn=<AddBackward0>)\n",
      "epoch: 13106 loss is tensor([-0.4684], grad_fn=<AddBackward0>)\n",
      "epoch: 13107 loss is tensor([-0.4762], grad_fn=<AddBackward0>)\n",
      "epoch: 13108 loss is tensor([-0.5519], grad_fn=<AddBackward0>)\n",
      "epoch: 13109 loss is tensor([-0.5134], grad_fn=<AddBackward0>)\n",
      "epoch: 13110 loss is tensor([-0.5593], grad_fn=<AddBackward0>)\n",
      "epoch: 13111 loss is tensor([-0.5291], grad_fn=<AddBackward0>)\n",
      "epoch: 13112 loss is tensor([-0.5593], grad_fn=<AddBackward0>)\n",
      "epoch: 13113 loss is tensor([-0.5465], grad_fn=<AddBackward0>)\n",
      "epoch: 13114 loss is tensor([-0.5447], grad_fn=<AddBackward0>)\n",
      "epoch: 13115 loss is tensor([-0.5151], grad_fn=<AddBackward0>)\n",
      "epoch: 13116 loss is tensor([-0.5417], grad_fn=<AddBackward0>)\n",
      "epoch: 13117 loss is tensor([-0.5081], grad_fn=<AddBackward0>)\n",
      "epoch: 13118 loss is tensor([-0.5461], grad_fn=<AddBackward0>)\n",
      "epoch: 13119 loss is tensor([-0.5225], grad_fn=<AddBackward0>)\n",
      "epoch: 13120 loss is tensor([-0.5001], grad_fn=<AddBackward0>)\n",
      "epoch: 13121 loss is tensor([-0.5686], grad_fn=<AddBackward0>)\n",
      "epoch: 13122 loss is tensor([-0.5821], grad_fn=<AddBackward0>)\n",
      "epoch: 13123 loss is tensor([-0.5535], grad_fn=<AddBackward0>)\n",
      "epoch: 13124 loss is tensor([-0.5522], grad_fn=<AddBackward0>)\n",
      "epoch: 13125 loss is tensor([-0.5346], grad_fn=<AddBackward0>)\n",
      "epoch: 13126 loss is tensor([-0.5665], grad_fn=<AddBackward0>)\n",
      "epoch: 13127 loss is tensor([-0.5565], grad_fn=<AddBackward0>)\n",
      "epoch: 13128 loss is tensor([-0.5151], grad_fn=<AddBackward0>)\n",
      "epoch: 13129 loss is tensor([-0.5267], grad_fn=<AddBackward0>)\n",
      "epoch: 13130 loss is tensor([-0.5155], grad_fn=<AddBackward0>)\n",
      "epoch: 13131 loss is tensor([-0.5372], grad_fn=<AddBackward0>)\n",
      "epoch: 13132 loss is tensor([-0.4776], grad_fn=<AddBackward0>)\n",
      "epoch: 13133 loss is tensor([-0.5337], grad_fn=<AddBackward0>)\n",
      "epoch: 13134 loss is tensor([-0.5004], grad_fn=<AddBackward0>)\n",
      "epoch: 13135 loss is tensor([-0.5237], grad_fn=<AddBackward0>)\n",
      "epoch: 13136 loss is tensor([-0.5660], grad_fn=<AddBackward0>)\n",
      "epoch: 13137 loss is tensor([-0.5957], grad_fn=<AddBackward0>)\n",
      "epoch: 13138 loss is tensor([-0.5953], grad_fn=<AddBackward0>)\n",
      "epoch: 13139 loss is tensor([-0.5591], grad_fn=<AddBackward0>)\n",
      "epoch: 13140 loss is tensor([-0.5754], grad_fn=<AddBackward0>)\n",
      "epoch: 13141 loss is tensor([-0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 13142 loss is tensor([-0.4866], grad_fn=<AddBackward0>)\n",
      "epoch: 13143 loss is tensor([-0.5821], grad_fn=<AddBackward0>)\n",
      "epoch: 13144 loss is tensor([-0.5196], grad_fn=<AddBackward0>)\n",
      "epoch: 13145 loss is tensor([-0.5699], grad_fn=<AddBackward0>)\n",
      "epoch: 13146 loss is tensor([-0.5344], grad_fn=<AddBackward0>)\n",
      "epoch: 13147 loss is tensor([-0.5176], grad_fn=<AddBackward0>)\n",
      "epoch: 13148 loss is tensor([-0.5561], grad_fn=<AddBackward0>)\n",
      "epoch: 13149 loss is tensor([-0.5636], grad_fn=<AddBackward0>)\n",
      "epoch: 13150 loss is tensor([-0.5397], grad_fn=<AddBackward0>)\n",
      "epoch: 13151 loss is tensor([-0.5466], grad_fn=<AddBackward0>)\n",
      "epoch: 13152 loss is tensor([-0.5168], grad_fn=<AddBackward0>)\n",
      "epoch: 13153 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 13154 loss is tensor([-0.5436], grad_fn=<AddBackward0>)\n",
      "epoch: 13155 loss is tensor([-0.5084], grad_fn=<AddBackward0>)\n",
      "epoch: 13156 loss is tensor([-0.5639], grad_fn=<AddBackward0>)\n",
      "epoch: 13157 loss is tensor([-0.5103], grad_fn=<AddBackward0>)\n",
      "epoch: 13158 loss is tensor([-0.5695], grad_fn=<AddBackward0>)\n",
      "epoch: 13159 loss is tensor([-0.6025], grad_fn=<AddBackward0>)\n",
      "epoch: 13160 loss is tensor([-0.5626], grad_fn=<AddBackward0>)\n",
      "epoch: 13161 loss is tensor([-0.5444], grad_fn=<AddBackward0>)\n",
      "epoch: 13162 loss is tensor([-0.5186], grad_fn=<AddBackward0>)\n",
      "epoch: 13163 loss is tensor([-0.5266], grad_fn=<AddBackward0>)\n",
      "epoch: 13164 loss is tensor([-0.4998], grad_fn=<AddBackward0>)\n",
      "epoch: 13165 loss is tensor([-0.5134], grad_fn=<AddBackward0>)\n",
      "epoch: 13166 loss is tensor([-0.5196], grad_fn=<AddBackward0>)\n",
      "epoch: 13167 loss is tensor([-0.3586], grad_fn=<AddBackward0>)\n",
      "epoch: 13168 loss is tensor([-0.3493], grad_fn=<AddBackward0>)\n",
      "epoch: 13169 loss is tensor([-0.5280], grad_fn=<AddBackward0>)\n",
      "epoch: 13170 loss is tensor([-0.5438], grad_fn=<AddBackward0>)\n",
      "epoch: 13171 loss is tensor([-0.4122], grad_fn=<AddBackward0>)\n",
      "epoch: 13172 loss is tensor([-0.4546], grad_fn=<AddBackward0>)\n",
      "epoch: 13173 loss is tensor([-0.5316], grad_fn=<AddBackward0>)\n",
      "epoch: 13174 loss is tensor([-0.4897], grad_fn=<AddBackward0>)\n",
      "epoch: 13175 loss is tensor([-0.4843], grad_fn=<AddBackward0>)\n",
      "epoch: 13176 loss is tensor([-0.5294], grad_fn=<AddBackward0>)\n",
      "epoch: 13177 loss is tensor([-0.4675], grad_fn=<AddBackward0>)\n",
      "epoch: 13178 loss is tensor([-0.5326], grad_fn=<AddBackward0>)\n",
      "epoch: 13179 loss is tensor([-0.5679], grad_fn=<AddBackward0>)\n",
      "epoch: 13180 loss is tensor([-0.5208], grad_fn=<AddBackward0>)\n",
      "epoch: 13181 loss is tensor([-0.4497], grad_fn=<AddBackward0>)\n",
      "epoch: 13182 loss is tensor([-0.5100], grad_fn=<AddBackward0>)\n",
      "epoch: 13183 loss is tensor([-0.5218], grad_fn=<AddBackward0>)\n",
      "epoch: 13184 loss is tensor([-0.5749], grad_fn=<AddBackward0>)\n",
      "epoch: 13185 loss is tensor([-0.4889], grad_fn=<AddBackward0>)\n",
      "epoch: 13186 loss is tensor([-0.5357], grad_fn=<AddBackward0>)\n",
      "epoch: 13187 loss is tensor([-0.5460], grad_fn=<AddBackward0>)\n",
      "epoch: 13188 loss is tensor([-0.5551], grad_fn=<AddBackward0>)\n",
      "epoch: 13189 loss is tensor([-0.4970], grad_fn=<AddBackward0>)\n",
      "epoch: 13190 loss is tensor([-0.5393], grad_fn=<AddBackward0>)\n",
      "epoch: 13191 loss is tensor([-0.5067], grad_fn=<AddBackward0>)\n",
      "epoch: 13192 loss is tensor([-0.5473], grad_fn=<AddBackward0>)\n",
      "epoch: 13193 loss is tensor([-0.5654], grad_fn=<AddBackward0>)\n",
      "epoch: 13194 loss is tensor([-0.5235], grad_fn=<AddBackward0>)\n",
      "epoch: 13195 loss is tensor([-0.4873], grad_fn=<AddBackward0>)\n",
      "epoch: 13196 loss is tensor([-0.5786], grad_fn=<AddBackward0>)\n",
      "epoch: 13197 loss is tensor([-0.5010], grad_fn=<AddBackward0>)\n",
      "epoch: 13198 loss is tensor([-0.5675], grad_fn=<AddBackward0>)\n",
      "epoch: 13199 loss is tensor([-0.5349], grad_fn=<AddBackward0>)\n",
      "epoch: 13200 loss is tensor([-0.5257], grad_fn=<AddBackward0>)\n",
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13201 loss is tensor([-0.5423], grad_fn=<AddBackward0>)\n",
      "epoch: 13202 loss is tensor([-0.6006], grad_fn=<AddBackward0>)\n",
      "epoch: 13203 loss is tensor([-0.4797], grad_fn=<AddBackward0>)\n",
      "epoch: 13204 loss is tensor([-0.5214], grad_fn=<AddBackward0>)\n",
      "epoch: 13205 loss is tensor([-0.5665], grad_fn=<AddBackward0>)\n",
      "epoch: 13206 loss is tensor([-0.6303], grad_fn=<AddBackward0>)\n",
      "epoch: 13207 loss is tensor([-0.4873], grad_fn=<AddBackward0>)\n",
      "epoch: 13208 loss is tensor([-0.5026], grad_fn=<AddBackward0>)\n",
      "epoch: 13209 loss is tensor([-0.5538], grad_fn=<AddBackward0>)\n",
      "epoch: 13210 loss is tensor([-0.5007], grad_fn=<AddBackward0>)\n",
      "epoch: 13211 loss is tensor([-0.5474], grad_fn=<AddBackward0>)\n",
      "epoch: 13212 loss is tensor([-0.5329], grad_fn=<AddBackward0>)\n",
      "epoch: 13213 loss is tensor([-0.5407], grad_fn=<AddBackward0>)\n",
      "epoch: 13214 loss is tensor([-0.5462], grad_fn=<AddBackward0>)\n",
      "epoch: 13215 loss is tensor([-0.5012], grad_fn=<AddBackward0>)\n",
      "epoch: 13216 loss is tensor([-0.5405], grad_fn=<AddBackward0>)\n",
      "epoch: 13217 loss is tensor([-0.5482], grad_fn=<AddBackward0>)\n",
      "epoch: 13218 loss is tensor([-0.6171], grad_fn=<AddBackward0>)\n",
      "epoch: 13219 loss is tensor([-0.5581], grad_fn=<AddBackward0>)\n",
      "epoch: 13220 loss is tensor([-0.5706], grad_fn=<AddBackward0>)\n",
      "epoch: 13221 loss is tensor([-0.5134], grad_fn=<AddBackward0>)\n",
      "epoch: 13222 loss is tensor([-0.5844], grad_fn=<AddBackward0>)\n",
      "epoch: 13223 loss is tensor([-0.5751], grad_fn=<AddBackward0>)\n",
      "epoch: 13224 loss is tensor([-0.5824], grad_fn=<AddBackward0>)\n",
      "epoch: 13225 loss is tensor([-0.5638], grad_fn=<AddBackward0>)\n",
      "epoch: 13226 loss is tensor([-0.5435], grad_fn=<AddBackward0>)\n",
      "epoch: 13227 loss is tensor([-0.5823], grad_fn=<AddBackward0>)\n",
      "epoch: 13228 loss is tensor([-0.5972], grad_fn=<AddBackward0>)\n",
      "epoch: 13229 loss is tensor([-0.6228], grad_fn=<AddBackward0>)\n",
      "epoch: 13230 loss is tensor([-0.5738], grad_fn=<AddBackward0>)\n",
      "epoch: 13231 loss is tensor([-0.5839], grad_fn=<AddBackward0>)\n",
      "epoch: 13232 loss is tensor([-0.5535], grad_fn=<AddBackward0>)\n",
      "epoch: 13233 loss is tensor([-0.5492], grad_fn=<AddBackward0>)\n",
      "epoch: 13234 loss is tensor([-0.5379], grad_fn=<AddBackward0>)\n",
      "epoch: 13235 loss is tensor([-0.5882], grad_fn=<AddBackward0>)\n",
      "epoch: 13236 loss is tensor([-0.5348], grad_fn=<AddBackward0>)\n",
      "epoch: 13237 loss is tensor([-0.5291], grad_fn=<AddBackward0>)\n",
      "epoch: 13238 loss is tensor([-0.4834], grad_fn=<AddBackward0>)\n",
      "epoch: 13239 loss is tensor([-0.5257], grad_fn=<AddBackward0>)\n",
      "epoch: 13240 loss is tensor([-0.5255], grad_fn=<AddBackward0>)\n",
      "epoch: 13241 loss is tensor([-0.5436], grad_fn=<AddBackward0>)\n",
      "epoch: 13242 loss is tensor([-0.5245], grad_fn=<AddBackward0>)\n",
      "epoch: 13243 loss is tensor([-0.5532], grad_fn=<AddBackward0>)\n",
      "epoch: 13244 loss is tensor([-0.5498], grad_fn=<AddBackward0>)\n",
      "epoch: 13245 loss is tensor([-0.5352], grad_fn=<AddBackward0>)\n",
      "epoch: 13246 loss is tensor([-0.5654], grad_fn=<AddBackward0>)\n",
      "epoch: 13247 loss is tensor([-0.5855], grad_fn=<AddBackward0>)\n",
      "epoch: 13248 loss is tensor([-0.5871], grad_fn=<AddBackward0>)\n",
      "epoch: 13249 loss is tensor([-0.6194], grad_fn=<AddBackward0>)\n",
      "epoch: 13250 loss is tensor([-0.5951], grad_fn=<AddBackward0>)\n",
      "epoch: 13251 loss is tensor([-0.5660], grad_fn=<AddBackward0>)\n",
      "epoch: 13252 loss is tensor([-0.5578], grad_fn=<AddBackward0>)\n",
      "epoch: 13253 loss is tensor([-0.5404], grad_fn=<AddBackward0>)\n",
      "epoch: 13254 loss is tensor([-0.5100], grad_fn=<AddBackward0>)\n",
      "epoch: 13255 loss is tensor([-0.5860], grad_fn=<AddBackward0>)\n",
      "epoch: 13256 loss is tensor([-0.5909], grad_fn=<AddBackward0>)\n",
      "epoch: 13257 loss is tensor([-0.5429], grad_fn=<AddBackward0>)\n",
      "epoch: 13258 loss is tensor([-0.5289], grad_fn=<AddBackward0>)\n",
      "epoch: 13259 loss is tensor([-0.5603], grad_fn=<AddBackward0>)\n",
      "epoch: 13260 loss is tensor([-0.6095], grad_fn=<AddBackward0>)\n",
      "epoch: 13261 loss is tensor([-0.5812], grad_fn=<AddBackward0>)\n",
      "epoch: 13262 loss is tensor([-0.6397], grad_fn=<AddBackward0>)\n",
      "epoch: 13263 loss is tensor([-0.6009], grad_fn=<AddBackward0>)\n",
      "epoch: 13264 loss is tensor([-0.5872], grad_fn=<AddBackward0>)\n",
      "epoch: 13265 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 13266 loss is tensor([-0.5462], grad_fn=<AddBackward0>)\n",
      "epoch: 13267 loss is tensor([-0.6060], grad_fn=<AddBackward0>)\n",
      "epoch: 13268 loss is tensor([-0.5864], grad_fn=<AddBackward0>)\n",
      "epoch: 13269 loss is tensor([-0.6007], grad_fn=<AddBackward0>)\n",
      "epoch: 13270 loss is tensor([-0.5906], grad_fn=<AddBackward0>)\n",
      "epoch: 13271 loss is tensor([-0.5791], grad_fn=<AddBackward0>)\n",
      "epoch: 13272 loss is tensor([-0.5850], grad_fn=<AddBackward0>)\n",
      "epoch: 13273 loss is tensor([-0.5981], grad_fn=<AddBackward0>)\n",
      "epoch: 13274 loss is tensor([-0.6158], grad_fn=<AddBackward0>)\n",
      "epoch: 13275 loss is tensor([-0.6054], grad_fn=<AddBackward0>)\n",
      "epoch: 13276 loss is tensor([-0.5611], grad_fn=<AddBackward0>)\n",
      "epoch: 13277 loss is tensor([-0.5366], grad_fn=<AddBackward0>)\n",
      "epoch: 13278 loss is tensor([-0.6067], grad_fn=<AddBackward0>)\n",
      "epoch: 13279 loss is tensor([-0.6321], grad_fn=<AddBackward0>)\n",
      "epoch: 13280 loss is tensor([-0.5888], grad_fn=<AddBackward0>)\n",
      "epoch: 13281 loss is tensor([-0.5784], grad_fn=<AddBackward0>)\n",
      "epoch: 13282 loss is tensor([-0.5680], grad_fn=<AddBackward0>)\n",
      "epoch: 13283 loss is tensor([-0.5750], grad_fn=<AddBackward0>)\n",
      "epoch: 13284 loss is tensor([-0.5613], grad_fn=<AddBackward0>)\n",
      "epoch: 13285 loss is tensor([-0.5919], grad_fn=<AddBackward0>)\n",
      "epoch: 13286 loss is tensor([-0.5407], grad_fn=<AddBackward0>)\n",
      "epoch: 13287 loss is tensor([-0.5910], grad_fn=<AddBackward0>)\n",
      "epoch: 13288 loss is tensor([-0.5457], grad_fn=<AddBackward0>)\n",
      "epoch: 13289 loss is tensor([-0.5969], grad_fn=<AddBackward0>)\n",
      "epoch: 13290 loss is tensor([-0.5712], grad_fn=<AddBackward0>)\n",
      "epoch: 13291 loss is tensor([-0.6090], grad_fn=<AddBackward0>)\n",
      "epoch: 13292 loss is tensor([-0.5712], grad_fn=<AddBackward0>)\n",
      "epoch: 13293 loss is tensor([-0.5636], grad_fn=<AddBackward0>)\n",
      "epoch: 13294 loss is tensor([-0.5725], grad_fn=<AddBackward0>)\n",
      "epoch: 13295 loss is tensor([-0.6070], grad_fn=<AddBackward0>)\n",
      "epoch: 13296 loss is tensor([-0.5056], grad_fn=<AddBackward0>)\n",
      "epoch: 13297 loss is tensor([-0.5536], grad_fn=<AddBackward0>)\n",
      "epoch: 13298 loss is tensor([-0.6050], grad_fn=<AddBackward0>)\n",
      "epoch: 13299 loss is tensor([-0.5765], grad_fn=<AddBackward0>)\n",
      "epoch: 13300 loss is tensor([-0.5549], grad_fn=<AddBackward0>)\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13301 loss is tensor([-0.5150], grad_fn=<AddBackward0>)\n",
      "epoch: 13302 loss is tensor([-0.5488], grad_fn=<AddBackward0>)\n",
      "epoch: 13303 loss is tensor([-0.5742], grad_fn=<AddBackward0>)\n",
      "epoch: 13304 loss is tensor([-0.5133], grad_fn=<AddBackward0>)\n",
      "epoch: 13305 loss is tensor([-0.5585], grad_fn=<AddBackward0>)\n",
      "epoch: 13306 loss is tensor([-0.5288], grad_fn=<AddBackward0>)\n",
      "epoch: 13307 loss is tensor([-0.5756], grad_fn=<AddBackward0>)\n",
      "epoch: 13308 loss is tensor([-0.5484], grad_fn=<AddBackward0>)\n",
      "epoch: 13309 loss is tensor([-0.5616], grad_fn=<AddBackward0>)\n",
      "epoch: 13310 loss is tensor([-0.5759], grad_fn=<AddBackward0>)\n",
      "epoch: 13311 loss is tensor([-0.5235], grad_fn=<AddBackward0>)\n",
      "epoch: 13312 loss is tensor([-0.4831], grad_fn=<AddBackward0>)\n",
      "epoch: 13313 loss is tensor([-0.5676], grad_fn=<AddBackward0>)\n",
      "epoch: 13314 loss is tensor([-0.5032], grad_fn=<AddBackward0>)\n",
      "epoch: 13315 loss is tensor([-0.4637], grad_fn=<AddBackward0>)\n",
      "epoch: 13316 loss is tensor([-0.5196], grad_fn=<AddBackward0>)\n",
      "epoch: 13317 loss is tensor([-0.5402], grad_fn=<AddBackward0>)\n",
      "epoch: 13318 loss is tensor([-0.5132], grad_fn=<AddBackward0>)\n",
      "epoch: 13319 loss is tensor([-0.4704], grad_fn=<AddBackward0>)\n",
      "epoch: 13320 loss is tensor([-0.5528], grad_fn=<AddBackward0>)\n",
      "epoch: 13321 loss is tensor([-0.4781], grad_fn=<AddBackward0>)\n",
      "epoch: 13322 loss is tensor([-0.4681], grad_fn=<AddBackward0>)\n",
      "epoch: 13323 loss is tensor([-0.4928], grad_fn=<AddBackward0>)\n",
      "epoch: 13324 loss is tensor([-0.5212], grad_fn=<AddBackward0>)\n",
      "epoch: 13325 loss is tensor([-0.5454], grad_fn=<AddBackward0>)\n",
      "epoch: 13326 loss is tensor([-0.5123], grad_fn=<AddBackward0>)\n",
      "epoch: 13327 loss is tensor([-0.4642], grad_fn=<AddBackward0>)\n",
      "epoch: 13328 loss is tensor([-0.5277], grad_fn=<AddBackward0>)\n",
      "epoch: 13329 loss is tensor([-0.5059], grad_fn=<AddBackward0>)\n",
      "epoch: 13330 loss is tensor([-0.3820], grad_fn=<AddBackward0>)\n",
      "epoch: 13331 loss is tensor([-0.3650], grad_fn=<AddBackward0>)\n",
      "epoch: 13332 loss is tensor([-0.4623], grad_fn=<AddBackward0>)\n",
      "epoch: 13333 loss is tensor([-0.4722], grad_fn=<AddBackward0>)\n",
      "epoch: 13334 loss is tensor([-0.4783], grad_fn=<AddBackward0>)\n",
      "epoch: 13335 loss is tensor([-0.4976], grad_fn=<AddBackward0>)\n",
      "epoch: 13336 loss is tensor([-0.4837], grad_fn=<AddBackward0>)\n",
      "epoch: 13337 loss is tensor([-0.4751], grad_fn=<AddBackward0>)\n",
      "epoch: 13338 loss is tensor([-0.5172], grad_fn=<AddBackward0>)\n",
      "epoch: 13339 loss is tensor([-0.5220], grad_fn=<AddBackward0>)\n",
      "epoch: 13340 loss is tensor([-0.5011], grad_fn=<AddBackward0>)\n",
      "epoch: 13341 loss is tensor([-0.5349], grad_fn=<AddBackward0>)\n",
      "epoch: 13342 loss is tensor([-0.4801], grad_fn=<AddBackward0>)\n",
      "epoch: 13343 loss is tensor([-0.5000], grad_fn=<AddBackward0>)\n",
      "epoch: 13344 loss is tensor([-0.5453], grad_fn=<AddBackward0>)\n",
      "epoch: 13345 loss is tensor([-0.4759], grad_fn=<AddBackward0>)\n",
      "epoch: 13346 loss is tensor([-0.5340], grad_fn=<AddBackward0>)\n",
      "epoch: 13347 loss is tensor([-0.5062], grad_fn=<AddBackward0>)\n",
      "epoch: 13348 loss is tensor([-0.5446], grad_fn=<AddBackward0>)\n",
      "epoch: 13349 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 13350 loss is tensor([-0.5426], grad_fn=<AddBackward0>)\n",
      "epoch: 13351 loss is tensor([-0.5578], grad_fn=<AddBackward0>)\n",
      "epoch: 13352 loss is tensor([-0.5863], grad_fn=<AddBackward0>)\n",
      "epoch: 13353 loss is tensor([-0.5565], grad_fn=<AddBackward0>)\n",
      "epoch: 13354 loss is tensor([-0.5627], grad_fn=<AddBackward0>)\n",
      "epoch: 13355 loss is tensor([-0.5766], grad_fn=<AddBackward0>)\n",
      "epoch: 13356 loss is tensor([-0.5742], grad_fn=<AddBackward0>)\n",
      "epoch: 13357 loss is tensor([-0.5093], grad_fn=<AddBackward0>)\n",
      "epoch: 13358 loss is tensor([-0.5336], grad_fn=<AddBackward0>)\n",
      "epoch: 13359 loss is tensor([-0.5461], grad_fn=<AddBackward0>)\n",
      "epoch: 13360 loss is tensor([-0.5345], grad_fn=<AddBackward0>)\n",
      "epoch: 13361 loss is tensor([-0.5173], grad_fn=<AddBackward0>)\n",
      "epoch: 13362 loss is tensor([-0.5672], grad_fn=<AddBackward0>)\n",
      "epoch: 13363 loss is tensor([-0.5883], grad_fn=<AddBackward0>)\n",
      "epoch: 13364 loss is tensor([-0.5138], grad_fn=<AddBackward0>)\n",
      "epoch: 13365 loss is tensor([-0.5791], grad_fn=<AddBackward0>)\n",
      "epoch: 13366 loss is tensor([-0.5630], grad_fn=<AddBackward0>)\n",
      "epoch: 13367 loss is tensor([-0.4868], grad_fn=<AddBackward0>)\n",
      "epoch: 13368 loss is tensor([-0.5293], grad_fn=<AddBackward0>)\n",
      "epoch: 13369 loss is tensor([-0.5320], grad_fn=<AddBackward0>)\n",
      "epoch: 13370 loss is tensor([-0.5206], grad_fn=<AddBackward0>)\n",
      "epoch: 13371 loss is tensor([-0.5230], grad_fn=<AddBackward0>)\n",
      "epoch: 13372 loss is tensor([-0.5998], grad_fn=<AddBackward0>)\n",
      "epoch: 13373 loss is tensor([-0.5423], grad_fn=<AddBackward0>)\n",
      "epoch: 13374 loss is tensor([-0.5033], grad_fn=<AddBackward0>)\n",
      "epoch: 13375 loss is tensor([-0.5510], grad_fn=<AddBackward0>)\n",
      "epoch: 13376 loss is tensor([-0.4954], grad_fn=<AddBackward0>)\n",
      "epoch: 13377 loss is tensor([-0.5083], grad_fn=<AddBackward0>)\n",
      "epoch: 13378 loss is tensor([-0.5498], grad_fn=<AddBackward0>)\n",
      "epoch: 13379 loss is tensor([-0.5571], grad_fn=<AddBackward0>)\n",
      "epoch: 13380 loss is tensor([-0.5392], grad_fn=<AddBackward0>)\n",
      "epoch: 13381 loss is tensor([-0.4937], grad_fn=<AddBackward0>)\n",
      "epoch: 13382 loss is tensor([-0.5227], grad_fn=<AddBackward0>)\n",
      "epoch: 13383 loss is tensor([-0.5957], grad_fn=<AddBackward0>)\n",
      "epoch: 13384 loss is tensor([-0.5474], grad_fn=<AddBackward0>)\n",
      "epoch: 13385 loss is tensor([-0.5667], grad_fn=<AddBackward0>)\n",
      "epoch: 13386 loss is tensor([-0.5590], grad_fn=<AddBackward0>)\n",
      "epoch: 13387 loss is tensor([-0.5212], grad_fn=<AddBackward0>)\n",
      "epoch: 13388 loss is tensor([-0.5407], grad_fn=<AddBackward0>)\n",
      "epoch: 13389 loss is tensor([-0.5106], grad_fn=<AddBackward0>)\n",
      "epoch: 13390 loss is tensor([-0.5028], grad_fn=<AddBackward0>)\n",
      "epoch: 13391 loss is tensor([-0.4742], grad_fn=<AddBackward0>)\n",
      "epoch: 13392 loss is tensor([-0.4906], grad_fn=<AddBackward0>)\n",
      "epoch: 13393 loss is tensor([-0.4940], grad_fn=<AddBackward0>)\n",
      "epoch: 13394 loss is tensor([-0.4596], grad_fn=<AddBackward0>)\n",
      "epoch: 13395 loss is tensor([-0.5067], grad_fn=<AddBackward0>)\n",
      "epoch: 13396 loss is tensor([-0.4787], grad_fn=<AddBackward0>)\n",
      "epoch: 13397 loss is tensor([-0.5090], grad_fn=<AddBackward0>)\n",
      "epoch: 13398 loss is tensor([-0.5157], grad_fn=<AddBackward0>)\n",
      "epoch: 13399 loss is tensor([-0.4976], grad_fn=<AddBackward0>)\n",
      "epoch: 13400 loss is tensor([-0.4923], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13401 loss is tensor([-0.5306], grad_fn=<AddBackward0>)\n",
      "epoch: 13402 loss is tensor([-0.5338], grad_fn=<AddBackward0>)\n",
      "epoch: 13403 loss is tensor([-0.5386], grad_fn=<AddBackward0>)\n",
      "epoch: 13404 loss is tensor([-0.5519], grad_fn=<AddBackward0>)\n",
      "epoch: 13405 loss is tensor([-0.5439], grad_fn=<AddBackward0>)\n",
      "epoch: 13406 loss is tensor([-0.5643], grad_fn=<AddBackward0>)\n",
      "epoch: 13407 loss is tensor([-0.5845], grad_fn=<AddBackward0>)\n",
      "epoch: 13408 loss is tensor([-0.5796], grad_fn=<AddBackward0>)\n",
      "epoch: 13409 loss is tensor([-0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 13410 loss is tensor([-0.5271], grad_fn=<AddBackward0>)\n",
      "epoch: 13411 loss is tensor([-0.5523], grad_fn=<AddBackward0>)\n",
      "epoch: 13412 loss is tensor([-0.5808], grad_fn=<AddBackward0>)\n",
      "epoch: 13413 loss is tensor([-0.5767], grad_fn=<AddBackward0>)\n",
      "epoch: 13414 loss is tensor([-0.5814], grad_fn=<AddBackward0>)\n",
      "epoch: 13415 loss is tensor([-0.5406], grad_fn=<AddBackward0>)\n",
      "epoch: 13416 loss is tensor([-0.5242], grad_fn=<AddBackward0>)\n",
      "epoch: 13417 loss is tensor([-0.5683], grad_fn=<AddBackward0>)\n",
      "epoch: 13418 loss is tensor([-0.5614], grad_fn=<AddBackward0>)\n",
      "epoch: 13419 loss is tensor([-0.5349], grad_fn=<AddBackward0>)\n",
      "epoch: 13420 loss is tensor([-0.5966], grad_fn=<AddBackward0>)\n",
      "epoch: 13421 loss is tensor([-0.5458], grad_fn=<AddBackward0>)\n",
      "epoch: 13422 loss is tensor([-0.5150], grad_fn=<AddBackward0>)\n",
      "epoch: 13423 loss is tensor([-0.5101], grad_fn=<AddBackward0>)\n",
      "epoch: 13424 loss is tensor([-0.5363], grad_fn=<AddBackward0>)\n",
      "epoch: 13425 loss is tensor([-0.5167], grad_fn=<AddBackward0>)\n",
      "epoch: 13426 loss is tensor([-0.5208], grad_fn=<AddBackward0>)\n",
      "epoch: 13427 loss is tensor([-0.5504], grad_fn=<AddBackward0>)\n",
      "epoch: 13428 loss is tensor([-0.5489], grad_fn=<AddBackward0>)\n",
      "epoch: 13429 loss is tensor([-0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 13430 loss is tensor([-0.5411], grad_fn=<AddBackward0>)\n",
      "epoch: 13431 loss is tensor([-0.5117], grad_fn=<AddBackward0>)\n",
      "epoch: 13432 loss is tensor([-0.5203], grad_fn=<AddBackward0>)\n",
      "epoch: 13433 loss is tensor([-0.5489], grad_fn=<AddBackward0>)\n",
      "epoch: 13434 loss is tensor([-0.5340], grad_fn=<AddBackward0>)\n",
      "epoch: 13435 loss is tensor([-0.4855], grad_fn=<AddBackward0>)\n",
      "epoch: 13436 loss is tensor([-0.5096], grad_fn=<AddBackward0>)\n",
      "epoch: 13437 loss is tensor([-0.4369], grad_fn=<AddBackward0>)\n",
      "epoch: 13438 loss is tensor([-0.5295], grad_fn=<AddBackward0>)\n",
      "epoch: 13439 loss is tensor([-0.4828], grad_fn=<AddBackward0>)\n",
      "epoch: 13440 loss is tensor([-0.4480], grad_fn=<AddBackward0>)\n",
      "epoch: 13441 loss is tensor([-0.4992], grad_fn=<AddBackward0>)\n",
      "epoch: 13442 loss is tensor([-0.4779], grad_fn=<AddBackward0>)\n",
      "epoch: 13443 loss is tensor([-0.5027], grad_fn=<AddBackward0>)\n",
      "epoch: 13444 loss is tensor([-0.4768], grad_fn=<AddBackward0>)\n",
      "epoch: 13445 loss is tensor([-0.5421], grad_fn=<AddBackward0>)\n",
      "epoch: 13446 loss is tensor([-0.5224], grad_fn=<AddBackward0>)\n",
      "epoch: 13447 loss is tensor([-0.5059], grad_fn=<AddBackward0>)\n",
      "epoch: 13448 loss is tensor([-0.5258], grad_fn=<AddBackward0>)\n",
      "epoch: 13449 loss is tensor([-0.5130], grad_fn=<AddBackward0>)\n",
      "epoch: 13450 loss is tensor([-0.5454], grad_fn=<AddBackward0>)\n",
      "epoch: 13451 loss is tensor([-0.5659], grad_fn=<AddBackward0>)\n",
      "epoch: 13452 loss is tensor([-0.5827], grad_fn=<AddBackward0>)\n",
      "epoch: 13453 loss is tensor([-0.5688], grad_fn=<AddBackward0>)\n",
      "epoch: 13454 loss is tensor([-0.5570], grad_fn=<AddBackward0>)\n",
      "epoch: 13455 loss is tensor([-0.5490], grad_fn=<AddBackward0>)\n",
      "epoch: 13456 loss is tensor([-0.5033], grad_fn=<AddBackward0>)\n",
      "epoch: 13457 loss is tensor([-0.5164], grad_fn=<AddBackward0>)\n",
      "epoch: 13458 loss is tensor([-0.5414], grad_fn=<AddBackward0>)\n",
      "epoch: 13459 loss is tensor([-0.5808], grad_fn=<AddBackward0>)\n",
      "epoch: 13460 loss is tensor([-0.5434], grad_fn=<AddBackward0>)\n",
      "epoch: 13461 loss is tensor([-0.5651], grad_fn=<AddBackward0>)\n",
      "epoch: 13462 loss is tensor([-0.5512], grad_fn=<AddBackward0>)\n",
      "epoch: 13463 loss is tensor([-0.5673], grad_fn=<AddBackward0>)\n",
      "epoch: 13464 loss is tensor([-0.5573], grad_fn=<AddBackward0>)\n",
      "epoch: 13465 loss is tensor([-0.5534], grad_fn=<AddBackward0>)\n",
      "epoch: 13466 loss is tensor([-0.5645], grad_fn=<AddBackward0>)\n",
      "epoch: 13467 loss is tensor([-0.6103], grad_fn=<AddBackward0>)\n",
      "epoch: 13468 loss is tensor([-0.5991], grad_fn=<AddBackward0>)\n",
      "epoch: 13469 loss is tensor([-0.5205], grad_fn=<AddBackward0>)\n",
      "epoch: 13470 loss is tensor([-0.5040], grad_fn=<AddBackward0>)\n",
      "epoch: 13471 loss is tensor([-0.6077], grad_fn=<AddBackward0>)\n",
      "epoch: 13472 loss is tensor([-0.5781], grad_fn=<AddBackward0>)\n",
      "epoch: 13473 loss is tensor([-0.4768], grad_fn=<AddBackward0>)\n",
      "epoch: 13474 loss is tensor([-0.5313], grad_fn=<AddBackward0>)\n",
      "epoch: 13475 loss is tensor([-0.5455], grad_fn=<AddBackward0>)\n",
      "epoch: 13476 loss is tensor([-0.5737], grad_fn=<AddBackward0>)\n",
      "epoch: 13477 loss is tensor([-0.5482], grad_fn=<AddBackward0>)\n",
      "epoch: 13478 loss is tensor([-0.6003], grad_fn=<AddBackward0>)\n",
      "epoch: 13479 loss is tensor([-0.5533], grad_fn=<AddBackward0>)\n",
      "epoch: 13480 loss is tensor([-0.5794], grad_fn=<AddBackward0>)\n",
      "epoch: 13481 loss is tensor([-0.5865], grad_fn=<AddBackward0>)\n",
      "epoch: 13482 loss is tensor([-0.5388], grad_fn=<AddBackward0>)\n",
      "epoch: 13483 loss is tensor([-0.5395], grad_fn=<AddBackward0>)\n",
      "epoch: 13484 loss is tensor([-0.5120], grad_fn=<AddBackward0>)\n",
      "epoch: 13485 loss is tensor([-0.5440], grad_fn=<AddBackward0>)\n",
      "epoch: 13486 loss is tensor([-0.5361], grad_fn=<AddBackward0>)\n",
      "epoch: 13487 loss is tensor([-0.5270], grad_fn=<AddBackward0>)\n",
      "epoch: 13488 loss is tensor([-0.6351], grad_fn=<AddBackward0>)\n",
      "epoch: 13489 loss is tensor([-0.5898], grad_fn=<AddBackward0>)\n",
      "epoch: 13490 loss is tensor([-0.5523], grad_fn=<AddBackward0>)\n",
      "epoch: 13491 loss is tensor([-0.5115], grad_fn=<AddBackward0>)\n",
      "epoch: 13492 loss is tensor([-0.5203], grad_fn=<AddBackward0>)\n",
      "epoch: 13493 loss is tensor([-0.5547], grad_fn=<AddBackward0>)\n",
      "epoch: 13494 loss is tensor([-0.5207], grad_fn=<AddBackward0>)\n",
      "epoch: 13495 loss is tensor([-0.5365], grad_fn=<AddBackward0>)\n",
      "epoch: 13496 loss is tensor([-0.5642], grad_fn=<AddBackward0>)\n",
      "epoch: 13497 loss is tensor([-0.4770], grad_fn=<AddBackward0>)\n",
      "epoch: 13498 loss is tensor([-0.5842], grad_fn=<AddBackward0>)\n",
      "epoch: 13499 loss is tensor([-0.5849], grad_fn=<AddBackward0>)\n",
      "epoch: 13500 loss is tensor([-0.5413], grad_fn=<AddBackward0>)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13501 loss is tensor([-0.5026], grad_fn=<AddBackward0>)\n",
      "epoch: 13502 loss is tensor([-0.5156], grad_fn=<AddBackward0>)\n",
      "epoch: 13503 loss is tensor([-0.4750], grad_fn=<AddBackward0>)\n",
      "epoch: 13504 loss is tensor([-0.5120], grad_fn=<AddBackward0>)\n",
      "epoch: 13505 loss is tensor([-0.5226], grad_fn=<AddBackward0>)\n",
      "epoch: 13506 loss is tensor([-0.5131], grad_fn=<AddBackward0>)\n",
      "epoch: 13507 loss is tensor([-0.4305], grad_fn=<AddBackward0>)\n",
      "epoch: 13508 loss is tensor([-0.5239], grad_fn=<AddBackward0>)\n",
      "epoch: 13509 loss is tensor([-0.5374], grad_fn=<AddBackward0>)\n",
      "epoch: 13510 loss is tensor([-0.5173], grad_fn=<AddBackward0>)\n",
      "epoch: 13511 loss is tensor([-0.5251], grad_fn=<AddBackward0>)\n",
      "epoch: 13512 loss is tensor([-0.5866], grad_fn=<AddBackward0>)\n",
      "epoch: 13513 loss is tensor([-0.5675], grad_fn=<AddBackward0>)\n",
      "epoch: 13514 loss is tensor([-0.4872], grad_fn=<AddBackward0>)\n",
      "epoch: 13515 loss is tensor([-0.5246], grad_fn=<AddBackward0>)\n",
      "epoch: 13516 loss is tensor([-0.5708], grad_fn=<AddBackward0>)\n",
      "epoch: 13517 loss is tensor([-0.5626], grad_fn=<AddBackward0>)\n",
      "epoch: 13518 loss is tensor([-0.5658], grad_fn=<AddBackward0>)\n",
      "epoch: 13519 loss is tensor([-0.5721], grad_fn=<AddBackward0>)\n",
      "epoch: 13520 loss is tensor([-0.5508], grad_fn=<AddBackward0>)\n",
      "epoch: 13521 loss is tensor([-0.6156], grad_fn=<AddBackward0>)\n",
      "epoch: 13522 loss is tensor([-0.5567], grad_fn=<AddBackward0>)\n",
      "epoch: 13523 loss is tensor([-0.5928], grad_fn=<AddBackward0>)\n",
      "epoch: 13524 loss is tensor([-0.5840], grad_fn=<AddBackward0>)\n",
      "epoch: 13525 loss is tensor([-0.5861], grad_fn=<AddBackward0>)\n",
      "epoch: 13526 loss is tensor([-0.5949], grad_fn=<AddBackward0>)\n",
      "epoch: 13527 loss is tensor([-0.5985], grad_fn=<AddBackward0>)\n",
      "epoch: 13528 loss is tensor([-0.5573], grad_fn=<AddBackward0>)\n",
      "epoch: 13529 loss is tensor([-0.5371], grad_fn=<AddBackward0>)\n",
      "epoch: 13530 loss is tensor([-0.5820], grad_fn=<AddBackward0>)\n",
      "epoch: 13531 loss is tensor([-0.5728], grad_fn=<AddBackward0>)\n",
      "epoch: 13532 loss is tensor([-0.5645], grad_fn=<AddBackward0>)\n",
      "epoch: 13533 loss is tensor([-0.6194], grad_fn=<AddBackward0>)\n",
      "epoch: 13534 loss is tensor([-0.5695], grad_fn=<AddBackward0>)\n",
      "epoch: 13535 loss is tensor([-0.5565], grad_fn=<AddBackward0>)\n",
      "epoch: 13536 loss is tensor([-0.5950], grad_fn=<AddBackward0>)\n",
      "epoch: 13537 loss is tensor([-0.6111], grad_fn=<AddBackward0>)\n",
      "epoch: 13538 loss is tensor([-0.6339], grad_fn=<AddBackward0>)\n",
      "epoch: 13539 loss is tensor([-0.5587], grad_fn=<AddBackward0>)\n",
      "epoch: 13540 loss is tensor([-0.5500], grad_fn=<AddBackward0>)\n",
      "epoch: 13541 loss is tensor([-0.5305], grad_fn=<AddBackward0>)\n",
      "epoch: 13542 loss is tensor([-0.5726], grad_fn=<AddBackward0>)\n",
      "epoch: 13543 loss is tensor([-0.5668], grad_fn=<AddBackward0>)\n",
      "epoch: 13544 loss is tensor([-0.5735], grad_fn=<AddBackward0>)\n",
      "epoch: 13545 loss is tensor([-0.5985], grad_fn=<AddBackward0>)\n",
      "epoch: 13546 loss is tensor([-0.5952], grad_fn=<AddBackward0>)\n",
      "epoch: 13547 loss is tensor([-0.5623], grad_fn=<AddBackward0>)\n",
      "epoch: 13548 loss is tensor([-0.4941], grad_fn=<AddBackward0>)\n",
      "epoch: 13549 loss is tensor([-0.5239], grad_fn=<AddBackward0>)\n",
      "epoch: 13550 loss is tensor([-0.5869], grad_fn=<AddBackward0>)\n",
      "epoch: 13551 loss is tensor([-0.5078], grad_fn=<AddBackward0>)\n",
      "epoch: 13552 loss is tensor([-0.5637], grad_fn=<AddBackward0>)\n",
      "epoch: 13553 loss is tensor([-0.5893], grad_fn=<AddBackward0>)\n",
      "epoch: 13554 loss is tensor([-0.5287], grad_fn=<AddBackward0>)\n",
      "epoch: 13555 loss is tensor([-0.5502], grad_fn=<AddBackward0>)\n",
      "epoch: 13556 loss is tensor([-0.5204], grad_fn=<AddBackward0>)\n",
      "epoch: 13557 loss is tensor([-0.5364], grad_fn=<AddBackward0>)\n",
      "epoch: 13558 loss is tensor([-0.5803], grad_fn=<AddBackward0>)\n",
      "epoch: 13559 loss is tensor([-0.6036], grad_fn=<AddBackward0>)\n",
      "epoch: 13560 loss is tensor([-0.5944], grad_fn=<AddBackward0>)\n",
      "epoch: 13561 loss is tensor([-0.5496], grad_fn=<AddBackward0>)\n",
      "epoch: 13562 loss is tensor([-0.5557], grad_fn=<AddBackward0>)\n",
      "epoch: 13563 loss is tensor([-0.6118], grad_fn=<AddBackward0>)\n",
      "epoch: 13564 loss is tensor([-0.5843], grad_fn=<AddBackward0>)\n",
      "epoch: 13565 loss is tensor([-0.5301], grad_fn=<AddBackward0>)\n",
      "epoch: 13566 loss is tensor([-0.5357], grad_fn=<AddBackward0>)\n",
      "epoch: 13567 loss is tensor([-0.5712], grad_fn=<AddBackward0>)\n",
      "epoch: 13568 loss is tensor([-0.5866], grad_fn=<AddBackward0>)\n",
      "epoch: 13569 loss is tensor([-0.5430], grad_fn=<AddBackward0>)\n",
      "epoch: 13570 loss is tensor([-0.5380], grad_fn=<AddBackward0>)\n",
      "epoch: 13571 loss is tensor([-0.5340], grad_fn=<AddBackward0>)\n",
      "epoch: 13572 loss is tensor([-0.5840], grad_fn=<AddBackward0>)\n",
      "epoch: 13573 loss is tensor([-0.5701], grad_fn=<AddBackward0>)\n",
      "epoch: 13574 loss is tensor([-0.5394], grad_fn=<AddBackward0>)\n",
      "epoch: 13575 loss is tensor([-0.5383], grad_fn=<AddBackward0>)\n",
      "epoch: 13576 loss is tensor([-0.5477], grad_fn=<AddBackward0>)\n",
      "epoch: 13577 loss is tensor([-0.5077], grad_fn=<AddBackward0>)\n",
      "epoch: 13578 loss is tensor([-0.5779], grad_fn=<AddBackward0>)\n",
      "epoch: 13579 loss is tensor([-0.5366], grad_fn=<AddBackward0>)\n",
      "epoch: 13580 loss is tensor([-0.5704], grad_fn=<AddBackward0>)\n",
      "epoch: 13581 loss is tensor([-0.5887], grad_fn=<AddBackward0>)\n",
      "epoch: 13582 loss is tensor([-0.5756], grad_fn=<AddBackward0>)\n",
      "epoch: 13583 loss is tensor([-0.5886], grad_fn=<AddBackward0>)\n",
      "epoch: 13584 loss is tensor([-0.5123], grad_fn=<AddBackward0>)\n",
      "epoch: 13585 loss is tensor([-0.5626], grad_fn=<AddBackward0>)\n",
      "epoch: 13586 loss is tensor([-0.6187], grad_fn=<AddBackward0>)\n",
      "epoch: 13587 loss is tensor([-0.5609], grad_fn=<AddBackward0>)\n",
      "epoch: 13588 loss is tensor([-0.6085], grad_fn=<AddBackward0>)\n",
      "epoch: 13589 loss is tensor([-0.6074], grad_fn=<AddBackward0>)\n",
      "epoch: 13590 loss is tensor([-0.6137], grad_fn=<AddBackward0>)\n",
      "epoch: 13591 loss is tensor([-0.5590], grad_fn=<AddBackward0>)\n",
      "epoch: 13592 loss is tensor([-0.5487], grad_fn=<AddBackward0>)\n",
      "epoch: 13593 loss is tensor([-0.6024], grad_fn=<AddBackward0>)\n",
      "epoch: 13594 loss is tensor([-0.6003], grad_fn=<AddBackward0>)\n",
      "epoch: 13595 loss is tensor([-0.5442], grad_fn=<AddBackward0>)\n",
      "epoch: 13596 loss is tensor([-0.6001], grad_fn=<AddBackward0>)\n",
      "epoch: 13597 loss is tensor([-0.6083], grad_fn=<AddBackward0>)\n",
      "epoch: 13598 loss is tensor([-0.5882], grad_fn=<AddBackward0>)\n",
      "epoch: 13599 loss is tensor([-0.6148], grad_fn=<AddBackward0>)\n",
      "epoch: 13600 loss is tensor([-0.5724], grad_fn=<AddBackward0>)\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13601 loss is tensor([-0.5560], grad_fn=<AddBackward0>)\n",
      "epoch: 13602 loss is tensor([-0.5380], grad_fn=<AddBackward0>)\n",
      "epoch: 13603 loss is tensor([-0.5225], grad_fn=<AddBackward0>)\n",
      "epoch: 13604 loss is tensor([-0.5772], grad_fn=<AddBackward0>)\n",
      "epoch: 13605 loss is tensor([-0.6019], grad_fn=<AddBackward0>)\n",
      "epoch: 13606 loss is tensor([-0.5927], grad_fn=<AddBackward0>)\n",
      "epoch: 13607 loss is tensor([-0.5618], grad_fn=<AddBackward0>)\n",
      "epoch: 13608 loss is tensor([-0.5898], grad_fn=<AddBackward0>)\n",
      "epoch: 13609 loss is tensor([-0.5872], grad_fn=<AddBackward0>)\n",
      "epoch: 13610 loss is tensor([-0.5583], grad_fn=<AddBackward0>)\n",
      "epoch: 13611 loss is tensor([-0.6169], grad_fn=<AddBackward0>)\n",
      "epoch: 13612 loss is tensor([-0.6136], grad_fn=<AddBackward0>)\n",
      "epoch: 13613 loss is tensor([-0.5914], grad_fn=<AddBackward0>)\n",
      "epoch: 13614 loss is tensor([-0.6023], grad_fn=<AddBackward0>)\n",
      "epoch: 13615 loss is tensor([-0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 13616 loss is tensor([-0.6280], grad_fn=<AddBackward0>)\n",
      "epoch: 13617 loss is tensor([-0.5961], grad_fn=<AddBackward0>)\n",
      "epoch: 13618 loss is tensor([-0.5857], grad_fn=<AddBackward0>)\n",
      "epoch: 13619 loss is tensor([-0.6409], grad_fn=<AddBackward0>)\n",
      "epoch: 13620 loss is tensor([-0.6225], grad_fn=<AddBackward0>)\n",
      "epoch: 13621 loss is tensor([-0.5531], grad_fn=<AddBackward0>)\n",
      "epoch: 13622 loss is tensor([-0.6222], grad_fn=<AddBackward0>)\n",
      "epoch: 13623 loss is tensor([-0.5980], grad_fn=<AddBackward0>)\n",
      "epoch: 13624 loss is tensor([-0.5347], grad_fn=<AddBackward0>)\n",
      "epoch: 13625 loss is tensor([-0.5063], grad_fn=<AddBackward0>)\n",
      "epoch: 13626 loss is tensor([-0.5885], grad_fn=<AddBackward0>)\n",
      "epoch: 13627 loss is tensor([-0.5533], grad_fn=<AddBackward0>)\n",
      "epoch: 13628 loss is tensor([-0.5414], grad_fn=<AddBackward0>)\n",
      "epoch: 13629 loss is tensor([-0.5586], grad_fn=<AddBackward0>)\n",
      "epoch: 13630 loss is tensor([-0.5216], grad_fn=<AddBackward0>)\n",
      "epoch: 13631 loss is tensor([-0.4878], grad_fn=<AddBackward0>)\n",
      "epoch: 13632 loss is tensor([-0.5256], grad_fn=<AddBackward0>)\n",
      "epoch: 13633 loss is tensor([-0.5219], grad_fn=<AddBackward0>)\n",
      "epoch: 13634 loss is tensor([-0.5044], grad_fn=<AddBackward0>)\n",
      "epoch: 13635 loss is tensor([-0.5353], grad_fn=<AddBackward0>)\n",
      "epoch: 13636 loss is tensor([-0.4591], grad_fn=<AddBackward0>)\n",
      "epoch: 13637 loss is tensor([-0.5583], grad_fn=<AddBackward0>)\n",
      "epoch: 13638 loss is tensor([-0.5519], grad_fn=<AddBackward0>)\n",
      "epoch: 13639 loss is tensor([-0.5100], grad_fn=<AddBackward0>)\n",
      "epoch: 13640 loss is tensor([-0.5568], grad_fn=<AddBackward0>)\n",
      "epoch: 13641 loss is tensor([-0.5448], grad_fn=<AddBackward0>)\n",
      "epoch: 13642 loss is tensor([-0.5416], grad_fn=<AddBackward0>)\n",
      "epoch: 13643 loss is tensor([-0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 13644 loss is tensor([-0.5726], grad_fn=<AddBackward0>)\n",
      "epoch: 13645 loss is tensor([-0.5618], grad_fn=<AddBackward0>)\n",
      "epoch: 13646 loss is tensor([-0.5526], grad_fn=<AddBackward0>)\n",
      "epoch: 13647 loss is tensor([-0.5817], grad_fn=<AddBackward0>)\n",
      "epoch: 13648 loss is tensor([-0.5491], grad_fn=<AddBackward0>)\n",
      "epoch: 13649 loss is tensor([-0.5748], grad_fn=<AddBackward0>)\n",
      "epoch: 13650 loss is tensor([-0.5597], grad_fn=<AddBackward0>)\n",
      "epoch: 13651 loss is tensor([-0.6091], grad_fn=<AddBackward0>)\n",
      "epoch: 13652 loss is tensor([-0.5569], grad_fn=<AddBackward0>)\n",
      "epoch: 13653 loss is tensor([-0.5485], grad_fn=<AddBackward0>)\n",
      "epoch: 13654 loss is tensor([-0.5764], grad_fn=<AddBackward0>)\n",
      "epoch: 13655 loss is tensor([-0.5529], grad_fn=<AddBackward0>)\n",
      "epoch: 13656 loss is tensor([-0.5701], grad_fn=<AddBackward0>)\n",
      "epoch: 13657 loss is tensor([-0.5970], grad_fn=<AddBackward0>)\n",
      "epoch: 13658 loss is tensor([-0.6246], grad_fn=<AddBackward0>)\n",
      "epoch: 13659 loss is tensor([-0.5748], grad_fn=<AddBackward0>)\n",
      "epoch: 13660 loss is tensor([-0.5645], grad_fn=<AddBackward0>)\n",
      "epoch: 13661 loss is tensor([-0.6158], grad_fn=<AddBackward0>)\n",
      "epoch: 13662 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 13663 loss is tensor([-0.5174], grad_fn=<AddBackward0>)\n",
      "epoch: 13664 loss is tensor([-0.5745], grad_fn=<AddBackward0>)\n",
      "epoch: 13665 loss is tensor([-0.5940], grad_fn=<AddBackward0>)\n",
      "epoch: 13666 loss is tensor([-0.5714], grad_fn=<AddBackward0>)\n",
      "epoch: 13667 loss is tensor([-0.6226], grad_fn=<AddBackward0>)\n",
      "epoch: 13668 loss is tensor([-0.6129], grad_fn=<AddBackward0>)\n",
      "epoch: 13669 loss is tensor([-0.5984], grad_fn=<AddBackward0>)\n",
      "epoch: 13670 loss is tensor([-0.5436], grad_fn=<AddBackward0>)\n",
      "epoch: 13671 loss is tensor([-0.6014], grad_fn=<AddBackward0>)\n",
      "epoch: 13672 loss is tensor([-0.6179], grad_fn=<AddBackward0>)\n",
      "epoch: 13673 loss is tensor([-0.5408], grad_fn=<AddBackward0>)\n",
      "epoch: 13674 loss is tensor([-0.5585], grad_fn=<AddBackward0>)\n",
      "epoch: 13675 loss is tensor([-0.5394], grad_fn=<AddBackward0>)\n",
      "epoch: 13676 loss is tensor([-0.5535], grad_fn=<AddBackward0>)\n",
      "epoch: 13677 loss is tensor([-0.5999], grad_fn=<AddBackward0>)\n",
      "epoch: 13678 loss is tensor([-0.5391], grad_fn=<AddBackward0>)\n",
      "epoch: 13679 loss is tensor([-0.5602], grad_fn=<AddBackward0>)\n",
      "epoch: 13680 loss is tensor([-0.5729], grad_fn=<AddBackward0>)\n",
      "epoch: 13681 loss is tensor([-0.5764], grad_fn=<AddBackward0>)\n",
      "epoch: 13682 loss is tensor([-0.6278], grad_fn=<AddBackward0>)\n",
      "epoch: 13683 loss is tensor([-0.5992], grad_fn=<AddBackward0>)\n",
      "epoch: 13684 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 13685 loss is tensor([-0.4935], grad_fn=<AddBackward0>)\n",
      "epoch: 13686 loss is tensor([-0.5564], grad_fn=<AddBackward0>)\n",
      "epoch: 13687 loss is tensor([-0.5587], grad_fn=<AddBackward0>)\n",
      "epoch: 13688 loss is tensor([-0.5674], grad_fn=<AddBackward0>)\n",
      "epoch: 13689 loss is tensor([-0.5863], grad_fn=<AddBackward0>)\n",
      "epoch: 13690 loss is tensor([-0.6033], grad_fn=<AddBackward0>)\n",
      "epoch: 13691 loss is tensor([-0.6063], grad_fn=<AddBackward0>)\n",
      "epoch: 13692 loss is tensor([-0.5564], grad_fn=<AddBackward0>)\n",
      "epoch: 13693 loss is tensor([-0.5516], grad_fn=<AddBackward0>)\n",
      "epoch: 13694 loss is tensor([-0.5997], grad_fn=<AddBackward0>)\n",
      "epoch: 13695 loss is tensor([-0.6142], grad_fn=<AddBackward0>)\n",
      "epoch: 13696 loss is tensor([-0.5948], grad_fn=<AddBackward0>)\n",
      "epoch: 13697 loss is tensor([-0.5532], grad_fn=<AddBackward0>)\n",
      "epoch: 13698 loss is tensor([-0.5795], grad_fn=<AddBackward0>)\n",
      "epoch: 13699 loss is tensor([-0.5668], grad_fn=<AddBackward0>)\n",
      "epoch: 13700 loss is tensor([-0.5982], grad_fn=<AddBackward0>)\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13701 loss is tensor([-0.6169], grad_fn=<AddBackward0>)\n",
      "epoch: 13702 loss is tensor([-0.5997], grad_fn=<AddBackward0>)\n",
      "epoch: 13703 loss is tensor([-0.5719], grad_fn=<AddBackward0>)\n",
      "epoch: 13704 loss is tensor([-0.6106], grad_fn=<AddBackward0>)\n",
      "epoch: 13705 loss is tensor([-0.5877], grad_fn=<AddBackward0>)\n",
      "epoch: 13706 loss is tensor([-0.5418], grad_fn=<AddBackward0>)\n",
      "epoch: 13707 loss is tensor([-0.6077], grad_fn=<AddBackward0>)\n",
      "epoch: 13708 loss is tensor([-0.6348], grad_fn=<AddBackward0>)\n",
      "epoch: 13709 loss is tensor([-0.6282], grad_fn=<AddBackward0>)\n",
      "epoch: 13710 loss is tensor([-0.5719], grad_fn=<AddBackward0>)\n",
      "epoch: 13711 loss is tensor([-0.6163], grad_fn=<AddBackward0>)\n",
      "epoch: 13712 loss is tensor([-0.6309], grad_fn=<AddBackward0>)\n",
      "epoch: 13713 loss is tensor([-0.5497], grad_fn=<AddBackward0>)\n",
      "epoch: 13714 loss is tensor([-0.5472], grad_fn=<AddBackward0>)\n",
      "epoch: 13715 loss is tensor([-0.5787], grad_fn=<AddBackward0>)\n",
      "epoch: 13716 loss is tensor([-0.6071], grad_fn=<AddBackward0>)\n",
      "epoch: 13717 loss is tensor([-0.5748], grad_fn=<AddBackward0>)\n",
      "epoch: 13718 loss is tensor([-0.5875], grad_fn=<AddBackward0>)\n",
      "epoch: 13719 loss is tensor([-0.5990], grad_fn=<AddBackward0>)\n",
      "epoch: 13720 loss is tensor([-0.5168], grad_fn=<AddBackward0>)\n",
      "epoch: 13721 loss is tensor([-0.5532], grad_fn=<AddBackward0>)\n",
      "epoch: 13722 loss is tensor([-0.5918], grad_fn=<AddBackward0>)\n",
      "epoch: 13723 loss is tensor([-0.5169], grad_fn=<AddBackward0>)\n",
      "epoch: 13724 loss is tensor([-0.5812], grad_fn=<AddBackward0>)\n",
      "epoch: 13725 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 13726 loss is tensor([-0.5794], grad_fn=<AddBackward0>)\n",
      "epoch: 13727 loss is tensor([-0.5436], grad_fn=<AddBackward0>)\n",
      "epoch: 13728 loss is tensor([-0.5268], grad_fn=<AddBackward0>)\n",
      "epoch: 13729 loss is tensor([-0.5172], grad_fn=<AddBackward0>)\n",
      "epoch: 13730 loss is tensor([-0.5560], grad_fn=<AddBackward0>)\n",
      "epoch: 13731 loss is tensor([-0.5369], grad_fn=<AddBackward0>)\n",
      "epoch: 13732 loss is tensor([-0.5981], grad_fn=<AddBackward0>)\n",
      "epoch: 13733 loss is tensor([-0.5474], grad_fn=<AddBackward0>)\n",
      "epoch: 13734 loss is tensor([-0.5309], grad_fn=<AddBackward0>)\n",
      "epoch: 13735 loss is tensor([-0.5701], grad_fn=<AddBackward0>)\n",
      "epoch: 13736 loss is tensor([-0.5978], grad_fn=<AddBackward0>)\n",
      "epoch: 13737 loss is tensor([-0.5792], grad_fn=<AddBackward0>)\n",
      "epoch: 13738 loss is tensor([-0.5202], grad_fn=<AddBackward0>)\n",
      "epoch: 13739 loss is tensor([-0.5463], grad_fn=<AddBackward0>)\n",
      "epoch: 13740 loss is tensor([-0.5329], grad_fn=<AddBackward0>)\n",
      "epoch: 13741 loss is tensor([-0.5842], grad_fn=<AddBackward0>)\n",
      "epoch: 13742 loss is tensor([-0.5345], grad_fn=<AddBackward0>)\n",
      "epoch: 13743 loss is tensor([-0.6206], grad_fn=<AddBackward0>)\n",
      "epoch: 13744 loss is tensor([-0.5884], grad_fn=<AddBackward0>)\n",
      "epoch: 13745 loss is tensor([-0.5401], grad_fn=<AddBackward0>)\n",
      "epoch: 13746 loss is tensor([-0.5562], grad_fn=<AddBackward0>)\n",
      "epoch: 13747 loss is tensor([-0.5747], grad_fn=<AddBackward0>)\n",
      "epoch: 13748 loss is tensor([-0.5602], grad_fn=<AddBackward0>)\n",
      "epoch: 13749 loss is tensor([-0.6001], grad_fn=<AddBackward0>)\n",
      "epoch: 13750 loss is tensor([-0.5956], grad_fn=<AddBackward0>)\n",
      "epoch: 13751 loss is tensor([-0.5788], grad_fn=<AddBackward0>)\n",
      "epoch: 13752 loss is tensor([-0.5509], grad_fn=<AddBackward0>)\n",
      "epoch: 13753 loss is tensor([-0.6077], grad_fn=<AddBackward0>)\n",
      "epoch: 13754 loss is tensor([-0.6055], grad_fn=<AddBackward0>)\n",
      "epoch: 13755 loss is tensor([-0.6082], grad_fn=<AddBackward0>)\n",
      "epoch: 13756 loss is tensor([-0.5406], grad_fn=<AddBackward0>)\n",
      "epoch: 13757 loss is tensor([-0.5900], grad_fn=<AddBackward0>)\n",
      "epoch: 13758 loss is tensor([-0.5441], grad_fn=<AddBackward0>)\n",
      "epoch: 13759 loss is tensor([-0.5551], grad_fn=<AddBackward0>)\n",
      "epoch: 13760 loss is tensor([-0.5767], grad_fn=<AddBackward0>)\n",
      "epoch: 13761 loss is tensor([-0.6022], grad_fn=<AddBackward0>)\n",
      "epoch: 13762 loss is tensor([-0.5543], grad_fn=<AddBackward0>)\n",
      "epoch: 13763 loss is tensor([-0.5332], grad_fn=<AddBackward0>)\n",
      "epoch: 13764 loss is tensor([-0.6177], grad_fn=<AddBackward0>)\n",
      "epoch: 13765 loss is tensor([-0.5797], grad_fn=<AddBackward0>)\n",
      "epoch: 13766 loss is tensor([-0.5717], grad_fn=<AddBackward0>)\n",
      "epoch: 13767 loss is tensor([-0.5879], grad_fn=<AddBackward0>)\n",
      "epoch: 13768 loss is tensor([-0.5310], grad_fn=<AddBackward0>)\n",
      "epoch: 13769 loss is tensor([-0.5859], grad_fn=<AddBackward0>)\n",
      "epoch: 13770 loss is tensor([-0.5432], grad_fn=<AddBackward0>)\n",
      "epoch: 13771 loss is tensor([-0.5439], grad_fn=<AddBackward0>)\n",
      "epoch: 13772 loss is tensor([-0.6254], grad_fn=<AddBackward0>)\n",
      "epoch: 13773 loss is tensor([-0.5572], grad_fn=<AddBackward0>)\n",
      "epoch: 13774 loss is tensor([-0.6089], grad_fn=<AddBackward0>)\n",
      "epoch: 13775 loss is tensor([-0.5820], grad_fn=<AddBackward0>)\n",
      "epoch: 13776 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 13777 loss is tensor([-0.5632], grad_fn=<AddBackward0>)\n",
      "epoch: 13778 loss is tensor([-0.5720], grad_fn=<AddBackward0>)\n",
      "epoch: 13779 loss is tensor([-0.6256], grad_fn=<AddBackward0>)\n",
      "epoch: 13780 loss is tensor([-0.5806], grad_fn=<AddBackward0>)\n",
      "epoch: 13781 loss is tensor([-0.6016], grad_fn=<AddBackward0>)\n",
      "epoch: 13782 loss is tensor([-0.6412], grad_fn=<AddBackward0>)\n",
      "epoch: 13783 loss is tensor([-0.5867], grad_fn=<AddBackward0>)\n",
      "epoch: 13784 loss is tensor([-0.5958], grad_fn=<AddBackward0>)\n",
      "epoch: 13785 loss is tensor([-0.6566], grad_fn=<AddBackward0>)\n",
      "epoch: 13786 loss is tensor([-0.5836], grad_fn=<AddBackward0>)\n",
      "epoch: 13787 loss is tensor([-0.5834], grad_fn=<AddBackward0>)\n",
      "epoch: 13788 loss is tensor([-0.6184], grad_fn=<AddBackward0>)\n",
      "epoch: 13789 loss is tensor([-0.6140], grad_fn=<AddBackward0>)\n",
      "epoch: 13790 loss is tensor([-0.5471], grad_fn=<AddBackward0>)\n",
      "epoch: 13791 loss is tensor([-0.5692], grad_fn=<AddBackward0>)\n",
      "epoch: 13792 loss is tensor([-0.6434], grad_fn=<AddBackward0>)\n",
      "epoch: 13793 loss is tensor([-0.5541], grad_fn=<AddBackward0>)\n",
      "epoch: 13794 loss is tensor([-0.6024], grad_fn=<AddBackward0>)\n",
      "epoch: 13795 loss is tensor([-0.5654], grad_fn=<AddBackward0>)\n",
      "epoch: 13796 loss is tensor([-0.5845], grad_fn=<AddBackward0>)\n",
      "epoch: 13797 loss is tensor([-0.6098], grad_fn=<AddBackward0>)\n",
      "epoch: 13798 loss is tensor([-0.5932], grad_fn=<AddBackward0>)\n",
      "epoch: 13799 loss is tensor([-0.5604], grad_fn=<AddBackward0>)\n",
      "epoch: 13800 loss is tensor([-0.5379], grad_fn=<AddBackward0>)\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13801 loss is tensor([-0.6020], grad_fn=<AddBackward0>)\n",
      "epoch: 13802 loss is tensor([-0.6197], grad_fn=<AddBackward0>)\n",
      "epoch: 13803 loss is tensor([-0.6188], grad_fn=<AddBackward0>)\n",
      "epoch: 13804 loss is tensor([-0.5743], grad_fn=<AddBackward0>)\n",
      "epoch: 13805 loss is tensor([-0.5781], grad_fn=<AddBackward0>)\n",
      "epoch: 13806 loss is tensor([-0.5937], grad_fn=<AddBackward0>)\n",
      "epoch: 13807 loss is tensor([-0.6151], grad_fn=<AddBackward0>)\n",
      "epoch: 13808 loss is tensor([-0.5463], grad_fn=<AddBackward0>)\n",
      "epoch: 13809 loss is tensor([-0.5743], grad_fn=<AddBackward0>)\n",
      "epoch: 13810 loss is tensor([-0.5341], grad_fn=<AddBackward0>)\n",
      "epoch: 13811 loss is tensor([-0.5751], grad_fn=<AddBackward0>)\n",
      "epoch: 13812 loss is tensor([-0.5912], grad_fn=<AddBackward0>)\n",
      "epoch: 13813 loss is tensor([-0.5818], grad_fn=<AddBackward0>)\n",
      "epoch: 13814 loss is tensor([-0.5684], grad_fn=<AddBackward0>)\n",
      "epoch: 13815 loss is tensor([-0.5399], grad_fn=<AddBackward0>)\n",
      "epoch: 13816 loss is tensor([-0.5320], grad_fn=<AddBackward0>)\n",
      "epoch: 13817 loss is tensor([-0.5501], grad_fn=<AddBackward0>)\n",
      "epoch: 13818 loss is tensor([-0.5023], grad_fn=<AddBackward0>)\n",
      "epoch: 13819 loss is tensor([-0.5365], grad_fn=<AddBackward0>)\n",
      "epoch: 13820 loss is tensor([-0.4730], grad_fn=<AddBackward0>)\n",
      "epoch: 13821 loss is tensor([-0.5550], grad_fn=<AddBackward0>)\n",
      "epoch: 13822 loss is tensor([-0.5428], grad_fn=<AddBackward0>)\n",
      "epoch: 13823 loss is tensor([-0.5758], grad_fn=<AddBackward0>)\n",
      "epoch: 13824 loss is tensor([-0.5591], grad_fn=<AddBackward0>)\n",
      "epoch: 13825 loss is tensor([-0.5269], grad_fn=<AddBackward0>)\n",
      "epoch: 13826 loss is tensor([-0.5080], grad_fn=<AddBackward0>)\n",
      "epoch: 13827 loss is tensor([-0.5537], grad_fn=<AddBackward0>)\n",
      "epoch: 13828 loss is tensor([-0.5740], grad_fn=<AddBackward0>)\n",
      "epoch: 13829 loss is tensor([-0.6367], grad_fn=<AddBackward0>)\n",
      "epoch: 13830 loss is tensor([-0.5922], grad_fn=<AddBackward0>)\n",
      "epoch: 13831 loss is tensor([-0.6092], grad_fn=<AddBackward0>)\n",
      "epoch: 13832 loss is tensor([-0.6031], grad_fn=<AddBackward0>)\n",
      "epoch: 13833 loss is tensor([-0.5433], grad_fn=<AddBackward0>)\n",
      "epoch: 13834 loss is tensor([-0.5437], grad_fn=<AddBackward0>)\n",
      "epoch: 13835 loss is tensor([-0.5605], grad_fn=<AddBackward0>)\n",
      "epoch: 13836 loss is tensor([-0.5593], grad_fn=<AddBackward0>)\n",
      "epoch: 13837 loss is tensor([-0.4910], grad_fn=<AddBackward0>)\n",
      "epoch: 13838 loss is tensor([-0.5632], grad_fn=<AddBackward0>)\n",
      "epoch: 13839 loss is tensor([-0.5155], grad_fn=<AddBackward0>)\n",
      "epoch: 13840 loss is tensor([-0.5327], grad_fn=<AddBackward0>)\n",
      "epoch: 13841 loss is tensor([-0.5924], grad_fn=<AddBackward0>)\n",
      "epoch: 13842 loss is tensor([-0.5145], grad_fn=<AddBackward0>)\n",
      "epoch: 13843 loss is tensor([-0.5064], grad_fn=<AddBackward0>)\n",
      "epoch: 13844 loss is tensor([-0.5719], grad_fn=<AddBackward0>)\n",
      "epoch: 13845 loss is tensor([-0.5728], grad_fn=<AddBackward0>)\n",
      "epoch: 13846 loss is tensor([-0.5425], grad_fn=<AddBackward0>)\n",
      "epoch: 13847 loss is tensor([-0.5412], grad_fn=<AddBackward0>)\n",
      "epoch: 13848 loss is tensor([-0.6200], grad_fn=<AddBackward0>)\n",
      "epoch: 13849 loss is tensor([-0.5999], grad_fn=<AddBackward0>)\n",
      "epoch: 13850 loss is tensor([-0.5782], grad_fn=<AddBackward0>)\n",
      "epoch: 13851 loss is tensor([-0.5649], grad_fn=<AddBackward0>)\n",
      "epoch: 13852 loss is tensor([-0.6054], grad_fn=<AddBackward0>)\n",
      "epoch: 13853 loss is tensor([-0.5712], grad_fn=<AddBackward0>)\n",
      "epoch: 13854 loss is tensor([-0.5904], grad_fn=<AddBackward0>)\n",
      "epoch: 13855 loss is tensor([-0.6071], grad_fn=<AddBackward0>)\n",
      "epoch: 13856 loss is tensor([-0.5685], grad_fn=<AddBackward0>)\n",
      "epoch: 13857 loss is tensor([-0.6294], grad_fn=<AddBackward0>)\n",
      "epoch: 13858 loss is tensor([-0.5361], grad_fn=<AddBackward0>)\n",
      "epoch: 13859 loss is tensor([-0.5830], grad_fn=<AddBackward0>)\n",
      "epoch: 13860 loss is tensor([-0.5562], grad_fn=<AddBackward0>)\n",
      "epoch: 13861 loss is tensor([-0.4966], grad_fn=<AddBackward0>)\n",
      "epoch: 13862 loss is tensor([-0.5611], grad_fn=<AddBackward0>)\n",
      "epoch: 13863 loss is tensor([-0.5312], grad_fn=<AddBackward0>)\n",
      "epoch: 13864 loss is tensor([-0.5957], grad_fn=<AddBackward0>)\n",
      "epoch: 13865 loss is tensor([-0.5489], grad_fn=<AddBackward0>)\n",
      "epoch: 13866 loss is tensor([-0.5605], grad_fn=<AddBackward0>)\n",
      "epoch: 13867 loss is tensor([-0.5382], grad_fn=<AddBackward0>)\n",
      "epoch: 13868 loss is tensor([-0.5755], grad_fn=<AddBackward0>)\n",
      "epoch: 13869 loss is tensor([-0.4951], grad_fn=<AddBackward0>)\n",
      "epoch: 13870 loss is tensor([-0.5547], grad_fn=<AddBackward0>)\n",
      "epoch: 13871 loss is tensor([-0.5777], grad_fn=<AddBackward0>)\n",
      "epoch: 13872 loss is tensor([-0.4748], grad_fn=<AddBackward0>)\n",
      "epoch: 13873 loss is tensor([-0.5772], grad_fn=<AddBackward0>)\n",
      "epoch: 13874 loss is tensor([-0.5355], grad_fn=<AddBackward0>)\n",
      "epoch: 13875 loss is tensor([-0.5297], grad_fn=<AddBackward0>)\n",
      "epoch: 13876 loss is tensor([-0.5252], grad_fn=<AddBackward0>)\n",
      "epoch: 13877 loss is tensor([-0.4944], grad_fn=<AddBackward0>)\n",
      "epoch: 13878 loss is tensor([-0.5200], grad_fn=<AddBackward0>)\n",
      "epoch: 13879 loss is tensor([-0.5520], grad_fn=<AddBackward0>)\n",
      "epoch: 13880 loss is tensor([-0.5795], grad_fn=<AddBackward0>)\n",
      "epoch: 13881 loss is tensor([-0.5098], grad_fn=<AddBackward0>)\n",
      "epoch: 13882 loss is tensor([-0.5360], grad_fn=<AddBackward0>)\n",
      "epoch: 13883 loss is tensor([-0.5888], grad_fn=<AddBackward0>)\n",
      "epoch: 13884 loss is tensor([-0.5808], grad_fn=<AddBackward0>)\n",
      "epoch: 13885 loss is tensor([-0.5624], grad_fn=<AddBackward0>)\n",
      "epoch: 13886 loss is tensor([-0.5064], grad_fn=<AddBackward0>)\n",
      "epoch: 13887 loss is tensor([-0.4897], grad_fn=<AddBackward0>)\n",
      "epoch: 13888 loss is tensor([-0.5580], grad_fn=<AddBackward0>)\n",
      "epoch: 13889 loss is tensor([-0.5107], grad_fn=<AddBackward0>)\n",
      "epoch: 13890 loss is tensor([-0.5203], grad_fn=<AddBackward0>)\n",
      "epoch: 13891 loss is tensor([-0.5326], grad_fn=<AddBackward0>)\n",
      "epoch: 13892 loss is tensor([-0.5623], grad_fn=<AddBackward0>)\n",
      "epoch: 13893 loss is tensor([-0.5538], grad_fn=<AddBackward0>)\n",
      "epoch: 13894 loss is tensor([-0.5248], grad_fn=<AddBackward0>)\n",
      "epoch: 13895 loss is tensor([-0.5225], grad_fn=<AddBackward0>)\n",
      "epoch: 13896 loss is tensor([-0.5545], grad_fn=<AddBackward0>)\n",
      "epoch: 13897 loss is tensor([-0.5554], grad_fn=<AddBackward0>)\n",
      "epoch: 13898 loss is tensor([-0.5299], grad_fn=<AddBackward0>)\n",
      "epoch: 13899 loss is tensor([-0.5748], grad_fn=<AddBackward0>)\n",
      "epoch: 13900 loss is tensor([-0.5686], grad_fn=<AddBackward0>)\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 13901 loss is tensor([-0.5744], grad_fn=<AddBackward0>)\n",
      "epoch: 13902 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 13903 loss is tensor([-0.5632], grad_fn=<AddBackward0>)\n",
      "epoch: 13904 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 13905 loss is tensor([-0.5625], grad_fn=<AddBackward0>)\n",
      "epoch: 13906 loss is tensor([-0.5703], grad_fn=<AddBackward0>)\n",
      "epoch: 13907 loss is tensor([-0.5748], grad_fn=<AddBackward0>)\n",
      "epoch: 13908 loss is tensor([-0.5489], grad_fn=<AddBackward0>)\n",
      "epoch: 13909 loss is tensor([-0.5877], grad_fn=<AddBackward0>)\n",
      "epoch: 13910 loss is tensor([-0.6157], grad_fn=<AddBackward0>)\n",
      "epoch: 13911 loss is tensor([-0.5566], grad_fn=<AddBackward0>)\n",
      "epoch: 13912 loss is tensor([-0.6270], grad_fn=<AddBackward0>)\n",
      "epoch: 13913 loss is tensor([-0.5912], grad_fn=<AddBackward0>)\n",
      "epoch: 13914 loss is tensor([-0.5282], grad_fn=<AddBackward0>)\n",
      "epoch: 13915 loss is tensor([-0.5736], grad_fn=<AddBackward0>)\n",
      "epoch: 13916 loss is tensor([-0.5919], grad_fn=<AddBackward0>)\n",
      "epoch: 13917 loss is tensor([-0.5470], grad_fn=<AddBackward0>)\n",
      "epoch: 13918 loss is tensor([-0.5686], grad_fn=<AddBackward0>)\n",
      "epoch: 13919 loss is tensor([-0.6090], grad_fn=<AddBackward0>)\n",
      "epoch: 13920 loss is tensor([-0.5782], grad_fn=<AddBackward0>)\n",
      "epoch: 13921 loss is tensor([-0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 13922 loss is tensor([-0.5701], grad_fn=<AddBackward0>)\n",
      "epoch: 13923 loss is tensor([-0.5860], grad_fn=<AddBackward0>)\n",
      "epoch: 13924 loss is tensor([-0.5932], grad_fn=<AddBackward0>)\n",
      "epoch: 13925 loss is tensor([-0.6115], grad_fn=<AddBackward0>)\n",
      "epoch: 13926 loss is tensor([-0.6226], grad_fn=<AddBackward0>)\n",
      "epoch: 13927 loss is tensor([-0.5273], grad_fn=<AddBackward0>)\n",
      "epoch: 13928 loss is tensor([-0.5611], grad_fn=<AddBackward0>)\n",
      "epoch: 13929 loss is tensor([-0.5455], grad_fn=<AddBackward0>)\n",
      "epoch: 13930 loss is tensor([-0.5777], grad_fn=<AddBackward0>)\n",
      "epoch: 13931 loss is tensor([-0.5610], grad_fn=<AddBackward0>)\n",
      "epoch: 13932 loss is tensor([-0.5697], grad_fn=<AddBackward0>)\n",
      "epoch: 13933 loss is tensor([-0.5619], grad_fn=<AddBackward0>)\n",
      "epoch: 13934 loss is tensor([-0.4634], grad_fn=<AddBackward0>)\n",
      "epoch: 13935 loss is tensor([-0.6050], grad_fn=<AddBackward0>)\n",
      "epoch: 13936 loss is tensor([-0.5473], grad_fn=<AddBackward0>)\n",
      "epoch: 13937 loss is tensor([-0.5625], grad_fn=<AddBackward0>)\n",
      "epoch: 13938 loss is tensor([-0.5939], grad_fn=<AddBackward0>)\n",
      "epoch: 13939 loss is tensor([-0.5989], grad_fn=<AddBackward0>)\n",
      "epoch: 13940 loss is tensor([-0.5753], grad_fn=<AddBackward0>)\n",
      "epoch: 13941 loss is tensor([-0.5813], grad_fn=<AddBackward0>)\n",
      "epoch: 13942 loss is tensor([-0.5385], grad_fn=<AddBackward0>)\n",
      "epoch: 13943 loss is tensor([-0.5679], grad_fn=<AddBackward0>)\n",
      "epoch: 13944 loss is tensor([-0.5313], grad_fn=<AddBackward0>)\n",
      "epoch: 13945 loss is tensor([-0.6076], grad_fn=<AddBackward0>)\n",
      "epoch: 13946 loss is tensor([-0.5559], grad_fn=<AddBackward0>)\n",
      "epoch: 13947 loss is tensor([-0.5370], grad_fn=<AddBackward0>)\n",
      "epoch: 13948 loss is tensor([-0.5852], grad_fn=<AddBackward0>)\n",
      "epoch: 13949 loss is tensor([-0.5950], grad_fn=<AddBackward0>)\n",
      "epoch: 13950 loss is tensor([-0.5235], grad_fn=<AddBackward0>)\n",
      "epoch: 13951 loss is tensor([-0.5760], grad_fn=<AddBackward0>)\n",
      "epoch: 13952 loss is tensor([-0.5937], grad_fn=<AddBackward0>)\n",
      "epoch: 13953 loss is tensor([-0.5323], grad_fn=<AddBackward0>)\n",
      "epoch: 13954 loss is tensor([-0.5893], grad_fn=<AddBackward0>)\n",
      "epoch: 13955 loss is tensor([-0.5935], grad_fn=<AddBackward0>)\n",
      "epoch: 13956 loss is tensor([-0.5842], grad_fn=<AddBackward0>)\n",
      "epoch: 13957 loss is tensor([-0.5673], grad_fn=<AddBackward0>)\n",
      "epoch: 13958 loss is tensor([-0.5938], grad_fn=<AddBackward0>)\n",
      "epoch: 13959 loss is tensor([-0.5926], grad_fn=<AddBackward0>)\n",
      "epoch: 13960 loss is tensor([-0.5589], grad_fn=<AddBackward0>)\n",
      "epoch: 13961 loss is tensor([-0.5634], grad_fn=<AddBackward0>)\n",
      "epoch: 13962 loss is tensor([-0.5774], grad_fn=<AddBackward0>)\n",
      "epoch: 13963 loss is tensor([-0.5737], grad_fn=<AddBackward0>)\n",
      "epoch: 13964 loss is tensor([-0.5641], grad_fn=<AddBackward0>)\n",
      "epoch: 13965 loss is tensor([-0.5878], grad_fn=<AddBackward0>)\n",
      "epoch: 13966 loss is tensor([-0.5250], grad_fn=<AddBackward0>)\n",
      "epoch: 13967 loss is tensor([-0.6262], grad_fn=<AddBackward0>)\n",
      "epoch: 13968 loss is tensor([-0.6060], grad_fn=<AddBackward0>)\n",
      "epoch: 13969 loss is tensor([-0.6151], grad_fn=<AddBackward0>)\n",
      "epoch: 13970 loss is tensor([-0.5572], grad_fn=<AddBackward0>)\n",
      "epoch: 13971 loss is tensor([-0.5865], grad_fn=<AddBackward0>)\n",
      "epoch: 13972 loss is tensor([-0.5516], grad_fn=<AddBackward0>)\n",
      "epoch: 13973 loss is tensor([-0.6087], grad_fn=<AddBackward0>)\n",
      "epoch: 13974 loss is tensor([-0.5725], grad_fn=<AddBackward0>)\n",
      "epoch: 13975 loss is tensor([-0.5245], grad_fn=<AddBackward0>)\n",
      "epoch: 13976 loss is tensor([-0.5904], grad_fn=<AddBackward0>)\n",
      "epoch: 13977 loss is tensor([-0.5519], grad_fn=<AddBackward0>)\n",
      "epoch: 13978 loss is tensor([-0.5971], grad_fn=<AddBackward0>)\n",
      "epoch: 13979 loss is tensor([-0.5834], grad_fn=<AddBackward0>)\n",
      "epoch: 13980 loss is tensor([-0.5861], grad_fn=<AddBackward0>)\n",
      "epoch: 13981 loss is tensor([-0.5401], grad_fn=<AddBackward0>)\n",
      "epoch: 13982 loss is tensor([-0.6002], grad_fn=<AddBackward0>)\n",
      "epoch: 13983 loss is tensor([-0.6110], grad_fn=<AddBackward0>)\n",
      "epoch: 13984 loss is tensor([-0.5621], grad_fn=<AddBackward0>)\n",
      "epoch: 13985 loss is tensor([-0.5331], grad_fn=<AddBackward0>)\n",
      "epoch: 13986 loss is tensor([-0.5644], grad_fn=<AddBackward0>)\n",
      "epoch: 13987 loss is tensor([-0.6057], grad_fn=<AddBackward0>)\n",
      "epoch: 13988 loss is tensor([-0.6104], grad_fn=<AddBackward0>)\n",
      "epoch: 13989 loss is tensor([-0.5721], grad_fn=<AddBackward0>)\n",
      "epoch: 13990 loss is tensor([-0.6537], grad_fn=<AddBackward0>)\n",
      "epoch: 13991 loss is tensor([-0.5532], grad_fn=<AddBackward0>)\n",
      "epoch: 13992 loss is tensor([-0.5433], grad_fn=<AddBackward0>)\n",
      "epoch: 13993 loss is tensor([-0.5537], grad_fn=<AddBackward0>)\n",
      "epoch: 13994 loss is tensor([-0.5842], grad_fn=<AddBackward0>)\n",
      "epoch: 13995 loss is tensor([-0.5572], grad_fn=<AddBackward0>)\n",
      "epoch: 13996 loss is tensor([-0.6007], grad_fn=<AddBackward0>)\n",
      "epoch: 13997 loss is tensor([-0.5743], grad_fn=<AddBackward0>)\n",
      "epoch: 13998 loss is tensor([-0.5883], grad_fn=<AddBackward0>)\n",
      "epoch: 13999 loss is tensor([-0.5622], grad_fn=<AddBackward0>)\n",
      "epoch: 14000 loss is tensor([-0.5061], grad_fn=<AddBackward0>)\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14001 loss is tensor([-0.5830], grad_fn=<AddBackward0>)\n",
      "epoch: 14002 loss is tensor([-0.5846], grad_fn=<AddBackward0>)\n",
      "epoch: 14003 loss is tensor([-0.5637], grad_fn=<AddBackward0>)\n",
      "epoch: 14004 loss is tensor([-0.5954], grad_fn=<AddBackward0>)\n",
      "epoch: 14005 loss is tensor([-0.5571], grad_fn=<AddBackward0>)\n",
      "epoch: 14006 loss is tensor([-0.5707], grad_fn=<AddBackward0>)\n",
      "epoch: 14007 loss is tensor([-0.5028], grad_fn=<AddBackward0>)\n",
      "epoch: 14008 loss is tensor([-0.6046], grad_fn=<AddBackward0>)\n",
      "epoch: 14009 loss is tensor([-0.5654], grad_fn=<AddBackward0>)\n",
      "epoch: 14010 loss is tensor([-0.6144], grad_fn=<AddBackward0>)\n",
      "epoch: 14011 loss is tensor([-0.5772], grad_fn=<AddBackward0>)\n",
      "epoch: 14012 loss is tensor([-0.5885], grad_fn=<AddBackward0>)\n",
      "epoch: 14013 loss is tensor([-0.5991], grad_fn=<AddBackward0>)\n",
      "epoch: 14014 loss is tensor([-0.6229], grad_fn=<AddBackward0>)\n",
      "epoch: 14015 loss is tensor([-0.5562], grad_fn=<AddBackward0>)\n",
      "epoch: 14016 loss is tensor([-0.5900], grad_fn=<AddBackward0>)\n",
      "epoch: 14017 loss is tensor([-0.5274], grad_fn=<AddBackward0>)\n",
      "epoch: 14018 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "epoch: 14019 loss is tensor([-0.6157], grad_fn=<AddBackward0>)\n",
      "epoch: 14020 loss is tensor([-0.5290], grad_fn=<AddBackward0>)\n",
      "epoch: 14021 loss is tensor([-0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 14022 loss is tensor([-0.4911], grad_fn=<AddBackward0>)\n",
      "epoch: 14023 loss is tensor([-0.5618], grad_fn=<AddBackward0>)\n",
      "epoch: 14024 loss is tensor([-0.5831], grad_fn=<AddBackward0>)\n",
      "epoch: 14025 loss is tensor([-0.5782], grad_fn=<AddBackward0>)\n",
      "epoch: 14026 loss is tensor([-0.5590], grad_fn=<AddBackward0>)\n",
      "epoch: 14027 loss is tensor([-0.5855], grad_fn=<AddBackward0>)\n",
      "epoch: 14028 loss is tensor([-0.5666], grad_fn=<AddBackward0>)\n",
      "epoch: 14029 loss is tensor([-0.5316], grad_fn=<AddBackward0>)\n",
      "epoch: 14030 loss is tensor([-0.5066], grad_fn=<AddBackward0>)\n",
      "epoch: 14031 loss is tensor([-0.5154], grad_fn=<AddBackward0>)\n",
      "epoch: 14032 loss is tensor([-0.5511], grad_fn=<AddBackward0>)\n",
      "epoch: 14033 loss is tensor([-0.5801], grad_fn=<AddBackward0>)\n",
      "epoch: 14034 loss is tensor([-0.5526], grad_fn=<AddBackward0>)\n",
      "epoch: 14035 loss is tensor([-0.5226], grad_fn=<AddBackward0>)\n",
      "epoch: 14036 loss is tensor([-0.5347], grad_fn=<AddBackward0>)\n",
      "epoch: 14037 loss is tensor([-0.5483], grad_fn=<AddBackward0>)\n",
      "epoch: 14038 loss is tensor([-0.5481], grad_fn=<AddBackward0>)\n",
      "epoch: 14039 loss is tensor([-0.5528], grad_fn=<AddBackward0>)\n",
      "epoch: 14040 loss is tensor([-0.5814], grad_fn=<AddBackward0>)\n",
      "epoch: 14041 loss is tensor([-0.5601], grad_fn=<AddBackward0>)\n",
      "epoch: 14042 loss is tensor([-0.5506], grad_fn=<AddBackward0>)\n",
      "epoch: 14043 loss is tensor([-0.6273], grad_fn=<AddBackward0>)\n",
      "epoch: 14044 loss is tensor([-0.5499], grad_fn=<AddBackward0>)\n",
      "epoch: 14045 loss is tensor([-0.5546], grad_fn=<AddBackward0>)\n",
      "epoch: 14046 loss is tensor([-0.5558], grad_fn=<AddBackward0>)\n",
      "epoch: 14047 loss is tensor([-0.5066], grad_fn=<AddBackward0>)\n",
      "epoch: 14048 loss is tensor([-0.5969], grad_fn=<AddBackward0>)\n",
      "epoch: 14049 loss is tensor([-0.5608], grad_fn=<AddBackward0>)\n",
      "epoch: 14050 loss is tensor([-0.5599], grad_fn=<AddBackward0>)\n",
      "epoch: 14051 loss is tensor([-0.5922], grad_fn=<AddBackward0>)\n",
      "epoch: 14052 loss is tensor([-0.5572], grad_fn=<AddBackward0>)\n",
      "epoch: 14053 loss is tensor([-0.5751], grad_fn=<AddBackward0>)\n",
      "epoch: 14054 loss is tensor([-0.5488], grad_fn=<AddBackward0>)\n",
      "epoch: 14055 loss is tensor([-0.6206], grad_fn=<AddBackward0>)\n",
      "epoch: 14056 loss is tensor([-0.6097], grad_fn=<AddBackward0>)\n",
      "epoch: 14057 loss is tensor([-0.5594], grad_fn=<AddBackward0>)\n",
      "epoch: 14058 loss is tensor([-0.5663], grad_fn=<AddBackward0>)\n",
      "epoch: 14059 loss is tensor([-0.5403], grad_fn=<AddBackward0>)\n",
      "epoch: 14060 loss is tensor([-0.5792], grad_fn=<AddBackward0>)\n",
      "epoch: 14061 loss is tensor([-0.5710], grad_fn=<AddBackward0>)\n",
      "epoch: 14062 loss is tensor([-0.5398], grad_fn=<AddBackward0>)\n",
      "epoch: 14063 loss is tensor([-0.5525], grad_fn=<AddBackward0>)\n",
      "epoch: 14064 loss is tensor([-0.5682], grad_fn=<AddBackward0>)\n",
      "epoch: 14065 loss is tensor([-0.5793], grad_fn=<AddBackward0>)\n",
      "epoch: 14066 loss is tensor([-0.5770], grad_fn=<AddBackward0>)\n",
      "epoch: 14067 loss is tensor([-0.6175], grad_fn=<AddBackward0>)\n",
      "epoch: 14068 loss is tensor([-0.5859], grad_fn=<AddBackward0>)\n",
      "epoch: 14069 loss is tensor([-0.5380], grad_fn=<AddBackward0>)\n",
      "epoch: 14070 loss is tensor([-0.6284], grad_fn=<AddBackward0>)\n",
      "epoch: 14071 loss is tensor([-0.5939], grad_fn=<AddBackward0>)\n",
      "epoch: 14072 loss is tensor([-0.5281], grad_fn=<AddBackward0>)\n",
      "epoch: 14073 loss is tensor([-0.6269], grad_fn=<AddBackward0>)\n",
      "epoch: 14074 loss is tensor([-0.6283], grad_fn=<AddBackward0>)\n",
      "epoch: 14075 loss is tensor([-0.5372], grad_fn=<AddBackward0>)\n",
      "epoch: 14076 loss is tensor([-0.6290], grad_fn=<AddBackward0>)\n",
      "epoch: 14077 loss is tensor([-0.5892], grad_fn=<AddBackward0>)\n",
      "epoch: 14078 loss is tensor([-0.6293], grad_fn=<AddBackward0>)\n",
      "epoch: 14079 loss is tensor([-0.6293], grad_fn=<AddBackward0>)\n",
      "epoch: 14080 loss is tensor([-0.6224], grad_fn=<AddBackward0>)\n",
      "epoch: 14081 loss is tensor([-0.6128], grad_fn=<AddBackward0>)\n",
      "epoch: 14082 loss is tensor([-0.5918], grad_fn=<AddBackward0>)\n",
      "epoch: 14083 loss is tensor([-0.5167], grad_fn=<AddBackward0>)\n",
      "epoch: 14084 loss is tensor([-0.6290], grad_fn=<AddBackward0>)\n",
      "epoch: 14085 loss is tensor([-0.5978], grad_fn=<AddBackward0>)\n",
      "epoch: 14086 loss is tensor([-0.5673], grad_fn=<AddBackward0>)\n",
      "epoch: 14087 loss is tensor([-0.5987], grad_fn=<AddBackward0>)\n",
      "epoch: 14088 loss is tensor([-0.5814], grad_fn=<AddBackward0>)\n",
      "epoch: 14089 loss is tensor([-0.5794], grad_fn=<AddBackward0>)\n",
      "epoch: 14090 loss is tensor([-0.5693], grad_fn=<AddBackward0>)\n",
      "epoch: 14091 loss is tensor([-0.5378], grad_fn=<AddBackward0>)\n",
      "epoch: 14092 loss is tensor([-0.5521], grad_fn=<AddBackward0>)\n",
      "epoch: 14093 loss is tensor([-0.5122], grad_fn=<AddBackward0>)\n",
      "epoch: 14094 loss is tensor([-0.5414], grad_fn=<AddBackward0>)\n",
      "epoch: 14095 loss is tensor([-0.5939], grad_fn=<AddBackward0>)\n",
      "epoch: 14096 loss is tensor([-0.5680], grad_fn=<AddBackward0>)\n",
      "epoch: 14097 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 14098 loss is tensor([-0.6124], grad_fn=<AddBackward0>)\n",
      "epoch: 14099 loss is tensor([-0.5641], grad_fn=<AddBackward0>)\n",
      "epoch: 14100 loss is tensor([-0.5576], grad_fn=<AddBackward0>)\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14101 loss is tensor([-0.5990], grad_fn=<AddBackward0>)\n",
      "epoch: 14102 loss is tensor([-0.5926], grad_fn=<AddBackward0>)\n",
      "epoch: 14103 loss is tensor([-0.5439], grad_fn=<AddBackward0>)\n",
      "epoch: 14104 loss is tensor([-0.5860], grad_fn=<AddBackward0>)\n",
      "epoch: 14105 loss is tensor([-0.6190], grad_fn=<AddBackward0>)\n",
      "epoch: 14106 loss is tensor([-0.5792], grad_fn=<AddBackward0>)\n",
      "epoch: 14107 loss is tensor([-0.5455], grad_fn=<AddBackward0>)\n",
      "epoch: 14108 loss is tensor([-0.5475], grad_fn=<AddBackward0>)\n",
      "epoch: 14109 loss is tensor([-0.5845], grad_fn=<AddBackward0>)\n",
      "epoch: 14110 loss is tensor([-0.5596], grad_fn=<AddBackward0>)\n",
      "epoch: 14111 loss is tensor([-0.5721], grad_fn=<AddBackward0>)\n",
      "epoch: 14112 loss is tensor([-0.4959], grad_fn=<AddBackward0>)\n",
      "epoch: 14113 loss is tensor([-0.5625], grad_fn=<AddBackward0>)\n",
      "epoch: 14114 loss is tensor([-0.5446], grad_fn=<AddBackward0>)\n",
      "epoch: 14115 loss is tensor([-0.4875], grad_fn=<AddBackward0>)\n",
      "epoch: 14116 loss is tensor([-0.5637], grad_fn=<AddBackward0>)\n",
      "epoch: 14117 loss is tensor([-0.5314], grad_fn=<AddBackward0>)\n",
      "epoch: 14118 loss is tensor([-0.5614], grad_fn=<AddBackward0>)\n",
      "epoch: 14119 loss is tensor([-0.5339], grad_fn=<AddBackward0>)\n",
      "epoch: 14120 loss is tensor([-0.5645], grad_fn=<AddBackward0>)\n",
      "epoch: 14121 loss is tensor([-0.5838], grad_fn=<AddBackward0>)\n",
      "epoch: 14122 loss is tensor([-0.5110], grad_fn=<AddBackward0>)\n",
      "epoch: 14123 loss is tensor([-0.5759], grad_fn=<AddBackward0>)\n",
      "epoch: 14124 loss is tensor([-0.5551], grad_fn=<AddBackward0>)\n",
      "epoch: 14125 loss is tensor([-0.5606], grad_fn=<AddBackward0>)\n",
      "epoch: 14126 loss is tensor([-0.5057], grad_fn=<AddBackward0>)\n",
      "epoch: 14127 loss is tensor([-0.6134], grad_fn=<AddBackward0>)\n",
      "epoch: 14128 loss is tensor([-0.5618], grad_fn=<AddBackward0>)\n",
      "epoch: 14129 loss is tensor([-0.5846], grad_fn=<AddBackward0>)\n",
      "epoch: 14130 loss is tensor([-0.5059], grad_fn=<AddBackward0>)\n",
      "epoch: 14131 loss is tensor([-0.5834], grad_fn=<AddBackward0>)\n",
      "epoch: 14132 loss is tensor([-0.5519], grad_fn=<AddBackward0>)\n",
      "epoch: 14133 loss is tensor([-0.5218], grad_fn=<AddBackward0>)\n",
      "epoch: 14134 loss is tensor([-0.5229], grad_fn=<AddBackward0>)\n",
      "epoch: 14135 loss is tensor([-0.6102], grad_fn=<AddBackward0>)\n",
      "epoch: 14136 loss is tensor([-0.5052], grad_fn=<AddBackward0>)\n",
      "epoch: 14137 loss is tensor([-0.5754], grad_fn=<AddBackward0>)\n",
      "epoch: 14138 loss is tensor([-0.5700], grad_fn=<AddBackward0>)\n",
      "epoch: 14139 loss is tensor([-0.5378], grad_fn=<AddBackward0>)\n",
      "epoch: 14140 loss is tensor([-0.5425], grad_fn=<AddBackward0>)\n",
      "epoch: 14141 loss is tensor([-0.5944], grad_fn=<AddBackward0>)\n",
      "epoch: 14142 loss is tensor([-0.5309], grad_fn=<AddBackward0>)\n",
      "epoch: 14143 loss is tensor([-0.5983], grad_fn=<AddBackward0>)\n",
      "epoch: 14144 loss is tensor([-0.5531], grad_fn=<AddBackward0>)\n",
      "epoch: 14145 loss is tensor([-0.5910], grad_fn=<AddBackward0>)\n",
      "epoch: 14146 loss is tensor([-0.5928], grad_fn=<AddBackward0>)\n",
      "epoch: 14147 loss is tensor([-0.6305], grad_fn=<AddBackward0>)\n",
      "epoch: 14148 loss is tensor([-0.5797], grad_fn=<AddBackward0>)\n",
      "epoch: 14149 loss is tensor([-0.5945], grad_fn=<AddBackward0>)\n",
      "epoch: 14150 loss is tensor([-0.5858], grad_fn=<AddBackward0>)\n",
      "epoch: 14151 loss is tensor([-0.5908], grad_fn=<AddBackward0>)\n",
      "epoch: 14152 loss is tensor([-0.6044], grad_fn=<AddBackward0>)\n",
      "epoch: 14153 loss is tensor([-0.5527], grad_fn=<AddBackward0>)\n",
      "epoch: 14154 loss is tensor([-0.5719], grad_fn=<AddBackward0>)\n",
      "epoch: 14155 loss is tensor([-0.5784], grad_fn=<AddBackward0>)\n",
      "epoch: 14156 loss is tensor([-0.5821], grad_fn=<AddBackward0>)\n",
      "epoch: 14157 loss is tensor([-0.5611], grad_fn=<AddBackward0>)\n",
      "epoch: 14158 loss is tensor([-0.5622], grad_fn=<AddBackward0>)\n",
      "epoch: 14159 loss is tensor([-0.5318], grad_fn=<AddBackward0>)\n",
      "epoch: 14160 loss is tensor([-0.5550], grad_fn=<AddBackward0>)\n",
      "epoch: 14161 loss is tensor([-0.5723], grad_fn=<AddBackward0>)\n",
      "epoch: 14162 loss is tensor([-0.5622], grad_fn=<AddBackward0>)\n",
      "epoch: 14163 loss is tensor([-0.5836], grad_fn=<AddBackward0>)\n",
      "epoch: 14164 loss is tensor([-0.5905], grad_fn=<AddBackward0>)\n",
      "epoch: 14165 loss is tensor([-0.5578], grad_fn=<AddBackward0>)\n",
      "epoch: 14166 loss is tensor([-0.5548], grad_fn=<AddBackward0>)\n",
      "epoch: 14167 loss is tensor([-0.6134], grad_fn=<AddBackward0>)\n",
      "epoch: 14168 loss is tensor([-0.5702], grad_fn=<AddBackward0>)\n",
      "epoch: 14169 loss is tensor([-0.5848], grad_fn=<AddBackward0>)\n",
      "epoch: 14170 loss is tensor([-0.5036], grad_fn=<AddBackward0>)\n",
      "epoch: 14171 loss is tensor([-0.5718], grad_fn=<AddBackward0>)\n",
      "epoch: 14172 loss is tensor([-0.5314], grad_fn=<AddBackward0>)\n",
      "epoch: 14173 loss is tensor([-0.5320], grad_fn=<AddBackward0>)\n",
      "epoch: 14174 loss is tensor([-0.5991], grad_fn=<AddBackward0>)\n",
      "epoch: 14175 loss is tensor([-0.5488], grad_fn=<AddBackward0>)\n",
      "epoch: 14176 loss is tensor([-0.5258], grad_fn=<AddBackward0>)\n",
      "epoch: 14177 loss is tensor([-0.5277], grad_fn=<AddBackward0>)\n",
      "epoch: 14178 loss is tensor([-0.5154], grad_fn=<AddBackward0>)\n",
      "epoch: 14179 loss is tensor([-0.5373], grad_fn=<AddBackward0>)\n",
      "epoch: 14180 loss is tensor([-0.5304], grad_fn=<AddBackward0>)\n",
      "epoch: 14181 loss is tensor([-0.4743], grad_fn=<AddBackward0>)\n",
      "epoch: 14182 loss is tensor([-0.5760], grad_fn=<AddBackward0>)\n",
      "epoch: 14183 loss is tensor([-0.5518], grad_fn=<AddBackward0>)\n",
      "epoch: 14184 loss is tensor([-0.5585], grad_fn=<AddBackward0>)\n",
      "epoch: 14185 loss is tensor([-0.5106], grad_fn=<AddBackward0>)\n",
      "epoch: 14186 loss is tensor([-0.4805], grad_fn=<AddBackward0>)\n",
      "epoch: 14187 loss is tensor([-0.5165], grad_fn=<AddBackward0>)\n",
      "epoch: 14188 loss is tensor([-0.5669], grad_fn=<AddBackward0>)\n",
      "epoch: 14189 loss is tensor([-0.5397], grad_fn=<AddBackward0>)\n",
      "epoch: 14190 loss is tensor([-0.5292], grad_fn=<AddBackward0>)\n",
      "epoch: 14191 loss is tensor([-0.5403], grad_fn=<AddBackward0>)\n",
      "epoch: 14192 loss is tensor([-0.5004], grad_fn=<AddBackward0>)\n",
      "epoch: 14193 loss is tensor([-0.5987], grad_fn=<AddBackward0>)\n",
      "epoch: 14194 loss is tensor([-0.5481], grad_fn=<AddBackward0>)\n",
      "epoch: 14195 loss is tensor([-0.5553], grad_fn=<AddBackward0>)\n",
      "epoch: 14196 loss is tensor([-0.5216], grad_fn=<AddBackward0>)\n",
      "epoch: 14197 loss is tensor([-0.4999], grad_fn=<AddBackward0>)\n",
      "epoch: 14198 loss is tensor([-0.5100], grad_fn=<AddBackward0>)\n",
      "epoch: 14199 loss is tensor([-0.5639], grad_fn=<AddBackward0>)\n",
      "epoch: 14200 loss is tensor([-0.5280], grad_fn=<AddBackward0>)\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14201 loss is tensor([-0.5457], grad_fn=<AddBackward0>)\n",
      "epoch: 14202 loss is tensor([-0.5378], grad_fn=<AddBackward0>)\n",
      "epoch: 14203 loss is tensor([-0.5435], grad_fn=<AddBackward0>)\n",
      "epoch: 14204 loss is tensor([-0.5576], grad_fn=<AddBackward0>)\n",
      "epoch: 14205 loss is tensor([-0.5189], grad_fn=<AddBackward0>)\n",
      "epoch: 14206 loss is tensor([-0.5715], grad_fn=<AddBackward0>)\n",
      "epoch: 14207 loss is tensor([-0.5644], grad_fn=<AddBackward0>)\n",
      "epoch: 14208 loss is tensor([-0.5545], grad_fn=<AddBackward0>)\n",
      "epoch: 14209 loss is tensor([-0.5891], grad_fn=<AddBackward0>)\n",
      "epoch: 14210 loss is tensor([-0.4721], grad_fn=<AddBackward0>)\n",
      "epoch: 14211 loss is tensor([-0.5478], grad_fn=<AddBackward0>)\n",
      "epoch: 14212 loss is tensor([-0.5460], grad_fn=<AddBackward0>)\n",
      "epoch: 14213 loss is tensor([-0.5298], grad_fn=<AddBackward0>)\n",
      "epoch: 14214 loss is tensor([-0.5058], grad_fn=<AddBackward0>)\n",
      "epoch: 14215 loss is tensor([-0.5661], grad_fn=<AddBackward0>)\n",
      "epoch: 14216 loss is tensor([-0.5407], grad_fn=<AddBackward0>)\n",
      "epoch: 14217 loss is tensor([-0.5742], grad_fn=<AddBackward0>)\n",
      "epoch: 14218 loss is tensor([-0.5903], grad_fn=<AddBackward0>)\n",
      "epoch: 14219 loss is tensor([-0.5330], grad_fn=<AddBackward0>)\n",
      "epoch: 14220 loss is tensor([-0.5881], grad_fn=<AddBackward0>)\n",
      "epoch: 14221 loss is tensor([-0.5821], grad_fn=<AddBackward0>)\n",
      "epoch: 14222 loss is tensor([-0.5610], grad_fn=<AddBackward0>)\n",
      "epoch: 14223 loss is tensor([-0.5983], grad_fn=<AddBackward0>)\n",
      "epoch: 14224 loss is tensor([-0.5470], grad_fn=<AddBackward0>)\n",
      "epoch: 14225 loss is tensor([-0.5675], grad_fn=<AddBackward0>)\n",
      "epoch: 14226 loss is tensor([-0.5647], grad_fn=<AddBackward0>)\n",
      "epoch: 14227 loss is tensor([-0.5585], grad_fn=<AddBackward0>)\n",
      "epoch: 14228 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 14229 loss is tensor([-0.5256], grad_fn=<AddBackward0>)\n",
      "epoch: 14230 loss is tensor([-0.5939], grad_fn=<AddBackward0>)\n",
      "epoch: 14231 loss is tensor([-0.5888], grad_fn=<AddBackward0>)\n",
      "epoch: 14232 loss is tensor([-0.6359], grad_fn=<AddBackward0>)\n",
      "epoch: 14233 loss is tensor([-0.5915], grad_fn=<AddBackward0>)\n",
      "epoch: 14234 loss is tensor([-0.5451], grad_fn=<AddBackward0>)\n",
      "epoch: 14235 loss is tensor([-0.5389], grad_fn=<AddBackward0>)\n",
      "epoch: 14236 loss is tensor([-0.5690], grad_fn=<AddBackward0>)\n",
      "epoch: 14237 loss is tensor([-0.6015], grad_fn=<AddBackward0>)\n",
      "epoch: 14238 loss is tensor([-0.5722], grad_fn=<AddBackward0>)\n",
      "epoch: 14239 loss is tensor([-0.5351], grad_fn=<AddBackward0>)\n",
      "epoch: 14240 loss is tensor([-0.5217], grad_fn=<AddBackward0>)\n",
      "epoch: 14241 loss is tensor([-0.5214], grad_fn=<AddBackward0>)\n",
      "epoch: 14242 loss is tensor([-0.5348], grad_fn=<AddBackward0>)\n",
      "epoch: 14243 loss is tensor([-0.5621], grad_fn=<AddBackward0>)\n",
      "epoch: 14244 loss is tensor([-0.6188], grad_fn=<AddBackward0>)\n",
      "epoch: 14245 loss is tensor([-0.5779], grad_fn=<AddBackward0>)\n",
      "epoch: 14246 loss is tensor([-0.5779], grad_fn=<AddBackward0>)\n",
      "epoch: 14247 loss is tensor([-0.5976], grad_fn=<AddBackward0>)\n",
      "epoch: 14248 loss is tensor([-0.5892], grad_fn=<AddBackward0>)\n",
      "epoch: 14249 loss is tensor([-0.5343], grad_fn=<AddBackward0>)\n",
      "epoch: 14250 loss is tensor([-0.5222], grad_fn=<AddBackward0>)\n",
      "epoch: 14251 loss is tensor([-0.6006], grad_fn=<AddBackward0>)\n",
      "epoch: 14252 loss is tensor([-0.5503], grad_fn=<AddBackward0>)\n",
      "epoch: 14253 loss is tensor([-0.5650], grad_fn=<AddBackward0>)\n",
      "epoch: 14254 loss is tensor([-0.5975], grad_fn=<AddBackward0>)\n",
      "epoch: 14255 loss is tensor([-0.5767], grad_fn=<AddBackward0>)\n",
      "epoch: 14256 loss is tensor([-0.5981], grad_fn=<AddBackward0>)\n",
      "epoch: 14257 loss is tensor([-0.5717], grad_fn=<AddBackward0>)\n",
      "epoch: 14258 loss is tensor([-0.5815], grad_fn=<AddBackward0>)\n",
      "epoch: 14259 loss is tensor([-0.5607], grad_fn=<AddBackward0>)\n",
      "epoch: 14260 loss is tensor([-0.5753], grad_fn=<AddBackward0>)\n",
      "epoch: 14261 loss is tensor([-0.5817], grad_fn=<AddBackward0>)\n",
      "epoch: 14262 loss is tensor([-0.5970], grad_fn=<AddBackward0>)\n",
      "epoch: 14263 loss is tensor([-0.5518], grad_fn=<AddBackward0>)\n",
      "epoch: 14264 loss is tensor([-0.6178], grad_fn=<AddBackward0>)\n",
      "epoch: 14265 loss is tensor([-0.5734], grad_fn=<AddBackward0>)\n",
      "epoch: 14266 loss is tensor([-0.5684], grad_fn=<AddBackward0>)\n",
      "epoch: 14267 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 14268 loss is tensor([-0.5767], grad_fn=<AddBackward0>)\n",
      "epoch: 14269 loss is tensor([-0.5905], grad_fn=<AddBackward0>)\n",
      "epoch: 14270 loss is tensor([-0.5524], grad_fn=<AddBackward0>)\n",
      "epoch: 14271 loss is tensor([-0.5681], grad_fn=<AddBackward0>)\n",
      "epoch: 14272 loss is tensor([-0.4893], grad_fn=<AddBackward0>)\n",
      "epoch: 14273 loss is tensor([-0.5039], grad_fn=<AddBackward0>)\n",
      "epoch: 14274 loss is tensor([-0.5367], grad_fn=<AddBackward0>)\n",
      "epoch: 14275 loss is tensor([-0.4926], grad_fn=<AddBackward0>)\n",
      "epoch: 14276 loss is tensor([-0.5320], grad_fn=<AddBackward0>)\n",
      "epoch: 14277 loss is tensor([-0.5969], grad_fn=<AddBackward0>)\n",
      "epoch: 14278 loss is tensor([-0.5153], grad_fn=<AddBackward0>)\n",
      "epoch: 14279 loss is tensor([-0.5407], grad_fn=<AddBackward0>)\n",
      "epoch: 14280 loss is tensor([-0.5964], grad_fn=<AddBackward0>)\n",
      "epoch: 14281 loss is tensor([-0.5627], grad_fn=<AddBackward0>)\n",
      "epoch: 14282 loss is tensor([-0.5337], grad_fn=<AddBackward0>)\n",
      "epoch: 14283 loss is tensor([-0.5345], grad_fn=<AddBackward0>)\n",
      "epoch: 14284 loss is tensor([-0.6041], grad_fn=<AddBackward0>)\n",
      "epoch: 14285 loss is tensor([-0.5745], grad_fn=<AddBackward0>)\n",
      "epoch: 14286 loss is tensor([-0.5911], grad_fn=<AddBackward0>)\n",
      "epoch: 14287 loss is tensor([-0.6286], grad_fn=<AddBackward0>)\n",
      "epoch: 14288 loss is tensor([-0.5614], grad_fn=<AddBackward0>)\n",
      "epoch: 14289 loss is tensor([-0.5435], grad_fn=<AddBackward0>)\n",
      "epoch: 14290 loss is tensor([-0.5695], grad_fn=<AddBackward0>)\n",
      "epoch: 14291 loss is tensor([-0.6114], grad_fn=<AddBackward0>)\n",
      "epoch: 14292 loss is tensor([-0.5969], grad_fn=<AddBackward0>)\n",
      "epoch: 14293 loss is tensor([-0.5906], grad_fn=<AddBackward0>)\n",
      "epoch: 14294 loss is tensor([-0.6355], grad_fn=<AddBackward0>)\n",
      "epoch: 14295 loss is tensor([-0.5868], grad_fn=<AddBackward0>)\n",
      "epoch: 14296 loss is tensor([-0.5831], grad_fn=<AddBackward0>)\n",
      "epoch: 14297 loss is tensor([-0.6144], grad_fn=<AddBackward0>)\n",
      "epoch: 14298 loss is tensor([-0.5662], grad_fn=<AddBackward0>)\n",
      "epoch: 14299 loss is tensor([-0.5797], grad_fn=<AddBackward0>)\n",
      "epoch: 14300 loss is tensor([-0.5711], grad_fn=<AddBackward0>)\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14301 loss is tensor([-0.5428], grad_fn=<AddBackward0>)\n",
      "epoch: 14302 loss is tensor([-0.5853], grad_fn=<AddBackward0>)\n",
      "epoch: 14303 loss is tensor([-0.5700], grad_fn=<AddBackward0>)\n",
      "epoch: 14304 loss is tensor([-0.6059], grad_fn=<AddBackward0>)\n",
      "epoch: 14305 loss is tensor([-0.5754], grad_fn=<AddBackward0>)\n",
      "epoch: 14306 loss is tensor([-0.5771], grad_fn=<AddBackward0>)\n",
      "epoch: 14307 loss is tensor([-0.5981], grad_fn=<AddBackward0>)\n",
      "epoch: 14308 loss is tensor([-0.4979], grad_fn=<AddBackward0>)\n",
      "epoch: 14309 loss is tensor([-0.5642], grad_fn=<AddBackward0>)\n",
      "epoch: 14310 loss is tensor([-0.6016], grad_fn=<AddBackward0>)\n",
      "epoch: 14311 loss is tensor([-0.5673], grad_fn=<AddBackward0>)\n",
      "epoch: 14312 loss is tensor([-0.5726], grad_fn=<AddBackward0>)\n",
      "epoch: 14313 loss is tensor([-0.5982], grad_fn=<AddBackward0>)\n",
      "epoch: 14314 loss is tensor([-0.6126], grad_fn=<AddBackward0>)\n",
      "epoch: 14315 loss is tensor([-0.5518], grad_fn=<AddBackward0>)\n",
      "epoch: 14316 loss is tensor([-0.5543], grad_fn=<AddBackward0>)\n",
      "epoch: 14317 loss is tensor([-0.5907], grad_fn=<AddBackward0>)\n",
      "epoch: 14318 loss is tensor([-0.6054], grad_fn=<AddBackward0>)\n",
      "epoch: 14319 loss is tensor([-0.6374], grad_fn=<AddBackward0>)\n",
      "epoch: 14320 loss is tensor([-0.6020], grad_fn=<AddBackward0>)\n",
      "epoch: 14321 loss is tensor([-0.5556], grad_fn=<AddBackward0>)\n",
      "epoch: 14322 loss is tensor([-0.5439], grad_fn=<AddBackward0>)\n",
      "epoch: 14323 loss is tensor([-0.5905], grad_fn=<AddBackward0>)\n",
      "epoch: 14324 loss is tensor([-0.5982], grad_fn=<AddBackward0>)\n",
      "epoch: 14325 loss is tensor([-0.6082], grad_fn=<AddBackward0>)\n",
      "epoch: 14326 loss is tensor([-0.5510], grad_fn=<AddBackward0>)\n",
      "epoch: 14327 loss is tensor([-0.6134], grad_fn=<AddBackward0>)\n",
      "epoch: 14328 loss is tensor([-0.6309], grad_fn=<AddBackward0>)\n",
      "epoch: 14329 loss is tensor([-0.6447], grad_fn=<AddBackward0>)\n",
      "epoch: 14330 loss is tensor([-0.5504], grad_fn=<AddBackward0>)\n",
      "epoch: 14331 loss is tensor([-0.5392], grad_fn=<AddBackward0>)\n",
      "epoch: 14332 loss is tensor([-0.5818], grad_fn=<AddBackward0>)\n",
      "epoch: 14333 loss is tensor([-0.4862], grad_fn=<AddBackward0>)\n",
      "epoch: 14334 loss is tensor([-0.5490], grad_fn=<AddBackward0>)\n",
      "epoch: 14335 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 14336 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 14337 loss is tensor([-0.5390], grad_fn=<AddBackward0>)\n",
      "epoch: 14338 loss is tensor([-0.5782], grad_fn=<AddBackward0>)\n",
      "epoch: 14339 loss is tensor([-0.5980], grad_fn=<AddBackward0>)\n",
      "epoch: 14340 loss is tensor([-0.6375], grad_fn=<AddBackward0>)\n",
      "epoch: 14341 loss is tensor([-0.6045], grad_fn=<AddBackward0>)\n",
      "epoch: 14342 loss is tensor([-0.6254], grad_fn=<AddBackward0>)\n",
      "epoch: 14343 loss is tensor([-0.5703], grad_fn=<AddBackward0>)\n",
      "epoch: 14344 loss is tensor([-0.5767], grad_fn=<AddBackward0>)\n",
      "epoch: 14345 loss is tensor([-0.5961], grad_fn=<AddBackward0>)\n",
      "epoch: 14346 loss is tensor([-0.5330], grad_fn=<AddBackward0>)\n",
      "epoch: 14347 loss is tensor([-0.5902], grad_fn=<AddBackward0>)\n",
      "epoch: 14348 loss is tensor([-0.6020], grad_fn=<AddBackward0>)\n",
      "epoch: 14349 loss is tensor([-0.6141], grad_fn=<AddBackward0>)\n",
      "epoch: 14350 loss is tensor([-0.6189], grad_fn=<AddBackward0>)\n",
      "epoch: 14351 loss is tensor([-0.5843], grad_fn=<AddBackward0>)\n",
      "epoch: 14352 loss is tensor([-0.6389], grad_fn=<AddBackward0>)\n",
      "epoch: 14353 loss is tensor([-0.6019], grad_fn=<AddBackward0>)\n",
      "epoch: 14354 loss is tensor([-0.5923], grad_fn=<AddBackward0>)\n",
      "epoch: 14355 loss is tensor([-0.6286], grad_fn=<AddBackward0>)\n",
      "epoch: 14356 loss is tensor([-0.5661], grad_fn=<AddBackward0>)\n",
      "epoch: 14357 loss is tensor([-0.6087], grad_fn=<AddBackward0>)\n",
      "epoch: 14358 loss is tensor([-0.5764], grad_fn=<AddBackward0>)\n",
      "epoch: 14359 loss is tensor([-0.5945], grad_fn=<AddBackward0>)\n",
      "epoch: 14360 loss is tensor([-0.5848], grad_fn=<AddBackward0>)\n",
      "epoch: 14361 loss is tensor([-0.6386], grad_fn=<AddBackward0>)\n",
      "epoch: 14362 loss is tensor([-0.5640], grad_fn=<AddBackward0>)\n",
      "epoch: 14363 loss is tensor([-0.6068], grad_fn=<AddBackward0>)\n",
      "epoch: 14364 loss is tensor([-0.5935], grad_fn=<AddBackward0>)\n",
      "epoch: 14365 loss is tensor([-0.5759], grad_fn=<AddBackward0>)\n",
      "epoch: 14366 loss is tensor([-0.6112], grad_fn=<AddBackward0>)\n",
      "epoch: 14367 loss is tensor([-0.5142], grad_fn=<AddBackward0>)\n",
      "epoch: 14368 loss is tensor([-0.5468], grad_fn=<AddBackward0>)\n",
      "epoch: 14369 loss is tensor([-0.5040], grad_fn=<AddBackward0>)\n",
      "epoch: 14370 loss is tensor([-0.5696], grad_fn=<AddBackward0>)\n",
      "epoch: 14371 loss is tensor([-0.5771], grad_fn=<AddBackward0>)\n",
      "epoch: 14372 loss is tensor([-0.5109], grad_fn=<AddBackward0>)\n",
      "epoch: 14373 loss is tensor([-0.5707], grad_fn=<AddBackward0>)\n",
      "epoch: 14374 loss is tensor([-0.5882], grad_fn=<AddBackward0>)\n",
      "epoch: 14375 loss is tensor([-0.5280], grad_fn=<AddBackward0>)\n",
      "epoch: 14376 loss is tensor([-0.6168], grad_fn=<AddBackward0>)\n",
      "epoch: 14377 loss is tensor([-0.5839], grad_fn=<AddBackward0>)\n",
      "epoch: 14378 loss is tensor([-0.5695], grad_fn=<AddBackward0>)\n",
      "epoch: 14379 loss is tensor([-0.6199], grad_fn=<AddBackward0>)\n",
      "epoch: 14380 loss is tensor([-0.5810], grad_fn=<AddBackward0>)\n",
      "epoch: 14381 loss is tensor([-0.6127], grad_fn=<AddBackward0>)\n",
      "epoch: 14382 loss is tensor([-0.5903], grad_fn=<AddBackward0>)\n",
      "epoch: 14383 loss is tensor([-0.6025], grad_fn=<AddBackward0>)\n",
      "epoch: 14384 loss is tensor([-0.5921], grad_fn=<AddBackward0>)\n",
      "epoch: 14385 loss is tensor([-0.5483], grad_fn=<AddBackward0>)\n",
      "epoch: 14386 loss is tensor([-0.6500], grad_fn=<AddBackward0>)\n",
      "epoch: 14387 loss is tensor([-0.6407], grad_fn=<AddBackward0>)\n",
      "epoch: 14388 loss is tensor([-0.6494], grad_fn=<AddBackward0>)\n",
      "epoch: 14389 loss is tensor([-0.6040], grad_fn=<AddBackward0>)\n",
      "epoch: 14390 loss is tensor([-0.5492], grad_fn=<AddBackward0>)\n",
      "epoch: 14391 loss is tensor([-0.6067], grad_fn=<AddBackward0>)\n",
      "epoch: 14392 loss is tensor([-0.6204], grad_fn=<AddBackward0>)\n",
      "epoch: 14393 loss is tensor([-0.5815], grad_fn=<AddBackward0>)\n",
      "epoch: 14394 loss is tensor([-0.6318], grad_fn=<AddBackward0>)\n",
      "epoch: 14395 loss is tensor([-0.6186], grad_fn=<AddBackward0>)\n",
      "epoch: 14396 loss is tensor([-0.5800], grad_fn=<AddBackward0>)\n",
      "epoch: 14397 loss is tensor([-0.6080], grad_fn=<AddBackward0>)\n",
      "epoch: 14398 loss is tensor([-0.5272], grad_fn=<AddBackward0>)\n",
      "epoch: 14399 loss is tensor([-0.5471], grad_fn=<AddBackward0>)\n",
      "epoch: 14400 loss is tensor([-0.5786], grad_fn=<AddBackward0>)\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14401 loss is tensor([-0.5680], grad_fn=<AddBackward0>)\n",
      "epoch: 14402 loss is tensor([-0.5688], grad_fn=<AddBackward0>)\n",
      "epoch: 14403 loss is tensor([-0.5535], grad_fn=<AddBackward0>)\n",
      "epoch: 14404 loss is tensor([-0.6525], grad_fn=<AddBackward0>)\n",
      "epoch: 14405 loss is tensor([-0.5515], grad_fn=<AddBackward0>)\n",
      "epoch: 14406 loss is tensor([-0.5877], grad_fn=<AddBackward0>)\n",
      "epoch: 14407 loss is tensor([-0.5856], grad_fn=<AddBackward0>)\n",
      "epoch: 14408 loss is tensor([-0.5514], grad_fn=<AddBackward0>)\n",
      "epoch: 14409 loss is tensor([-0.6477], grad_fn=<AddBackward0>)\n",
      "epoch: 14410 loss is tensor([-0.5806], grad_fn=<AddBackward0>)\n",
      "epoch: 14411 loss is tensor([-0.5939], grad_fn=<AddBackward0>)\n",
      "epoch: 14412 loss is tensor([-0.6158], grad_fn=<AddBackward0>)\n",
      "epoch: 14413 loss is tensor([-0.6189], grad_fn=<AddBackward0>)\n",
      "epoch: 14414 loss is tensor([-0.5693], grad_fn=<AddBackward0>)\n",
      "epoch: 14415 loss is tensor([-0.6024], grad_fn=<AddBackward0>)\n",
      "epoch: 14416 loss is tensor([-0.6607], grad_fn=<AddBackward0>)\n",
      "epoch: 14417 loss is tensor([-0.6292], grad_fn=<AddBackward0>)\n",
      "epoch: 14418 loss is tensor([-0.6223], grad_fn=<AddBackward0>)\n",
      "epoch: 14419 loss is tensor([-0.6017], grad_fn=<AddBackward0>)\n",
      "epoch: 14420 loss is tensor([-0.6051], grad_fn=<AddBackward0>)\n",
      "epoch: 14421 loss is tensor([-0.6282], grad_fn=<AddBackward0>)\n",
      "epoch: 14422 loss is tensor([-0.6280], grad_fn=<AddBackward0>)\n",
      "epoch: 14423 loss is tensor([-0.6093], grad_fn=<AddBackward0>)\n",
      "epoch: 14424 loss is tensor([-0.5806], grad_fn=<AddBackward0>)\n",
      "epoch: 14425 loss is tensor([-0.6244], grad_fn=<AddBackward0>)\n",
      "epoch: 14426 loss is tensor([-0.6534], grad_fn=<AddBackward0>)\n",
      "epoch: 14427 loss is tensor([-0.6511], grad_fn=<AddBackward0>)\n",
      "epoch: 14428 loss is tensor([-0.6142], grad_fn=<AddBackward0>)\n",
      "epoch: 14429 loss is tensor([-0.6624], grad_fn=<AddBackward0>)\n",
      "epoch: 14430 loss is tensor([-0.6456], grad_fn=<AddBackward0>)\n",
      "epoch: 14431 loss is tensor([-0.5978], grad_fn=<AddBackward0>)\n",
      "epoch: 14432 loss is tensor([-0.5177], grad_fn=<AddBackward0>)\n",
      "epoch: 14433 loss is tensor([-0.5750], grad_fn=<AddBackward0>)\n",
      "epoch: 14434 loss is tensor([-0.5843], grad_fn=<AddBackward0>)\n",
      "epoch: 14435 loss is tensor([-0.5819], grad_fn=<AddBackward0>)\n",
      "epoch: 14436 loss is tensor([-0.5211], grad_fn=<AddBackward0>)\n",
      "epoch: 14437 loss is tensor([-0.5845], grad_fn=<AddBackward0>)\n",
      "epoch: 14438 loss is tensor([-0.6061], grad_fn=<AddBackward0>)\n",
      "epoch: 14439 loss is tensor([-0.5866], grad_fn=<AddBackward0>)\n",
      "epoch: 14440 loss is tensor([-0.6253], grad_fn=<AddBackward0>)\n",
      "epoch: 14441 loss is tensor([-0.5661], grad_fn=<AddBackward0>)\n",
      "epoch: 14442 loss is tensor([-0.5965], grad_fn=<AddBackward0>)\n",
      "epoch: 14443 loss is tensor([-0.5847], grad_fn=<AddBackward0>)\n",
      "epoch: 14444 loss is tensor([-0.6072], grad_fn=<AddBackward0>)\n",
      "epoch: 14445 loss is tensor([-0.5510], grad_fn=<AddBackward0>)\n",
      "epoch: 14446 loss is tensor([-0.5926], grad_fn=<AddBackward0>)\n",
      "epoch: 14447 loss is tensor([-0.5443], grad_fn=<AddBackward0>)\n",
      "epoch: 14448 loss is tensor([-0.6020], grad_fn=<AddBackward0>)\n",
      "epoch: 14449 loss is tensor([-0.5653], grad_fn=<AddBackward0>)\n",
      "epoch: 14450 loss is tensor([-0.5520], grad_fn=<AddBackward0>)\n",
      "epoch: 14451 loss is tensor([-0.5867], grad_fn=<AddBackward0>)\n",
      "epoch: 14452 loss is tensor([-0.5534], grad_fn=<AddBackward0>)\n",
      "epoch: 14453 loss is tensor([-0.6119], grad_fn=<AddBackward0>)\n",
      "epoch: 14454 loss is tensor([-0.4877], grad_fn=<AddBackward0>)\n",
      "epoch: 14455 loss is tensor([-0.5808], grad_fn=<AddBackward0>)\n",
      "epoch: 14456 loss is tensor([-0.6058], grad_fn=<AddBackward0>)\n",
      "epoch: 14457 loss is tensor([-0.6386], grad_fn=<AddBackward0>)\n",
      "epoch: 14458 loss is tensor([-0.6478], grad_fn=<AddBackward0>)\n",
      "epoch: 14459 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 14460 loss is tensor([-0.5781], grad_fn=<AddBackward0>)\n",
      "epoch: 14461 loss is tensor([-0.5330], grad_fn=<AddBackward0>)\n",
      "epoch: 14462 loss is tensor([-0.5636], grad_fn=<AddBackward0>)\n",
      "epoch: 14463 loss is tensor([-0.5959], grad_fn=<AddBackward0>)\n",
      "epoch: 14464 loss is tensor([-0.5756], grad_fn=<AddBackward0>)\n",
      "epoch: 14465 loss is tensor([-0.6253], grad_fn=<AddBackward0>)\n",
      "epoch: 14466 loss is tensor([-0.6305], grad_fn=<AddBackward0>)\n",
      "epoch: 14467 loss is tensor([-0.5758], grad_fn=<AddBackward0>)\n",
      "epoch: 14468 loss is tensor([-0.5571], grad_fn=<AddBackward0>)\n",
      "epoch: 14469 loss is tensor([-0.5863], grad_fn=<AddBackward0>)\n",
      "epoch: 14470 loss is tensor([-0.5981], grad_fn=<AddBackward0>)\n",
      "epoch: 14471 loss is tensor([-0.5575], grad_fn=<AddBackward0>)\n",
      "epoch: 14472 loss is tensor([-0.4960], grad_fn=<AddBackward0>)\n",
      "epoch: 14473 loss is tensor([-0.5956], grad_fn=<AddBackward0>)\n",
      "epoch: 14474 loss is tensor([-0.6050], grad_fn=<AddBackward0>)\n",
      "epoch: 14475 loss is tensor([-0.6075], grad_fn=<AddBackward0>)\n",
      "epoch: 14476 loss is tensor([-0.6120], grad_fn=<AddBackward0>)\n",
      "epoch: 14477 loss is tensor([-0.5934], grad_fn=<AddBackward0>)\n",
      "epoch: 14478 loss is tensor([-0.5998], grad_fn=<AddBackward0>)\n",
      "epoch: 14479 loss is tensor([-0.6762], grad_fn=<AddBackward0>)\n",
      "epoch: 14480 loss is tensor([-0.6094], grad_fn=<AddBackward0>)\n",
      "epoch: 14481 loss is tensor([-0.5884], grad_fn=<AddBackward0>)\n",
      "epoch: 14482 loss is tensor([-0.5792], grad_fn=<AddBackward0>)\n",
      "epoch: 14483 loss is tensor([-0.5605], grad_fn=<AddBackward0>)\n",
      "epoch: 14484 loss is tensor([-0.6106], grad_fn=<AddBackward0>)\n",
      "epoch: 14485 loss is tensor([-0.6088], grad_fn=<AddBackward0>)\n",
      "epoch: 14486 loss is tensor([-0.5948], grad_fn=<AddBackward0>)\n",
      "epoch: 14487 loss is tensor([-0.5731], grad_fn=<AddBackward0>)\n",
      "epoch: 14488 loss is tensor([-0.6498], grad_fn=<AddBackward0>)\n",
      "epoch: 14489 loss is tensor([-0.5754], grad_fn=<AddBackward0>)\n",
      "epoch: 14490 loss is tensor([-0.6185], grad_fn=<AddBackward0>)\n",
      "epoch: 14491 loss is tensor([-0.6090], grad_fn=<AddBackward0>)\n",
      "epoch: 14492 loss is tensor([-0.6294], grad_fn=<AddBackward0>)\n",
      "epoch: 14493 loss is tensor([-0.6022], grad_fn=<AddBackward0>)\n",
      "epoch: 14494 loss is tensor([-0.6013], grad_fn=<AddBackward0>)\n",
      "epoch: 14495 loss is tensor([-0.5750], grad_fn=<AddBackward0>)\n",
      "epoch: 14496 loss is tensor([-0.6287], grad_fn=<AddBackward0>)\n",
      "epoch: 14497 loss is tensor([-0.5983], grad_fn=<AddBackward0>)\n",
      "epoch: 14498 loss is tensor([-0.6021], grad_fn=<AddBackward0>)\n",
      "epoch: 14499 loss is tensor([-0.5445], grad_fn=<AddBackward0>)\n",
      "epoch: 14500 loss is tensor([-0.6622], grad_fn=<AddBackward0>)\n",
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14501 loss is tensor([-0.6419], grad_fn=<AddBackward0>)\n",
      "epoch: 14502 loss is tensor([-0.6326], grad_fn=<AddBackward0>)\n",
      "epoch: 14503 loss is tensor([-0.5934], grad_fn=<AddBackward0>)\n",
      "epoch: 14504 loss is tensor([-0.5794], grad_fn=<AddBackward0>)\n",
      "epoch: 14505 loss is tensor([-0.5927], grad_fn=<AddBackward0>)\n",
      "epoch: 14506 loss is tensor([-0.6091], grad_fn=<AddBackward0>)\n",
      "epoch: 14507 loss is tensor([-0.6062], grad_fn=<AddBackward0>)\n",
      "epoch: 14508 loss is tensor([-0.6036], grad_fn=<AddBackward0>)\n",
      "epoch: 14509 loss is tensor([-0.5635], grad_fn=<AddBackward0>)\n",
      "epoch: 14510 loss is tensor([-0.5901], grad_fn=<AddBackward0>)\n",
      "epoch: 14511 loss is tensor([-0.6261], grad_fn=<AddBackward0>)\n",
      "epoch: 14512 loss is tensor([-0.5322], grad_fn=<AddBackward0>)\n",
      "epoch: 14513 loss is tensor([-0.5813], grad_fn=<AddBackward0>)\n",
      "epoch: 14514 loss is tensor([-0.5561], grad_fn=<AddBackward0>)\n",
      "epoch: 14515 loss is tensor([-0.6192], grad_fn=<AddBackward0>)\n",
      "epoch: 14516 loss is tensor([-0.5958], grad_fn=<AddBackward0>)\n",
      "epoch: 14517 loss is tensor([-0.6242], grad_fn=<AddBackward0>)\n",
      "epoch: 14518 loss is tensor([-0.5944], grad_fn=<AddBackward0>)\n",
      "epoch: 14519 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 14520 loss is tensor([-0.4993], grad_fn=<AddBackward0>)\n",
      "epoch: 14521 loss is tensor([-0.6019], grad_fn=<AddBackward0>)\n",
      "epoch: 14522 loss is tensor([-0.5521], grad_fn=<AddBackward0>)\n",
      "epoch: 14523 loss is tensor([-0.5406], grad_fn=<AddBackward0>)\n",
      "epoch: 14524 loss is tensor([-0.5705], grad_fn=<AddBackward0>)\n",
      "epoch: 14525 loss is tensor([-0.5451], grad_fn=<AddBackward0>)\n",
      "epoch: 14526 loss is tensor([-0.5923], grad_fn=<AddBackward0>)\n",
      "epoch: 14527 loss is tensor([-0.5938], grad_fn=<AddBackward0>)\n",
      "epoch: 14528 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 14529 loss is tensor([-0.5726], grad_fn=<AddBackward0>)\n",
      "epoch: 14530 loss is tensor([-0.4716], grad_fn=<AddBackward0>)\n",
      "epoch: 14531 loss is tensor([-0.5945], grad_fn=<AddBackward0>)\n",
      "epoch: 14532 loss is tensor([-0.5718], grad_fn=<AddBackward0>)\n",
      "epoch: 14533 loss is tensor([-0.5928], grad_fn=<AddBackward0>)\n",
      "epoch: 14534 loss is tensor([-0.5188], grad_fn=<AddBackward0>)\n",
      "epoch: 14535 loss is tensor([-0.5537], grad_fn=<AddBackward0>)\n",
      "epoch: 14536 loss is tensor([-0.4958], grad_fn=<AddBackward0>)\n",
      "epoch: 14537 loss is tensor([-0.5279], grad_fn=<AddBackward0>)\n",
      "epoch: 14538 loss is tensor([-0.5604], grad_fn=<AddBackward0>)\n",
      "epoch: 14539 loss is tensor([-0.5521], grad_fn=<AddBackward0>)\n",
      "epoch: 14540 loss is tensor([-0.5524], grad_fn=<AddBackward0>)\n",
      "epoch: 14541 loss is tensor([-0.5506], grad_fn=<AddBackward0>)\n",
      "epoch: 14542 loss is tensor([-0.5543], grad_fn=<AddBackward0>)\n",
      "epoch: 14543 loss is tensor([-0.5362], grad_fn=<AddBackward0>)\n",
      "epoch: 14544 loss is tensor([-0.5540], grad_fn=<AddBackward0>)\n",
      "epoch: 14545 loss is tensor([-0.6013], grad_fn=<AddBackward0>)\n",
      "epoch: 14546 loss is tensor([-0.5780], grad_fn=<AddBackward0>)\n",
      "epoch: 14547 loss is tensor([-0.5907], grad_fn=<AddBackward0>)\n",
      "epoch: 14548 loss is tensor([-0.5150], grad_fn=<AddBackward0>)\n",
      "epoch: 14549 loss is tensor([-0.5887], grad_fn=<AddBackward0>)\n",
      "epoch: 14550 loss is tensor([-0.5855], grad_fn=<AddBackward0>)\n",
      "epoch: 14551 loss is tensor([-0.5596], grad_fn=<AddBackward0>)\n",
      "epoch: 14552 loss is tensor([-0.6281], grad_fn=<AddBackward0>)\n",
      "epoch: 14553 loss is tensor([-0.5688], grad_fn=<AddBackward0>)\n",
      "epoch: 14554 loss is tensor([-0.5281], grad_fn=<AddBackward0>)\n",
      "epoch: 14555 loss is tensor([-0.6002], grad_fn=<AddBackward0>)\n",
      "epoch: 14556 loss is tensor([-0.5169], grad_fn=<AddBackward0>)\n",
      "epoch: 14557 loss is tensor([-0.4914], grad_fn=<AddBackward0>)\n",
      "epoch: 14558 loss is tensor([-0.4923], grad_fn=<AddBackward0>)\n",
      "epoch: 14559 loss is tensor([-0.5342], grad_fn=<AddBackward0>)\n",
      "epoch: 14560 loss is tensor([-0.5524], grad_fn=<AddBackward0>)\n",
      "epoch: 14561 loss is tensor([-0.5420], grad_fn=<AddBackward0>)\n",
      "epoch: 14562 loss is tensor([-0.5116], grad_fn=<AddBackward0>)\n",
      "epoch: 14563 loss is tensor([-0.5052], grad_fn=<AddBackward0>)\n",
      "epoch: 14564 loss is tensor([-0.5191], grad_fn=<AddBackward0>)\n",
      "epoch: 14565 loss is tensor([-0.5157], grad_fn=<AddBackward0>)\n",
      "epoch: 14566 loss is tensor([-0.5569], grad_fn=<AddBackward0>)\n",
      "epoch: 14567 loss is tensor([-0.5298], grad_fn=<AddBackward0>)\n",
      "epoch: 14568 loss is tensor([-0.5363], grad_fn=<AddBackward0>)\n",
      "epoch: 14569 loss is tensor([-0.5971], grad_fn=<AddBackward0>)\n",
      "epoch: 14570 loss is tensor([-0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 14571 loss is tensor([-0.5862], grad_fn=<AddBackward0>)\n",
      "epoch: 14572 loss is tensor([-0.6083], grad_fn=<AddBackward0>)\n",
      "epoch: 14573 loss is tensor([-0.5435], grad_fn=<AddBackward0>)\n",
      "epoch: 14574 loss is tensor([-0.5500], grad_fn=<AddBackward0>)\n",
      "epoch: 14575 loss is tensor([-0.5203], grad_fn=<AddBackward0>)\n",
      "epoch: 14576 loss is tensor([-0.4879], grad_fn=<AddBackward0>)\n",
      "epoch: 14577 loss is tensor([-0.5537], grad_fn=<AddBackward0>)\n",
      "epoch: 14578 loss is tensor([-0.5773], grad_fn=<AddBackward0>)\n",
      "epoch: 14579 loss is tensor([-0.5883], grad_fn=<AddBackward0>)\n",
      "epoch: 14580 loss is tensor([-0.5395], grad_fn=<AddBackward0>)\n",
      "epoch: 14581 loss is tensor([-0.6178], grad_fn=<AddBackward0>)\n",
      "epoch: 14582 loss is tensor([-0.5841], grad_fn=<AddBackward0>)\n",
      "epoch: 14583 loss is tensor([-0.5758], grad_fn=<AddBackward0>)\n",
      "epoch: 14584 loss is tensor([-0.5738], grad_fn=<AddBackward0>)\n",
      "epoch: 14585 loss is tensor([-0.5645], grad_fn=<AddBackward0>)\n",
      "epoch: 14586 loss is tensor([-0.5570], grad_fn=<AddBackward0>)\n",
      "epoch: 14587 loss is tensor([-0.5930], grad_fn=<AddBackward0>)\n",
      "epoch: 14588 loss is tensor([-0.5886], grad_fn=<AddBackward0>)\n",
      "epoch: 14589 loss is tensor([-0.5728], grad_fn=<AddBackward0>)\n",
      "epoch: 14590 loss is tensor([-0.6034], grad_fn=<AddBackward0>)\n",
      "epoch: 14591 loss is tensor([-0.6197], grad_fn=<AddBackward0>)\n",
      "epoch: 14592 loss is tensor([-0.6384], grad_fn=<AddBackward0>)\n",
      "epoch: 14593 loss is tensor([-0.5311], grad_fn=<AddBackward0>)\n",
      "epoch: 14594 loss is tensor([-0.5918], grad_fn=<AddBackward0>)\n",
      "epoch: 14595 loss is tensor([-0.5836], grad_fn=<AddBackward0>)\n",
      "epoch: 14596 loss is tensor([-0.6262], grad_fn=<AddBackward0>)\n",
      "epoch: 14597 loss is tensor([-0.5519], grad_fn=<AddBackward0>)\n",
      "epoch: 14598 loss is tensor([-0.6091], grad_fn=<AddBackward0>)\n",
      "epoch: 14599 loss is tensor([-0.5765], grad_fn=<AddBackward0>)\n",
      "epoch: 14600 loss is tensor([-0.6000], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14601 loss is tensor([-0.5477], grad_fn=<AddBackward0>)\n",
      "epoch: 14602 loss is tensor([-0.5840], grad_fn=<AddBackward0>)\n",
      "epoch: 14603 loss is tensor([-0.6034], grad_fn=<AddBackward0>)\n",
      "epoch: 14604 loss is tensor([-0.5947], grad_fn=<AddBackward0>)\n",
      "epoch: 14605 loss is tensor([-0.5895], grad_fn=<AddBackward0>)\n",
      "epoch: 14606 loss is tensor([-0.6067], grad_fn=<AddBackward0>)\n",
      "epoch: 14607 loss is tensor([-0.6192], grad_fn=<AddBackward0>)\n",
      "epoch: 14608 loss is tensor([-0.6271], grad_fn=<AddBackward0>)\n",
      "epoch: 14609 loss is tensor([-0.5941], grad_fn=<AddBackward0>)\n",
      "epoch: 14610 loss is tensor([-0.6226], grad_fn=<AddBackward0>)\n",
      "epoch: 14611 loss is tensor([-0.6438], grad_fn=<AddBackward0>)\n",
      "epoch: 14612 loss is tensor([-0.5464], grad_fn=<AddBackward0>)\n",
      "epoch: 14613 loss is tensor([-0.5140], grad_fn=<AddBackward0>)\n",
      "epoch: 14614 loss is tensor([-0.5231], grad_fn=<AddBackward0>)\n",
      "epoch: 14615 loss is tensor([-0.5217], grad_fn=<AddBackward0>)\n",
      "epoch: 14616 loss is tensor([-0.5703], grad_fn=<AddBackward0>)\n",
      "epoch: 14617 loss is tensor([-0.5603], grad_fn=<AddBackward0>)\n",
      "epoch: 14618 loss is tensor([-0.5595], grad_fn=<AddBackward0>)\n",
      "epoch: 14619 loss is tensor([-0.5739], grad_fn=<AddBackward0>)\n",
      "epoch: 14620 loss is tensor([-0.5924], grad_fn=<AddBackward0>)\n",
      "epoch: 14621 loss is tensor([-0.5449], grad_fn=<AddBackward0>)\n",
      "epoch: 14622 loss is tensor([-0.5890], grad_fn=<AddBackward0>)\n",
      "epoch: 14623 loss is tensor([-0.5780], grad_fn=<AddBackward0>)\n",
      "epoch: 14624 loss is tensor([-0.5850], grad_fn=<AddBackward0>)\n",
      "epoch: 14625 loss is tensor([-0.5154], grad_fn=<AddBackward0>)\n",
      "epoch: 14626 loss is tensor([-0.5977], grad_fn=<AddBackward0>)\n",
      "epoch: 14627 loss is tensor([-0.5602], grad_fn=<AddBackward0>)\n",
      "epoch: 14628 loss is tensor([-0.5773], grad_fn=<AddBackward0>)\n",
      "epoch: 14629 loss is tensor([-0.5618], grad_fn=<AddBackward0>)\n",
      "epoch: 14630 loss is tensor([-0.5829], grad_fn=<AddBackward0>)\n",
      "epoch: 14631 loss is tensor([-0.5538], grad_fn=<AddBackward0>)\n",
      "epoch: 14632 loss is tensor([-0.5590], grad_fn=<AddBackward0>)\n",
      "epoch: 14633 loss is tensor([-0.6064], grad_fn=<AddBackward0>)\n",
      "epoch: 14634 loss is tensor([-0.5930], grad_fn=<AddBackward0>)\n",
      "epoch: 14635 loss is tensor([-0.5850], grad_fn=<AddBackward0>)\n",
      "epoch: 14636 loss is tensor([-0.5870], grad_fn=<AddBackward0>)\n",
      "epoch: 14637 loss is tensor([-0.5300], grad_fn=<AddBackward0>)\n",
      "epoch: 14638 loss is tensor([-0.6028], grad_fn=<AddBackward0>)\n",
      "epoch: 14639 loss is tensor([-0.5109], grad_fn=<AddBackward0>)\n",
      "epoch: 14640 loss is tensor([-0.6045], grad_fn=<AddBackward0>)\n",
      "epoch: 14641 loss is tensor([-0.6216], grad_fn=<AddBackward0>)\n",
      "epoch: 14642 loss is tensor([-0.6415], grad_fn=<AddBackward0>)\n",
      "epoch: 14643 loss is tensor([-0.5956], grad_fn=<AddBackward0>)\n",
      "epoch: 14644 loss is tensor([-0.5647], grad_fn=<AddBackward0>)\n",
      "epoch: 14645 loss is tensor([-0.6190], grad_fn=<AddBackward0>)\n",
      "epoch: 14646 loss is tensor([-0.5997], grad_fn=<AddBackward0>)\n",
      "epoch: 14647 loss is tensor([-0.5808], grad_fn=<AddBackward0>)\n",
      "epoch: 14648 loss is tensor([-0.6093], grad_fn=<AddBackward0>)\n",
      "epoch: 14649 loss is tensor([-0.5882], grad_fn=<AddBackward0>)\n",
      "epoch: 14650 loss is tensor([-0.5755], grad_fn=<AddBackward0>)\n",
      "epoch: 14651 loss is tensor([-0.5790], grad_fn=<AddBackward0>)\n",
      "epoch: 14652 loss is tensor([-0.6210], grad_fn=<AddBackward0>)\n",
      "epoch: 14653 loss is tensor([-0.5961], grad_fn=<AddBackward0>)\n",
      "epoch: 14654 loss is tensor([-0.5173], grad_fn=<AddBackward0>)\n",
      "epoch: 14655 loss is tensor([-0.6202], grad_fn=<AddBackward0>)\n",
      "epoch: 14656 loss is tensor([-0.5992], grad_fn=<AddBackward0>)\n",
      "epoch: 14657 loss is tensor([-0.5620], grad_fn=<AddBackward0>)\n",
      "epoch: 14658 loss is tensor([-0.5293], grad_fn=<AddBackward0>)\n",
      "epoch: 14659 loss is tensor([-0.5802], grad_fn=<AddBackward0>)\n",
      "epoch: 14660 loss is tensor([-0.6137], grad_fn=<AddBackward0>)\n",
      "epoch: 14661 loss is tensor([-0.6036], grad_fn=<AddBackward0>)\n",
      "epoch: 14662 loss is tensor([-0.5787], grad_fn=<AddBackward0>)\n",
      "epoch: 14663 loss is tensor([-0.6255], grad_fn=<AddBackward0>)\n",
      "epoch: 14664 loss is tensor([-0.5859], grad_fn=<AddBackward0>)\n",
      "epoch: 14665 loss is tensor([-0.6000], grad_fn=<AddBackward0>)\n",
      "epoch: 14666 loss is tensor([-0.5812], grad_fn=<AddBackward0>)\n",
      "epoch: 14667 loss is tensor([-0.5674], grad_fn=<AddBackward0>)\n",
      "epoch: 14668 loss is tensor([-0.6164], grad_fn=<AddBackward0>)\n",
      "epoch: 14669 loss is tensor([-0.6045], grad_fn=<AddBackward0>)\n",
      "epoch: 14670 loss is tensor([-0.5747], grad_fn=<AddBackward0>)\n",
      "epoch: 14671 loss is tensor([-0.5210], grad_fn=<AddBackward0>)\n",
      "epoch: 14672 loss is tensor([-0.6378], grad_fn=<AddBackward0>)\n",
      "epoch: 14673 loss is tensor([-0.5693], grad_fn=<AddBackward0>)\n",
      "epoch: 14674 loss is tensor([-0.6095], grad_fn=<AddBackward0>)\n",
      "epoch: 14675 loss is tensor([-0.6341], grad_fn=<AddBackward0>)\n",
      "epoch: 14676 loss is tensor([-0.6289], grad_fn=<AddBackward0>)\n",
      "epoch: 14677 loss is tensor([-0.5899], grad_fn=<AddBackward0>)\n",
      "epoch: 14678 loss is tensor([-0.6187], grad_fn=<AddBackward0>)\n",
      "epoch: 14679 loss is tensor([-0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 14680 loss is tensor([-0.6444], grad_fn=<AddBackward0>)\n",
      "epoch: 14681 loss is tensor([-0.5870], grad_fn=<AddBackward0>)\n",
      "epoch: 14682 loss is tensor([-0.6030], grad_fn=<AddBackward0>)\n",
      "epoch: 14683 loss is tensor([-0.6558], grad_fn=<AddBackward0>)\n",
      "epoch: 14684 loss is tensor([-0.6195], grad_fn=<AddBackward0>)\n",
      "epoch: 14685 loss is tensor([-0.5690], grad_fn=<AddBackward0>)\n",
      "epoch: 14686 loss is tensor([-0.6600], grad_fn=<AddBackward0>)\n",
      "epoch: 14687 loss is tensor([-0.5466], grad_fn=<AddBackward0>)\n",
      "epoch: 14688 loss is tensor([-0.6363], grad_fn=<AddBackward0>)\n",
      "epoch: 14689 loss is tensor([-0.5917], grad_fn=<AddBackward0>)\n",
      "epoch: 14690 loss is tensor([-0.5852], grad_fn=<AddBackward0>)\n",
      "epoch: 14691 loss is tensor([-0.5247], grad_fn=<AddBackward0>)\n",
      "epoch: 14692 loss is tensor([-0.5863], grad_fn=<AddBackward0>)\n",
      "epoch: 14693 loss is tensor([-0.5740], grad_fn=<AddBackward0>)\n",
      "epoch: 14694 loss is tensor([-0.5211], grad_fn=<AddBackward0>)\n",
      "epoch: 14695 loss is tensor([-0.6264], grad_fn=<AddBackward0>)\n",
      "epoch: 14696 loss is tensor([-0.6113], grad_fn=<AddBackward0>)\n",
      "epoch: 14697 loss is tensor([-0.5897], grad_fn=<AddBackward0>)\n",
      "epoch: 14698 loss is tensor([-0.6042], grad_fn=<AddBackward0>)\n",
      "epoch: 14699 loss is tensor([-0.6180], grad_fn=<AddBackward0>)\n",
      "epoch: 14700 loss is tensor([-0.5868], grad_fn=<AddBackward0>)\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14701 loss is tensor([-0.5890], grad_fn=<AddBackward0>)\n",
      "epoch: 14702 loss is tensor([-0.5481], grad_fn=<AddBackward0>)\n",
      "epoch: 14703 loss is tensor([-0.6293], grad_fn=<AddBackward0>)\n",
      "epoch: 14704 loss is tensor([-0.6266], grad_fn=<AddBackward0>)\n",
      "epoch: 14705 loss is tensor([-0.6415], grad_fn=<AddBackward0>)\n",
      "epoch: 14706 loss is tensor([-0.5807], grad_fn=<AddBackward0>)\n",
      "epoch: 14707 loss is tensor([-0.6221], grad_fn=<AddBackward0>)\n",
      "epoch: 14708 loss is tensor([-0.6248], grad_fn=<AddBackward0>)\n",
      "epoch: 14709 loss is tensor([-0.5515], grad_fn=<AddBackward0>)\n",
      "epoch: 14710 loss is tensor([-0.6125], grad_fn=<AddBackward0>)\n",
      "epoch: 14711 loss is tensor([-0.5545], grad_fn=<AddBackward0>)\n",
      "epoch: 14712 loss is tensor([-0.6005], grad_fn=<AddBackward0>)\n",
      "epoch: 14713 loss is tensor([-0.5419], grad_fn=<AddBackward0>)\n",
      "epoch: 14714 loss is tensor([-0.5717], grad_fn=<AddBackward0>)\n",
      "epoch: 14715 loss is tensor([-0.5639], grad_fn=<AddBackward0>)\n",
      "epoch: 14716 loss is tensor([-0.6180], grad_fn=<AddBackward0>)\n",
      "epoch: 14717 loss is tensor([-0.6054], grad_fn=<AddBackward0>)\n",
      "epoch: 14718 loss is tensor([-0.5756], grad_fn=<AddBackward0>)\n",
      "epoch: 14719 loss is tensor([-0.5836], grad_fn=<AddBackward0>)\n",
      "epoch: 14720 loss is tensor([-0.5988], grad_fn=<AddBackward0>)\n",
      "epoch: 14721 loss is tensor([-0.5764], grad_fn=<AddBackward0>)\n",
      "epoch: 14722 loss is tensor([-0.5931], grad_fn=<AddBackward0>)\n",
      "epoch: 14723 loss is tensor([-0.6686], grad_fn=<AddBackward0>)\n",
      "epoch: 14724 loss is tensor([-0.6191], grad_fn=<AddBackward0>)\n",
      "epoch: 14725 loss is tensor([-0.6429], grad_fn=<AddBackward0>)\n",
      "epoch: 14726 loss is tensor([-0.6100], grad_fn=<AddBackward0>)\n",
      "epoch: 14727 loss is tensor([-0.6213], grad_fn=<AddBackward0>)\n",
      "epoch: 14728 loss is tensor([-0.6060], grad_fn=<AddBackward0>)\n",
      "epoch: 14729 loss is tensor([-0.6312], grad_fn=<AddBackward0>)\n",
      "epoch: 14730 loss is tensor([-0.6165], grad_fn=<AddBackward0>)\n",
      "epoch: 14731 loss is tensor([-0.6134], grad_fn=<AddBackward0>)\n",
      "epoch: 14732 loss is tensor([-0.6178], grad_fn=<AddBackward0>)\n",
      "epoch: 14733 loss is tensor([-0.6472], grad_fn=<AddBackward0>)\n",
      "epoch: 14734 loss is tensor([-0.6224], grad_fn=<AddBackward0>)\n",
      "epoch: 14735 loss is tensor([-0.6057], grad_fn=<AddBackward0>)\n",
      "epoch: 14736 loss is tensor([-0.5927], grad_fn=<AddBackward0>)\n",
      "epoch: 14737 loss is tensor([-0.5891], grad_fn=<AddBackward0>)\n",
      "epoch: 14738 loss is tensor([-0.6145], grad_fn=<AddBackward0>)\n",
      "epoch: 14739 loss is tensor([-0.6206], grad_fn=<AddBackward0>)\n",
      "epoch: 14740 loss is tensor([-0.6101], grad_fn=<AddBackward0>)\n",
      "epoch: 14741 loss is tensor([-0.6565], grad_fn=<AddBackward0>)\n",
      "epoch: 14742 loss is tensor([-0.6769], grad_fn=<AddBackward0>)\n",
      "epoch: 14743 loss is tensor([-0.6433], grad_fn=<AddBackward0>)\n",
      "epoch: 14744 loss is tensor([-0.5921], grad_fn=<AddBackward0>)\n",
      "epoch: 14745 loss is tensor([-0.6691], grad_fn=<AddBackward0>)\n",
      "epoch: 14746 loss is tensor([-0.6220], grad_fn=<AddBackward0>)\n",
      "epoch: 14747 loss is tensor([-0.5818], grad_fn=<AddBackward0>)\n",
      "epoch: 14748 loss is tensor([-0.5996], grad_fn=<AddBackward0>)\n",
      "epoch: 14749 loss is tensor([-0.6359], grad_fn=<AddBackward0>)\n",
      "epoch: 14750 loss is tensor([-0.6102], grad_fn=<AddBackward0>)\n",
      "epoch: 14751 loss is tensor([-0.6810], grad_fn=<AddBackward0>)\n",
      "epoch: 14752 loss is tensor([-0.6325], grad_fn=<AddBackward0>)\n",
      "epoch: 14753 loss is tensor([-0.5706], grad_fn=<AddBackward0>)\n",
      "epoch: 14754 loss is tensor([-0.6614], grad_fn=<AddBackward0>)\n",
      "epoch: 14755 loss is tensor([-0.5794], grad_fn=<AddBackward0>)\n",
      "epoch: 14756 loss is tensor([-0.6354], grad_fn=<AddBackward0>)\n",
      "epoch: 14757 loss is tensor([-0.5798], grad_fn=<AddBackward0>)\n",
      "epoch: 14758 loss is tensor([-0.6492], grad_fn=<AddBackward0>)\n",
      "epoch: 14759 loss is tensor([-0.6510], grad_fn=<AddBackward0>)\n",
      "epoch: 14760 loss is tensor([-0.5924], grad_fn=<AddBackward0>)\n",
      "epoch: 14761 loss is tensor([-0.5828], grad_fn=<AddBackward0>)\n",
      "epoch: 14762 loss is tensor([-0.6402], grad_fn=<AddBackward0>)\n",
      "epoch: 14763 loss is tensor([-0.5280], grad_fn=<AddBackward0>)\n",
      "epoch: 14764 loss is tensor([-0.5980], grad_fn=<AddBackward0>)\n",
      "epoch: 14765 loss is tensor([-0.5871], grad_fn=<AddBackward0>)\n",
      "epoch: 14766 loss is tensor([-0.5881], grad_fn=<AddBackward0>)\n",
      "epoch: 14767 loss is tensor([-0.6037], grad_fn=<AddBackward0>)\n",
      "epoch: 14768 loss is tensor([-0.5498], grad_fn=<AddBackward0>)\n",
      "epoch: 14769 loss is tensor([-0.6095], grad_fn=<AddBackward0>)\n",
      "epoch: 14770 loss is tensor([-0.5509], grad_fn=<AddBackward0>)\n",
      "epoch: 14771 loss is tensor([-0.5820], grad_fn=<AddBackward0>)\n",
      "epoch: 14772 loss is tensor([-0.5698], grad_fn=<AddBackward0>)\n",
      "epoch: 14773 loss is tensor([-0.5647], grad_fn=<AddBackward0>)\n",
      "epoch: 14774 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 14775 loss is tensor([-0.5357], grad_fn=<AddBackward0>)\n",
      "epoch: 14776 loss is tensor([-0.5938], grad_fn=<AddBackward0>)\n",
      "epoch: 14777 loss is tensor([-0.6140], grad_fn=<AddBackward0>)\n",
      "epoch: 14778 loss is tensor([-0.5514], grad_fn=<AddBackward0>)\n",
      "epoch: 14779 loss is tensor([-0.5638], grad_fn=<AddBackward0>)\n",
      "epoch: 14780 loss is tensor([-0.5473], grad_fn=<AddBackward0>)\n",
      "epoch: 14781 loss is tensor([-0.5385], grad_fn=<AddBackward0>)\n",
      "epoch: 14782 loss is tensor([-0.5331], grad_fn=<AddBackward0>)\n",
      "epoch: 14783 loss is tensor([-0.5412], grad_fn=<AddBackward0>)\n",
      "epoch: 14784 loss is tensor([-0.5710], grad_fn=<AddBackward0>)\n",
      "epoch: 14785 loss is tensor([-0.5522], grad_fn=<AddBackward0>)\n",
      "epoch: 14786 loss is tensor([-0.5782], grad_fn=<AddBackward0>)\n",
      "epoch: 14787 loss is tensor([-0.5783], grad_fn=<AddBackward0>)\n",
      "epoch: 14788 loss is tensor([-0.5862], grad_fn=<AddBackward0>)\n",
      "epoch: 14789 loss is tensor([-0.5279], grad_fn=<AddBackward0>)\n",
      "epoch: 14790 loss is tensor([-0.5835], grad_fn=<AddBackward0>)\n",
      "epoch: 14791 loss is tensor([-0.5814], grad_fn=<AddBackward0>)\n",
      "epoch: 14792 loss is tensor([-0.5209], grad_fn=<AddBackward0>)\n",
      "epoch: 14793 loss is tensor([-0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 14794 loss is tensor([-0.4830], grad_fn=<AddBackward0>)\n",
      "epoch: 14795 loss is tensor([-0.5900], grad_fn=<AddBackward0>)\n",
      "epoch: 14796 loss is tensor([-0.5752], grad_fn=<AddBackward0>)\n",
      "epoch: 14797 loss is tensor([-0.5989], grad_fn=<AddBackward0>)\n",
      "epoch: 14798 loss is tensor([-0.6397], grad_fn=<AddBackward0>)\n",
      "epoch: 14799 loss is tensor([-0.5990], grad_fn=<AddBackward0>)\n",
      "epoch: 14800 loss is tensor([-0.5695], grad_fn=<AddBackward0>)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14801 loss is tensor([-0.5941], grad_fn=<AddBackward0>)\n",
      "epoch: 14802 loss is tensor([-0.6304], grad_fn=<AddBackward0>)\n",
      "epoch: 14803 loss is tensor([-0.5960], grad_fn=<AddBackward0>)\n",
      "epoch: 14804 loss is tensor([-0.6356], grad_fn=<AddBackward0>)\n",
      "epoch: 14805 loss is tensor([-0.5713], grad_fn=<AddBackward0>)\n",
      "epoch: 14806 loss is tensor([-0.6107], grad_fn=<AddBackward0>)\n",
      "epoch: 14807 loss is tensor([-0.6806], grad_fn=<AddBackward0>)\n",
      "epoch: 14808 loss is tensor([-0.5608], grad_fn=<AddBackward0>)\n",
      "epoch: 14809 loss is tensor([-0.6605], grad_fn=<AddBackward0>)\n",
      "epoch: 14810 loss is tensor([-0.6077], grad_fn=<AddBackward0>)\n",
      "epoch: 14811 loss is tensor([-0.5674], grad_fn=<AddBackward0>)\n",
      "epoch: 14812 loss is tensor([-0.5713], grad_fn=<AddBackward0>)\n",
      "epoch: 14813 loss is tensor([-0.5242], grad_fn=<AddBackward0>)\n",
      "epoch: 14814 loss is tensor([-0.5681], grad_fn=<AddBackward0>)\n",
      "epoch: 14815 loss is tensor([-0.5480], grad_fn=<AddBackward0>)\n",
      "epoch: 14816 loss is tensor([-0.6216], grad_fn=<AddBackward0>)\n",
      "epoch: 14817 loss is tensor([-0.6401], grad_fn=<AddBackward0>)\n",
      "epoch: 14818 loss is tensor([-0.5524], grad_fn=<AddBackward0>)\n",
      "epoch: 14819 loss is tensor([-0.5089], grad_fn=<AddBackward0>)\n",
      "epoch: 14820 loss is tensor([-0.5342], grad_fn=<AddBackward0>)\n",
      "epoch: 14821 loss is tensor([-0.5881], grad_fn=<AddBackward0>)\n",
      "epoch: 14822 loss is tensor([-0.6165], grad_fn=<AddBackward0>)\n",
      "epoch: 14823 loss is tensor([-0.6380], grad_fn=<AddBackward0>)\n",
      "epoch: 14824 loss is tensor([-0.5748], grad_fn=<AddBackward0>)\n",
      "epoch: 14825 loss is tensor([-0.5847], grad_fn=<AddBackward0>)\n",
      "epoch: 14826 loss is tensor([-0.5812], grad_fn=<AddBackward0>)\n",
      "epoch: 14827 loss is tensor([-0.5863], grad_fn=<AddBackward0>)\n",
      "epoch: 14828 loss is tensor([-0.4916], grad_fn=<AddBackward0>)\n",
      "epoch: 14829 loss is tensor([-0.5797], grad_fn=<AddBackward0>)\n",
      "epoch: 14830 loss is tensor([-0.5970], grad_fn=<AddBackward0>)\n",
      "epoch: 14831 loss is tensor([-0.6246], grad_fn=<AddBackward0>)\n",
      "epoch: 14832 loss is tensor([-0.5457], grad_fn=<AddBackward0>)\n",
      "epoch: 14833 loss is tensor([-0.5825], grad_fn=<AddBackward0>)\n",
      "epoch: 14834 loss is tensor([-0.6135], grad_fn=<AddBackward0>)\n",
      "epoch: 14835 loss is tensor([-0.5377], grad_fn=<AddBackward0>)\n",
      "epoch: 14836 loss is tensor([-0.5945], grad_fn=<AddBackward0>)\n",
      "epoch: 14837 loss is tensor([-0.6361], grad_fn=<AddBackward0>)\n",
      "epoch: 14838 loss is tensor([-0.5915], grad_fn=<AddBackward0>)\n",
      "epoch: 14839 loss is tensor([-0.5918], grad_fn=<AddBackward0>)\n",
      "epoch: 14840 loss is tensor([-0.6003], grad_fn=<AddBackward0>)\n",
      "epoch: 14841 loss is tensor([-0.5948], grad_fn=<AddBackward0>)\n",
      "epoch: 14842 loss is tensor([-0.6416], grad_fn=<AddBackward0>)\n",
      "epoch: 14843 loss is tensor([-0.6171], grad_fn=<AddBackward0>)\n",
      "epoch: 14844 loss is tensor([-0.6015], grad_fn=<AddBackward0>)\n",
      "epoch: 14845 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 14846 loss is tensor([-0.5991], grad_fn=<AddBackward0>)\n",
      "epoch: 14847 loss is tensor([-0.6601], grad_fn=<AddBackward0>)\n",
      "epoch: 14848 loss is tensor([-0.5865], grad_fn=<AddBackward0>)\n",
      "epoch: 14849 loss is tensor([-0.5877], grad_fn=<AddBackward0>)\n",
      "epoch: 14850 loss is tensor([-0.5910], grad_fn=<AddBackward0>)\n",
      "epoch: 14851 loss is tensor([-0.6193], grad_fn=<AddBackward0>)\n",
      "epoch: 14852 loss is tensor([-0.5832], grad_fn=<AddBackward0>)\n",
      "epoch: 14853 loss is tensor([-0.6495], grad_fn=<AddBackward0>)\n",
      "epoch: 14854 loss is tensor([-0.6350], grad_fn=<AddBackward0>)\n",
      "epoch: 14855 loss is tensor([-0.6267], grad_fn=<AddBackward0>)\n",
      "epoch: 14856 loss is tensor([-0.6329], grad_fn=<AddBackward0>)\n",
      "epoch: 14857 loss is tensor([-0.6722], grad_fn=<AddBackward0>)\n",
      "epoch: 14858 loss is tensor([-0.6420], grad_fn=<AddBackward0>)\n",
      "epoch: 14859 loss is tensor([-0.5574], grad_fn=<AddBackward0>)\n",
      "epoch: 14860 loss is tensor([-0.6264], grad_fn=<AddBackward0>)\n",
      "epoch: 14861 loss is tensor([-0.6134], grad_fn=<AddBackward0>)\n",
      "epoch: 14862 loss is tensor([-0.6747], grad_fn=<AddBackward0>)\n",
      "epoch: 14863 loss is tensor([-0.6075], grad_fn=<AddBackward0>)\n",
      "epoch: 14864 loss is tensor([-0.6351], grad_fn=<AddBackward0>)\n",
      "epoch: 14865 loss is tensor([-0.5725], grad_fn=<AddBackward0>)\n",
      "epoch: 14866 loss is tensor([-0.5744], grad_fn=<AddBackward0>)\n",
      "epoch: 14867 loss is tensor([-0.5836], grad_fn=<AddBackward0>)\n",
      "epoch: 14868 loss is tensor([-0.6489], grad_fn=<AddBackward0>)\n",
      "epoch: 14869 loss is tensor([-0.6503], grad_fn=<AddBackward0>)\n",
      "epoch: 14870 loss is tensor([-0.5968], grad_fn=<AddBackward0>)\n",
      "epoch: 14871 loss is tensor([-0.5726], grad_fn=<AddBackward0>)\n",
      "epoch: 14872 loss is tensor([-0.6158], grad_fn=<AddBackward0>)\n",
      "epoch: 14873 loss is tensor([-0.6727], grad_fn=<AddBackward0>)\n",
      "epoch: 14874 loss is tensor([-0.6598], grad_fn=<AddBackward0>)\n",
      "epoch: 14875 loss is tensor([-0.6242], grad_fn=<AddBackward0>)\n",
      "epoch: 14876 loss is tensor([-0.6275], grad_fn=<AddBackward0>)\n",
      "epoch: 14877 loss is tensor([-0.5968], grad_fn=<AddBackward0>)\n",
      "epoch: 14878 loss is tensor([-0.6465], grad_fn=<AddBackward0>)\n",
      "epoch: 14879 loss is tensor([-0.5883], grad_fn=<AddBackward0>)\n",
      "epoch: 14880 loss is tensor([-0.6207], grad_fn=<AddBackward0>)\n",
      "epoch: 14881 loss is tensor([-0.6321], grad_fn=<AddBackward0>)\n",
      "epoch: 14882 loss is tensor([-0.6546], grad_fn=<AddBackward0>)\n",
      "epoch: 14883 loss is tensor([-0.6295], grad_fn=<AddBackward0>)\n",
      "epoch: 14884 loss is tensor([-0.6671], grad_fn=<AddBackward0>)\n",
      "epoch: 14885 loss is tensor([-0.6344], grad_fn=<AddBackward0>)\n",
      "epoch: 14886 loss is tensor([-0.6095], grad_fn=<AddBackward0>)\n",
      "epoch: 14887 loss is tensor([-0.6457], grad_fn=<AddBackward0>)\n",
      "epoch: 14888 loss is tensor([-0.5784], grad_fn=<AddBackward0>)\n",
      "epoch: 14889 loss is tensor([-0.5651], grad_fn=<AddBackward0>)\n",
      "epoch: 14890 loss is tensor([-0.5766], grad_fn=<AddBackward0>)\n",
      "epoch: 14891 loss is tensor([-0.5897], grad_fn=<AddBackward0>)\n",
      "epoch: 14892 loss is tensor([-0.5484], grad_fn=<AddBackward0>)\n",
      "epoch: 14893 loss is tensor([-0.6057], grad_fn=<AddBackward0>)\n",
      "epoch: 14894 loss is tensor([-0.5936], grad_fn=<AddBackward0>)\n",
      "epoch: 14895 loss is tensor([-0.6387], grad_fn=<AddBackward0>)\n",
      "epoch: 14896 loss is tensor([-0.6013], grad_fn=<AddBackward0>)\n",
      "epoch: 14897 loss is tensor([-0.6401], grad_fn=<AddBackward0>)\n",
      "epoch: 14898 loss is tensor([-0.5895], grad_fn=<AddBackward0>)\n",
      "epoch: 14899 loss is tensor([-0.6160], grad_fn=<AddBackward0>)\n",
      "epoch: 14900 loss is tensor([-0.6335], grad_fn=<AddBackward0>)\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 14901 loss is tensor([-0.5427], grad_fn=<AddBackward0>)\n",
      "epoch: 14902 loss is tensor([-0.5766], grad_fn=<AddBackward0>)\n",
      "epoch: 14903 loss is tensor([-0.6066], grad_fn=<AddBackward0>)\n",
      "epoch: 14904 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 14905 loss is tensor([-0.6162], grad_fn=<AddBackward0>)\n",
      "epoch: 14906 loss is tensor([-0.5889], grad_fn=<AddBackward0>)\n",
      "epoch: 14907 loss is tensor([-0.6113], grad_fn=<AddBackward0>)\n",
      "epoch: 14908 loss is tensor([-0.6139], grad_fn=<AddBackward0>)\n",
      "epoch: 14909 loss is tensor([-0.6125], grad_fn=<AddBackward0>)\n",
      "epoch: 14910 loss is tensor([-0.6129], grad_fn=<AddBackward0>)\n",
      "epoch: 14911 loss is tensor([-0.6324], grad_fn=<AddBackward0>)\n",
      "epoch: 14912 loss is tensor([-0.5869], grad_fn=<AddBackward0>)\n",
      "epoch: 14913 loss is tensor([-0.6178], grad_fn=<AddBackward0>)\n",
      "epoch: 14914 loss is tensor([-0.6236], grad_fn=<AddBackward0>)\n",
      "epoch: 14915 loss is tensor([-0.6367], grad_fn=<AddBackward0>)\n",
      "epoch: 14916 loss is tensor([-0.5995], grad_fn=<AddBackward0>)\n",
      "epoch: 14917 loss is tensor([-0.5596], grad_fn=<AddBackward0>)\n",
      "epoch: 14918 loss is tensor([-0.6318], grad_fn=<AddBackward0>)\n",
      "epoch: 14919 loss is tensor([-0.5809], grad_fn=<AddBackward0>)\n",
      "epoch: 14920 loss is tensor([-0.6252], grad_fn=<AddBackward0>)\n",
      "epoch: 14921 loss is tensor([-0.5516], grad_fn=<AddBackward0>)\n",
      "epoch: 14922 loss is tensor([-0.6159], grad_fn=<AddBackward0>)\n",
      "epoch: 14923 loss is tensor([-0.5586], grad_fn=<AddBackward0>)\n",
      "epoch: 14924 loss is tensor([-0.5325], grad_fn=<AddBackward0>)\n",
      "epoch: 14925 loss is tensor([-0.5500], grad_fn=<AddBackward0>)\n",
      "epoch: 14926 loss is tensor([-0.5553], grad_fn=<AddBackward0>)\n",
      "epoch: 14927 loss is tensor([-0.6063], grad_fn=<AddBackward0>)\n",
      "epoch: 14928 loss is tensor([-0.6368], grad_fn=<AddBackward0>)\n",
      "epoch: 14929 loss is tensor([-0.5510], grad_fn=<AddBackward0>)\n",
      "epoch: 14930 loss is tensor([-0.5883], grad_fn=<AddBackward0>)\n",
      "epoch: 14931 loss is tensor([-0.5663], grad_fn=<AddBackward0>)\n",
      "epoch: 14932 loss is tensor([-0.6048], grad_fn=<AddBackward0>)\n",
      "epoch: 14933 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "epoch: 14934 loss is tensor([-0.5788], grad_fn=<AddBackward0>)\n",
      "epoch: 14935 loss is tensor([-0.5709], grad_fn=<AddBackward0>)\n",
      "epoch: 14936 loss is tensor([-0.5537], grad_fn=<AddBackward0>)\n",
      "epoch: 14937 loss is tensor([-0.6384], grad_fn=<AddBackward0>)\n",
      "epoch: 14938 loss is tensor([-0.5928], grad_fn=<AddBackward0>)\n",
      "epoch: 14939 loss is tensor([-0.6039], grad_fn=<AddBackward0>)\n",
      "epoch: 14940 loss is tensor([-0.5896], grad_fn=<AddBackward0>)\n",
      "epoch: 14941 loss is tensor([-0.6079], grad_fn=<AddBackward0>)\n",
      "epoch: 14942 loss is tensor([-0.5506], grad_fn=<AddBackward0>)\n",
      "epoch: 14943 loss is tensor([-0.6048], grad_fn=<AddBackward0>)\n",
      "epoch: 14944 loss is tensor([-0.6022], grad_fn=<AddBackward0>)\n",
      "epoch: 14945 loss is tensor([-0.6014], grad_fn=<AddBackward0>)\n",
      "epoch: 14946 loss is tensor([-0.5647], grad_fn=<AddBackward0>)\n",
      "epoch: 14947 loss is tensor([-0.5960], grad_fn=<AddBackward0>)\n",
      "epoch: 14948 loss is tensor([-0.5633], grad_fn=<AddBackward0>)\n",
      "epoch: 14949 loss is tensor([-0.6191], grad_fn=<AddBackward0>)\n",
      "epoch: 14950 loss is tensor([-0.5887], grad_fn=<AddBackward0>)\n",
      "epoch: 14951 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 14952 loss is tensor([-0.5772], grad_fn=<AddBackward0>)\n",
      "epoch: 14953 loss is tensor([-0.6106], grad_fn=<AddBackward0>)\n",
      "epoch: 14954 loss is tensor([-0.5125], grad_fn=<AddBackward0>)\n",
      "epoch: 14955 loss is tensor([-0.5744], grad_fn=<AddBackward0>)\n",
      "epoch: 14956 loss is tensor([-0.5948], grad_fn=<AddBackward0>)\n",
      "epoch: 14957 loss is tensor([-0.5977], grad_fn=<AddBackward0>)\n",
      "epoch: 14958 loss is tensor([-0.6195], grad_fn=<AddBackward0>)\n",
      "epoch: 14959 loss is tensor([-0.6020], grad_fn=<AddBackward0>)\n",
      "epoch: 14960 loss is tensor([-0.6443], grad_fn=<AddBackward0>)\n",
      "epoch: 14961 loss is tensor([-0.5768], grad_fn=<AddBackward0>)\n",
      "epoch: 14962 loss is tensor([-0.5796], grad_fn=<AddBackward0>)\n",
      "epoch: 14963 loss is tensor([-0.5742], grad_fn=<AddBackward0>)\n",
      "epoch: 14964 loss is tensor([-0.5614], grad_fn=<AddBackward0>)\n",
      "epoch: 14965 loss is tensor([-0.6104], grad_fn=<AddBackward0>)\n",
      "epoch: 14966 loss is tensor([-0.5594], grad_fn=<AddBackward0>)\n",
      "epoch: 14967 loss is tensor([-0.5275], grad_fn=<AddBackward0>)\n",
      "epoch: 14968 loss is tensor([-0.5405], grad_fn=<AddBackward0>)\n",
      "epoch: 14969 loss is tensor([-0.6120], grad_fn=<AddBackward0>)\n",
      "epoch: 14970 loss is tensor([-0.6036], grad_fn=<AddBackward0>)\n",
      "epoch: 14971 loss is tensor([-0.6349], grad_fn=<AddBackward0>)\n",
      "epoch: 14972 loss is tensor([-0.6393], grad_fn=<AddBackward0>)\n",
      "epoch: 14973 loss is tensor([-0.5915], grad_fn=<AddBackward0>)\n",
      "epoch: 14974 loss is tensor([-0.5957], grad_fn=<AddBackward0>)\n",
      "epoch: 14975 loss is tensor([-0.5737], grad_fn=<AddBackward0>)\n",
      "epoch: 14976 loss is tensor([-0.5962], grad_fn=<AddBackward0>)\n",
      "epoch: 14977 loss is tensor([-0.5683], grad_fn=<AddBackward0>)\n",
      "epoch: 14978 loss is tensor([-0.5763], grad_fn=<AddBackward0>)\n",
      "epoch: 14979 loss is tensor([-0.5915], grad_fn=<AddBackward0>)\n",
      "epoch: 14980 loss is tensor([-0.5894], grad_fn=<AddBackward0>)\n",
      "epoch: 14981 loss is tensor([-0.5551], grad_fn=<AddBackward0>)\n",
      "epoch: 14982 loss is tensor([-0.3769], grad_fn=<AddBackward0>)\n",
      "epoch: 14983 loss is tensor([-0.4595], grad_fn=<AddBackward0>)\n",
      "epoch: 14984 loss is tensor([-0.4857], grad_fn=<AddBackward0>)\n",
      "epoch: 14985 loss is tensor([-0.4597], grad_fn=<AddBackward0>)\n",
      "epoch: 14986 loss is tensor([-0.5377], grad_fn=<AddBackward0>)\n",
      "epoch: 14987 loss is tensor([-0.4970], grad_fn=<AddBackward0>)\n",
      "epoch: 14988 loss is tensor([-0.5476], grad_fn=<AddBackward0>)\n",
      "epoch: 14989 loss is tensor([-0.4829], grad_fn=<AddBackward0>)\n",
      "epoch: 14990 loss is tensor([-0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 14991 loss is tensor([-0.5585], grad_fn=<AddBackward0>)\n",
      "epoch: 14992 loss is tensor([-0.5316], grad_fn=<AddBackward0>)\n",
      "epoch: 14993 loss is tensor([-0.5669], grad_fn=<AddBackward0>)\n",
      "epoch: 14994 loss is tensor([-0.5539], grad_fn=<AddBackward0>)\n",
      "epoch: 14995 loss is tensor([-0.5571], grad_fn=<AddBackward0>)\n",
      "epoch: 14996 loss is tensor([-0.5791], grad_fn=<AddBackward0>)\n",
      "epoch: 14997 loss is tensor([-0.5465], grad_fn=<AddBackward0>)\n",
      "epoch: 14998 loss is tensor([-0.6087], grad_fn=<AddBackward0>)\n",
      "epoch: 14999 loss is tensor([-0.5776], grad_fn=<AddBackward0>)\n",
      "epoch: 15000 loss is tensor([-0.5313], grad_fn=<AddBackward0>)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15001 loss is tensor([-0.5627], grad_fn=<AddBackward0>)\n",
      "epoch: 15002 loss is tensor([-0.5590], grad_fn=<AddBackward0>)\n",
      "epoch: 15003 loss is tensor([-0.6026], grad_fn=<AddBackward0>)\n",
      "epoch: 15004 loss is tensor([-0.6161], grad_fn=<AddBackward0>)\n",
      "epoch: 15005 loss is tensor([-0.5541], grad_fn=<AddBackward0>)\n",
      "epoch: 15006 loss is tensor([-0.5687], grad_fn=<AddBackward0>)\n",
      "epoch: 15007 loss is tensor([-0.5197], grad_fn=<AddBackward0>)\n",
      "epoch: 15008 loss is tensor([-0.5599], grad_fn=<AddBackward0>)\n",
      "epoch: 15009 loss is tensor([-0.5451], grad_fn=<AddBackward0>)\n",
      "epoch: 15010 loss is tensor([-0.5578], grad_fn=<AddBackward0>)\n",
      "epoch: 15011 loss is tensor([-0.5546], grad_fn=<AddBackward0>)\n",
      "epoch: 15012 loss is tensor([-0.5425], grad_fn=<AddBackward0>)\n",
      "epoch: 15013 loss is tensor([-0.6092], grad_fn=<AddBackward0>)\n",
      "epoch: 15014 loss is tensor([-0.5969], grad_fn=<AddBackward0>)\n",
      "epoch: 15015 loss is tensor([-0.5459], grad_fn=<AddBackward0>)\n",
      "epoch: 15016 loss is tensor([-0.6082], grad_fn=<AddBackward0>)\n",
      "epoch: 15017 loss is tensor([-0.5476], grad_fn=<AddBackward0>)\n",
      "epoch: 15018 loss is tensor([-0.5758], grad_fn=<AddBackward0>)\n",
      "epoch: 15019 loss is tensor([-0.5769], grad_fn=<AddBackward0>)\n",
      "epoch: 15020 loss is tensor([-0.6150], grad_fn=<AddBackward0>)\n",
      "epoch: 15021 loss is tensor([-0.6216], grad_fn=<AddBackward0>)\n",
      "epoch: 15022 loss is tensor([-0.5551], grad_fn=<AddBackward0>)\n",
      "epoch: 15023 loss is tensor([-0.5635], grad_fn=<AddBackward0>)\n",
      "epoch: 15024 loss is tensor([-0.5957], grad_fn=<AddBackward0>)\n",
      "epoch: 15025 loss is tensor([-0.6122], grad_fn=<AddBackward0>)\n",
      "epoch: 15026 loss is tensor([-0.5724], grad_fn=<AddBackward0>)\n",
      "epoch: 15027 loss is tensor([-0.5842], grad_fn=<AddBackward0>)\n",
      "epoch: 15028 loss is tensor([-0.5964], grad_fn=<AddBackward0>)\n",
      "epoch: 15029 loss is tensor([-0.5752], grad_fn=<AddBackward0>)\n",
      "epoch: 15030 loss is tensor([-0.6043], grad_fn=<AddBackward0>)\n",
      "epoch: 15031 loss is tensor([-0.5914], grad_fn=<AddBackward0>)\n",
      "epoch: 15032 loss is tensor([-0.5729], grad_fn=<AddBackward0>)\n",
      "epoch: 15033 loss is tensor([-0.6305], grad_fn=<AddBackward0>)\n",
      "epoch: 15034 loss is tensor([-0.6101], grad_fn=<AddBackward0>)\n",
      "epoch: 15035 loss is tensor([-0.6528], grad_fn=<AddBackward0>)\n",
      "epoch: 15036 loss is tensor([-0.6291], grad_fn=<AddBackward0>)\n",
      "epoch: 15037 loss is tensor([-0.5474], grad_fn=<AddBackward0>)\n",
      "epoch: 15038 loss is tensor([-0.6154], grad_fn=<AddBackward0>)\n",
      "epoch: 15039 loss is tensor([-0.5740], grad_fn=<AddBackward0>)\n",
      "epoch: 15040 loss is tensor([-0.6467], grad_fn=<AddBackward0>)\n",
      "epoch: 15041 loss is tensor([-0.6103], grad_fn=<AddBackward0>)\n",
      "epoch: 15042 loss is tensor([-0.4958], grad_fn=<AddBackward0>)\n",
      "epoch: 15043 loss is tensor([-0.5418], grad_fn=<AddBackward0>)\n",
      "epoch: 15044 loss is tensor([-0.5608], grad_fn=<AddBackward0>)\n",
      "epoch: 15045 loss is tensor([-0.5728], grad_fn=<AddBackward0>)\n",
      "epoch: 15046 loss is tensor([-0.5950], grad_fn=<AddBackward0>)\n",
      "epoch: 15047 loss is tensor([-0.6007], grad_fn=<AddBackward0>)\n",
      "epoch: 15048 loss is tensor([-0.5876], grad_fn=<AddBackward0>)\n",
      "epoch: 15049 loss is tensor([-0.5283], grad_fn=<AddBackward0>)\n",
      "epoch: 15050 loss is tensor([-0.6311], grad_fn=<AddBackward0>)\n",
      "epoch: 15051 loss is tensor([-0.5678], grad_fn=<AddBackward0>)\n",
      "epoch: 15052 loss is tensor([-0.6333], grad_fn=<AddBackward0>)\n",
      "epoch: 15053 loss is tensor([-0.5298], grad_fn=<AddBackward0>)\n",
      "epoch: 15054 loss is tensor([-0.5705], grad_fn=<AddBackward0>)\n",
      "epoch: 15055 loss is tensor([-0.5755], grad_fn=<AddBackward0>)\n",
      "epoch: 15056 loss is tensor([-0.5732], grad_fn=<AddBackward0>)\n",
      "epoch: 15057 loss is tensor([-0.6059], grad_fn=<AddBackward0>)\n",
      "epoch: 15058 loss is tensor([-0.6033], grad_fn=<AddBackward0>)\n",
      "epoch: 15059 loss is tensor([-0.6079], grad_fn=<AddBackward0>)\n",
      "epoch: 15060 loss is tensor([-0.6030], grad_fn=<AddBackward0>)\n",
      "epoch: 15061 loss is tensor([-0.6459], grad_fn=<AddBackward0>)\n",
      "epoch: 15062 loss is tensor([-0.6121], grad_fn=<AddBackward0>)\n",
      "epoch: 15063 loss is tensor([-0.6431], grad_fn=<AddBackward0>)\n",
      "epoch: 15064 loss is tensor([-0.5908], grad_fn=<AddBackward0>)\n",
      "epoch: 15065 loss is tensor([-0.6313], grad_fn=<AddBackward0>)\n",
      "epoch: 15066 loss is tensor([-0.5977], grad_fn=<AddBackward0>)\n",
      "epoch: 15067 loss is tensor([-0.6255], grad_fn=<AddBackward0>)\n",
      "epoch: 15068 loss is tensor([-0.6555], grad_fn=<AddBackward0>)\n",
      "epoch: 15069 loss is tensor([-0.6263], grad_fn=<AddBackward0>)\n",
      "epoch: 15070 loss is tensor([-0.5948], grad_fn=<AddBackward0>)\n",
      "epoch: 15071 loss is tensor([-0.5884], grad_fn=<AddBackward0>)\n",
      "epoch: 15072 loss is tensor([-0.5123], grad_fn=<AddBackward0>)\n",
      "epoch: 15073 loss is tensor([-0.6005], grad_fn=<AddBackward0>)\n",
      "epoch: 15074 loss is tensor([-0.5725], grad_fn=<AddBackward0>)\n",
      "epoch: 15075 loss is tensor([-0.5660], grad_fn=<AddBackward0>)\n",
      "epoch: 15076 loss is tensor([-0.5902], grad_fn=<AddBackward0>)\n",
      "epoch: 15077 loss is tensor([-0.6228], grad_fn=<AddBackward0>)\n",
      "epoch: 15078 loss is tensor([-0.5613], grad_fn=<AddBackward0>)\n",
      "epoch: 15079 loss is tensor([-0.5813], grad_fn=<AddBackward0>)\n",
      "epoch: 15080 loss is tensor([-0.5808], grad_fn=<AddBackward0>)\n",
      "epoch: 15081 loss is tensor([-0.6087], grad_fn=<AddBackward0>)\n",
      "epoch: 15082 loss is tensor([-0.6471], grad_fn=<AddBackward0>)\n",
      "epoch: 15083 loss is tensor([-0.6158], grad_fn=<AddBackward0>)\n",
      "epoch: 15084 loss is tensor([-0.5553], grad_fn=<AddBackward0>)\n",
      "epoch: 15085 loss is tensor([-0.5956], grad_fn=<AddBackward0>)\n",
      "epoch: 15086 loss is tensor([-0.6840], grad_fn=<AddBackward0>)\n",
      "epoch: 15087 loss is tensor([-0.6628], grad_fn=<AddBackward0>)\n",
      "epoch: 15088 loss is tensor([-0.6077], grad_fn=<AddBackward0>)\n",
      "epoch: 15089 loss is tensor([-0.6691], grad_fn=<AddBackward0>)\n",
      "epoch: 15090 loss is tensor([-0.6129], grad_fn=<AddBackward0>)\n",
      "epoch: 15091 loss is tensor([-0.6515], grad_fn=<AddBackward0>)\n",
      "epoch: 15092 loss is tensor([-0.5307], grad_fn=<AddBackward0>)\n",
      "epoch: 15093 loss is tensor([-0.4230], grad_fn=<AddBackward0>)\n",
      "epoch: 15094 loss is tensor([-0.4817], grad_fn=<AddBackward0>)\n",
      "epoch: 15095 loss is tensor([-0.5341], grad_fn=<AddBackward0>)\n",
      "epoch: 15096 loss is tensor([-0.5108], grad_fn=<AddBackward0>)\n",
      "epoch: 15097 loss is tensor([-0.5319], grad_fn=<AddBackward0>)\n",
      "epoch: 15098 loss is tensor([-0.5270], grad_fn=<AddBackward0>)\n",
      "epoch: 15099 loss is tensor([-0.5625], grad_fn=<AddBackward0>)\n",
      "epoch: 15100 loss is tensor([-0.5797], grad_fn=<AddBackward0>)\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15101 loss is tensor([-0.4829], grad_fn=<AddBackward0>)\n",
      "epoch: 15102 loss is tensor([-0.5575], grad_fn=<AddBackward0>)\n",
      "epoch: 15103 loss is tensor([-0.5437], grad_fn=<AddBackward0>)\n",
      "epoch: 15104 loss is tensor([-0.5490], grad_fn=<AddBackward0>)\n",
      "epoch: 15105 loss is tensor([-0.6187], grad_fn=<AddBackward0>)\n",
      "epoch: 15106 loss is tensor([-0.5458], grad_fn=<AddBackward0>)\n",
      "epoch: 15107 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "epoch: 15108 loss is tensor([-0.6087], grad_fn=<AddBackward0>)\n",
      "epoch: 15109 loss is tensor([-0.6124], grad_fn=<AddBackward0>)\n",
      "epoch: 15110 loss is tensor([-0.5990], grad_fn=<AddBackward0>)\n",
      "epoch: 15111 loss is tensor([-0.5934], grad_fn=<AddBackward0>)\n",
      "epoch: 15112 loss is tensor([-0.6508], grad_fn=<AddBackward0>)\n",
      "epoch: 15113 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 15114 loss is tensor([-0.6549], grad_fn=<AddBackward0>)\n",
      "epoch: 15115 loss is tensor([-0.6074], grad_fn=<AddBackward0>)\n",
      "epoch: 15116 loss is tensor([-0.5948], grad_fn=<AddBackward0>)\n",
      "epoch: 15117 loss is tensor([-0.6427], grad_fn=<AddBackward0>)\n",
      "epoch: 15118 loss is tensor([-0.6170], grad_fn=<AddBackward0>)\n",
      "epoch: 15119 loss is tensor([-0.6436], grad_fn=<AddBackward0>)\n",
      "epoch: 15120 loss is tensor([-0.6245], grad_fn=<AddBackward0>)\n",
      "epoch: 15121 loss is tensor([-0.5700], grad_fn=<AddBackward0>)\n",
      "epoch: 15122 loss is tensor([-0.6112], grad_fn=<AddBackward0>)\n",
      "epoch: 15123 loss is tensor([-0.6066], grad_fn=<AddBackward0>)\n",
      "epoch: 15124 loss is tensor([-0.6155], grad_fn=<AddBackward0>)\n",
      "epoch: 15125 loss is tensor([-0.6335], grad_fn=<AddBackward0>)\n",
      "epoch: 15126 loss is tensor([-0.6143], grad_fn=<AddBackward0>)\n",
      "epoch: 15127 loss is tensor([-0.6327], grad_fn=<AddBackward0>)\n",
      "epoch: 15128 loss is tensor([-0.6546], grad_fn=<AddBackward0>)\n",
      "epoch: 15129 loss is tensor([-0.6636], grad_fn=<AddBackward0>)\n",
      "epoch: 15130 loss is tensor([-0.6375], grad_fn=<AddBackward0>)\n",
      "epoch: 15131 loss is tensor([-0.6277], grad_fn=<AddBackward0>)\n",
      "epoch: 15132 loss is tensor([-0.6285], grad_fn=<AddBackward0>)\n",
      "epoch: 15133 loss is tensor([-0.6262], grad_fn=<AddBackward0>)\n",
      "epoch: 15134 loss is tensor([-0.5774], grad_fn=<AddBackward0>)\n",
      "epoch: 15135 loss is tensor([-0.6390], grad_fn=<AddBackward0>)\n",
      "epoch: 15136 loss is tensor([-0.6019], grad_fn=<AddBackward0>)\n",
      "epoch: 15137 loss is tensor([-0.6439], grad_fn=<AddBackward0>)\n",
      "epoch: 15138 loss is tensor([-0.6352], grad_fn=<AddBackward0>)\n",
      "epoch: 15139 loss is tensor([-0.6289], grad_fn=<AddBackward0>)\n",
      "epoch: 15140 loss is tensor([-0.6463], grad_fn=<AddBackward0>)\n",
      "epoch: 15141 loss is tensor([-0.6456], grad_fn=<AddBackward0>)\n",
      "epoch: 15142 loss is tensor([-0.6549], grad_fn=<AddBackward0>)\n",
      "epoch: 15143 loss is tensor([-0.5921], grad_fn=<AddBackward0>)\n",
      "epoch: 15144 loss is tensor([-0.6494], grad_fn=<AddBackward0>)\n",
      "epoch: 15145 loss is tensor([-0.6105], grad_fn=<AddBackward0>)\n",
      "epoch: 15146 loss is tensor([-0.6639], grad_fn=<AddBackward0>)\n",
      "epoch: 15147 loss is tensor([-0.6481], grad_fn=<AddBackward0>)\n",
      "epoch: 15148 loss is tensor([-0.6095], grad_fn=<AddBackward0>)\n",
      "epoch: 15149 loss is tensor([-0.6393], grad_fn=<AddBackward0>)\n",
      "epoch: 15150 loss is tensor([-0.6313], grad_fn=<AddBackward0>)\n",
      "epoch: 15151 loss is tensor([-0.5754], grad_fn=<AddBackward0>)\n",
      "epoch: 15152 loss is tensor([-0.5898], grad_fn=<AddBackward0>)\n",
      "epoch: 15153 loss is tensor([-0.6274], grad_fn=<AddBackward0>)\n",
      "epoch: 15154 loss is tensor([-0.5935], grad_fn=<AddBackward0>)\n",
      "epoch: 15155 loss is tensor([-0.6755], grad_fn=<AddBackward0>)\n",
      "epoch: 15156 loss is tensor([-0.6055], grad_fn=<AddBackward0>)\n",
      "epoch: 15157 loss is tensor([-0.6496], grad_fn=<AddBackward0>)\n",
      "epoch: 15158 loss is tensor([-0.6640], grad_fn=<AddBackward0>)\n",
      "epoch: 15159 loss is tensor([-0.5859], grad_fn=<AddBackward0>)\n",
      "epoch: 15160 loss is tensor([-0.6407], grad_fn=<AddBackward0>)\n",
      "epoch: 15161 loss is tensor([-0.6386], grad_fn=<AddBackward0>)\n",
      "epoch: 15162 loss is tensor([-0.6454], grad_fn=<AddBackward0>)\n",
      "epoch: 15163 loss is tensor([-0.6002], grad_fn=<AddBackward0>)\n",
      "epoch: 15164 loss is tensor([-0.6686], grad_fn=<AddBackward0>)\n",
      "epoch: 15165 loss is tensor([-0.6643], grad_fn=<AddBackward0>)\n",
      "epoch: 15166 loss is tensor([-0.5784], grad_fn=<AddBackward0>)\n",
      "epoch: 15167 loss is tensor([-0.6213], grad_fn=<AddBackward0>)\n",
      "epoch: 15168 loss is tensor([-0.6511], grad_fn=<AddBackward0>)\n",
      "epoch: 15169 loss is tensor([-0.6265], grad_fn=<AddBackward0>)\n",
      "epoch: 15170 loss is tensor([-0.6007], grad_fn=<AddBackward0>)\n",
      "epoch: 15171 loss is tensor([-0.5879], grad_fn=<AddBackward0>)\n",
      "epoch: 15172 loss is tensor([-0.6287], grad_fn=<AddBackward0>)\n",
      "epoch: 15173 loss is tensor([-0.6090], grad_fn=<AddBackward0>)\n",
      "epoch: 15174 loss is tensor([-0.6350], grad_fn=<AddBackward0>)\n",
      "epoch: 15175 loss is tensor([-0.6155], grad_fn=<AddBackward0>)\n",
      "epoch: 15176 loss is tensor([-0.6320], grad_fn=<AddBackward0>)\n",
      "epoch: 15177 loss is tensor([-0.6368], grad_fn=<AddBackward0>)\n",
      "epoch: 15178 loss is tensor([-0.6207], grad_fn=<AddBackward0>)\n",
      "epoch: 15179 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 15180 loss is tensor([-0.6480], grad_fn=<AddBackward0>)\n",
      "epoch: 15181 loss is tensor([-0.6336], grad_fn=<AddBackward0>)\n",
      "epoch: 15182 loss is tensor([-0.5690], grad_fn=<AddBackward0>)\n",
      "epoch: 15183 loss is tensor([-0.6213], grad_fn=<AddBackward0>)\n",
      "epoch: 15184 loss is tensor([-0.6607], grad_fn=<AddBackward0>)\n",
      "epoch: 15185 loss is tensor([-0.6099], grad_fn=<AddBackward0>)\n",
      "epoch: 15186 loss is tensor([-0.5795], grad_fn=<AddBackward0>)\n",
      "epoch: 15187 loss is tensor([-0.5923], grad_fn=<AddBackward0>)\n",
      "epoch: 15188 loss is tensor([-0.6187], grad_fn=<AddBackward0>)\n",
      "epoch: 15189 loss is tensor([-0.5755], grad_fn=<AddBackward0>)\n",
      "epoch: 15190 loss is tensor([-0.5813], grad_fn=<AddBackward0>)\n",
      "epoch: 15191 loss is tensor([-0.7137], grad_fn=<AddBackward0>)\n",
      "epoch: 15192 loss is tensor([-0.6418], grad_fn=<AddBackward0>)\n",
      "epoch: 15193 loss is tensor([-0.5879], grad_fn=<AddBackward0>)\n",
      "epoch: 15194 loss is tensor([-0.5897], grad_fn=<AddBackward0>)\n",
      "epoch: 15195 loss is tensor([-0.5626], grad_fn=<AddBackward0>)\n",
      "epoch: 15196 loss is tensor([-0.6220], grad_fn=<AddBackward0>)\n",
      "epoch: 15197 loss is tensor([-0.5965], grad_fn=<AddBackward0>)\n",
      "epoch: 15198 loss is tensor([-0.6381], grad_fn=<AddBackward0>)\n",
      "epoch: 15199 loss is tensor([-0.5962], grad_fn=<AddBackward0>)\n",
      "epoch: 15200 loss is tensor([-0.6435], grad_fn=<AddBackward0>)\n",
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15201 loss is tensor([-0.5755], grad_fn=<AddBackward0>)\n",
      "epoch: 15202 loss is tensor([-0.6065], grad_fn=<AddBackward0>)\n",
      "epoch: 15203 loss is tensor([-0.6100], grad_fn=<AddBackward0>)\n",
      "epoch: 15204 loss is tensor([-0.5659], grad_fn=<AddBackward0>)\n",
      "epoch: 15205 loss is tensor([-0.5786], grad_fn=<AddBackward0>)\n",
      "epoch: 15206 loss is tensor([-0.5718], grad_fn=<AddBackward0>)\n",
      "epoch: 15207 loss is tensor([-0.6331], grad_fn=<AddBackward0>)\n",
      "epoch: 15208 loss is tensor([-0.6117], grad_fn=<AddBackward0>)\n",
      "epoch: 15209 loss is tensor([-0.6279], grad_fn=<AddBackward0>)\n",
      "epoch: 15210 loss is tensor([-0.5796], grad_fn=<AddBackward0>)\n",
      "epoch: 15211 loss is tensor([-0.5913], grad_fn=<AddBackward0>)\n",
      "epoch: 15212 loss is tensor([-0.6059], grad_fn=<AddBackward0>)\n",
      "epoch: 15213 loss is tensor([-0.6166], grad_fn=<AddBackward0>)\n",
      "epoch: 15214 loss is tensor([-0.6334], grad_fn=<AddBackward0>)\n",
      "epoch: 15215 loss is tensor([-0.5941], grad_fn=<AddBackward0>)\n",
      "epoch: 15216 loss is tensor([-0.6186], grad_fn=<AddBackward0>)\n",
      "epoch: 15217 loss is tensor([-0.6521], grad_fn=<AddBackward0>)\n",
      "epoch: 15218 loss is tensor([-0.6451], grad_fn=<AddBackward0>)\n",
      "epoch: 15219 loss is tensor([-0.6629], grad_fn=<AddBackward0>)\n",
      "epoch: 15220 loss is tensor([-0.5705], grad_fn=<AddBackward0>)\n",
      "epoch: 15221 loss is tensor([-0.6080], grad_fn=<AddBackward0>)\n",
      "epoch: 15222 loss is tensor([-0.6462], grad_fn=<AddBackward0>)\n",
      "epoch: 15223 loss is tensor([-0.6684], grad_fn=<AddBackward0>)\n",
      "epoch: 15224 loss is tensor([-0.5433], grad_fn=<AddBackward0>)\n",
      "epoch: 15225 loss is tensor([-0.5489], grad_fn=<AddBackward0>)\n",
      "epoch: 15226 loss is tensor([-0.6048], grad_fn=<AddBackward0>)\n",
      "epoch: 15227 loss is tensor([-0.6169], grad_fn=<AddBackward0>)\n",
      "epoch: 15228 loss is tensor([-0.5730], grad_fn=<AddBackward0>)\n",
      "epoch: 15229 loss is tensor([-0.5752], grad_fn=<AddBackward0>)\n",
      "epoch: 15230 loss is tensor([-0.5632], grad_fn=<AddBackward0>)\n",
      "epoch: 15231 loss is tensor([-0.5504], grad_fn=<AddBackward0>)\n",
      "epoch: 15232 loss is tensor([-0.5877], grad_fn=<AddBackward0>)\n",
      "epoch: 15233 loss is tensor([-0.5698], grad_fn=<AddBackward0>)\n",
      "epoch: 15234 loss is tensor([-0.5907], grad_fn=<AddBackward0>)\n",
      "epoch: 15235 loss is tensor([-0.6422], grad_fn=<AddBackward0>)\n",
      "epoch: 15236 loss is tensor([-0.5741], grad_fn=<AddBackward0>)\n",
      "epoch: 15237 loss is tensor([-0.6099], grad_fn=<AddBackward0>)\n",
      "epoch: 15238 loss is tensor([-0.6078], grad_fn=<AddBackward0>)\n",
      "epoch: 15239 loss is tensor([-0.5780], grad_fn=<AddBackward0>)\n",
      "epoch: 15240 loss is tensor([-0.5860], grad_fn=<AddBackward0>)\n",
      "epoch: 15241 loss is tensor([-0.5939], grad_fn=<AddBackward0>)\n",
      "epoch: 15242 loss is tensor([-0.5375], grad_fn=<AddBackward0>)\n",
      "epoch: 15243 loss is tensor([-0.5450], grad_fn=<AddBackward0>)\n",
      "epoch: 15244 loss is tensor([-0.5773], grad_fn=<AddBackward0>)\n",
      "epoch: 15245 loss is tensor([-0.5659], grad_fn=<AddBackward0>)\n",
      "epoch: 15246 loss is tensor([-0.6098], grad_fn=<AddBackward0>)\n",
      "epoch: 15247 loss is tensor([-0.5870], grad_fn=<AddBackward0>)\n",
      "epoch: 15248 loss is tensor([-0.5635], grad_fn=<AddBackward0>)\n",
      "epoch: 15249 loss is tensor([-0.5755], grad_fn=<AddBackward0>)\n",
      "epoch: 15250 loss is tensor([-0.6135], grad_fn=<AddBackward0>)\n",
      "epoch: 15251 loss is tensor([-0.5973], grad_fn=<AddBackward0>)\n",
      "epoch: 15252 loss is tensor([-0.5835], grad_fn=<AddBackward0>)\n",
      "epoch: 15253 loss is tensor([-0.6134], grad_fn=<AddBackward0>)\n",
      "epoch: 15254 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 15255 loss is tensor([-0.6558], grad_fn=<AddBackward0>)\n",
      "epoch: 15256 loss is tensor([-0.6091], grad_fn=<AddBackward0>)\n",
      "epoch: 15257 loss is tensor([-0.6064], grad_fn=<AddBackward0>)\n",
      "epoch: 15258 loss is tensor([-0.5851], grad_fn=<AddBackward0>)\n",
      "epoch: 15259 loss is tensor([-0.6156], grad_fn=<AddBackward0>)\n",
      "epoch: 15260 loss is tensor([-0.6977], grad_fn=<AddBackward0>)\n",
      "epoch: 15261 loss is tensor([-0.6260], grad_fn=<AddBackward0>)\n",
      "epoch: 15262 loss is tensor([-0.5879], grad_fn=<AddBackward0>)\n",
      "epoch: 15263 loss is tensor([-0.5590], grad_fn=<AddBackward0>)\n",
      "epoch: 15264 loss is tensor([-0.5073], grad_fn=<AddBackward0>)\n",
      "epoch: 15265 loss is tensor([-0.6037], grad_fn=<AddBackward0>)\n",
      "epoch: 15266 loss is tensor([-0.5068], grad_fn=<AddBackward0>)\n",
      "epoch: 15267 loss is tensor([-0.5688], grad_fn=<AddBackward0>)\n",
      "epoch: 15268 loss is tensor([-0.5696], grad_fn=<AddBackward0>)\n",
      "epoch: 15269 loss is tensor([-0.5544], grad_fn=<AddBackward0>)\n",
      "epoch: 15270 loss is tensor([-0.5879], grad_fn=<AddBackward0>)\n",
      "epoch: 15271 loss is tensor([-0.5767], grad_fn=<AddBackward0>)\n",
      "epoch: 15272 loss is tensor([-0.6473], grad_fn=<AddBackward0>)\n",
      "epoch: 15273 loss is tensor([-0.5822], grad_fn=<AddBackward0>)\n",
      "epoch: 15274 loss is tensor([-0.6044], grad_fn=<AddBackward0>)\n",
      "epoch: 15275 loss is tensor([-0.5730], grad_fn=<AddBackward0>)\n",
      "epoch: 15276 loss is tensor([-0.5660], grad_fn=<AddBackward0>)\n",
      "epoch: 15277 loss is tensor([-0.6154], grad_fn=<AddBackward0>)\n",
      "epoch: 15278 loss is tensor([-0.6030], grad_fn=<AddBackward0>)\n",
      "epoch: 15279 loss is tensor([-0.5564], grad_fn=<AddBackward0>)\n",
      "epoch: 15280 loss is tensor([-0.5860], grad_fn=<AddBackward0>)\n",
      "epoch: 15281 loss is tensor([-0.5357], grad_fn=<AddBackward0>)\n",
      "epoch: 15282 loss is tensor([-0.5739], grad_fn=<AddBackward0>)\n",
      "epoch: 15283 loss is tensor([-0.6408], grad_fn=<AddBackward0>)\n",
      "epoch: 15284 loss is tensor([-0.5774], grad_fn=<AddBackward0>)\n",
      "epoch: 15285 loss is tensor([-0.5876], grad_fn=<AddBackward0>)\n",
      "epoch: 15286 loss is tensor([-0.6501], grad_fn=<AddBackward0>)\n",
      "epoch: 15287 loss is tensor([-0.6149], grad_fn=<AddBackward0>)\n",
      "epoch: 15288 loss is tensor([-0.6455], grad_fn=<AddBackward0>)\n",
      "epoch: 15289 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 15290 loss is tensor([-0.6367], grad_fn=<AddBackward0>)\n",
      "epoch: 15291 loss is tensor([-0.7014], grad_fn=<AddBackward0>)\n",
      "epoch: 15292 loss is tensor([-0.6242], grad_fn=<AddBackward0>)\n",
      "epoch: 15293 loss is tensor([-0.6303], grad_fn=<AddBackward0>)\n",
      "epoch: 15294 loss is tensor([-0.5901], grad_fn=<AddBackward0>)\n",
      "epoch: 15295 loss is tensor([-0.6238], grad_fn=<AddBackward0>)\n",
      "epoch: 15296 loss is tensor([-0.6531], grad_fn=<AddBackward0>)\n",
      "epoch: 15297 loss is tensor([-0.6505], grad_fn=<AddBackward0>)\n",
      "epoch: 15298 loss is tensor([-0.6215], grad_fn=<AddBackward0>)\n",
      "epoch: 15299 loss is tensor([-0.5924], grad_fn=<AddBackward0>)\n",
      "epoch: 15300 loss is tensor([-0.6695], grad_fn=<AddBackward0>)\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15301 loss is tensor([-0.6647], grad_fn=<AddBackward0>)\n",
      "epoch: 15302 loss is tensor([-0.6569], grad_fn=<AddBackward0>)\n",
      "epoch: 15303 loss is tensor([-0.6843], grad_fn=<AddBackward0>)\n",
      "epoch: 15304 loss is tensor([-0.6874], grad_fn=<AddBackward0>)\n",
      "epoch: 15305 loss is tensor([-0.6815], grad_fn=<AddBackward0>)\n",
      "epoch: 15306 loss is tensor([-0.6641], grad_fn=<AddBackward0>)\n",
      "epoch: 15307 loss is tensor([-0.5952], grad_fn=<AddBackward0>)\n",
      "epoch: 15308 loss is tensor([-0.6698], grad_fn=<AddBackward0>)\n",
      "epoch: 15309 loss is tensor([-0.6494], grad_fn=<AddBackward0>)\n",
      "epoch: 15310 loss is tensor([-0.6411], grad_fn=<AddBackward0>)\n",
      "epoch: 15311 loss is tensor([-0.6092], grad_fn=<AddBackward0>)\n",
      "epoch: 15312 loss is tensor([-0.6265], grad_fn=<AddBackward0>)\n",
      "epoch: 15313 loss is tensor([-0.6278], grad_fn=<AddBackward0>)\n",
      "epoch: 15314 loss is tensor([-0.6510], grad_fn=<AddBackward0>)\n",
      "epoch: 15315 loss is tensor([-0.6637], grad_fn=<AddBackward0>)\n",
      "epoch: 15316 loss is tensor([-0.6020], grad_fn=<AddBackward0>)\n",
      "epoch: 15317 loss is tensor([-0.6531], grad_fn=<AddBackward0>)\n",
      "epoch: 15318 loss is tensor([-0.6328], grad_fn=<AddBackward0>)\n",
      "epoch: 15319 loss is tensor([-0.6730], grad_fn=<AddBackward0>)\n",
      "epoch: 15320 loss is tensor([-0.6028], grad_fn=<AddBackward0>)\n",
      "epoch: 15321 loss is tensor([-0.6301], grad_fn=<AddBackward0>)\n",
      "epoch: 15322 loss is tensor([-0.6627], grad_fn=<AddBackward0>)\n",
      "epoch: 15323 loss is tensor([-0.6877], grad_fn=<AddBackward0>)\n",
      "epoch: 15324 loss is tensor([-0.7004], grad_fn=<AddBackward0>)\n",
      "epoch: 15325 loss is tensor([-0.6499], grad_fn=<AddBackward0>)\n",
      "epoch: 15326 loss is tensor([-0.6232], grad_fn=<AddBackward0>)\n",
      "epoch: 15327 loss is tensor([-0.6425], grad_fn=<AddBackward0>)\n",
      "epoch: 15328 loss is tensor([-0.6326], grad_fn=<AddBackward0>)\n",
      "epoch: 15329 loss is tensor([-0.6797], grad_fn=<AddBackward0>)\n",
      "epoch: 15330 loss is tensor([-0.6506], grad_fn=<AddBackward0>)\n",
      "epoch: 15331 loss is tensor([-0.6201], grad_fn=<AddBackward0>)\n",
      "epoch: 15332 loss is tensor([-0.6632], grad_fn=<AddBackward0>)\n",
      "epoch: 15333 loss is tensor([-0.6810], grad_fn=<AddBackward0>)\n",
      "epoch: 15334 loss is tensor([-0.6176], grad_fn=<AddBackward0>)\n",
      "epoch: 15335 loss is tensor([-0.6882], grad_fn=<AddBackward0>)\n",
      "epoch: 15336 loss is tensor([-0.6451], grad_fn=<AddBackward0>)\n",
      "epoch: 15337 loss is tensor([-0.6248], grad_fn=<AddBackward0>)\n",
      "epoch: 15338 loss is tensor([-0.6300], grad_fn=<AddBackward0>)\n",
      "epoch: 15339 loss is tensor([-0.6369], grad_fn=<AddBackward0>)\n",
      "epoch: 15340 loss is tensor([-0.6240], grad_fn=<AddBackward0>)\n",
      "epoch: 15341 loss is tensor([-0.6004], grad_fn=<AddBackward0>)\n",
      "epoch: 15342 loss is tensor([-0.6461], grad_fn=<AddBackward0>)\n",
      "epoch: 15343 loss is tensor([-0.6058], grad_fn=<AddBackward0>)\n",
      "epoch: 15344 loss is tensor([-0.5810], grad_fn=<AddBackward0>)\n",
      "epoch: 15345 loss is tensor([-0.5991], grad_fn=<AddBackward0>)\n",
      "epoch: 15346 loss is tensor([-0.6881], grad_fn=<AddBackward0>)\n",
      "epoch: 15347 loss is tensor([-0.6652], grad_fn=<AddBackward0>)\n",
      "epoch: 15348 loss is tensor([-0.6385], grad_fn=<AddBackward0>)\n",
      "epoch: 15349 loss is tensor([-0.6038], grad_fn=<AddBackward0>)\n",
      "epoch: 15350 loss is tensor([-0.5566], grad_fn=<AddBackward0>)\n",
      "epoch: 15351 loss is tensor([-0.6421], grad_fn=<AddBackward0>)\n",
      "epoch: 15352 loss is tensor([-0.5934], grad_fn=<AddBackward0>)\n",
      "epoch: 15353 loss is tensor([-0.6140], grad_fn=<AddBackward0>)\n",
      "epoch: 15354 loss is tensor([-0.6372], grad_fn=<AddBackward0>)\n",
      "epoch: 15355 loss is tensor([-0.6289], grad_fn=<AddBackward0>)\n",
      "epoch: 15356 loss is tensor([-0.6057], grad_fn=<AddBackward0>)\n",
      "epoch: 15357 loss is tensor([-0.6535], grad_fn=<AddBackward0>)\n",
      "epoch: 15358 loss is tensor([-0.6062], grad_fn=<AddBackward0>)\n",
      "epoch: 15359 loss is tensor([-0.6330], grad_fn=<AddBackward0>)\n",
      "epoch: 15360 loss is tensor([-0.6680], grad_fn=<AddBackward0>)\n",
      "epoch: 15361 loss is tensor([-0.6548], grad_fn=<AddBackward0>)\n",
      "epoch: 15362 loss is tensor([-0.6286], grad_fn=<AddBackward0>)\n",
      "epoch: 15363 loss is tensor([-0.6116], grad_fn=<AddBackward0>)\n",
      "epoch: 15364 loss is tensor([-0.6397], grad_fn=<AddBackward0>)\n",
      "epoch: 15365 loss is tensor([-0.5908], grad_fn=<AddBackward0>)\n",
      "epoch: 15366 loss is tensor([-0.6231], grad_fn=<AddBackward0>)\n",
      "epoch: 15367 loss is tensor([-0.6321], grad_fn=<AddBackward0>)\n",
      "epoch: 15368 loss is tensor([-0.6589], grad_fn=<AddBackward0>)\n",
      "epoch: 15369 loss is tensor([-0.6349], grad_fn=<AddBackward0>)\n",
      "epoch: 15370 loss is tensor([-0.6323], grad_fn=<AddBackward0>)\n",
      "epoch: 15371 loss is tensor([-0.6344], grad_fn=<AddBackward0>)\n",
      "epoch: 15372 loss is tensor([-0.6587], grad_fn=<AddBackward0>)\n",
      "epoch: 15373 loss is tensor([-0.6394], grad_fn=<AddBackward0>)\n",
      "epoch: 15374 loss is tensor([-0.6526], grad_fn=<AddBackward0>)\n",
      "epoch: 15375 loss is tensor([-0.6212], grad_fn=<AddBackward0>)\n",
      "epoch: 15376 loss is tensor([-0.6397], grad_fn=<AddBackward0>)\n",
      "epoch: 15377 loss is tensor([-0.6370], grad_fn=<AddBackward0>)\n",
      "epoch: 15378 loss is tensor([-0.6188], grad_fn=<AddBackward0>)\n",
      "epoch: 15379 loss is tensor([-0.6104], grad_fn=<AddBackward0>)\n",
      "epoch: 15380 loss is tensor([-0.6129], grad_fn=<AddBackward0>)\n",
      "epoch: 15381 loss is tensor([-0.6280], grad_fn=<AddBackward0>)\n",
      "epoch: 15382 loss is tensor([-0.6550], grad_fn=<AddBackward0>)\n",
      "epoch: 15383 loss is tensor([-0.5811], grad_fn=<AddBackward0>)\n",
      "epoch: 15384 loss is tensor([-0.6555], grad_fn=<AddBackward0>)\n",
      "epoch: 15385 loss is tensor([-0.6128], grad_fn=<AddBackward0>)\n",
      "epoch: 15386 loss is tensor([-0.6671], grad_fn=<AddBackward0>)\n",
      "epoch: 15387 loss is tensor([-0.6221], grad_fn=<AddBackward0>)\n",
      "epoch: 15388 loss is tensor([-0.6307], grad_fn=<AddBackward0>)\n",
      "epoch: 15389 loss is tensor([-0.6437], grad_fn=<AddBackward0>)\n",
      "epoch: 15390 loss is tensor([-0.6181], grad_fn=<AddBackward0>)\n",
      "epoch: 15391 loss is tensor([-0.6365], grad_fn=<AddBackward0>)\n",
      "epoch: 15392 loss is tensor([-0.6248], grad_fn=<AddBackward0>)\n",
      "epoch: 15393 loss is tensor([-0.6313], grad_fn=<AddBackward0>)\n",
      "epoch: 15394 loss is tensor([-0.6262], grad_fn=<AddBackward0>)\n",
      "epoch: 15395 loss is tensor([-0.6593], grad_fn=<AddBackward0>)\n",
      "epoch: 15396 loss is tensor([-0.6145], grad_fn=<AddBackward0>)\n",
      "epoch: 15397 loss is tensor([-0.6581], grad_fn=<AddBackward0>)\n",
      "epoch: 15398 loss is tensor([-0.6181], grad_fn=<AddBackward0>)\n",
      "epoch: 15399 loss is tensor([-0.6205], grad_fn=<AddBackward0>)\n",
      "epoch: 15400 loss is tensor([-0.6451], grad_fn=<AddBackward0>)\n",
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15401 loss is tensor([-0.6685], grad_fn=<AddBackward0>)\n",
      "epoch: 15402 loss is tensor([-0.6460], grad_fn=<AddBackward0>)\n",
      "epoch: 15403 loss is tensor([-0.6464], grad_fn=<AddBackward0>)\n",
      "epoch: 15404 loss is tensor([-0.6485], grad_fn=<AddBackward0>)\n",
      "epoch: 15405 loss is tensor([-0.6538], grad_fn=<AddBackward0>)\n",
      "epoch: 15406 loss is tensor([-0.6211], grad_fn=<AddBackward0>)\n",
      "epoch: 15407 loss is tensor([-0.7023], grad_fn=<AddBackward0>)\n",
      "epoch: 15408 loss is tensor([-0.6471], grad_fn=<AddBackward0>)\n",
      "epoch: 15409 loss is tensor([-0.6550], grad_fn=<AddBackward0>)\n",
      "epoch: 15410 loss is tensor([-0.5969], grad_fn=<AddBackward0>)\n",
      "epoch: 15411 loss is tensor([-0.6177], grad_fn=<AddBackward0>)\n",
      "epoch: 15412 loss is tensor([-0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 15413 loss is tensor([-0.6456], grad_fn=<AddBackward0>)\n",
      "epoch: 15414 loss is tensor([-0.5661], grad_fn=<AddBackward0>)\n",
      "epoch: 15415 loss is tensor([-0.6820], grad_fn=<AddBackward0>)\n",
      "epoch: 15416 loss is tensor([-0.6132], grad_fn=<AddBackward0>)\n",
      "epoch: 15417 loss is tensor([-0.6002], grad_fn=<AddBackward0>)\n",
      "epoch: 15418 loss is tensor([-0.6184], grad_fn=<AddBackward0>)\n",
      "epoch: 15419 loss is tensor([-0.5329], grad_fn=<AddBackward0>)\n",
      "epoch: 15420 loss is tensor([-0.6215], grad_fn=<AddBackward0>)\n",
      "epoch: 15421 loss is tensor([-0.5701], grad_fn=<AddBackward0>)\n",
      "epoch: 15422 loss is tensor([-0.5459], grad_fn=<AddBackward0>)\n",
      "epoch: 15423 loss is tensor([-0.5919], grad_fn=<AddBackward0>)\n",
      "epoch: 15424 loss is tensor([-0.6431], grad_fn=<AddBackward0>)\n",
      "epoch: 15425 loss is tensor([-0.6681], grad_fn=<AddBackward0>)\n",
      "epoch: 15426 loss is tensor([-0.6075], grad_fn=<AddBackward0>)\n",
      "epoch: 15427 loss is tensor([-0.5923], grad_fn=<AddBackward0>)\n",
      "epoch: 15428 loss is tensor([-0.6205], grad_fn=<AddBackward0>)\n",
      "epoch: 15429 loss is tensor([-0.6344], grad_fn=<AddBackward0>)\n",
      "epoch: 15430 loss is tensor([-0.5821], grad_fn=<AddBackward0>)\n",
      "epoch: 15431 loss is tensor([-0.6505], grad_fn=<AddBackward0>)\n",
      "epoch: 15432 loss is tensor([-0.6190], grad_fn=<AddBackward0>)\n",
      "epoch: 15433 loss is tensor([-0.6066], grad_fn=<AddBackward0>)\n",
      "epoch: 15434 loss is tensor([-0.6417], grad_fn=<AddBackward0>)\n",
      "epoch: 15435 loss is tensor([-0.5862], grad_fn=<AddBackward0>)\n",
      "epoch: 15436 loss is tensor([-0.6136], grad_fn=<AddBackward0>)\n",
      "epoch: 15437 loss is tensor([-0.5459], grad_fn=<AddBackward0>)\n",
      "epoch: 15438 loss is tensor([-0.5490], grad_fn=<AddBackward0>)\n",
      "epoch: 15439 loss is tensor([-0.6076], grad_fn=<AddBackward0>)\n",
      "epoch: 15440 loss is tensor([-0.5721], grad_fn=<AddBackward0>)\n",
      "epoch: 15441 loss is tensor([-0.6397], grad_fn=<AddBackward0>)\n",
      "epoch: 15442 loss is tensor([-0.5995], grad_fn=<AddBackward0>)\n",
      "epoch: 15443 loss is tensor([-0.6078], grad_fn=<AddBackward0>)\n",
      "epoch: 15444 loss is tensor([-0.6186], grad_fn=<AddBackward0>)\n",
      "epoch: 15445 loss is tensor([-0.5586], grad_fn=<AddBackward0>)\n",
      "epoch: 15446 loss is tensor([-0.5569], grad_fn=<AddBackward0>)\n",
      "epoch: 15447 loss is tensor([-0.6144], grad_fn=<AddBackward0>)\n",
      "epoch: 15448 loss is tensor([-0.5664], grad_fn=<AddBackward0>)\n",
      "epoch: 15449 loss is tensor([-0.6454], grad_fn=<AddBackward0>)\n",
      "epoch: 15450 loss is tensor([-0.5394], grad_fn=<AddBackward0>)\n",
      "epoch: 15451 loss is tensor([-0.6258], grad_fn=<AddBackward0>)\n",
      "epoch: 15452 loss is tensor([-0.5667], grad_fn=<AddBackward0>)\n",
      "epoch: 15453 loss is tensor([-0.5133], grad_fn=<AddBackward0>)\n",
      "epoch: 15454 loss is tensor([-0.5500], grad_fn=<AddBackward0>)\n",
      "epoch: 15455 loss is tensor([-0.6197], grad_fn=<AddBackward0>)\n",
      "epoch: 15456 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "epoch: 15457 loss is tensor([-0.5555], grad_fn=<AddBackward0>)\n",
      "epoch: 15458 loss is tensor([-0.6140], grad_fn=<AddBackward0>)\n",
      "epoch: 15459 loss is tensor([-0.6438], grad_fn=<AddBackward0>)\n",
      "epoch: 15460 loss is tensor([-0.6351], grad_fn=<AddBackward0>)\n",
      "epoch: 15461 loss is tensor([-0.6015], grad_fn=<AddBackward0>)\n",
      "epoch: 15462 loss is tensor([-0.6254], grad_fn=<AddBackward0>)\n",
      "epoch: 15463 loss is tensor([-0.5686], grad_fn=<AddBackward0>)\n",
      "epoch: 15464 loss is tensor([-0.6039], grad_fn=<AddBackward0>)\n",
      "epoch: 15465 loss is tensor([-0.5903], grad_fn=<AddBackward0>)\n",
      "epoch: 15466 loss is tensor([-0.5855], grad_fn=<AddBackward0>)\n",
      "epoch: 15467 loss is tensor([-0.6018], grad_fn=<AddBackward0>)\n",
      "epoch: 15468 loss is tensor([-0.5757], grad_fn=<AddBackward0>)\n",
      "epoch: 15469 loss is tensor([-0.5710], grad_fn=<AddBackward0>)\n",
      "epoch: 15470 loss is tensor([-0.6567], grad_fn=<AddBackward0>)\n",
      "epoch: 15471 loss is tensor([-0.6157], grad_fn=<AddBackward0>)\n",
      "epoch: 15472 loss is tensor([-0.6240], grad_fn=<AddBackward0>)\n",
      "epoch: 15473 loss is tensor([-0.6292], grad_fn=<AddBackward0>)\n",
      "epoch: 15474 loss is tensor([-0.6071], grad_fn=<AddBackward0>)\n",
      "epoch: 15475 loss is tensor([-0.5992], grad_fn=<AddBackward0>)\n",
      "epoch: 15476 loss is tensor([-0.5878], grad_fn=<AddBackward0>)\n",
      "epoch: 15477 loss is tensor([-0.5523], grad_fn=<AddBackward0>)\n",
      "epoch: 15478 loss is tensor([-0.5760], grad_fn=<AddBackward0>)\n",
      "epoch: 15479 loss is tensor([-0.5964], grad_fn=<AddBackward0>)\n",
      "epoch: 15480 loss is tensor([-0.5918], grad_fn=<AddBackward0>)\n",
      "epoch: 15481 loss is tensor([-0.5978], grad_fn=<AddBackward0>)\n",
      "epoch: 15482 loss is tensor([-0.6469], grad_fn=<AddBackward0>)\n",
      "epoch: 15483 loss is tensor([-0.5369], grad_fn=<AddBackward0>)\n",
      "epoch: 15484 loss is tensor([-0.6047], grad_fn=<AddBackward0>)\n",
      "epoch: 15485 loss is tensor([-0.5831], grad_fn=<AddBackward0>)\n",
      "epoch: 15486 loss is tensor([-0.4802], grad_fn=<AddBackward0>)\n",
      "epoch: 15487 loss is tensor([-0.4358], grad_fn=<AddBackward0>)\n",
      "epoch: 15488 loss is tensor([-0.4925], grad_fn=<AddBackward0>)\n",
      "epoch: 15489 loss is tensor([-0.5589], grad_fn=<AddBackward0>)\n",
      "epoch: 15490 loss is tensor([-0.5415], grad_fn=<AddBackward0>)\n",
      "epoch: 15491 loss is tensor([-0.5611], grad_fn=<AddBackward0>)\n",
      "epoch: 15492 loss is tensor([-0.6276], grad_fn=<AddBackward0>)\n",
      "epoch: 15493 loss is tensor([-0.5892], grad_fn=<AddBackward0>)\n",
      "epoch: 15494 loss is tensor([-0.5988], grad_fn=<AddBackward0>)\n",
      "epoch: 15495 loss is tensor([-0.5559], grad_fn=<AddBackward0>)\n",
      "epoch: 15496 loss is tensor([-0.6521], grad_fn=<AddBackward0>)\n",
      "epoch: 15497 loss is tensor([-0.5982], grad_fn=<AddBackward0>)\n",
      "epoch: 15498 loss is tensor([-0.6314], grad_fn=<AddBackward0>)\n",
      "epoch: 15499 loss is tensor([-0.5767], grad_fn=<AddBackward0>)\n",
      "epoch: 15500 loss is tensor([-0.6310], grad_fn=<AddBackward0>)\n",
      "51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15501 loss is tensor([-0.5985], grad_fn=<AddBackward0>)\n",
      "epoch: 15502 loss is tensor([-0.6201], grad_fn=<AddBackward0>)\n",
      "epoch: 15503 loss is tensor([-0.5411], grad_fn=<AddBackward0>)\n",
      "epoch: 15504 loss is tensor([-0.6220], grad_fn=<AddBackward0>)\n",
      "epoch: 15505 loss is tensor([-0.5743], grad_fn=<AddBackward0>)\n",
      "epoch: 15506 loss is tensor([-0.6600], grad_fn=<AddBackward0>)\n",
      "epoch: 15507 loss is tensor([-0.6316], grad_fn=<AddBackward0>)\n",
      "epoch: 15508 loss is tensor([-0.6284], grad_fn=<AddBackward0>)\n",
      "epoch: 15509 loss is tensor([-0.6001], grad_fn=<AddBackward0>)\n",
      "epoch: 15510 loss is tensor([-0.6700], grad_fn=<AddBackward0>)\n",
      "epoch: 15511 loss is tensor([-0.6559], grad_fn=<AddBackward0>)\n",
      "epoch: 15512 loss is tensor([-0.6167], grad_fn=<AddBackward0>)\n",
      "epoch: 15513 loss is tensor([-0.6420], grad_fn=<AddBackward0>)\n",
      "epoch: 15514 loss is tensor([-0.6835], grad_fn=<AddBackward0>)\n",
      "epoch: 15515 loss is tensor([-0.6326], grad_fn=<AddBackward0>)\n",
      "epoch: 15516 loss is tensor([-0.5881], grad_fn=<AddBackward0>)\n",
      "epoch: 15517 loss is tensor([-0.5950], grad_fn=<AddBackward0>)\n",
      "epoch: 15518 loss is tensor([-0.6183], grad_fn=<AddBackward0>)\n",
      "epoch: 15519 loss is tensor([-0.5492], grad_fn=<AddBackward0>)\n",
      "epoch: 15520 loss is tensor([-0.6438], grad_fn=<AddBackward0>)\n",
      "epoch: 15521 loss is tensor([-0.5813], grad_fn=<AddBackward0>)\n",
      "epoch: 15522 loss is tensor([-0.6515], grad_fn=<AddBackward0>)\n",
      "epoch: 15523 loss is tensor([-0.5703], grad_fn=<AddBackward0>)\n",
      "epoch: 15524 loss is tensor([-0.6291], grad_fn=<AddBackward0>)\n",
      "epoch: 15525 loss is tensor([-0.6175], grad_fn=<AddBackward0>)\n",
      "epoch: 15526 loss is tensor([-0.6401], grad_fn=<AddBackward0>)\n",
      "epoch: 15527 loss is tensor([-0.6346], grad_fn=<AddBackward0>)\n",
      "epoch: 15528 loss is tensor([-0.6074], grad_fn=<AddBackward0>)\n",
      "epoch: 15529 loss is tensor([-0.6369], grad_fn=<AddBackward0>)\n",
      "epoch: 15530 loss is tensor([-0.6385], grad_fn=<AddBackward0>)\n",
      "epoch: 15531 loss is tensor([-0.6578], grad_fn=<AddBackward0>)\n",
      "epoch: 15532 loss is tensor([-0.5924], grad_fn=<AddBackward0>)\n",
      "epoch: 15533 loss is tensor([-0.5971], grad_fn=<AddBackward0>)\n",
      "epoch: 15534 loss is tensor([-0.5757], grad_fn=<AddBackward0>)\n",
      "epoch: 15535 loss is tensor([-0.6551], grad_fn=<AddBackward0>)\n",
      "epoch: 15536 loss is tensor([-0.5879], grad_fn=<AddBackward0>)\n",
      "epoch: 15537 loss is tensor([-0.6435], grad_fn=<AddBackward0>)\n",
      "epoch: 15538 loss is tensor([-0.6250], grad_fn=<AddBackward0>)\n",
      "epoch: 15539 loss is tensor([-0.5793], grad_fn=<AddBackward0>)\n",
      "epoch: 15540 loss is tensor([-0.5866], grad_fn=<AddBackward0>)\n",
      "epoch: 15541 loss is tensor([-0.6368], grad_fn=<AddBackward0>)\n",
      "epoch: 15542 loss is tensor([-0.6000], grad_fn=<AddBackward0>)\n",
      "epoch: 15543 loss is tensor([-0.6155], grad_fn=<AddBackward0>)\n",
      "epoch: 15544 loss is tensor([-0.5884], grad_fn=<AddBackward0>)\n",
      "epoch: 15545 loss is tensor([-0.6007], grad_fn=<AddBackward0>)\n",
      "epoch: 15546 loss is tensor([-0.6485], grad_fn=<AddBackward0>)\n",
      "epoch: 15547 loss is tensor([-0.6195], grad_fn=<AddBackward0>)\n",
      "epoch: 15548 loss is tensor([-0.5678], grad_fn=<AddBackward0>)\n",
      "epoch: 15549 loss is tensor([-0.6024], grad_fn=<AddBackward0>)\n",
      "epoch: 15550 loss is tensor([-0.6636], grad_fn=<AddBackward0>)\n",
      "epoch: 15551 loss is tensor([-0.6173], grad_fn=<AddBackward0>)\n",
      "epoch: 15552 loss is tensor([-0.6070], grad_fn=<AddBackward0>)\n",
      "epoch: 15553 loss is tensor([-0.5940], grad_fn=<AddBackward0>)\n",
      "epoch: 15554 loss is tensor([-0.6228], grad_fn=<AddBackward0>)\n",
      "epoch: 15555 loss is tensor([-0.5891], grad_fn=<AddBackward0>)\n",
      "epoch: 15556 loss is tensor([-0.6146], grad_fn=<AddBackward0>)\n",
      "epoch: 15557 loss is tensor([-0.6415], grad_fn=<AddBackward0>)\n",
      "epoch: 15558 loss is tensor([-0.5226], grad_fn=<AddBackward0>)\n",
      "epoch: 15559 loss is tensor([-0.6131], grad_fn=<AddBackward0>)\n",
      "epoch: 15560 loss is tensor([-0.6196], grad_fn=<AddBackward0>)\n",
      "epoch: 15561 loss is tensor([-0.5827], grad_fn=<AddBackward0>)\n",
      "epoch: 15562 loss is tensor([-0.6200], grad_fn=<AddBackward0>)\n",
      "epoch: 15563 loss is tensor([-0.6426], grad_fn=<AddBackward0>)\n",
      "epoch: 15564 loss is tensor([-0.6285], grad_fn=<AddBackward0>)\n",
      "epoch: 15565 loss is tensor([-0.6388], grad_fn=<AddBackward0>)\n",
      "epoch: 15566 loss is tensor([-0.6104], grad_fn=<AddBackward0>)\n",
      "epoch: 15567 loss is tensor([-0.6264], grad_fn=<AddBackward0>)\n",
      "epoch: 15568 loss is tensor([-0.5876], grad_fn=<AddBackward0>)\n",
      "epoch: 15569 loss is tensor([-0.5865], grad_fn=<AddBackward0>)\n",
      "epoch: 15570 loss is tensor([-0.6426], grad_fn=<AddBackward0>)\n",
      "epoch: 15571 loss is tensor([-0.6033], grad_fn=<AddBackward0>)\n",
      "epoch: 15572 loss is tensor([-0.5238], grad_fn=<AddBackward0>)\n",
      "epoch: 15573 loss is tensor([-0.6386], grad_fn=<AddBackward0>)\n",
      "epoch: 15574 loss is tensor([-0.6519], grad_fn=<AddBackward0>)\n",
      "epoch: 15575 loss is tensor([-0.6294], grad_fn=<AddBackward0>)\n",
      "epoch: 15576 loss is tensor([-0.5861], grad_fn=<AddBackward0>)\n",
      "epoch: 15577 loss is tensor([-0.6052], grad_fn=<AddBackward0>)\n",
      "epoch: 15578 loss is tensor([-0.6610], grad_fn=<AddBackward0>)\n",
      "epoch: 15579 loss is tensor([-0.5779], grad_fn=<AddBackward0>)\n",
      "epoch: 15580 loss is tensor([-0.6221], grad_fn=<AddBackward0>)\n",
      "epoch: 15581 loss is tensor([-0.5722], grad_fn=<AddBackward0>)\n",
      "epoch: 15582 loss is tensor([-0.6492], grad_fn=<AddBackward0>)\n",
      "epoch: 15583 loss is tensor([-0.6277], grad_fn=<AddBackward0>)\n",
      "epoch: 15584 loss is tensor([-0.5981], grad_fn=<AddBackward0>)\n",
      "epoch: 15585 loss is tensor([-0.6466], grad_fn=<AddBackward0>)\n",
      "epoch: 15586 loss is tensor([-0.6173], grad_fn=<AddBackward0>)\n",
      "epoch: 15587 loss is tensor([-0.6145], grad_fn=<AddBackward0>)\n",
      "epoch: 15588 loss is tensor([-0.6023], grad_fn=<AddBackward0>)\n",
      "epoch: 15589 loss is tensor([-0.6747], grad_fn=<AddBackward0>)\n",
      "epoch: 15590 loss is tensor([-0.6187], grad_fn=<AddBackward0>)\n",
      "epoch: 15591 loss is tensor([-0.6386], grad_fn=<AddBackward0>)\n",
      "epoch: 15592 loss is tensor([-0.6297], grad_fn=<AddBackward0>)\n",
      "epoch: 15593 loss is tensor([-0.6522], grad_fn=<AddBackward0>)\n",
      "epoch: 15594 loss is tensor([-0.6587], grad_fn=<AddBackward0>)\n",
      "epoch: 15595 loss is tensor([-0.6535], grad_fn=<AddBackward0>)\n",
      "epoch: 15596 loss is tensor([-0.6074], grad_fn=<AddBackward0>)\n",
      "epoch: 15597 loss is tensor([-0.6191], grad_fn=<AddBackward0>)\n",
      "epoch: 15598 loss is tensor([-0.6044], grad_fn=<AddBackward0>)\n",
      "epoch: 15599 loss is tensor([-0.5790], grad_fn=<AddBackward0>)\n",
      "epoch: 15600 loss is tensor([-0.6198], grad_fn=<AddBackward0>)\n",
      "28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15601 loss is tensor([-0.6419], grad_fn=<AddBackward0>)\n",
      "epoch: 15602 loss is tensor([-0.6003], grad_fn=<AddBackward0>)\n",
      "epoch: 15603 loss is tensor([-0.5683], grad_fn=<AddBackward0>)\n",
      "epoch: 15604 loss is tensor([-0.5823], grad_fn=<AddBackward0>)\n",
      "epoch: 15605 loss is tensor([-0.6058], grad_fn=<AddBackward0>)\n",
      "epoch: 15606 loss is tensor([-0.6340], grad_fn=<AddBackward0>)\n",
      "epoch: 15607 loss is tensor([-0.6021], grad_fn=<AddBackward0>)\n",
      "epoch: 15608 loss is tensor([-0.5646], grad_fn=<AddBackward0>)\n",
      "epoch: 15609 loss is tensor([-0.6030], grad_fn=<AddBackward0>)\n",
      "epoch: 15610 loss is tensor([-0.5976], grad_fn=<AddBackward0>)\n",
      "epoch: 15611 loss is tensor([-0.5490], grad_fn=<AddBackward0>)\n",
      "epoch: 15612 loss is tensor([-0.6117], grad_fn=<AddBackward0>)\n",
      "epoch: 15613 loss is tensor([-0.6240], grad_fn=<AddBackward0>)\n",
      "epoch: 15614 loss is tensor([-0.5178], grad_fn=<AddBackward0>)\n",
      "epoch: 15615 loss is tensor([-0.4878], grad_fn=<AddBackward0>)\n",
      "epoch: 15616 loss is tensor([-0.6273], grad_fn=<AddBackward0>)\n",
      "epoch: 15617 loss is tensor([-0.5839], grad_fn=<AddBackward0>)\n",
      "epoch: 15618 loss is tensor([-0.5966], grad_fn=<AddBackward0>)\n",
      "epoch: 15619 loss is tensor([-0.5651], grad_fn=<AddBackward0>)\n",
      "epoch: 15620 loss is tensor([-0.5376], grad_fn=<AddBackward0>)\n",
      "epoch: 15621 loss is tensor([-0.5914], grad_fn=<AddBackward0>)\n",
      "epoch: 15622 loss is tensor([-0.6163], grad_fn=<AddBackward0>)\n",
      "epoch: 15623 loss is tensor([-0.5924], grad_fn=<AddBackward0>)\n",
      "epoch: 15624 loss is tensor([-0.5885], grad_fn=<AddBackward0>)\n",
      "epoch: 15625 loss is tensor([-0.6111], grad_fn=<AddBackward0>)\n",
      "epoch: 15626 loss is tensor([-0.6315], grad_fn=<AddBackward0>)\n",
      "epoch: 15627 loss is tensor([-0.6147], grad_fn=<AddBackward0>)\n",
      "epoch: 15628 loss is tensor([-0.6420], grad_fn=<AddBackward0>)\n",
      "epoch: 15629 loss is tensor([-0.6384], grad_fn=<AddBackward0>)\n",
      "epoch: 15630 loss is tensor([-0.5954], grad_fn=<AddBackward0>)\n",
      "epoch: 15631 loss is tensor([-0.5637], grad_fn=<AddBackward0>)\n",
      "epoch: 15632 loss is tensor([-0.5820], grad_fn=<AddBackward0>)\n",
      "epoch: 15633 loss is tensor([-0.5773], grad_fn=<AddBackward0>)\n",
      "epoch: 15634 loss is tensor([-0.6143], grad_fn=<AddBackward0>)\n",
      "epoch: 15635 loss is tensor([-0.6061], grad_fn=<AddBackward0>)\n",
      "epoch: 15636 loss is tensor([-0.6090], grad_fn=<AddBackward0>)\n",
      "epoch: 15637 loss is tensor([-0.5772], grad_fn=<AddBackward0>)\n",
      "epoch: 15638 loss is tensor([-0.6013], grad_fn=<AddBackward0>)\n",
      "epoch: 15639 loss is tensor([-0.6643], grad_fn=<AddBackward0>)\n",
      "epoch: 15640 loss is tensor([-0.6551], grad_fn=<AddBackward0>)\n",
      "epoch: 15641 loss is tensor([-0.6442], grad_fn=<AddBackward0>)\n",
      "epoch: 15642 loss is tensor([-0.6121], grad_fn=<AddBackward0>)\n",
      "epoch: 15643 loss is tensor([-0.5802], grad_fn=<AddBackward0>)\n",
      "epoch: 15644 loss is tensor([-0.5481], grad_fn=<AddBackward0>)\n",
      "epoch: 15645 loss is tensor([-0.6022], grad_fn=<AddBackward0>)\n",
      "epoch: 15646 loss is tensor([-0.6230], grad_fn=<AddBackward0>)\n",
      "epoch: 15647 loss is tensor([-0.6043], grad_fn=<AddBackward0>)\n",
      "epoch: 15648 loss is tensor([-0.6445], grad_fn=<AddBackward0>)\n",
      "epoch: 15649 loss is tensor([-0.6414], grad_fn=<AddBackward0>)\n",
      "epoch: 15650 loss is tensor([-0.6072], grad_fn=<AddBackward0>)\n",
      "epoch: 15651 loss is tensor([-0.6334], grad_fn=<AddBackward0>)\n",
      "epoch: 15652 loss is tensor([-0.5972], grad_fn=<AddBackward0>)\n",
      "epoch: 15653 loss is tensor([-0.5417], grad_fn=<AddBackward0>)\n",
      "epoch: 15654 loss is tensor([-0.5472], grad_fn=<AddBackward0>)\n",
      "epoch: 15655 loss is tensor([-0.5616], grad_fn=<AddBackward0>)\n",
      "epoch: 15656 loss is tensor([-0.5938], grad_fn=<AddBackward0>)\n",
      "epoch: 15657 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 15658 loss is tensor([-0.6405], grad_fn=<AddBackward0>)\n",
      "epoch: 15659 loss is tensor([-0.6183], grad_fn=<AddBackward0>)\n",
      "epoch: 15660 loss is tensor([-0.5921], grad_fn=<AddBackward0>)\n",
      "epoch: 15661 loss is tensor([-0.5326], grad_fn=<AddBackward0>)\n",
      "epoch: 15662 loss is tensor([-0.5938], grad_fn=<AddBackward0>)\n",
      "epoch: 15663 loss is tensor([-0.6276], grad_fn=<AddBackward0>)\n",
      "epoch: 15664 loss is tensor([-0.5991], grad_fn=<AddBackward0>)\n",
      "epoch: 15665 loss is tensor([-0.6405], grad_fn=<AddBackward0>)\n",
      "epoch: 15666 loss is tensor([-0.6406], grad_fn=<AddBackward0>)\n",
      "epoch: 15667 loss is tensor([-0.6021], grad_fn=<AddBackward0>)\n",
      "epoch: 15668 loss is tensor([-0.6075], grad_fn=<AddBackward0>)\n",
      "epoch: 15669 loss is tensor([-0.5993], grad_fn=<AddBackward0>)\n",
      "epoch: 15670 loss is tensor([-0.6109], grad_fn=<AddBackward0>)\n",
      "epoch: 15671 loss is tensor([-0.5310], grad_fn=<AddBackward0>)\n",
      "epoch: 15672 loss is tensor([-0.6430], grad_fn=<AddBackward0>)\n",
      "epoch: 15673 loss is tensor([-0.5540], grad_fn=<AddBackward0>)\n",
      "epoch: 15674 loss is tensor([-0.6513], grad_fn=<AddBackward0>)\n",
      "epoch: 15675 loss is tensor([-0.6798], grad_fn=<AddBackward0>)\n",
      "epoch: 15676 loss is tensor([-0.5986], grad_fn=<AddBackward0>)\n",
      "epoch: 15677 loss is tensor([-0.5332], grad_fn=<AddBackward0>)\n",
      "epoch: 15678 loss is tensor([-0.6355], grad_fn=<AddBackward0>)\n",
      "epoch: 15679 loss is tensor([-0.6351], grad_fn=<AddBackward0>)\n",
      "epoch: 15680 loss is tensor([-0.5920], grad_fn=<AddBackward0>)\n",
      "epoch: 15681 loss is tensor([-0.6007], grad_fn=<AddBackward0>)\n",
      "epoch: 15682 loss is tensor([-0.6055], grad_fn=<AddBackward0>)\n",
      "epoch: 15683 loss is tensor([-0.5947], grad_fn=<AddBackward0>)\n",
      "epoch: 15684 loss is tensor([-0.6142], grad_fn=<AddBackward0>)\n",
      "epoch: 15685 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "epoch: 15686 loss is tensor([-0.5729], grad_fn=<AddBackward0>)\n",
      "epoch: 15687 loss is tensor([-0.5984], grad_fn=<AddBackward0>)\n",
      "epoch: 15688 loss is tensor([-0.5949], grad_fn=<AddBackward0>)\n",
      "epoch: 15689 loss is tensor([-0.5310], grad_fn=<AddBackward0>)\n",
      "epoch: 15690 loss is tensor([-0.5657], grad_fn=<AddBackward0>)\n",
      "epoch: 15691 loss is tensor([-0.5979], grad_fn=<AddBackward0>)\n",
      "epoch: 15692 loss is tensor([-0.6050], grad_fn=<AddBackward0>)\n",
      "epoch: 15693 loss is tensor([-0.5410], grad_fn=<AddBackward0>)\n",
      "epoch: 15694 loss is tensor([-0.5703], grad_fn=<AddBackward0>)\n",
      "epoch: 15695 loss is tensor([-0.5393], grad_fn=<AddBackward0>)\n",
      "epoch: 15696 loss is tensor([-0.6060], grad_fn=<AddBackward0>)\n",
      "epoch: 15697 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "epoch: 15698 loss is tensor([-0.6073], grad_fn=<AddBackward0>)\n",
      "epoch: 15699 loss is tensor([-0.5854], grad_fn=<AddBackward0>)\n",
      "epoch: 15700 loss is tensor([-0.5905], grad_fn=<AddBackward0>)\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15701 loss is tensor([-0.5718], grad_fn=<AddBackward0>)\n",
      "epoch: 15702 loss is tensor([-0.5884], grad_fn=<AddBackward0>)\n",
      "epoch: 15703 loss is tensor([-0.6105], grad_fn=<AddBackward0>)\n",
      "epoch: 15704 loss is tensor([-0.6369], grad_fn=<AddBackward0>)\n",
      "epoch: 15705 loss is tensor([-0.6825], grad_fn=<AddBackward0>)\n",
      "epoch: 15706 loss is tensor([-0.5763], grad_fn=<AddBackward0>)\n",
      "epoch: 15707 loss is tensor([-0.6171], grad_fn=<AddBackward0>)\n",
      "epoch: 15708 loss is tensor([-0.6207], grad_fn=<AddBackward0>)\n",
      "epoch: 15709 loss is tensor([-0.6299], grad_fn=<AddBackward0>)\n",
      "epoch: 15710 loss is tensor([-0.6214], grad_fn=<AddBackward0>)\n",
      "epoch: 15711 loss is tensor([-0.5986], grad_fn=<AddBackward0>)\n",
      "epoch: 15712 loss is tensor([-0.6500], grad_fn=<AddBackward0>)\n",
      "epoch: 15713 loss is tensor([-0.6755], grad_fn=<AddBackward0>)\n",
      "epoch: 15714 loss is tensor([-0.6518], grad_fn=<AddBackward0>)\n",
      "epoch: 15715 loss is tensor([-0.5701], grad_fn=<AddBackward0>)\n",
      "epoch: 15716 loss is tensor([-0.6221], grad_fn=<AddBackward0>)\n",
      "epoch: 15717 loss is tensor([-0.6234], grad_fn=<AddBackward0>)\n",
      "epoch: 15718 loss is tensor([-0.6434], grad_fn=<AddBackward0>)\n",
      "epoch: 15719 loss is tensor([-0.6247], grad_fn=<AddBackward0>)\n",
      "epoch: 15720 loss is tensor([-0.6571], grad_fn=<AddBackward0>)\n",
      "epoch: 15721 loss is tensor([-0.6324], grad_fn=<AddBackward0>)\n",
      "epoch: 15722 loss is tensor([-0.6408], grad_fn=<AddBackward0>)\n",
      "epoch: 15723 loss is tensor([-0.5903], grad_fn=<AddBackward0>)\n",
      "epoch: 15724 loss is tensor([-0.6651], grad_fn=<AddBackward0>)\n",
      "epoch: 15725 loss is tensor([-0.6528], grad_fn=<AddBackward0>)\n",
      "epoch: 15726 loss is tensor([-0.6295], grad_fn=<AddBackward0>)\n",
      "epoch: 15727 loss is tensor([-0.6661], grad_fn=<AddBackward0>)\n",
      "epoch: 15728 loss is tensor([-0.6168], grad_fn=<AddBackward0>)\n",
      "epoch: 15729 loss is tensor([-0.6722], grad_fn=<AddBackward0>)\n",
      "epoch: 15730 loss is tensor([-0.6506], grad_fn=<AddBackward0>)\n",
      "epoch: 15731 loss is tensor([-0.5968], grad_fn=<AddBackward0>)\n",
      "epoch: 15732 loss is tensor([-0.6466], grad_fn=<AddBackward0>)\n",
      "epoch: 15733 loss is tensor([-0.6290], grad_fn=<AddBackward0>)\n",
      "epoch: 15734 loss is tensor([-0.6440], grad_fn=<AddBackward0>)\n",
      "epoch: 15735 loss is tensor([-0.6711], grad_fn=<AddBackward0>)\n",
      "epoch: 15736 loss is tensor([-0.6595], grad_fn=<AddBackward0>)\n",
      "epoch: 15737 loss is tensor([-0.6730], grad_fn=<AddBackward0>)\n",
      "epoch: 15738 loss is tensor([-0.7090], grad_fn=<AddBackward0>)\n",
      "epoch: 15739 loss is tensor([-0.5987], grad_fn=<AddBackward0>)\n",
      "epoch: 15740 loss is tensor([-0.6490], grad_fn=<AddBackward0>)\n",
      "epoch: 15741 loss is tensor([-0.6250], grad_fn=<AddBackward0>)\n",
      "epoch: 15742 loss is tensor([-0.6188], grad_fn=<AddBackward0>)\n",
      "epoch: 15743 loss is tensor([-0.5994], grad_fn=<AddBackward0>)\n",
      "epoch: 15744 loss is tensor([-0.6624], grad_fn=<AddBackward0>)\n",
      "epoch: 15745 loss is tensor([-0.6925], grad_fn=<AddBackward0>)\n",
      "epoch: 15746 loss is tensor([-0.6864], grad_fn=<AddBackward0>)\n",
      "epoch: 15747 loss is tensor([-0.6233], grad_fn=<AddBackward0>)\n",
      "epoch: 15748 loss is tensor([-0.6463], grad_fn=<AddBackward0>)\n",
      "epoch: 15749 loss is tensor([-0.6747], grad_fn=<AddBackward0>)\n",
      "epoch: 15750 loss is tensor([-0.5646], grad_fn=<AddBackward0>)\n",
      "epoch: 15751 loss is tensor([-0.6478], grad_fn=<AddBackward0>)\n",
      "epoch: 15752 loss is tensor([-0.6647], grad_fn=<AddBackward0>)\n",
      "epoch: 15753 loss is tensor([-0.6540], grad_fn=<AddBackward0>)\n",
      "epoch: 15754 loss is tensor([-0.6457], grad_fn=<AddBackward0>)\n",
      "epoch: 15755 loss is tensor([-0.6223], grad_fn=<AddBackward0>)\n",
      "epoch: 15756 loss is tensor([-0.6415], grad_fn=<AddBackward0>)\n",
      "epoch: 15757 loss is tensor([-0.6731], grad_fn=<AddBackward0>)\n",
      "epoch: 15758 loss is tensor([-0.6688], grad_fn=<AddBackward0>)\n",
      "epoch: 15759 loss is tensor([-0.6836], grad_fn=<AddBackward0>)\n",
      "epoch: 15760 loss is tensor([-0.6320], grad_fn=<AddBackward0>)\n",
      "epoch: 15761 loss is tensor([-0.6228], grad_fn=<AddBackward0>)\n",
      "epoch: 15762 loss is tensor([-0.6572], grad_fn=<AddBackward0>)\n",
      "epoch: 15763 loss is tensor([-0.5878], grad_fn=<AddBackward0>)\n",
      "epoch: 15764 loss is tensor([-0.5661], grad_fn=<AddBackward0>)\n",
      "epoch: 15765 loss is tensor([-0.6543], grad_fn=<AddBackward0>)\n",
      "epoch: 15766 loss is tensor([-0.5656], grad_fn=<AddBackward0>)\n",
      "epoch: 15767 loss is tensor([-0.6319], grad_fn=<AddBackward0>)\n",
      "epoch: 15768 loss is tensor([-0.6325], grad_fn=<AddBackward0>)\n",
      "epoch: 15769 loss is tensor([-0.6054], grad_fn=<AddBackward0>)\n",
      "epoch: 15770 loss is tensor([-0.6278], grad_fn=<AddBackward0>)\n",
      "epoch: 15771 loss is tensor([-0.6313], grad_fn=<AddBackward0>)\n",
      "epoch: 15772 loss is tensor([-0.6771], grad_fn=<AddBackward0>)\n",
      "epoch: 15773 loss is tensor([-0.6314], grad_fn=<AddBackward0>)\n",
      "epoch: 15774 loss is tensor([-0.6305], grad_fn=<AddBackward0>)\n",
      "epoch: 15775 loss is tensor([-0.6266], grad_fn=<AddBackward0>)\n",
      "epoch: 15776 loss is tensor([-0.6192], grad_fn=<AddBackward0>)\n",
      "epoch: 15777 loss is tensor([-0.7142], grad_fn=<AddBackward0>)\n",
      "epoch: 15778 loss is tensor([-0.6206], grad_fn=<AddBackward0>)\n",
      "epoch: 15779 loss is tensor([-0.6415], grad_fn=<AddBackward0>)\n",
      "epoch: 15780 loss is tensor([-0.6278], grad_fn=<AddBackward0>)\n",
      "epoch: 15781 loss is tensor([-0.6602], grad_fn=<AddBackward0>)\n",
      "epoch: 15782 loss is tensor([-0.6383], grad_fn=<AddBackward0>)\n",
      "epoch: 15783 loss is tensor([-0.6801], grad_fn=<AddBackward0>)\n",
      "epoch: 15784 loss is tensor([-0.6594], grad_fn=<AddBackward0>)\n",
      "epoch: 15785 loss is tensor([-0.6232], grad_fn=<AddBackward0>)\n",
      "epoch: 15786 loss is tensor([-0.6281], grad_fn=<AddBackward0>)\n",
      "epoch: 15787 loss is tensor([-0.6356], grad_fn=<AddBackward0>)\n",
      "epoch: 15788 loss is tensor([-0.6613], grad_fn=<AddBackward0>)\n",
      "epoch: 15789 loss is tensor([-0.6806], grad_fn=<AddBackward0>)\n",
      "epoch: 15790 loss is tensor([-0.6012], grad_fn=<AddBackward0>)\n",
      "epoch: 15791 loss is tensor([-0.6924], grad_fn=<AddBackward0>)\n",
      "epoch: 15792 loss is tensor([-0.6904], grad_fn=<AddBackward0>)\n",
      "epoch: 15793 loss is tensor([-0.6439], grad_fn=<AddBackward0>)\n",
      "epoch: 15794 loss is tensor([-0.6636], grad_fn=<AddBackward0>)\n",
      "epoch: 15795 loss is tensor([-0.6553], grad_fn=<AddBackward0>)\n",
      "epoch: 15796 loss is tensor([-0.6691], grad_fn=<AddBackward0>)\n",
      "epoch: 15797 loss is tensor([-0.6698], grad_fn=<AddBackward0>)\n",
      "epoch: 15798 loss is tensor([-0.7037], grad_fn=<AddBackward0>)\n",
      "epoch: 15799 loss is tensor([-0.6169], grad_fn=<AddBackward0>)\n",
      "epoch: 15800 loss is tensor([-0.6409], grad_fn=<AddBackward0>)\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15801 loss is tensor([-0.6074], grad_fn=<AddBackward0>)\n",
      "epoch: 15802 loss is tensor([-0.6481], grad_fn=<AddBackward0>)\n",
      "epoch: 15803 loss is tensor([-0.6589], grad_fn=<AddBackward0>)\n",
      "epoch: 15804 loss is tensor([-0.6552], grad_fn=<AddBackward0>)\n",
      "epoch: 15805 loss is tensor([-0.6852], grad_fn=<AddBackward0>)\n",
      "epoch: 15806 loss is tensor([-0.6283], grad_fn=<AddBackward0>)\n",
      "epoch: 15807 loss is tensor([-0.6340], grad_fn=<AddBackward0>)\n",
      "epoch: 15808 loss is tensor([-0.6152], grad_fn=<AddBackward0>)\n",
      "epoch: 15809 loss is tensor([-0.6595], grad_fn=<AddBackward0>)\n",
      "epoch: 15810 loss is tensor([-0.6051], grad_fn=<AddBackward0>)\n",
      "epoch: 15811 loss is tensor([-0.6572], grad_fn=<AddBackward0>)\n",
      "epoch: 15812 loss is tensor([-0.6426], grad_fn=<AddBackward0>)\n",
      "epoch: 15813 loss is tensor([-0.6590], grad_fn=<AddBackward0>)\n",
      "epoch: 15814 loss is tensor([-0.6041], grad_fn=<AddBackward0>)\n",
      "epoch: 15815 loss is tensor([-0.6196], grad_fn=<AddBackward0>)\n",
      "epoch: 15816 loss is tensor([-0.6399], grad_fn=<AddBackward0>)\n",
      "epoch: 15817 loss is tensor([-0.6832], grad_fn=<AddBackward0>)\n",
      "epoch: 15818 loss is tensor([-0.6407], grad_fn=<AddBackward0>)\n",
      "epoch: 15819 loss is tensor([-0.6060], grad_fn=<AddBackward0>)\n",
      "epoch: 15820 loss is tensor([-0.6864], grad_fn=<AddBackward0>)\n",
      "epoch: 15821 loss is tensor([-0.6002], grad_fn=<AddBackward0>)\n",
      "epoch: 15822 loss is tensor([-0.5973], grad_fn=<AddBackward0>)\n",
      "epoch: 15823 loss is tensor([-0.6111], grad_fn=<AddBackward0>)\n",
      "epoch: 15824 loss is tensor([-0.6008], grad_fn=<AddBackward0>)\n",
      "epoch: 15825 loss is tensor([-0.6388], grad_fn=<AddBackward0>)\n",
      "epoch: 15826 loss is tensor([-0.6692], grad_fn=<AddBackward0>)\n",
      "epoch: 15827 loss is tensor([-0.6369], grad_fn=<AddBackward0>)\n",
      "epoch: 15828 loss is tensor([-0.6023], grad_fn=<AddBackward0>)\n",
      "epoch: 15829 loss is tensor([-0.6238], grad_fn=<AddBackward0>)\n",
      "epoch: 15830 loss is tensor([-0.6539], grad_fn=<AddBackward0>)\n",
      "epoch: 15831 loss is tensor([-0.5961], grad_fn=<AddBackward0>)\n",
      "epoch: 15832 loss is tensor([-0.5983], grad_fn=<AddBackward0>)\n",
      "epoch: 15833 loss is tensor([-0.6521], grad_fn=<AddBackward0>)\n",
      "epoch: 15834 loss is tensor([-0.6248], grad_fn=<AddBackward0>)\n",
      "epoch: 15835 loss is tensor([-0.6097], grad_fn=<AddBackward0>)\n",
      "epoch: 15836 loss is tensor([-0.6108], grad_fn=<AddBackward0>)\n",
      "epoch: 15837 loss is tensor([-0.6189], grad_fn=<AddBackward0>)\n",
      "epoch: 15838 loss is tensor([-0.6013], grad_fn=<AddBackward0>)\n",
      "epoch: 15839 loss is tensor([-0.5728], grad_fn=<AddBackward0>)\n",
      "epoch: 15840 loss is tensor([-0.6231], grad_fn=<AddBackward0>)\n",
      "epoch: 15841 loss is tensor([-0.6343], grad_fn=<AddBackward0>)\n",
      "epoch: 15842 loss is tensor([-0.6615], grad_fn=<AddBackward0>)\n",
      "epoch: 15843 loss is tensor([-0.5962], grad_fn=<AddBackward0>)\n",
      "epoch: 15844 loss is tensor([-0.6415], grad_fn=<AddBackward0>)\n",
      "epoch: 15845 loss is tensor([-0.6533], grad_fn=<AddBackward0>)\n",
      "epoch: 15846 loss is tensor([-0.6649], grad_fn=<AddBackward0>)\n",
      "epoch: 15847 loss is tensor([-0.6498], grad_fn=<AddBackward0>)\n",
      "epoch: 15848 loss is tensor([-0.6426], grad_fn=<AddBackward0>)\n",
      "epoch: 15849 loss is tensor([-0.6782], grad_fn=<AddBackward0>)\n",
      "epoch: 15850 loss is tensor([-0.6149], grad_fn=<AddBackward0>)\n",
      "epoch: 15851 loss is tensor([-0.6446], grad_fn=<AddBackward0>)\n",
      "epoch: 15852 loss is tensor([-0.6187], grad_fn=<AddBackward0>)\n",
      "epoch: 15853 loss is tensor([-0.6724], grad_fn=<AddBackward0>)\n",
      "epoch: 15854 loss is tensor([-0.6283], grad_fn=<AddBackward0>)\n",
      "epoch: 15855 loss is tensor([-0.6552], grad_fn=<AddBackward0>)\n",
      "epoch: 15856 loss is tensor([-0.6081], grad_fn=<AddBackward0>)\n",
      "epoch: 15857 loss is tensor([-0.6301], grad_fn=<AddBackward0>)\n",
      "epoch: 15858 loss is tensor([-0.6121], grad_fn=<AddBackward0>)\n",
      "epoch: 15859 loss is tensor([-0.6247], grad_fn=<AddBackward0>)\n",
      "epoch: 15860 loss is tensor([-0.6060], grad_fn=<AddBackward0>)\n",
      "epoch: 15861 loss is tensor([-0.6471], grad_fn=<AddBackward0>)\n",
      "epoch: 15862 loss is tensor([-0.6602], grad_fn=<AddBackward0>)\n",
      "epoch: 15863 loss is tensor([-0.6492], grad_fn=<AddBackward0>)\n",
      "epoch: 15864 loss is tensor([-0.6770], grad_fn=<AddBackward0>)\n",
      "epoch: 15865 loss is tensor([-0.6604], grad_fn=<AddBackward0>)\n",
      "epoch: 15866 loss is tensor([-0.6990], grad_fn=<AddBackward0>)\n",
      "epoch: 15867 loss is tensor([-0.6585], grad_fn=<AddBackward0>)\n",
      "epoch: 15868 loss is tensor([-0.6021], grad_fn=<AddBackward0>)\n",
      "epoch: 15869 loss is tensor([-0.6780], grad_fn=<AddBackward0>)\n",
      "epoch: 15870 loss is tensor([-0.6405], grad_fn=<AddBackward0>)\n",
      "epoch: 15871 loss is tensor([-0.7079], grad_fn=<AddBackward0>)\n",
      "epoch: 15872 loss is tensor([-0.6224], grad_fn=<AddBackward0>)\n",
      "epoch: 15873 loss is tensor([-0.6360], grad_fn=<AddBackward0>)\n",
      "epoch: 15874 loss is tensor([-0.6735], grad_fn=<AddBackward0>)\n",
      "epoch: 15875 loss is tensor([-0.6202], grad_fn=<AddBackward0>)\n",
      "epoch: 15876 loss is tensor([-0.6254], grad_fn=<AddBackward0>)\n",
      "epoch: 15877 loss is tensor([-0.6289], grad_fn=<AddBackward0>)\n",
      "epoch: 15878 loss is tensor([-0.6526], grad_fn=<AddBackward0>)\n",
      "epoch: 15879 loss is tensor([-0.6248], grad_fn=<AddBackward0>)\n",
      "epoch: 15880 loss is tensor([-0.6295], grad_fn=<AddBackward0>)\n",
      "epoch: 15881 loss is tensor([-0.5704], grad_fn=<AddBackward0>)\n",
      "epoch: 15882 loss is tensor([-0.5278], grad_fn=<AddBackward0>)\n",
      "epoch: 15883 loss is tensor([-0.5941], grad_fn=<AddBackward0>)\n",
      "epoch: 15884 loss is tensor([-0.5748], grad_fn=<AddBackward0>)\n",
      "epoch: 15885 loss is tensor([-0.6079], grad_fn=<AddBackward0>)\n",
      "epoch: 15886 loss is tensor([-0.6082], grad_fn=<AddBackward0>)\n",
      "epoch: 15887 loss is tensor([-0.6398], grad_fn=<AddBackward0>)\n",
      "epoch: 15888 loss is tensor([-0.6022], grad_fn=<AddBackward0>)\n",
      "epoch: 15889 loss is tensor([-0.6470], grad_fn=<AddBackward0>)\n",
      "epoch: 15890 loss is tensor([-0.6314], grad_fn=<AddBackward0>)\n",
      "epoch: 15891 loss is tensor([-0.6373], grad_fn=<AddBackward0>)\n",
      "epoch: 15892 loss is tensor([-0.6389], grad_fn=<AddBackward0>)\n",
      "epoch: 15893 loss is tensor([-0.6125], grad_fn=<AddBackward0>)\n",
      "epoch: 15894 loss is tensor([-0.6015], grad_fn=<AddBackward0>)\n",
      "epoch: 15895 loss is tensor([-0.6170], grad_fn=<AddBackward0>)\n",
      "epoch: 15896 loss is tensor([-0.5710], grad_fn=<AddBackward0>)\n",
      "epoch: 15897 loss is tensor([-0.5900], grad_fn=<AddBackward0>)\n",
      "epoch: 15898 loss is tensor([-0.6055], grad_fn=<AddBackward0>)\n",
      "epoch: 15899 loss is tensor([-0.5772], grad_fn=<AddBackward0>)\n",
      "epoch: 15900 loss is tensor([-0.6362], grad_fn=<AddBackward0>)\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 15901 loss is tensor([-0.6098], grad_fn=<AddBackward0>)\n",
      "epoch: 15902 loss is tensor([-0.6466], grad_fn=<AddBackward0>)\n",
      "epoch: 15903 loss is tensor([-0.6490], grad_fn=<AddBackward0>)\n",
      "epoch: 15904 loss is tensor([-0.6198], grad_fn=<AddBackward0>)\n",
      "epoch: 15905 loss is tensor([-0.6491], grad_fn=<AddBackward0>)\n",
      "epoch: 15906 loss is tensor([-0.5992], grad_fn=<AddBackward0>)\n",
      "epoch: 15907 loss is tensor([-0.6288], grad_fn=<AddBackward0>)\n",
      "epoch: 15908 loss is tensor([-0.5957], grad_fn=<AddBackward0>)\n",
      "epoch: 15909 loss is tensor([-0.6025], grad_fn=<AddBackward0>)\n",
      "epoch: 15910 loss is tensor([-0.5982], grad_fn=<AddBackward0>)\n",
      "epoch: 15911 loss is tensor([-0.6428], grad_fn=<AddBackward0>)\n",
      "epoch: 15912 loss is tensor([-0.5954], grad_fn=<AddBackward0>)\n",
      "epoch: 15913 loss is tensor([-0.5672], grad_fn=<AddBackward0>)\n",
      "epoch: 15914 loss is tensor([-0.6150], grad_fn=<AddBackward0>)\n",
      "epoch: 15915 loss is tensor([-0.5789], grad_fn=<AddBackward0>)\n",
      "epoch: 15916 loss is tensor([-0.5597], grad_fn=<AddBackward0>)\n",
      "epoch: 15917 loss is tensor([-0.6017], grad_fn=<AddBackward0>)\n",
      "epoch: 15918 loss is tensor([-0.6233], grad_fn=<AddBackward0>)\n",
      "epoch: 15919 loss is tensor([-0.5339], grad_fn=<AddBackward0>)\n",
      "epoch: 15920 loss is tensor([-0.6200], grad_fn=<AddBackward0>)\n",
      "epoch: 15921 loss is tensor([-0.6011], grad_fn=<AddBackward0>)\n",
      "epoch: 15922 loss is tensor([-0.5806], grad_fn=<AddBackward0>)\n",
      "epoch: 15923 loss is tensor([-0.6197], grad_fn=<AddBackward0>)\n",
      "epoch: 15924 loss is tensor([-0.5863], grad_fn=<AddBackward0>)\n",
      "epoch: 15925 loss is tensor([-0.6054], grad_fn=<AddBackward0>)\n",
      "epoch: 15926 loss is tensor([-0.6061], grad_fn=<AddBackward0>)\n",
      "epoch: 15927 loss is tensor([-0.5987], grad_fn=<AddBackward0>)\n",
      "epoch: 15928 loss is tensor([-0.6286], grad_fn=<AddBackward0>)\n",
      "epoch: 15929 loss is tensor([-0.5994], grad_fn=<AddBackward0>)\n",
      "epoch: 15930 loss is tensor([-0.5840], grad_fn=<AddBackward0>)\n",
      "epoch: 15931 loss is tensor([-0.6413], grad_fn=<AddBackward0>)\n",
      "epoch: 15932 loss is tensor([-0.6159], grad_fn=<AddBackward0>)\n",
      "epoch: 15933 loss is tensor([-0.6431], grad_fn=<AddBackward0>)\n",
      "epoch: 15934 loss is tensor([-0.6340], grad_fn=<AddBackward0>)\n",
      "epoch: 15935 loss is tensor([-0.6452], grad_fn=<AddBackward0>)\n",
      "epoch: 15936 loss is tensor([-0.6442], grad_fn=<AddBackward0>)\n",
      "epoch: 15937 loss is tensor([-0.6095], grad_fn=<AddBackward0>)\n",
      "epoch: 15938 loss is tensor([-0.5417], grad_fn=<AddBackward0>)\n",
      "epoch: 15939 loss is tensor([-0.5255], grad_fn=<AddBackward0>)\n",
      "epoch: 15940 loss is tensor([-0.5468], grad_fn=<AddBackward0>)\n",
      "epoch: 15941 loss is tensor([-0.6038], grad_fn=<AddBackward0>)\n",
      "epoch: 15942 loss is tensor([-0.5967], grad_fn=<AddBackward0>)\n",
      "epoch: 15943 loss is tensor([-0.5587], grad_fn=<AddBackward0>)\n",
      "epoch: 15944 loss is tensor([-0.6344], grad_fn=<AddBackward0>)\n",
      "epoch: 15945 loss is tensor([-0.5990], grad_fn=<AddBackward0>)\n",
      "epoch: 15946 loss is tensor([-0.5689], grad_fn=<AddBackward0>)\n",
      "epoch: 15947 loss is tensor([-0.6071], grad_fn=<AddBackward0>)\n",
      "epoch: 15948 loss is tensor([-0.6086], grad_fn=<AddBackward0>)\n",
      "epoch: 15949 loss is tensor([-0.6097], grad_fn=<AddBackward0>)\n",
      "epoch: 15950 loss is tensor([-0.6465], grad_fn=<AddBackward0>)\n",
      "epoch: 15951 loss is tensor([-0.6309], grad_fn=<AddBackward0>)\n",
      "epoch: 15952 loss is tensor([-0.6284], grad_fn=<AddBackward0>)\n",
      "epoch: 15953 loss is tensor([-0.5889], grad_fn=<AddBackward0>)\n",
      "epoch: 15954 loss is tensor([-0.6175], grad_fn=<AddBackward0>)\n",
      "epoch: 15955 loss is tensor([-0.5819], grad_fn=<AddBackward0>)\n",
      "epoch: 15956 loss is tensor([-0.6037], grad_fn=<AddBackward0>)\n",
      "epoch: 15957 loss is tensor([-0.5991], grad_fn=<AddBackward0>)\n",
      "epoch: 15958 loss is tensor([-0.5752], grad_fn=<AddBackward0>)\n",
      "epoch: 15959 loss is tensor([-0.5982], grad_fn=<AddBackward0>)\n",
      "epoch: 15960 loss is tensor([-0.5614], grad_fn=<AddBackward0>)\n",
      "epoch: 15961 loss is tensor([-0.5841], grad_fn=<AddBackward0>)\n",
      "epoch: 15962 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "epoch: 15963 loss is tensor([-0.5836], grad_fn=<AddBackward0>)\n",
      "epoch: 15964 loss is tensor([-0.6920], grad_fn=<AddBackward0>)\n",
      "epoch: 15965 loss is tensor([-0.6749], grad_fn=<AddBackward0>)\n",
      "epoch: 15966 loss is tensor([-0.6145], grad_fn=<AddBackward0>)\n",
      "epoch: 15967 loss is tensor([-0.5847], grad_fn=<AddBackward0>)\n",
      "epoch: 15968 loss is tensor([-0.6080], grad_fn=<AddBackward0>)\n",
      "epoch: 15969 loss is tensor([-0.6041], grad_fn=<AddBackward0>)\n",
      "epoch: 15970 loss is tensor([-0.6267], grad_fn=<AddBackward0>)\n",
      "epoch: 15971 loss is tensor([-0.6144], grad_fn=<AddBackward0>)\n",
      "epoch: 15972 loss is tensor([-0.6511], grad_fn=<AddBackward0>)\n",
      "epoch: 15973 loss is tensor([-0.6418], grad_fn=<AddBackward0>)\n",
      "epoch: 15974 loss is tensor([-0.6638], grad_fn=<AddBackward0>)\n",
      "epoch: 15975 loss is tensor([-0.6302], grad_fn=<AddBackward0>)\n",
      "epoch: 15976 loss is tensor([-0.5849], grad_fn=<AddBackward0>)\n",
      "epoch: 15977 loss is tensor([-0.6458], grad_fn=<AddBackward0>)\n",
      "epoch: 15978 loss is tensor([-0.6242], grad_fn=<AddBackward0>)\n",
      "epoch: 15979 loss is tensor([-0.6051], grad_fn=<AddBackward0>)\n",
      "epoch: 15980 loss is tensor([-0.6300], grad_fn=<AddBackward0>)\n",
      "epoch: 15981 loss is tensor([-0.5873], grad_fn=<AddBackward0>)\n",
      "epoch: 15982 loss is tensor([-0.6553], grad_fn=<AddBackward0>)\n",
      "epoch: 15983 loss is tensor([-0.6012], grad_fn=<AddBackward0>)\n",
      "epoch: 15984 loss is tensor([-0.6103], grad_fn=<AddBackward0>)\n",
      "epoch: 15985 loss is tensor([-0.6396], grad_fn=<AddBackward0>)\n",
      "epoch: 15986 loss is tensor([-0.6956], grad_fn=<AddBackward0>)\n",
      "epoch: 15987 loss is tensor([-0.6503], grad_fn=<AddBackward0>)\n",
      "epoch: 15988 loss is tensor([-0.6529], grad_fn=<AddBackward0>)\n",
      "epoch: 15989 loss is tensor([-0.6266], grad_fn=<AddBackward0>)\n",
      "epoch: 15990 loss is tensor([-0.6791], grad_fn=<AddBackward0>)\n",
      "epoch: 15991 loss is tensor([-0.6412], grad_fn=<AddBackward0>)\n",
      "epoch: 15992 loss is tensor([-0.6133], grad_fn=<AddBackward0>)\n",
      "epoch: 15993 loss is tensor([-0.6563], grad_fn=<AddBackward0>)\n",
      "epoch: 15994 loss is tensor([-0.6360], grad_fn=<AddBackward0>)\n",
      "epoch: 15995 loss is tensor([-0.6730], grad_fn=<AddBackward0>)\n",
      "epoch: 15996 loss is tensor([-0.6744], grad_fn=<AddBackward0>)\n",
      "epoch: 15997 loss is tensor([-0.6364], grad_fn=<AddBackward0>)\n",
      "epoch: 15998 loss is tensor([-0.6760], grad_fn=<AddBackward0>)\n",
      "epoch: 15999 loss is tensor([-0.6347], grad_fn=<AddBackward0>)\n",
      "epoch: 16000 loss is tensor([-0.6010], grad_fn=<AddBackward0>)\n",
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16001 loss is tensor([-0.6085], grad_fn=<AddBackward0>)\n",
      "epoch: 16002 loss is tensor([-0.6154], grad_fn=<AddBackward0>)\n",
      "epoch: 16003 loss is tensor([-0.6519], grad_fn=<AddBackward0>)\n",
      "epoch: 16004 loss is tensor([-0.6618], grad_fn=<AddBackward0>)\n",
      "epoch: 16005 loss is tensor([-0.6674], grad_fn=<AddBackward0>)\n",
      "epoch: 16006 loss is tensor([-0.6025], grad_fn=<AddBackward0>)\n",
      "epoch: 16007 loss is tensor([-0.6366], grad_fn=<AddBackward0>)\n",
      "epoch: 16008 loss is tensor([-0.6206], grad_fn=<AddBackward0>)\n",
      "epoch: 16009 loss is tensor([-0.6597], grad_fn=<AddBackward0>)\n",
      "epoch: 16010 loss is tensor([-0.6316], grad_fn=<AddBackward0>)\n",
      "epoch: 16011 loss is tensor([-0.6943], grad_fn=<AddBackward0>)\n",
      "epoch: 16012 loss is tensor([-0.6542], grad_fn=<AddBackward0>)\n",
      "epoch: 16013 loss is tensor([-0.6791], grad_fn=<AddBackward0>)\n",
      "epoch: 16014 loss is tensor([-0.6627], grad_fn=<AddBackward0>)\n",
      "epoch: 16015 loss is tensor([-0.6453], grad_fn=<AddBackward0>)\n",
      "epoch: 16016 loss is tensor([-0.6340], grad_fn=<AddBackward0>)\n",
      "epoch: 16017 loss is tensor([-0.6133], grad_fn=<AddBackward0>)\n",
      "epoch: 16018 loss is tensor([-0.6575], grad_fn=<AddBackward0>)\n",
      "epoch: 16019 loss is tensor([-0.6485], grad_fn=<AddBackward0>)\n",
      "epoch: 16020 loss is tensor([-0.6325], grad_fn=<AddBackward0>)\n",
      "epoch: 16021 loss is tensor([-0.6534], grad_fn=<AddBackward0>)\n",
      "epoch: 16022 loss is tensor([-0.6564], grad_fn=<AddBackward0>)\n",
      "epoch: 16023 loss is tensor([-0.6011], grad_fn=<AddBackward0>)\n",
      "epoch: 16024 loss is tensor([-0.6229], grad_fn=<AddBackward0>)\n",
      "epoch: 16025 loss is tensor([-0.6817], grad_fn=<AddBackward0>)\n",
      "epoch: 16026 loss is tensor([-0.6630], grad_fn=<AddBackward0>)\n",
      "epoch: 16027 loss is tensor([-0.6231], grad_fn=<AddBackward0>)\n",
      "epoch: 16028 loss is tensor([-0.6661], grad_fn=<AddBackward0>)\n",
      "epoch: 16029 loss is tensor([-0.6160], grad_fn=<AddBackward0>)\n",
      "epoch: 16030 loss is tensor([-0.6769], grad_fn=<AddBackward0>)\n",
      "epoch: 16031 loss is tensor([-0.6637], grad_fn=<AddBackward0>)\n",
      "epoch: 16032 loss is tensor([-0.6514], grad_fn=<AddBackward0>)\n",
      "epoch: 16033 loss is tensor([-0.6508], grad_fn=<AddBackward0>)\n",
      "epoch: 16034 loss is tensor([-0.7016], grad_fn=<AddBackward0>)\n",
      "epoch: 16035 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 16036 loss is tensor([-0.5527], grad_fn=<AddBackward0>)\n",
      "epoch: 16037 loss is tensor([-0.6198], grad_fn=<AddBackward0>)\n",
      "epoch: 16038 loss is tensor([-0.5641], grad_fn=<AddBackward0>)\n",
      "epoch: 16039 loss is tensor([-0.6332], grad_fn=<AddBackward0>)\n",
      "epoch: 16040 loss is tensor([-0.5949], grad_fn=<AddBackward0>)\n",
      "epoch: 16041 loss is tensor([-0.5999], grad_fn=<AddBackward0>)\n",
      "epoch: 16042 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 16043 loss is tensor([-0.6325], grad_fn=<AddBackward0>)\n",
      "epoch: 16044 loss is tensor([-0.6279], grad_fn=<AddBackward0>)\n",
      "epoch: 16045 loss is tensor([-0.6768], grad_fn=<AddBackward0>)\n",
      "epoch: 16046 loss is tensor([-0.6995], grad_fn=<AddBackward0>)\n",
      "epoch: 16047 loss is tensor([-0.6559], grad_fn=<AddBackward0>)\n",
      "epoch: 16048 loss is tensor([-0.6841], grad_fn=<AddBackward0>)\n",
      "epoch: 16049 loss is tensor([-0.6472], grad_fn=<AddBackward0>)\n",
      "epoch: 16050 loss is tensor([-0.6311], grad_fn=<AddBackward0>)\n",
      "epoch: 16051 loss is tensor([-0.5440], grad_fn=<AddBackward0>)\n",
      "epoch: 16052 loss is tensor([-0.6386], grad_fn=<AddBackward0>)\n",
      "epoch: 16053 loss is tensor([-0.6337], grad_fn=<AddBackward0>)\n",
      "epoch: 16054 loss is tensor([-0.6486], grad_fn=<AddBackward0>)\n",
      "epoch: 16055 loss is tensor([-0.6306], grad_fn=<AddBackward0>)\n",
      "epoch: 16056 loss is tensor([-0.5998], grad_fn=<AddBackward0>)\n",
      "epoch: 16057 loss is tensor([-0.6743], grad_fn=<AddBackward0>)\n",
      "epoch: 16058 loss is tensor([-0.6071], grad_fn=<AddBackward0>)\n",
      "epoch: 16059 loss is tensor([-0.6369], grad_fn=<AddBackward0>)\n",
      "epoch: 16060 loss is tensor([-0.6781], grad_fn=<AddBackward0>)\n",
      "epoch: 16061 loss is tensor([-0.6186], grad_fn=<AddBackward0>)\n",
      "epoch: 16062 loss is tensor([-0.5964], grad_fn=<AddBackward0>)\n",
      "epoch: 16063 loss is tensor([-0.6380], grad_fn=<AddBackward0>)\n",
      "epoch: 16064 loss is tensor([-0.6299], grad_fn=<AddBackward0>)\n",
      "epoch: 16065 loss is tensor([-0.5864], grad_fn=<AddBackward0>)\n",
      "epoch: 16066 loss is tensor([-0.6924], grad_fn=<AddBackward0>)\n",
      "epoch: 16067 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 16068 loss is tensor([-0.6434], grad_fn=<AddBackward0>)\n",
      "epoch: 16069 loss is tensor([-0.6250], grad_fn=<AddBackward0>)\n",
      "epoch: 16070 loss is tensor([-0.6062], grad_fn=<AddBackward0>)\n",
      "epoch: 16071 loss is tensor([-0.6508], grad_fn=<AddBackward0>)\n",
      "epoch: 16072 loss is tensor([-0.6617], grad_fn=<AddBackward0>)\n",
      "epoch: 16073 loss is tensor([-0.6309], grad_fn=<AddBackward0>)\n",
      "epoch: 16074 loss is tensor([-0.6005], grad_fn=<AddBackward0>)\n",
      "epoch: 16075 loss is tensor([-0.5714], grad_fn=<AddBackward0>)\n",
      "epoch: 16076 loss is tensor([-0.5125], grad_fn=<AddBackward0>)\n",
      "epoch: 16077 loss is tensor([-0.6296], grad_fn=<AddBackward0>)\n",
      "epoch: 16078 loss is tensor([-0.6177], grad_fn=<AddBackward0>)\n",
      "epoch: 16079 loss is tensor([-0.6352], grad_fn=<AddBackward0>)\n",
      "epoch: 16080 loss is tensor([-0.6258], grad_fn=<AddBackward0>)\n",
      "epoch: 16081 loss is tensor([-0.6569], grad_fn=<AddBackward0>)\n",
      "epoch: 16082 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 16083 loss is tensor([-0.6375], grad_fn=<AddBackward0>)\n",
      "epoch: 16084 loss is tensor([-0.6146], grad_fn=<AddBackward0>)\n",
      "epoch: 16085 loss is tensor([-0.6550], grad_fn=<AddBackward0>)\n",
      "epoch: 16086 loss is tensor([-0.6083], grad_fn=<AddBackward0>)\n",
      "epoch: 16087 loss is tensor([-0.6644], grad_fn=<AddBackward0>)\n",
      "epoch: 16088 loss is tensor([-0.6526], grad_fn=<AddBackward0>)\n",
      "epoch: 16089 loss is tensor([-0.6381], grad_fn=<AddBackward0>)\n",
      "epoch: 16090 loss is tensor([-0.6705], grad_fn=<AddBackward0>)\n",
      "epoch: 16091 loss is tensor([-0.6893], grad_fn=<AddBackward0>)\n",
      "epoch: 16092 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 16093 loss is tensor([-0.6323], grad_fn=<AddBackward0>)\n",
      "epoch: 16094 loss is tensor([-0.6140], grad_fn=<AddBackward0>)\n",
      "epoch: 16095 loss is tensor([-0.6230], grad_fn=<AddBackward0>)\n",
      "epoch: 16096 loss is tensor([-0.6158], grad_fn=<AddBackward0>)\n",
      "epoch: 16097 loss is tensor([-0.5801], grad_fn=<AddBackward0>)\n",
      "epoch: 16098 loss is tensor([-0.6622], grad_fn=<AddBackward0>)\n",
      "epoch: 16099 loss is tensor([-0.6569], grad_fn=<AddBackward0>)\n",
      "epoch: 16100 loss is tensor([-0.5973], grad_fn=<AddBackward0>)\n",
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16101 loss is tensor([-0.6598], grad_fn=<AddBackward0>)\n",
      "epoch: 16102 loss is tensor([-0.6287], grad_fn=<AddBackward0>)\n",
      "epoch: 16103 loss is tensor([-0.6470], grad_fn=<AddBackward0>)\n",
      "epoch: 16104 loss is tensor([-0.6486], grad_fn=<AddBackward0>)\n",
      "epoch: 16105 loss is tensor([-0.6950], grad_fn=<AddBackward0>)\n",
      "epoch: 16106 loss is tensor([-0.6644], grad_fn=<AddBackward0>)\n",
      "epoch: 16107 loss is tensor([-0.6709], grad_fn=<AddBackward0>)\n",
      "epoch: 16108 loss is tensor([-0.6206], grad_fn=<AddBackward0>)\n",
      "epoch: 16109 loss is tensor([-0.6325], grad_fn=<AddBackward0>)\n",
      "epoch: 16110 loss is tensor([-0.6487], grad_fn=<AddBackward0>)\n",
      "epoch: 16111 loss is tensor([-0.6463], grad_fn=<AddBackward0>)\n",
      "epoch: 16112 loss is tensor([-0.6401], grad_fn=<AddBackward0>)\n",
      "epoch: 16113 loss is tensor([-0.6046], grad_fn=<AddBackward0>)\n",
      "epoch: 16114 loss is tensor([-0.6572], grad_fn=<AddBackward0>)\n",
      "epoch: 16115 loss is tensor([-0.6160], grad_fn=<AddBackward0>)\n",
      "epoch: 16116 loss is tensor([-0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 16117 loss is tensor([-0.6690], grad_fn=<AddBackward0>)\n",
      "epoch: 16118 loss is tensor([-0.6648], grad_fn=<AddBackward0>)\n",
      "epoch: 16119 loss is tensor([-0.6338], grad_fn=<AddBackward0>)\n",
      "epoch: 16120 loss is tensor([-0.6619], grad_fn=<AddBackward0>)\n",
      "epoch: 16121 loss is tensor([-0.6291], grad_fn=<AddBackward0>)\n",
      "epoch: 16122 loss is tensor([-0.6048], grad_fn=<AddBackward0>)\n",
      "epoch: 16123 loss is tensor([-0.6225], grad_fn=<AddBackward0>)\n",
      "epoch: 16124 loss is tensor([-0.6206], grad_fn=<AddBackward0>)\n",
      "epoch: 16125 loss is tensor([-0.6167], grad_fn=<AddBackward0>)\n",
      "epoch: 16126 loss is tensor([-0.6336], grad_fn=<AddBackward0>)\n",
      "epoch: 16127 loss is tensor([-0.5901], grad_fn=<AddBackward0>)\n",
      "epoch: 16128 loss is tensor([-0.6122], grad_fn=<AddBackward0>)\n",
      "epoch: 16129 loss is tensor([-0.6060], grad_fn=<AddBackward0>)\n",
      "epoch: 16130 loss is tensor([-0.6152], grad_fn=<AddBackward0>)\n",
      "epoch: 16131 loss is tensor([-0.6339], grad_fn=<AddBackward0>)\n",
      "epoch: 16132 loss is tensor([-0.6613], grad_fn=<AddBackward0>)\n",
      "epoch: 16133 loss is tensor([-0.6812], grad_fn=<AddBackward0>)\n",
      "epoch: 16134 loss is tensor([-0.6428], grad_fn=<AddBackward0>)\n",
      "epoch: 16135 loss is tensor([-0.7080], grad_fn=<AddBackward0>)\n",
      "epoch: 16136 loss is tensor([-0.6188], grad_fn=<AddBackward0>)\n",
      "epoch: 16137 loss is tensor([-0.7066], grad_fn=<AddBackward0>)\n",
      "epoch: 16138 loss is tensor([-0.6922], grad_fn=<AddBackward0>)\n",
      "epoch: 16139 loss is tensor([-0.6670], grad_fn=<AddBackward0>)\n",
      "epoch: 16140 loss is tensor([-0.6426], grad_fn=<AddBackward0>)\n",
      "epoch: 16141 loss is tensor([-0.6656], grad_fn=<AddBackward0>)\n",
      "epoch: 16142 loss is tensor([-0.6729], grad_fn=<AddBackward0>)\n",
      "epoch: 16143 loss is tensor([-0.6934], grad_fn=<AddBackward0>)\n",
      "epoch: 16144 loss is tensor([-0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 16145 loss is tensor([-0.6166], grad_fn=<AddBackward0>)\n",
      "epoch: 16146 loss is tensor([-0.6793], grad_fn=<AddBackward0>)\n",
      "epoch: 16147 loss is tensor([-0.6583], grad_fn=<AddBackward0>)\n",
      "epoch: 16148 loss is tensor([-0.6449], grad_fn=<AddBackward0>)\n",
      "epoch: 16149 loss is tensor([-0.7079], grad_fn=<AddBackward0>)\n",
      "epoch: 16150 loss is tensor([-0.6332], grad_fn=<AddBackward0>)\n",
      "epoch: 16151 loss is tensor([-0.6749], grad_fn=<AddBackward0>)\n",
      "epoch: 16152 loss is tensor([-0.6147], grad_fn=<AddBackward0>)\n",
      "epoch: 16153 loss is tensor([-0.5845], grad_fn=<AddBackward0>)\n",
      "epoch: 16154 loss is tensor([-0.5960], grad_fn=<AddBackward0>)\n",
      "epoch: 16155 loss is tensor([-0.6146], grad_fn=<AddBackward0>)\n",
      "epoch: 16156 loss is tensor([-0.6182], grad_fn=<AddBackward0>)\n",
      "epoch: 16157 loss is tensor([-0.6421], grad_fn=<AddBackward0>)\n",
      "epoch: 16158 loss is tensor([-0.5977], grad_fn=<AddBackward0>)\n",
      "epoch: 16159 loss is tensor([-0.6097], grad_fn=<AddBackward0>)\n",
      "epoch: 16160 loss is tensor([-0.6199], grad_fn=<AddBackward0>)\n",
      "epoch: 16161 loss is tensor([-0.6636], grad_fn=<AddBackward0>)\n",
      "epoch: 16162 loss is tensor([-0.6785], grad_fn=<AddBackward0>)\n",
      "epoch: 16163 loss is tensor([-0.6835], grad_fn=<AddBackward0>)\n",
      "epoch: 16164 loss is tensor([-0.6786], grad_fn=<AddBackward0>)\n",
      "epoch: 16165 loss is tensor([-0.6403], grad_fn=<AddBackward0>)\n",
      "epoch: 16166 loss is tensor([-0.6824], grad_fn=<AddBackward0>)\n",
      "epoch: 16167 loss is tensor([-0.6452], grad_fn=<AddBackward0>)\n",
      "epoch: 16168 loss is tensor([-0.6757], grad_fn=<AddBackward0>)\n",
      "epoch: 16169 loss is tensor([-0.7148], grad_fn=<AddBackward0>)\n",
      "epoch: 16170 loss is tensor([-0.6615], grad_fn=<AddBackward0>)\n",
      "epoch: 16171 loss is tensor([-0.6728], grad_fn=<AddBackward0>)\n",
      "epoch: 16172 loss is tensor([-0.6834], grad_fn=<AddBackward0>)\n",
      "epoch: 16173 loss is tensor([-0.6265], grad_fn=<AddBackward0>)\n",
      "epoch: 16174 loss is tensor([-0.6101], grad_fn=<AddBackward0>)\n",
      "epoch: 16175 loss is tensor([-0.6246], grad_fn=<AddBackward0>)\n",
      "epoch: 16176 loss is tensor([-0.5998], grad_fn=<AddBackward0>)\n",
      "epoch: 16177 loss is tensor([-0.6507], grad_fn=<AddBackward0>)\n",
      "epoch: 16178 loss is tensor([-0.6431], grad_fn=<AddBackward0>)\n",
      "epoch: 16179 loss is tensor([-0.5493], grad_fn=<AddBackward0>)\n",
      "epoch: 16180 loss is tensor([-0.5349], grad_fn=<AddBackward0>)\n",
      "epoch: 16181 loss is tensor([-0.6214], grad_fn=<AddBackward0>)\n",
      "epoch: 16182 loss is tensor([-0.5332], grad_fn=<AddBackward0>)\n",
      "epoch: 16183 loss is tensor([-0.6000], grad_fn=<AddBackward0>)\n",
      "epoch: 16184 loss is tensor([-0.6343], grad_fn=<AddBackward0>)\n",
      "epoch: 16185 loss is tensor([-0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 16186 loss is tensor([-0.5646], grad_fn=<AddBackward0>)\n",
      "epoch: 16187 loss is tensor([-0.5458], grad_fn=<AddBackward0>)\n",
      "epoch: 16188 loss is tensor([-0.5925], grad_fn=<AddBackward0>)\n",
      "epoch: 16189 loss is tensor([-0.5957], grad_fn=<AddBackward0>)\n",
      "epoch: 16190 loss is tensor([-0.5968], grad_fn=<AddBackward0>)\n",
      "epoch: 16191 loss is tensor([-0.6041], grad_fn=<AddBackward0>)\n",
      "epoch: 16192 loss is tensor([-0.6261], grad_fn=<AddBackward0>)\n",
      "epoch: 16193 loss is tensor([-0.5545], grad_fn=<AddBackward0>)\n",
      "epoch: 16194 loss is tensor([-0.5965], grad_fn=<AddBackward0>)\n",
      "epoch: 16195 loss is tensor([-0.6086], grad_fn=<AddBackward0>)\n",
      "epoch: 16196 loss is tensor([-0.6462], grad_fn=<AddBackward0>)\n",
      "epoch: 16197 loss is tensor([-0.6566], grad_fn=<AddBackward0>)\n",
      "epoch: 16198 loss is tensor([-0.6908], grad_fn=<AddBackward0>)\n",
      "epoch: 16199 loss is tensor([-0.6073], grad_fn=<AddBackward0>)\n",
      "epoch: 16200 loss is tensor([-0.6681], grad_fn=<AddBackward0>)\n",
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16201 loss is tensor([-0.6241], grad_fn=<AddBackward0>)\n",
      "epoch: 16202 loss is tensor([-0.6297], grad_fn=<AddBackward0>)\n",
      "epoch: 16203 loss is tensor([-0.5811], grad_fn=<AddBackward0>)\n",
      "epoch: 16204 loss is tensor([-0.6193], grad_fn=<AddBackward0>)\n",
      "epoch: 16205 loss is tensor([-0.6286], grad_fn=<AddBackward0>)\n",
      "epoch: 16206 loss is tensor([-0.6512], grad_fn=<AddBackward0>)\n",
      "epoch: 16207 loss is tensor([-0.5856], grad_fn=<AddBackward0>)\n",
      "epoch: 16208 loss is tensor([-0.6222], grad_fn=<AddBackward0>)\n",
      "epoch: 16209 loss is tensor([-0.6238], grad_fn=<AddBackward0>)\n",
      "epoch: 16210 loss is tensor([-0.5794], grad_fn=<AddBackward0>)\n",
      "epoch: 16211 loss is tensor([-0.5916], grad_fn=<AddBackward0>)\n",
      "epoch: 16212 loss is tensor([-0.5977], grad_fn=<AddBackward0>)\n",
      "epoch: 16213 loss is tensor([-0.5566], grad_fn=<AddBackward0>)\n",
      "epoch: 16214 loss is tensor([-0.6497], grad_fn=<AddBackward0>)\n",
      "epoch: 16215 loss is tensor([-0.5547], grad_fn=<AddBackward0>)\n",
      "epoch: 16216 loss is tensor([-0.6088], grad_fn=<AddBackward0>)\n",
      "epoch: 16217 loss is tensor([-0.6213], grad_fn=<AddBackward0>)\n",
      "epoch: 16218 loss is tensor([-0.6184], grad_fn=<AddBackward0>)\n",
      "epoch: 16219 loss is tensor([-0.5648], grad_fn=<AddBackward0>)\n",
      "epoch: 16220 loss is tensor([-0.6889], grad_fn=<AddBackward0>)\n",
      "epoch: 16221 loss is tensor([-0.6282], grad_fn=<AddBackward0>)\n",
      "epoch: 16222 loss is tensor([-0.6065], grad_fn=<AddBackward0>)\n",
      "epoch: 16223 loss is tensor([-0.5931], grad_fn=<AddBackward0>)\n",
      "epoch: 16224 loss is tensor([-0.5803], grad_fn=<AddBackward0>)\n",
      "epoch: 16225 loss is tensor([-0.6173], grad_fn=<AddBackward0>)\n",
      "epoch: 16226 loss is tensor([-0.6240], grad_fn=<AddBackward0>)\n",
      "epoch: 16227 loss is tensor([-0.6119], grad_fn=<AddBackward0>)\n",
      "epoch: 16228 loss is tensor([-0.6362], grad_fn=<AddBackward0>)\n",
      "epoch: 16229 loss is tensor([-0.6812], grad_fn=<AddBackward0>)\n",
      "epoch: 16230 loss is tensor([-0.5998], grad_fn=<AddBackward0>)\n",
      "epoch: 16231 loss is tensor([-0.5748], grad_fn=<AddBackward0>)\n",
      "epoch: 16232 loss is tensor([-0.5884], grad_fn=<AddBackward0>)\n",
      "epoch: 16233 loss is tensor([-0.6807], grad_fn=<AddBackward0>)\n",
      "epoch: 16234 loss is tensor([-0.6528], grad_fn=<AddBackward0>)\n",
      "epoch: 16235 loss is tensor([-0.6630], grad_fn=<AddBackward0>)\n",
      "epoch: 16236 loss is tensor([-0.6425], grad_fn=<AddBackward0>)\n",
      "epoch: 16237 loss is tensor([-0.6784], grad_fn=<AddBackward0>)\n",
      "epoch: 16238 loss is tensor([-0.6119], grad_fn=<AddBackward0>)\n",
      "epoch: 16239 loss is tensor([-0.6006], grad_fn=<AddBackward0>)\n",
      "epoch: 16240 loss is tensor([-0.6745], grad_fn=<AddBackward0>)\n",
      "epoch: 16241 loss is tensor([-0.6567], grad_fn=<AddBackward0>)\n",
      "epoch: 16242 loss is tensor([-0.6528], grad_fn=<AddBackward0>)\n",
      "epoch: 16243 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 16244 loss is tensor([-0.6415], grad_fn=<AddBackward0>)\n",
      "epoch: 16245 loss is tensor([-0.6382], grad_fn=<AddBackward0>)\n",
      "epoch: 16246 loss is tensor([-0.6977], grad_fn=<AddBackward0>)\n",
      "epoch: 16247 loss is tensor([-0.6851], grad_fn=<AddBackward0>)\n",
      "epoch: 16248 loss is tensor([-0.6363], grad_fn=<AddBackward0>)\n",
      "epoch: 16249 loss is tensor([-0.7017], grad_fn=<AddBackward0>)\n",
      "epoch: 16250 loss is tensor([-0.6715], grad_fn=<AddBackward0>)\n",
      "epoch: 16251 loss is tensor([-0.7096], grad_fn=<AddBackward0>)\n",
      "epoch: 16252 loss is tensor([-0.6570], grad_fn=<AddBackward0>)\n",
      "epoch: 16253 loss is tensor([-0.6590], grad_fn=<AddBackward0>)\n",
      "epoch: 16254 loss is tensor([-0.6883], grad_fn=<AddBackward0>)\n",
      "epoch: 16255 loss is tensor([-0.6576], grad_fn=<AddBackward0>)\n",
      "epoch: 16256 loss is tensor([-0.6408], grad_fn=<AddBackward0>)\n",
      "epoch: 16257 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 16258 loss is tensor([-0.6370], grad_fn=<AddBackward0>)\n",
      "epoch: 16259 loss is tensor([-0.6979], grad_fn=<AddBackward0>)\n",
      "epoch: 16260 loss is tensor([-0.7227], grad_fn=<AddBackward0>)\n",
      "epoch: 16261 loss is tensor([-0.6617], grad_fn=<AddBackward0>)\n",
      "epoch: 16262 loss is tensor([-0.6573], grad_fn=<AddBackward0>)\n",
      "epoch: 16263 loss is tensor([-0.6533], grad_fn=<AddBackward0>)\n",
      "epoch: 16264 loss is tensor([-0.6490], grad_fn=<AddBackward0>)\n",
      "epoch: 16265 loss is tensor([-0.6148], grad_fn=<AddBackward0>)\n",
      "epoch: 16266 loss is tensor([-0.6440], grad_fn=<AddBackward0>)\n",
      "epoch: 16267 loss is tensor([-0.6475], grad_fn=<AddBackward0>)\n",
      "epoch: 16268 loss is tensor([-0.7189], grad_fn=<AddBackward0>)\n",
      "epoch: 16269 loss is tensor([-0.6226], grad_fn=<AddBackward0>)\n",
      "epoch: 16270 loss is tensor([-0.6151], grad_fn=<AddBackward0>)\n",
      "epoch: 16271 loss is tensor([-0.5813], grad_fn=<AddBackward0>)\n",
      "epoch: 16272 loss is tensor([-0.6191], grad_fn=<AddBackward0>)\n",
      "epoch: 16273 loss is tensor([-0.6367], grad_fn=<AddBackward0>)\n",
      "epoch: 16274 loss is tensor([-0.6518], grad_fn=<AddBackward0>)\n",
      "epoch: 16275 loss is tensor([-0.6324], grad_fn=<AddBackward0>)\n",
      "epoch: 16276 loss is tensor([-0.6627], grad_fn=<AddBackward0>)\n",
      "epoch: 16277 loss is tensor([-0.6322], grad_fn=<AddBackward0>)\n",
      "epoch: 16278 loss is tensor([-0.6834], grad_fn=<AddBackward0>)\n",
      "epoch: 16279 loss is tensor([-0.6104], grad_fn=<AddBackward0>)\n",
      "epoch: 16280 loss is tensor([-0.6097], grad_fn=<AddBackward0>)\n",
      "epoch: 16281 loss is tensor([-0.6594], grad_fn=<AddBackward0>)\n",
      "epoch: 16282 loss is tensor([-0.5882], grad_fn=<AddBackward0>)\n",
      "epoch: 16283 loss is tensor([-0.6567], grad_fn=<AddBackward0>)\n",
      "epoch: 16284 loss is tensor([-0.6840], grad_fn=<AddBackward0>)\n",
      "epoch: 16285 loss is tensor([-0.6120], grad_fn=<AddBackward0>)\n",
      "epoch: 16286 loss is tensor([-0.6822], grad_fn=<AddBackward0>)\n",
      "epoch: 16287 loss is tensor([-0.6268], grad_fn=<AddBackward0>)\n",
      "epoch: 16288 loss is tensor([-0.5999], grad_fn=<AddBackward0>)\n",
      "epoch: 16289 loss is tensor([-0.6518], grad_fn=<AddBackward0>)\n",
      "epoch: 16290 loss is tensor([-0.6122], grad_fn=<AddBackward0>)\n",
      "epoch: 16291 loss is tensor([-0.6654], grad_fn=<AddBackward0>)\n",
      "epoch: 16292 loss is tensor([-0.5527], grad_fn=<AddBackward0>)\n",
      "epoch: 16293 loss is tensor([-0.5973], grad_fn=<AddBackward0>)\n",
      "epoch: 16294 loss is tensor([-0.6000], grad_fn=<AddBackward0>)\n",
      "epoch: 16295 loss is tensor([-0.6147], grad_fn=<AddBackward0>)\n",
      "epoch: 16296 loss is tensor([-0.5327], grad_fn=<AddBackward0>)\n",
      "epoch: 16297 loss is tensor([-0.6145], grad_fn=<AddBackward0>)\n",
      "epoch: 16298 loss is tensor([-0.5857], grad_fn=<AddBackward0>)\n",
      "epoch: 16299 loss is tensor([-0.6414], grad_fn=<AddBackward0>)\n",
      "epoch: 16300 loss is tensor([-0.6460], grad_fn=<AddBackward0>)\n",
      "33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16301 loss is tensor([-0.5947], grad_fn=<AddBackward0>)\n",
      "epoch: 16302 loss is tensor([-0.6668], grad_fn=<AddBackward0>)\n",
      "epoch: 16303 loss is tensor([-0.6167], grad_fn=<AddBackward0>)\n",
      "epoch: 16304 loss is tensor([-0.6094], grad_fn=<AddBackward0>)\n",
      "epoch: 16305 loss is tensor([-0.6181], grad_fn=<AddBackward0>)\n",
      "epoch: 16306 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 16307 loss is tensor([-0.6454], grad_fn=<AddBackward0>)\n",
      "epoch: 16308 loss is tensor([-0.5893], grad_fn=<AddBackward0>)\n",
      "epoch: 16309 loss is tensor([-0.6337], grad_fn=<AddBackward0>)\n",
      "epoch: 16310 loss is tensor([-0.6421], grad_fn=<AddBackward0>)\n",
      "epoch: 16311 loss is tensor([-0.6597], grad_fn=<AddBackward0>)\n",
      "epoch: 16312 loss is tensor([-0.6656], grad_fn=<AddBackward0>)\n",
      "epoch: 16313 loss is tensor([-0.6290], grad_fn=<AddBackward0>)\n",
      "epoch: 16314 loss is tensor([-0.6530], grad_fn=<AddBackward0>)\n",
      "epoch: 16315 loss is tensor([-0.6723], grad_fn=<AddBackward0>)\n",
      "epoch: 16316 loss is tensor([-0.6451], grad_fn=<AddBackward0>)\n",
      "epoch: 16317 loss is tensor([-0.6011], grad_fn=<AddBackward0>)\n",
      "epoch: 16318 loss is tensor([-0.6306], grad_fn=<AddBackward0>)\n",
      "epoch: 16319 loss is tensor([-0.5539], grad_fn=<AddBackward0>)\n",
      "epoch: 16320 loss is tensor([-0.6462], grad_fn=<AddBackward0>)\n",
      "epoch: 16321 loss is tensor([-0.6025], grad_fn=<AddBackward0>)\n",
      "epoch: 16322 loss is tensor([-0.6393], grad_fn=<AddBackward0>)\n",
      "epoch: 16323 loss is tensor([-0.5738], grad_fn=<AddBackward0>)\n",
      "epoch: 16324 loss is tensor([-0.6811], grad_fn=<AddBackward0>)\n",
      "epoch: 16325 loss is tensor([-0.6519], grad_fn=<AddBackward0>)\n",
      "epoch: 16326 loss is tensor([-0.5912], grad_fn=<AddBackward0>)\n",
      "epoch: 16327 loss is tensor([-0.5771], grad_fn=<AddBackward0>)\n",
      "epoch: 16328 loss is tensor([-0.6333], grad_fn=<AddBackward0>)\n",
      "epoch: 16329 loss is tensor([-0.6640], grad_fn=<AddBackward0>)\n",
      "epoch: 16330 loss is tensor([-0.6211], grad_fn=<AddBackward0>)\n",
      "epoch: 16331 loss is tensor([-0.6412], grad_fn=<AddBackward0>)\n",
      "epoch: 16332 loss is tensor([-0.6231], grad_fn=<AddBackward0>)\n",
      "epoch: 16333 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 16334 loss is tensor([-0.6713], grad_fn=<AddBackward0>)\n",
      "epoch: 16335 loss is tensor([-0.6462], grad_fn=<AddBackward0>)\n",
      "epoch: 16336 loss is tensor([-0.6953], grad_fn=<AddBackward0>)\n",
      "epoch: 16337 loss is tensor([-0.6512], grad_fn=<AddBackward0>)\n",
      "epoch: 16338 loss is tensor([-0.6266], grad_fn=<AddBackward0>)\n",
      "epoch: 16339 loss is tensor([-0.7105], grad_fn=<AddBackward0>)\n",
      "epoch: 16340 loss is tensor([-0.6196], grad_fn=<AddBackward0>)\n",
      "epoch: 16341 loss is tensor([-0.6780], grad_fn=<AddBackward0>)\n",
      "epoch: 16342 loss is tensor([-0.6266], grad_fn=<AddBackward0>)\n",
      "epoch: 16343 loss is tensor([-0.6474], grad_fn=<AddBackward0>)\n",
      "epoch: 16344 loss is tensor([-0.5885], grad_fn=<AddBackward0>)\n",
      "epoch: 16345 loss is tensor([-0.6566], grad_fn=<AddBackward0>)\n",
      "epoch: 16346 loss is tensor([-0.6220], grad_fn=<AddBackward0>)\n",
      "epoch: 16347 loss is tensor([-0.6622], grad_fn=<AddBackward0>)\n",
      "epoch: 16348 loss is tensor([-0.6465], grad_fn=<AddBackward0>)\n",
      "epoch: 16349 loss is tensor([-0.6380], grad_fn=<AddBackward0>)\n",
      "epoch: 16350 loss is tensor([-0.6277], grad_fn=<AddBackward0>)\n",
      "epoch: 16351 loss is tensor([-0.6560], grad_fn=<AddBackward0>)\n",
      "epoch: 16352 loss is tensor([-0.7096], grad_fn=<AddBackward0>)\n",
      "epoch: 16353 loss is tensor([-0.6972], grad_fn=<AddBackward0>)\n",
      "epoch: 16354 loss is tensor([-0.6330], grad_fn=<AddBackward0>)\n",
      "epoch: 16355 loss is tensor([-0.6542], grad_fn=<AddBackward0>)\n",
      "epoch: 16356 loss is tensor([-0.6439], grad_fn=<AddBackward0>)\n",
      "epoch: 16357 loss is tensor([-0.6655], grad_fn=<AddBackward0>)\n",
      "epoch: 16358 loss is tensor([-0.7164], grad_fn=<AddBackward0>)\n",
      "epoch: 16359 loss is tensor([-0.6779], grad_fn=<AddBackward0>)\n",
      "epoch: 16360 loss is tensor([-0.6249], grad_fn=<AddBackward0>)\n",
      "epoch: 16361 loss is tensor([-0.6669], grad_fn=<AddBackward0>)\n",
      "epoch: 16362 loss is tensor([-0.6782], grad_fn=<AddBackward0>)\n",
      "epoch: 16363 loss is tensor([-0.6750], grad_fn=<AddBackward0>)\n",
      "epoch: 16364 loss is tensor([-0.6424], grad_fn=<AddBackward0>)\n",
      "epoch: 16365 loss is tensor([-0.6838], grad_fn=<AddBackward0>)\n",
      "epoch: 16366 loss is tensor([-0.6533], grad_fn=<AddBackward0>)\n",
      "epoch: 16367 loss is tensor([-0.6966], grad_fn=<AddBackward0>)\n",
      "epoch: 16368 loss is tensor([-0.6255], grad_fn=<AddBackward0>)\n",
      "epoch: 16369 loss is tensor([-0.6389], grad_fn=<AddBackward0>)\n",
      "epoch: 16370 loss is tensor([-0.6237], grad_fn=<AddBackward0>)\n",
      "epoch: 16371 loss is tensor([-0.5931], grad_fn=<AddBackward0>)\n",
      "epoch: 16372 loss is tensor([-0.6372], grad_fn=<AddBackward0>)\n",
      "epoch: 16373 loss is tensor([-0.6584], grad_fn=<AddBackward0>)\n",
      "epoch: 16374 loss is tensor([-0.6017], grad_fn=<AddBackward0>)\n",
      "epoch: 16375 loss is tensor([-0.6563], grad_fn=<AddBackward0>)\n",
      "epoch: 16376 loss is tensor([-0.6021], grad_fn=<AddBackward0>)\n",
      "epoch: 16377 loss is tensor([-0.6263], grad_fn=<AddBackward0>)\n",
      "epoch: 16378 loss is tensor([-0.6640], grad_fn=<AddBackward0>)\n",
      "epoch: 16379 loss is tensor([-0.6140], grad_fn=<AddBackward0>)\n",
      "epoch: 16380 loss is tensor([-0.6819], grad_fn=<AddBackward0>)\n",
      "epoch: 16381 loss is tensor([-0.6943], grad_fn=<AddBackward0>)\n",
      "epoch: 16382 loss is tensor([-0.6591], grad_fn=<AddBackward0>)\n",
      "epoch: 16383 loss is tensor([-0.6348], grad_fn=<AddBackward0>)\n",
      "epoch: 16384 loss is tensor([-0.6876], grad_fn=<AddBackward0>)\n",
      "epoch: 16385 loss is tensor([-0.6568], grad_fn=<AddBackward0>)\n",
      "epoch: 16386 loss is tensor([-0.7025], grad_fn=<AddBackward0>)\n",
      "epoch: 16387 loss is tensor([-0.6263], grad_fn=<AddBackward0>)\n",
      "epoch: 16388 loss is tensor([-0.6349], grad_fn=<AddBackward0>)\n",
      "epoch: 16389 loss is tensor([-0.6420], grad_fn=<AddBackward0>)\n",
      "epoch: 16390 loss is tensor([-0.6355], grad_fn=<AddBackward0>)\n",
      "epoch: 16391 loss is tensor([-0.6601], grad_fn=<AddBackward0>)\n",
      "epoch: 16392 loss is tensor([-0.6590], grad_fn=<AddBackward0>)\n",
      "epoch: 16393 loss is tensor([-0.6276], grad_fn=<AddBackward0>)\n",
      "epoch: 16394 loss is tensor([-0.6417], grad_fn=<AddBackward0>)\n",
      "epoch: 16395 loss is tensor([-0.6065], grad_fn=<AddBackward0>)\n",
      "epoch: 16396 loss is tensor([-0.6110], grad_fn=<AddBackward0>)\n",
      "epoch: 16397 loss is tensor([-0.6885], grad_fn=<AddBackward0>)\n",
      "epoch: 16398 loss is tensor([-0.6108], grad_fn=<AddBackward0>)\n",
      "epoch: 16399 loss is tensor([-0.6406], grad_fn=<AddBackward0>)\n",
      "epoch: 16400 loss is tensor([-0.6337], grad_fn=<AddBackward0>)\n",
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16401 loss is tensor([-0.6196], grad_fn=<AddBackward0>)\n",
      "epoch: 16402 loss is tensor([-0.6391], grad_fn=<AddBackward0>)\n",
      "epoch: 16403 loss is tensor([-0.6112], grad_fn=<AddBackward0>)\n",
      "epoch: 16404 loss is tensor([-0.6012], grad_fn=<AddBackward0>)\n",
      "epoch: 16405 loss is tensor([-0.6757], grad_fn=<AddBackward0>)\n",
      "epoch: 16406 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 16407 loss is tensor([-0.6325], grad_fn=<AddBackward0>)\n",
      "epoch: 16408 loss is tensor([-0.6141], grad_fn=<AddBackward0>)\n",
      "epoch: 16409 loss is tensor([-0.6443], grad_fn=<AddBackward0>)\n",
      "epoch: 16410 loss is tensor([-0.6055], grad_fn=<AddBackward0>)\n",
      "epoch: 16411 loss is tensor([-0.6585], grad_fn=<AddBackward0>)\n",
      "epoch: 16412 loss is tensor([-0.7006], grad_fn=<AddBackward0>)\n",
      "epoch: 16413 loss is tensor([-0.7099], grad_fn=<AddBackward0>)\n",
      "epoch: 16414 loss is tensor([-0.6396], grad_fn=<AddBackward0>)\n",
      "epoch: 16415 loss is tensor([-0.6983], grad_fn=<AddBackward0>)\n",
      "epoch: 16416 loss is tensor([-0.7162], grad_fn=<AddBackward0>)\n",
      "epoch: 16417 loss is tensor([-0.6138], grad_fn=<AddBackward0>)\n",
      "epoch: 16418 loss is tensor([-0.6473], grad_fn=<AddBackward0>)\n",
      "epoch: 16419 loss is tensor([-0.6461], grad_fn=<AddBackward0>)\n",
      "epoch: 16420 loss is tensor([-0.6426], grad_fn=<AddBackward0>)\n",
      "epoch: 16421 loss is tensor([-0.6432], grad_fn=<AddBackward0>)\n",
      "epoch: 16422 loss is tensor([-0.6287], grad_fn=<AddBackward0>)\n",
      "epoch: 16423 loss is tensor([-0.6231], grad_fn=<AddBackward0>)\n",
      "epoch: 16424 loss is tensor([-0.6374], grad_fn=<AddBackward0>)\n",
      "epoch: 16425 loss is tensor([-0.6459], grad_fn=<AddBackward0>)\n",
      "epoch: 16426 loss is tensor([-0.6206], grad_fn=<AddBackward0>)\n",
      "epoch: 16427 loss is tensor([-0.6406], grad_fn=<AddBackward0>)\n",
      "epoch: 16428 loss is tensor([-0.5829], grad_fn=<AddBackward0>)\n",
      "epoch: 16429 loss is tensor([-0.5593], grad_fn=<AddBackward0>)\n",
      "epoch: 16430 loss is tensor([-0.6791], grad_fn=<AddBackward0>)\n",
      "epoch: 16431 loss is tensor([-0.5557], grad_fn=<AddBackward0>)\n",
      "epoch: 16432 loss is tensor([-0.5691], grad_fn=<AddBackward0>)\n",
      "epoch: 16433 loss is tensor([-0.6622], grad_fn=<AddBackward0>)\n",
      "epoch: 16434 loss is tensor([-0.5880], grad_fn=<AddBackward0>)\n",
      "epoch: 16435 loss is tensor([-0.5760], grad_fn=<AddBackward0>)\n",
      "epoch: 16436 loss is tensor([-0.5788], grad_fn=<AddBackward0>)\n",
      "epoch: 16437 loss is tensor([-0.6235], grad_fn=<AddBackward0>)\n",
      "epoch: 16438 loss is tensor([-0.6425], grad_fn=<AddBackward0>)\n",
      "epoch: 16439 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 16440 loss is tensor([-0.6938], grad_fn=<AddBackward0>)\n",
      "epoch: 16441 loss is tensor([-0.5907], grad_fn=<AddBackward0>)\n",
      "epoch: 16442 loss is tensor([-0.6394], grad_fn=<AddBackward0>)\n",
      "epoch: 16443 loss is tensor([-0.5813], grad_fn=<AddBackward0>)\n",
      "epoch: 16444 loss is tensor([-0.5861], grad_fn=<AddBackward0>)\n",
      "epoch: 16445 loss is tensor([-0.6189], grad_fn=<AddBackward0>)\n",
      "epoch: 16446 loss is tensor([-0.6560], grad_fn=<AddBackward0>)\n",
      "epoch: 16447 loss is tensor([-0.6204], grad_fn=<AddBackward0>)\n",
      "epoch: 16448 loss is tensor([-0.6134], grad_fn=<AddBackward0>)\n",
      "epoch: 16449 loss is tensor([-0.6585], grad_fn=<AddBackward0>)\n",
      "epoch: 16450 loss is tensor([-0.6294], grad_fn=<AddBackward0>)\n",
      "epoch: 16451 loss is tensor([-0.5835], grad_fn=<AddBackward0>)\n",
      "epoch: 16452 loss is tensor([-0.6010], grad_fn=<AddBackward0>)\n",
      "epoch: 16453 loss is tensor([-0.6382], grad_fn=<AddBackward0>)\n",
      "epoch: 16454 loss is tensor([-0.6953], grad_fn=<AddBackward0>)\n",
      "epoch: 16455 loss is tensor([-0.6051], grad_fn=<AddBackward0>)\n",
      "epoch: 16456 loss is tensor([-0.6109], grad_fn=<AddBackward0>)\n",
      "epoch: 16457 loss is tensor([-0.6543], grad_fn=<AddBackward0>)\n",
      "epoch: 16458 loss is tensor([-0.6111], grad_fn=<AddBackward0>)\n",
      "epoch: 16459 loss is tensor([-0.6888], grad_fn=<AddBackward0>)\n",
      "epoch: 16460 loss is tensor([-0.6245], grad_fn=<AddBackward0>)\n",
      "epoch: 16461 loss is tensor([-0.6638], grad_fn=<AddBackward0>)\n",
      "epoch: 16462 loss is tensor([-0.6580], grad_fn=<AddBackward0>)\n",
      "epoch: 16463 loss is tensor([-0.6489], grad_fn=<AddBackward0>)\n",
      "epoch: 16464 loss is tensor([-0.6877], grad_fn=<AddBackward0>)\n",
      "epoch: 16465 loss is tensor([-0.6644], grad_fn=<AddBackward0>)\n",
      "epoch: 16466 loss is tensor([-0.7058], grad_fn=<AddBackward0>)\n",
      "epoch: 16467 loss is tensor([-0.6198], grad_fn=<AddBackward0>)\n",
      "epoch: 16468 loss is tensor([-0.6815], grad_fn=<AddBackward0>)\n",
      "epoch: 16469 loss is tensor([-0.6114], grad_fn=<AddBackward0>)\n",
      "epoch: 16470 loss is tensor([-0.6376], grad_fn=<AddBackward0>)\n",
      "epoch: 16471 loss is tensor([-0.6280], grad_fn=<AddBackward0>)\n",
      "epoch: 16472 loss is tensor([-0.6631], grad_fn=<AddBackward0>)\n",
      "epoch: 16473 loss is tensor([-0.6722], grad_fn=<AddBackward0>)\n",
      "epoch: 16474 loss is tensor([-0.6553], grad_fn=<AddBackward0>)\n",
      "epoch: 16475 loss is tensor([-0.6680], grad_fn=<AddBackward0>)\n",
      "epoch: 16476 loss is tensor([-0.6766], grad_fn=<AddBackward0>)\n",
      "epoch: 16477 loss is tensor([-0.6358], grad_fn=<AddBackward0>)\n",
      "epoch: 16478 loss is tensor([-0.6099], grad_fn=<AddBackward0>)\n",
      "epoch: 16479 loss is tensor([-0.6414], grad_fn=<AddBackward0>)\n",
      "epoch: 16480 loss is tensor([-0.6623], grad_fn=<AddBackward0>)\n",
      "epoch: 16481 loss is tensor([-0.6144], grad_fn=<AddBackward0>)\n",
      "epoch: 16482 loss is tensor([-0.6679], grad_fn=<AddBackward0>)\n",
      "epoch: 16483 loss is tensor([-0.6326], grad_fn=<AddBackward0>)\n",
      "epoch: 16484 loss is tensor([-0.6233], grad_fn=<AddBackward0>)\n",
      "epoch: 16485 loss is tensor([-0.5785], grad_fn=<AddBackward0>)\n",
      "epoch: 16486 loss is tensor([-0.5960], grad_fn=<AddBackward0>)\n",
      "epoch: 16487 loss is tensor([-0.6016], grad_fn=<AddBackward0>)\n",
      "epoch: 16488 loss is tensor([-0.5755], grad_fn=<AddBackward0>)\n",
      "epoch: 16489 loss is tensor([-0.5814], grad_fn=<AddBackward0>)\n",
      "epoch: 16490 loss is tensor([-0.6253], grad_fn=<AddBackward0>)\n",
      "epoch: 16491 loss is tensor([-0.5959], grad_fn=<AddBackward0>)\n",
      "epoch: 16492 loss is tensor([-0.6884], grad_fn=<AddBackward0>)\n",
      "epoch: 16493 loss is tensor([-0.6186], grad_fn=<AddBackward0>)\n",
      "epoch: 16494 loss is tensor([-0.6455], grad_fn=<AddBackward0>)\n",
      "epoch: 16495 loss is tensor([-0.6784], grad_fn=<AddBackward0>)\n",
      "epoch: 16496 loss is tensor([-0.6215], grad_fn=<AddBackward0>)\n",
      "epoch: 16497 loss is tensor([-0.6809], grad_fn=<AddBackward0>)\n",
      "epoch: 16498 loss is tensor([-0.6592], grad_fn=<AddBackward0>)\n",
      "epoch: 16499 loss is tensor([-0.6650], grad_fn=<AddBackward0>)\n",
      "epoch: 16500 loss is tensor([-0.6022], grad_fn=<AddBackward0>)\n",
      "34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16501 loss is tensor([-0.6196], grad_fn=<AddBackward0>)\n",
      "epoch: 16502 loss is tensor([-0.6759], grad_fn=<AddBackward0>)\n",
      "epoch: 16503 loss is tensor([-0.6519], grad_fn=<AddBackward0>)\n",
      "epoch: 16504 loss is tensor([-0.6104], grad_fn=<AddBackward0>)\n",
      "epoch: 16505 loss is tensor([-0.7139], grad_fn=<AddBackward0>)\n",
      "epoch: 16506 loss is tensor([-0.6632], grad_fn=<AddBackward0>)\n",
      "epoch: 16507 loss is tensor([-0.7126], grad_fn=<AddBackward0>)\n",
      "epoch: 16508 loss is tensor([-0.6259], grad_fn=<AddBackward0>)\n",
      "epoch: 16509 loss is tensor([-0.6552], grad_fn=<AddBackward0>)\n",
      "epoch: 16510 loss is tensor([-0.6410], grad_fn=<AddBackward0>)\n",
      "epoch: 16511 loss is tensor([-0.6109], grad_fn=<AddBackward0>)\n",
      "epoch: 16512 loss is tensor([-0.6349], grad_fn=<AddBackward0>)\n",
      "epoch: 16513 loss is tensor([-0.6447], grad_fn=<AddBackward0>)\n",
      "epoch: 16514 loss is tensor([-0.6254], grad_fn=<AddBackward0>)\n",
      "epoch: 16515 loss is tensor([-0.6967], grad_fn=<AddBackward0>)\n",
      "epoch: 16516 loss is tensor([-0.6507], grad_fn=<AddBackward0>)\n",
      "epoch: 16517 loss is tensor([-0.6094], grad_fn=<AddBackward0>)\n",
      "epoch: 16518 loss is tensor([-0.5973], grad_fn=<AddBackward0>)\n",
      "epoch: 16519 loss is tensor([-0.6818], grad_fn=<AddBackward0>)\n",
      "epoch: 16520 loss is tensor([-0.6481], grad_fn=<AddBackward0>)\n",
      "epoch: 16521 loss is tensor([-0.6183], grad_fn=<AddBackward0>)\n",
      "epoch: 16522 loss is tensor([-0.5681], grad_fn=<AddBackward0>)\n",
      "epoch: 16523 loss is tensor([-0.6300], grad_fn=<AddBackward0>)\n",
      "epoch: 16524 loss is tensor([-0.6421], grad_fn=<AddBackward0>)\n",
      "epoch: 16525 loss is tensor([-0.5935], grad_fn=<AddBackward0>)\n",
      "epoch: 16526 loss is tensor([-0.6077], grad_fn=<AddBackward0>)\n",
      "epoch: 16527 loss is tensor([-0.6394], grad_fn=<AddBackward0>)\n",
      "epoch: 16528 loss is tensor([-0.6144], grad_fn=<AddBackward0>)\n",
      "epoch: 16529 loss is tensor([-0.5713], grad_fn=<AddBackward0>)\n",
      "epoch: 16530 loss is tensor([-0.6392], grad_fn=<AddBackward0>)\n",
      "epoch: 16531 loss is tensor([-0.6636], grad_fn=<AddBackward0>)\n",
      "epoch: 16532 loss is tensor([-0.6796], grad_fn=<AddBackward0>)\n",
      "epoch: 16533 loss is tensor([-0.6436], grad_fn=<AddBackward0>)\n",
      "epoch: 16534 loss is tensor([-0.6279], grad_fn=<AddBackward0>)\n",
      "epoch: 16535 loss is tensor([-0.6447], grad_fn=<AddBackward0>)\n",
      "epoch: 16536 loss is tensor([-0.6410], grad_fn=<AddBackward0>)\n",
      "epoch: 16537 loss is tensor([-0.6756], grad_fn=<AddBackward0>)\n",
      "epoch: 16538 loss is tensor([-0.6988], grad_fn=<AddBackward0>)\n",
      "epoch: 16539 loss is tensor([-0.6582], grad_fn=<AddBackward0>)\n",
      "epoch: 16540 loss is tensor([-0.6540], grad_fn=<AddBackward0>)\n",
      "epoch: 16541 loss is tensor([-0.6064], grad_fn=<AddBackward0>)\n",
      "epoch: 16542 loss is tensor([-0.7070], grad_fn=<AddBackward0>)\n",
      "epoch: 16543 loss is tensor([-0.6440], grad_fn=<AddBackward0>)\n",
      "epoch: 16544 loss is tensor([-0.7008], grad_fn=<AddBackward0>)\n",
      "epoch: 16545 loss is tensor([-0.6255], grad_fn=<AddBackward0>)\n",
      "epoch: 16546 loss is tensor([-0.6693], grad_fn=<AddBackward0>)\n",
      "epoch: 16547 loss is tensor([-0.6673], grad_fn=<AddBackward0>)\n",
      "epoch: 16548 loss is tensor([-0.6615], grad_fn=<AddBackward0>)\n",
      "epoch: 16549 loss is tensor([-0.6750], grad_fn=<AddBackward0>)\n",
      "epoch: 16550 loss is tensor([-0.6323], grad_fn=<AddBackward0>)\n",
      "epoch: 16551 loss is tensor([-0.6821], grad_fn=<AddBackward0>)\n",
      "epoch: 16552 loss is tensor([-0.6116], grad_fn=<AddBackward0>)\n",
      "epoch: 16553 loss is tensor([-0.6204], grad_fn=<AddBackward0>)\n",
      "epoch: 16554 loss is tensor([-0.5674], grad_fn=<AddBackward0>)\n",
      "epoch: 16555 loss is tensor([-0.6013], grad_fn=<AddBackward0>)\n",
      "epoch: 16556 loss is tensor([-0.6482], grad_fn=<AddBackward0>)\n",
      "epoch: 16557 loss is tensor([-0.5859], grad_fn=<AddBackward0>)\n",
      "epoch: 16558 loss is tensor([-0.6000], grad_fn=<AddBackward0>)\n",
      "epoch: 16559 loss is tensor([-0.6363], grad_fn=<AddBackward0>)\n",
      "epoch: 16560 loss is tensor([-0.5870], grad_fn=<AddBackward0>)\n",
      "epoch: 16561 loss is tensor([-0.6632], grad_fn=<AddBackward0>)\n",
      "epoch: 16562 loss is tensor([-0.6376], grad_fn=<AddBackward0>)\n",
      "epoch: 16563 loss is tensor([-0.6483], grad_fn=<AddBackward0>)\n",
      "epoch: 16564 loss is tensor([-0.6025], grad_fn=<AddBackward0>)\n",
      "epoch: 16565 loss is tensor([-0.6551], grad_fn=<AddBackward0>)\n",
      "epoch: 16566 loss is tensor([-0.6108], grad_fn=<AddBackward0>)\n",
      "epoch: 16567 loss is tensor([-0.5799], grad_fn=<AddBackward0>)\n",
      "epoch: 16568 loss is tensor([-0.6265], grad_fn=<AddBackward0>)\n",
      "epoch: 16569 loss is tensor([-0.6169], grad_fn=<AddBackward0>)\n",
      "epoch: 16570 loss is tensor([-0.6505], grad_fn=<AddBackward0>)\n",
      "epoch: 16571 loss is tensor([-0.6589], grad_fn=<AddBackward0>)\n",
      "epoch: 16572 loss is tensor([-0.6218], grad_fn=<AddBackward0>)\n",
      "epoch: 16573 loss is tensor([-0.7145], grad_fn=<AddBackward0>)\n",
      "epoch: 16574 loss is tensor([-0.5701], grad_fn=<AddBackward0>)\n",
      "epoch: 16575 loss is tensor([-0.6651], grad_fn=<AddBackward0>)\n",
      "epoch: 16576 loss is tensor([-0.6655], grad_fn=<AddBackward0>)\n",
      "epoch: 16577 loss is tensor([-0.7047], grad_fn=<AddBackward0>)\n",
      "epoch: 16578 loss is tensor([-0.6377], grad_fn=<AddBackward0>)\n",
      "epoch: 16579 loss is tensor([-0.6878], grad_fn=<AddBackward0>)\n",
      "epoch: 16580 loss is tensor([-0.6715], grad_fn=<AddBackward0>)\n",
      "epoch: 16581 loss is tensor([-0.6714], grad_fn=<AddBackward0>)\n",
      "epoch: 16582 loss is tensor([-0.6609], grad_fn=<AddBackward0>)\n",
      "epoch: 16583 loss is tensor([-0.6748], grad_fn=<AddBackward0>)\n",
      "epoch: 16584 loss is tensor([-0.6545], grad_fn=<AddBackward0>)\n",
      "epoch: 16585 loss is tensor([-0.6410], grad_fn=<AddBackward0>)\n",
      "epoch: 16586 loss is tensor([-0.6501], grad_fn=<AddBackward0>)\n",
      "epoch: 16587 loss is tensor([-0.6429], grad_fn=<AddBackward0>)\n",
      "epoch: 16588 loss is tensor([-0.5862], grad_fn=<AddBackward0>)\n",
      "epoch: 16589 loss is tensor([-0.6491], grad_fn=<AddBackward0>)\n",
      "epoch: 16590 loss is tensor([-0.6812], grad_fn=<AddBackward0>)\n",
      "epoch: 16591 loss is tensor([-0.6379], grad_fn=<AddBackward0>)\n",
      "epoch: 16592 loss is tensor([-0.6384], grad_fn=<AddBackward0>)\n",
      "epoch: 16593 loss is tensor([-0.6791], grad_fn=<AddBackward0>)\n",
      "epoch: 16594 loss is tensor([-0.5910], grad_fn=<AddBackward0>)\n",
      "epoch: 16595 loss is tensor([-0.6743], grad_fn=<AddBackward0>)\n",
      "epoch: 16596 loss is tensor([-0.6382], grad_fn=<AddBackward0>)\n",
      "epoch: 16597 loss is tensor([-0.5992], grad_fn=<AddBackward0>)\n",
      "epoch: 16598 loss is tensor([-0.6480], grad_fn=<AddBackward0>)\n",
      "epoch: 16599 loss is tensor([-0.6235], grad_fn=<AddBackward0>)\n",
      "epoch: 16600 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16601 loss is tensor([-0.5661], grad_fn=<AddBackward0>)\n",
      "epoch: 16602 loss is tensor([-0.6057], grad_fn=<AddBackward0>)\n",
      "epoch: 16603 loss is tensor([-0.6294], grad_fn=<AddBackward0>)\n",
      "epoch: 16604 loss is tensor([-0.6481], grad_fn=<AddBackward0>)\n",
      "epoch: 16605 loss is tensor([-0.6909], grad_fn=<AddBackward0>)\n",
      "epoch: 16606 loss is tensor([-0.6677], grad_fn=<AddBackward0>)\n",
      "epoch: 16607 loss is tensor([-0.6305], grad_fn=<AddBackward0>)\n",
      "epoch: 16608 loss is tensor([-0.6710], grad_fn=<AddBackward0>)\n",
      "epoch: 16609 loss is tensor([-0.6819], grad_fn=<AddBackward0>)\n",
      "epoch: 16610 loss is tensor([-0.6708], grad_fn=<AddBackward0>)\n",
      "epoch: 16611 loss is tensor([-0.6160], grad_fn=<AddBackward0>)\n",
      "epoch: 16612 loss is tensor([-0.6687], grad_fn=<AddBackward0>)\n",
      "epoch: 16613 loss is tensor([-0.6582], grad_fn=<AddBackward0>)\n",
      "epoch: 16614 loss is tensor([-0.7220], grad_fn=<AddBackward0>)\n",
      "epoch: 16615 loss is tensor([-0.6590], grad_fn=<AddBackward0>)\n",
      "epoch: 16616 loss is tensor([-0.6652], grad_fn=<AddBackward0>)\n",
      "epoch: 16617 loss is tensor([-0.6133], grad_fn=<AddBackward0>)\n",
      "epoch: 16618 loss is tensor([-0.6690], grad_fn=<AddBackward0>)\n",
      "epoch: 16619 loss is tensor([-0.6175], grad_fn=<AddBackward0>)\n",
      "epoch: 16620 loss is tensor([-0.6840], grad_fn=<AddBackward0>)\n",
      "epoch: 16621 loss is tensor([-0.6647], grad_fn=<AddBackward0>)\n",
      "epoch: 16622 loss is tensor([-0.7046], grad_fn=<AddBackward0>)\n",
      "epoch: 16623 loss is tensor([-0.6538], grad_fn=<AddBackward0>)\n",
      "epoch: 16624 loss is tensor([-0.6873], grad_fn=<AddBackward0>)\n",
      "epoch: 16625 loss is tensor([-0.6190], grad_fn=<AddBackward0>)\n",
      "epoch: 16626 loss is tensor([-0.6806], grad_fn=<AddBackward0>)\n",
      "epoch: 16627 loss is tensor([-0.6891], grad_fn=<AddBackward0>)\n",
      "epoch: 16628 loss is tensor([-0.6002], grad_fn=<AddBackward0>)\n",
      "epoch: 16629 loss is tensor([-0.5643], grad_fn=<AddBackward0>)\n",
      "epoch: 16630 loss is tensor([-0.6127], grad_fn=<AddBackward0>)\n",
      "epoch: 16631 loss is tensor([-0.6148], grad_fn=<AddBackward0>)\n",
      "epoch: 16632 loss is tensor([-0.5935], grad_fn=<AddBackward0>)\n",
      "epoch: 16633 loss is tensor([-0.6284], grad_fn=<AddBackward0>)\n",
      "epoch: 16634 loss is tensor([-0.6324], grad_fn=<AddBackward0>)\n",
      "epoch: 16635 loss is tensor([-0.6149], grad_fn=<AddBackward0>)\n",
      "epoch: 16636 loss is tensor([-0.6041], grad_fn=<AddBackward0>)\n",
      "epoch: 16637 loss is tensor([-0.5318], grad_fn=<AddBackward0>)\n",
      "epoch: 16638 loss is tensor([-0.6090], grad_fn=<AddBackward0>)\n",
      "epoch: 16639 loss is tensor([-0.5427], grad_fn=<AddBackward0>)\n",
      "epoch: 16640 loss is tensor([-0.6249], grad_fn=<AddBackward0>)\n",
      "epoch: 16641 loss is tensor([-0.6241], grad_fn=<AddBackward0>)\n",
      "epoch: 16642 loss is tensor([-0.5975], grad_fn=<AddBackward0>)\n",
      "epoch: 16643 loss is tensor([-0.6355], grad_fn=<AddBackward0>)\n",
      "epoch: 16644 loss is tensor([-0.6514], grad_fn=<AddBackward0>)\n",
      "epoch: 16645 loss is tensor([-0.6151], grad_fn=<AddBackward0>)\n",
      "epoch: 16646 loss is tensor([-0.5880], grad_fn=<AddBackward0>)\n",
      "epoch: 16647 loss is tensor([-0.5813], grad_fn=<AddBackward0>)\n",
      "epoch: 16648 loss is tensor([-0.6408], grad_fn=<AddBackward0>)\n",
      "epoch: 16649 loss is tensor([-0.6697], grad_fn=<AddBackward0>)\n",
      "epoch: 16650 loss is tensor([-0.6641], grad_fn=<AddBackward0>)\n",
      "epoch: 16651 loss is tensor([-0.5832], grad_fn=<AddBackward0>)\n",
      "epoch: 16652 loss is tensor([-0.5761], grad_fn=<AddBackward0>)\n",
      "epoch: 16653 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 16654 loss is tensor([-0.6580], grad_fn=<AddBackward0>)\n",
      "epoch: 16655 loss is tensor([-0.6106], grad_fn=<AddBackward0>)\n",
      "epoch: 16656 loss is tensor([-0.5958], grad_fn=<AddBackward0>)\n",
      "epoch: 16657 loss is tensor([-0.6516], grad_fn=<AddBackward0>)\n",
      "epoch: 16658 loss is tensor([-0.6156], grad_fn=<AddBackward0>)\n",
      "epoch: 16659 loss is tensor([-0.5933], grad_fn=<AddBackward0>)\n",
      "epoch: 16660 loss is tensor([-0.5727], grad_fn=<AddBackward0>)\n",
      "epoch: 16661 loss is tensor([-0.6462], grad_fn=<AddBackward0>)\n",
      "epoch: 16662 loss is tensor([-0.6312], grad_fn=<AddBackward0>)\n",
      "epoch: 16663 loss is tensor([-0.6254], grad_fn=<AddBackward0>)\n",
      "epoch: 16664 loss is tensor([-0.6450], grad_fn=<AddBackward0>)\n",
      "epoch: 16665 loss is tensor([-0.6261], grad_fn=<AddBackward0>)\n",
      "epoch: 16666 loss is tensor([-0.6366], grad_fn=<AddBackward0>)\n",
      "epoch: 16667 loss is tensor([-0.6702], grad_fn=<AddBackward0>)\n",
      "epoch: 16668 loss is tensor([-0.6138], grad_fn=<AddBackward0>)\n",
      "epoch: 16669 loss is tensor([-0.6702], grad_fn=<AddBackward0>)\n",
      "epoch: 16670 loss is tensor([-0.5409], grad_fn=<AddBackward0>)\n",
      "epoch: 16671 loss is tensor([-0.6542], grad_fn=<AddBackward0>)\n",
      "epoch: 16672 loss is tensor([-0.6428], grad_fn=<AddBackward0>)\n",
      "epoch: 16673 loss is tensor([-0.6686], grad_fn=<AddBackward0>)\n",
      "epoch: 16674 loss is tensor([-0.6091], grad_fn=<AddBackward0>)\n",
      "epoch: 16675 loss is tensor([-0.6027], grad_fn=<AddBackward0>)\n",
      "epoch: 16676 loss is tensor([-0.6230], grad_fn=<AddBackward0>)\n",
      "epoch: 16677 loss is tensor([-0.6227], grad_fn=<AddBackward0>)\n",
      "epoch: 16678 loss is tensor([-0.5986], grad_fn=<AddBackward0>)\n",
      "epoch: 16679 loss is tensor([-0.6805], grad_fn=<AddBackward0>)\n",
      "epoch: 16680 loss is tensor([-0.6239], grad_fn=<AddBackward0>)\n",
      "epoch: 16681 loss is tensor([-0.6326], grad_fn=<AddBackward0>)\n",
      "epoch: 16682 loss is tensor([-0.6462], grad_fn=<AddBackward0>)\n",
      "epoch: 16683 loss is tensor([-0.6701], grad_fn=<AddBackward0>)\n",
      "epoch: 16684 loss is tensor([-0.6090], grad_fn=<AddBackward0>)\n",
      "epoch: 16685 loss is tensor([-0.6766], grad_fn=<AddBackward0>)\n",
      "epoch: 16686 loss is tensor([-0.6257], grad_fn=<AddBackward0>)\n",
      "epoch: 16687 loss is tensor([-0.6043], grad_fn=<AddBackward0>)\n",
      "epoch: 16688 loss is tensor([-0.6046], grad_fn=<AddBackward0>)\n",
      "epoch: 16689 loss is tensor([-0.6200], grad_fn=<AddBackward0>)\n",
      "epoch: 16690 loss is tensor([-0.6483], grad_fn=<AddBackward0>)\n",
      "epoch: 16691 loss is tensor([-0.6051], grad_fn=<AddBackward0>)\n",
      "epoch: 16692 loss is tensor([-0.7043], grad_fn=<AddBackward0>)\n",
      "epoch: 16693 loss is tensor([-0.6318], grad_fn=<AddBackward0>)\n",
      "epoch: 16694 loss is tensor([-0.5965], grad_fn=<AddBackward0>)\n",
      "epoch: 16695 loss is tensor([-0.6385], grad_fn=<AddBackward0>)\n",
      "epoch: 16696 loss is tensor([-0.6772], grad_fn=<AddBackward0>)\n",
      "epoch: 16697 loss is tensor([-0.6338], grad_fn=<AddBackward0>)\n",
      "epoch: 16698 loss is tensor([-0.6480], grad_fn=<AddBackward0>)\n",
      "epoch: 16699 loss is tensor([-0.6212], grad_fn=<AddBackward0>)\n",
      "epoch: 16700 loss is tensor([-0.6621], grad_fn=<AddBackward0>)\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16701 loss is tensor([-0.6803], grad_fn=<AddBackward0>)\n",
      "epoch: 16702 loss is tensor([-0.7064], grad_fn=<AddBackward0>)\n",
      "epoch: 16703 loss is tensor([-0.6778], grad_fn=<AddBackward0>)\n",
      "epoch: 16704 loss is tensor([-0.6629], grad_fn=<AddBackward0>)\n",
      "epoch: 16705 loss is tensor([-0.6385], grad_fn=<AddBackward0>)\n",
      "epoch: 16706 loss is tensor([-0.5910], grad_fn=<AddBackward0>)\n",
      "epoch: 16707 loss is tensor([-0.6625], grad_fn=<AddBackward0>)\n",
      "epoch: 16708 loss is tensor([-0.6140], grad_fn=<AddBackward0>)\n",
      "epoch: 16709 loss is tensor([-0.6897], grad_fn=<AddBackward0>)\n",
      "epoch: 16710 loss is tensor([-0.6284], grad_fn=<AddBackward0>)\n",
      "epoch: 16711 loss is tensor([-0.6266], grad_fn=<AddBackward0>)\n",
      "epoch: 16712 loss is tensor([-0.6836], grad_fn=<AddBackward0>)\n",
      "epoch: 16713 loss is tensor([-0.6824], grad_fn=<AddBackward0>)\n",
      "epoch: 16714 loss is tensor([-0.6622], grad_fn=<AddBackward0>)\n",
      "epoch: 16715 loss is tensor([-0.7036], grad_fn=<AddBackward0>)\n",
      "epoch: 16716 loss is tensor([-0.6943], grad_fn=<AddBackward0>)\n",
      "epoch: 16717 loss is tensor([-0.7068], grad_fn=<AddBackward0>)\n",
      "epoch: 16718 loss is tensor([-0.6732], grad_fn=<AddBackward0>)\n",
      "epoch: 16719 loss is tensor([-0.6809], grad_fn=<AddBackward0>)\n",
      "epoch: 16720 loss is tensor([-0.6475], grad_fn=<AddBackward0>)\n",
      "epoch: 16721 loss is tensor([-0.7032], grad_fn=<AddBackward0>)\n",
      "epoch: 16722 loss is tensor([-0.7078], grad_fn=<AddBackward0>)\n",
      "epoch: 16723 loss is tensor([-0.6779], grad_fn=<AddBackward0>)\n",
      "epoch: 16724 loss is tensor([-0.6853], grad_fn=<AddBackward0>)\n",
      "epoch: 16725 loss is tensor([-0.6713], grad_fn=<AddBackward0>)\n",
      "epoch: 16726 loss is tensor([-0.6911], grad_fn=<AddBackward0>)\n",
      "epoch: 16727 loss is tensor([-0.6843], grad_fn=<AddBackward0>)\n",
      "epoch: 16728 loss is tensor([-0.6531], grad_fn=<AddBackward0>)\n",
      "epoch: 16729 loss is tensor([-0.5917], grad_fn=<AddBackward0>)\n",
      "epoch: 16730 loss is tensor([-0.6125], grad_fn=<AddBackward0>)\n",
      "epoch: 16731 loss is tensor([-0.6153], grad_fn=<AddBackward0>)\n",
      "epoch: 16732 loss is tensor([-0.6539], grad_fn=<AddBackward0>)\n",
      "epoch: 16733 loss is tensor([-0.5701], grad_fn=<AddBackward0>)\n",
      "epoch: 16734 loss is tensor([-0.6508], grad_fn=<AddBackward0>)\n",
      "epoch: 16735 loss is tensor([-0.6096], grad_fn=<AddBackward0>)\n",
      "epoch: 16736 loss is tensor([-0.6503], grad_fn=<AddBackward0>)\n",
      "epoch: 16737 loss is tensor([-0.6393], grad_fn=<AddBackward0>)\n",
      "epoch: 16738 loss is tensor([-0.6630], grad_fn=<AddBackward0>)\n",
      "epoch: 16739 loss is tensor([-0.6832], grad_fn=<AddBackward0>)\n",
      "epoch: 16740 loss is tensor([-0.6384], grad_fn=<AddBackward0>)\n",
      "epoch: 16741 loss is tensor([-0.6711], grad_fn=<AddBackward0>)\n",
      "epoch: 16742 loss is tensor([-0.6347], grad_fn=<AddBackward0>)\n",
      "epoch: 16743 loss is tensor([-0.6638], grad_fn=<AddBackward0>)\n",
      "epoch: 16744 loss is tensor([-0.6879], grad_fn=<AddBackward0>)\n",
      "epoch: 16745 loss is tensor([-0.6938], grad_fn=<AddBackward0>)\n",
      "epoch: 16746 loss is tensor([-0.6061], grad_fn=<AddBackward0>)\n",
      "epoch: 16747 loss is tensor([-0.6998], grad_fn=<AddBackward0>)\n",
      "epoch: 16748 loss is tensor([-0.7039], grad_fn=<AddBackward0>)\n",
      "epoch: 16749 loss is tensor([-0.6922], grad_fn=<AddBackward0>)\n",
      "epoch: 16750 loss is tensor([-0.6585], grad_fn=<AddBackward0>)\n",
      "epoch: 16751 loss is tensor([-0.6134], grad_fn=<AddBackward0>)\n",
      "epoch: 16752 loss is tensor([-0.6637], grad_fn=<AddBackward0>)\n",
      "epoch: 16753 loss is tensor([-0.6732], grad_fn=<AddBackward0>)\n",
      "epoch: 16754 loss is tensor([-0.6657], grad_fn=<AddBackward0>)\n",
      "epoch: 16755 loss is tensor([-0.6324], grad_fn=<AddBackward0>)\n",
      "epoch: 16756 loss is tensor([-0.7040], grad_fn=<AddBackward0>)\n",
      "epoch: 16757 loss is tensor([-0.6932], grad_fn=<AddBackward0>)\n",
      "epoch: 16758 loss is tensor([-0.6800], grad_fn=<AddBackward0>)\n",
      "epoch: 16759 loss is tensor([-0.6367], grad_fn=<AddBackward0>)\n",
      "epoch: 16760 loss is tensor([-0.7197], grad_fn=<AddBackward0>)\n",
      "epoch: 16761 loss is tensor([-0.7097], grad_fn=<AddBackward0>)\n",
      "epoch: 16762 loss is tensor([-0.6963], grad_fn=<AddBackward0>)\n",
      "epoch: 16763 loss is tensor([-0.6645], grad_fn=<AddBackward0>)\n",
      "epoch: 16764 loss is tensor([-0.6619], grad_fn=<AddBackward0>)\n",
      "epoch: 16765 loss is tensor([-0.6770], grad_fn=<AddBackward0>)\n",
      "epoch: 16766 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 16767 loss is tensor([-0.6367], grad_fn=<AddBackward0>)\n",
      "epoch: 16768 loss is tensor([-0.6747], grad_fn=<AddBackward0>)\n",
      "epoch: 16769 loss is tensor([-0.6789], grad_fn=<AddBackward0>)\n",
      "epoch: 16770 loss is tensor([-0.7017], grad_fn=<AddBackward0>)\n",
      "epoch: 16771 loss is tensor([-0.6437], grad_fn=<AddBackward0>)\n",
      "epoch: 16772 loss is tensor([-0.6962], grad_fn=<AddBackward0>)\n",
      "epoch: 16773 loss is tensor([-0.6654], grad_fn=<AddBackward0>)\n",
      "epoch: 16774 loss is tensor([-0.7099], grad_fn=<AddBackward0>)\n",
      "epoch: 16775 loss is tensor([-0.6901], grad_fn=<AddBackward0>)\n",
      "epoch: 16776 loss is tensor([-0.6961], grad_fn=<AddBackward0>)\n",
      "epoch: 16777 loss is tensor([-0.7534], grad_fn=<AddBackward0>)\n",
      "epoch: 16778 loss is tensor([-0.7415], grad_fn=<AddBackward0>)\n",
      "epoch: 16779 loss is tensor([-0.7160], grad_fn=<AddBackward0>)\n",
      "epoch: 16780 loss is tensor([-0.6723], grad_fn=<AddBackward0>)\n",
      "epoch: 16781 loss is tensor([-0.6234], grad_fn=<AddBackward0>)\n",
      "epoch: 16782 loss is tensor([-0.7000], grad_fn=<AddBackward0>)\n",
      "epoch: 16783 loss is tensor([-0.6313], grad_fn=<AddBackward0>)\n",
      "epoch: 16784 loss is tensor([-0.6706], grad_fn=<AddBackward0>)\n",
      "epoch: 16785 loss is tensor([-0.6069], grad_fn=<AddBackward0>)\n",
      "epoch: 16786 loss is tensor([-0.6949], grad_fn=<AddBackward0>)\n",
      "epoch: 16787 loss is tensor([-0.6532], grad_fn=<AddBackward0>)\n",
      "epoch: 16788 loss is tensor([-0.6495], grad_fn=<AddBackward0>)\n",
      "epoch: 16789 loss is tensor([-0.6393], grad_fn=<AddBackward0>)\n",
      "epoch: 16790 loss is tensor([-0.6824], grad_fn=<AddBackward0>)\n",
      "epoch: 16791 loss is tensor([-0.6810], grad_fn=<AddBackward0>)\n",
      "epoch: 16792 loss is tensor([-0.6636], grad_fn=<AddBackward0>)\n",
      "epoch: 16793 loss is tensor([-0.6589], grad_fn=<AddBackward0>)\n",
      "epoch: 16794 loss is tensor([-0.6694], grad_fn=<AddBackward0>)\n",
      "epoch: 16795 loss is tensor([-0.6642], grad_fn=<AddBackward0>)\n",
      "epoch: 16796 loss is tensor([-0.6487], grad_fn=<AddBackward0>)\n",
      "epoch: 16797 loss is tensor([-0.6594], grad_fn=<AddBackward0>)\n",
      "epoch: 16798 loss is tensor([-0.6786], grad_fn=<AddBackward0>)\n",
      "epoch: 16799 loss is tensor([-0.6825], grad_fn=<AddBackward0>)\n",
      "epoch: 16800 loss is tensor([-0.6330], grad_fn=<AddBackward0>)\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16801 loss is tensor([-0.6991], grad_fn=<AddBackward0>)\n",
      "epoch: 16802 loss is tensor([-0.7150], grad_fn=<AddBackward0>)\n",
      "epoch: 16803 loss is tensor([-0.7229], grad_fn=<AddBackward0>)\n",
      "epoch: 16804 loss is tensor([-0.6589], grad_fn=<AddBackward0>)\n",
      "epoch: 16805 loss is tensor([-0.7051], grad_fn=<AddBackward0>)\n",
      "epoch: 16806 loss is tensor([-0.6450], grad_fn=<AddBackward0>)\n",
      "epoch: 16807 loss is tensor([-0.6770], grad_fn=<AddBackward0>)\n",
      "epoch: 16808 loss is tensor([-0.6667], grad_fn=<AddBackward0>)\n",
      "epoch: 16809 loss is tensor([-0.6767], grad_fn=<AddBackward0>)\n",
      "epoch: 16810 loss is tensor([-0.6999], grad_fn=<AddBackward0>)\n",
      "epoch: 16811 loss is tensor([-0.6544], grad_fn=<AddBackward0>)\n",
      "epoch: 16812 loss is tensor([-0.6732], grad_fn=<AddBackward0>)\n",
      "epoch: 16813 loss is tensor([-0.7057], grad_fn=<AddBackward0>)\n",
      "epoch: 16814 loss is tensor([-0.6367], grad_fn=<AddBackward0>)\n",
      "epoch: 16815 loss is tensor([-0.6795], grad_fn=<AddBackward0>)\n",
      "epoch: 16816 loss is tensor([-0.6689], grad_fn=<AddBackward0>)\n",
      "epoch: 16817 loss is tensor([-0.7104], grad_fn=<AddBackward0>)\n",
      "epoch: 16818 loss is tensor([-0.6726], grad_fn=<AddBackward0>)\n",
      "epoch: 16819 loss is tensor([-0.6726], grad_fn=<AddBackward0>)\n",
      "epoch: 16820 loss is tensor([-0.7278], grad_fn=<AddBackward0>)\n",
      "epoch: 16821 loss is tensor([-0.6697], grad_fn=<AddBackward0>)\n",
      "epoch: 16822 loss is tensor([-0.6738], grad_fn=<AddBackward0>)\n",
      "epoch: 16823 loss is tensor([-0.6732], grad_fn=<AddBackward0>)\n",
      "epoch: 16824 loss is tensor([-0.6263], grad_fn=<AddBackward0>)\n",
      "epoch: 16825 loss is tensor([-0.6451], grad_fn=<AddBackward0>)\n",
      "epoch: 16826 loss is tensor([-0.7016], grad_fn=<AddBackward0>)\n",
      "epoch: 16827 loss is tensor([-0.6863], grad_fn=<AddBackward0>)\n",
      "epoch: 16828 loss is tensor([-0.6550], grad_fn=<AddBackward0>)\n",
      "epoch: 16829 loss is tensor([-0.7035], grad_fn=<AddBackward0>)\n",
      "epoch: 16830 loss is tensor([-0.6572], grad_fn=<AddBackward0>)\n",
      "epoch: 16831 loss is tensor([-0.6275], grad_fn=<AddBackward0>)\n",
      "epoch: 16832 loss is tensor([-0.7261], grad_fn=<AddBackward0>)\n",
      "epoch: 16833 loss is tensor([-0.6970], grad_fn=<AddBackward0>)\n",
      "epoch: 16834 loss is tensor([-0.7194], grad_fn=<AddBackward0>)\n",
      "epoch: 16835 loss is tensor([-0.7388], grad_fn=<AddBackward0>)\n",
      "epoch: 16836 loss is tensor([-0.6903], grad_fn=<AddBackward0>)\n",
      "epoch: 16837 loss is tensor([-0.7037], grad_fn=<AddBackward0>)\n",
      "epoch: 16838 loss is tensor([-0.6435], grad_fn=<AddBackward0>)\n",
      "epoch: 16839 loss is tensor([-0.6876], grad_fn=<AddBackward0>)\n",
      "epoch: 16840 loss is tensor([-0.6423], grad_fn=<AddBackward0>)\n",
      "epoch: 16841 loss is tensor([-0.7170], grad_fn=<AddBackward0>)\n",
      "epoch: 16842 loss is tensor([-0.6881], grad_fn=<AddBackward0>)\n",
      "epoch: 16843 loss is tensor([-0.6852], grad_fn=<AddBackward0>)\n",
      "epoch: 16844 loss is tensor([-0.6515], grad_fn=<AddBackward0>)\n",
      "epoch: 16845 loss is tensor([-0.7333], grad_fn=<AddBackward0>)\n",
      "epoch: 16846 loss is tensor([-0.7318], grad_fn=<AddBackward0>)\n",
      "epoch: 16847 loss is tensor([-0.6320], grad_fn=<AddBackward0>)\n",
      "epoch: 16848 loss is tensor([-0.6812], grad_fn=<AddBackward0>)\n",
      "epoch: 16849 loss is tensor([-0.6643], grad_fn=<AddBackward0>)\n",
      "epoch: 16850 loss is tensor([-0.6614], grad_fn=<AddBackward0>)\n",
      "epoch: 16851 loss is tensor([-0.6882], grad_fn=<AddBackward0>)\n",
      "epoch: 16852 loss is tensor([-0.6649], grad_fn=<AddBackward0>)\n",
      "epoch: 16853 loss is tensor([-0.6410], grad_fn=<AddBackward0>)\n",
      "epoch: 16854 loss is tensor([-0.5927], grad_fn=<AddBackward0>)\n",
      "epoch: 16855 loss is tensor([-0.7470], grad_fn=<AddBackward0>)\n",
      "epoch: 16856 loss is tensor([-0.7375], grad_fn=<AddBackward0>)\n",
      "epoch: 16857 loss is tensor([-0.7150], grad_fn=<AddBackward0>)\n",
      "epoch: 16858 loss is tensor([-0.7166], grad_fn=<AddBackward0>)\n",
      "epoch: 16859 loss is tensor([-0.7153], grad_fn=<AddBackward0>)\n",
      "epoch: 16860 loss is tensor([-0.6473], grad_fn=<AddBackward0>)\n",
      "epoch: 16861 loss is tensor([-0.6781], grad_fn=<AddBackward0>)\n",
      "epoch: 16862 loss is tensor([-0.7210], grad_fn=<AddBackward0>)\n",
      "epoch: 16863 loss is tensor([-0.6369], grad_fn=<AddBackward0>)\n",
      "epoch: 16864 loss is tensor([-0.6864], grad_fn=<AddBackward0>)\n",
      "epoch: 16865 loss is tensor([-0.6623], grad_fn=<AddBackward0>)\n",
      "epoch: 16866 loss is tensor([-0.6996], grad_fn=<AddBackward0>)\n",
      "epoch: 16867 loss is tensor([-0.6359], grad_fn=<AddBackward0>)\n",
      "epoch: 16868 loss is tensor([-0.6715], grad_fn=<AddBackward0>)\n",
      "epoch: 16869 loss is tensor([-0.6431], grad_fn=<AddBackward0>)\n",
      "epoch: 16870 loss is tensor([-0.6390], grad_fn=<AddBackward0>)\n",
      "epoch: 16871 loss is tensor([-0.7230], grad_fn=<AddBackward0>)\n",
      "epoch: 16872 loss is tensor([-0.6456], grad_fn=<AddBackward0>)\n",
      "epoch: 16873 loss is tensor([-0.6844], grad_fn=<AddBackward0>)\n",
      "epoch: 16874 loss is tensor([-0.7026], grad_fn=<AddBackward0>)\n",
      "epoch: 16875 loss is tensor([-0.6798], grad_fn=<AddBackward0>)\n",
      "epoch: 16876 loss is tensor([-0.6937], grad_fn=<AddBackward0>)\n",
      "epoch: 16877 loss is tensor([-0.6664], grad_fn=<AddBackward0>)\n",
      "epoch: 16878 loss is tensor([-0.6840], grad_fn=<AddBackward0>)\n",
      "epoch: 16879 loss is tensor([-0.6332], grad_fn=<AddBackward0>)\n",
      "epoch: 16880 loss is tensor([-0.6225], grad_fn=<AddBackward0>)\n",
      "epoch: 16881 loss is tensor([-0.6527], grad_fn=<AddBackward0>)\n",
      "epoch: 16882 loss is tensor([-0.6671], grad_fn=<AddBackward0>)\n",
      "epoch: 16883 loss is tensor([-0.7112], grad_fn=<AddBackward0>)\n",
      "epoch: 16884 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 16885 loss is tensor([-0.6741], grad_fn=<AddBackward0>)\n",
      "epoch: 16886 loss is tensor([-0.6565], grad_fn=<AddBackward0>)\n",
      "epoch: 16887 loss is tensor([-0.6571], grad_fn=<AddBackward0>)\n",
      "epoch: 16888 loss is tensor([-0.6547], grad_fn=<AddBackward0>)\n",
      "epoch: 16889 loss is tensor([-0.7307], grad_fn=<AddBackward0>)\n",
      "epoch: 16890 loss is tensor([-0.6489], grad_fn=<AddBackward0>)\n",
      "epoch: 16891 loss is tensor([-0.7009], grad_fn=<AddBackward0>)\n",
      "epoch: 16892 loss is tensor([-0.6848], grad_fn=<AddBackward0>)\n",
      "epoch: 16893 loss is tensor([-0.7067], grad_fn=<AddBackward0>)\n",
      "epoch: 16894 loss is tensor([-0.6777], grad_fn=<AddBackward0>)\n",
      "epoch: 16895 loss is tensor([-0.6699], grad_fn=<AddBackward0>)\n",
      "epoch: 16896 loss is tensor([-0.7067], grad_fn=<AddBackward0>)\n",
      "epoch: 16897 loss is tensor([-0.6845], grad_fn=<AddBackward0>)\n",
      "epoch: 16898 loss is tensor([-0.6268], grad_fn=<AddBackward0>)\n",
      "epoch: 16899 loss is tensor([-0.7117], grad_fn=<AddBackward0>)\n",
      "epoch: 16900 loss is tensor([-0.6852], grad_fn=<AddBackward0>)\n",
      "54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 16901 loss is tensor([-0.6403], grad_fn=<AddBackward0>)\n",
      "epoch: 16902 loss is tensor([-0.6676], grad_fn=<AddBackward0>)\n",
      "epoch: 16903 loss is tensor([-0.6500], grad_fn=<AddBackward0>)\n",
      "epoch: 16904 loss is tensor([-0.6441], grad_fn=<AddBackward0>)\n",
      "epoch: 16905 loss is tensor([-0.6627], grad_fn=<AddBackward0>)\n",
      "epoch: 16906 loss is tensor([-0.6726], grad_fn=<AddBackward0>)\n",
      "epoch: 16907 loss is tensor([-0.6474], grad_fn=<AddBackward0>)\n",
      "epoch: 16908 loss is tensor([-0.6294], grad_fn=<AddBackward0>)\n",
      "epoch: 16909 loss is tensor([-0.6634], grad_fn=<AddBackward0>)\n",
      "epoch: 16910 loss is tensor([-0.6024], grad_fn=<AddBackward0>)\n",
      "epoch: 16911 loss is tensor([-0.6690], grad_fn=<AddBackward0>)\n",
      "epoch: 16912 loss is tensor([-0.6510], grad_fn=<AddBackward0>)\n",
      "epoch: 16913 loss is tensor([-0.6535], grad_fn=<AddBackward0>)\n",
      "epoch: 16914 loss is tensor([-0.6583], grad_fn=<AddBackward0>)\n",
      "epoch: 16915 loss is tensor([-0.6357], grad_fn=<AddBackward0>)\n",
      "epoch: 16916 loss is tensor([-0.6482], grad_fn=<AddBackward0>)\n",
      "epoch: 16917 loss is tensor([-0.6973], grad_fn=<AddBackward0>)\n",
      "epoch: 16918 loss is tensor([-0.6976], grad_fn=<AddBackward0>)\n",
      "epoch: 16919 loss is tensor([-0.6391], grad_fn=<AddBackward0>)\n",
      "epoch: 16920 loss is tensor([-0.6501], grad_fn=<AddBackward0>)\n",
      "epoch: 16921 loss is tensor([-0.6441], grad_fn=<AddBackward0>)\n",
      "epoch: 16922 loss is tensor([-0.6869], grad_fn=<AddBackward0>)\n",
      "epoch: 16923 loss is tensor([-0.7148], grad_fn=<AddBackward0>)\n",
      "epoch: 16924 loss is tensor([-0.6518], grad_fn=<AddBackward0>)\n",
      "epoch: 16925 loss is tensor([-0.6679], grad_fn=<AddBackward0>)\n",
      "epoch: 16926 loss is tensor([-0.7049], grad_fn=<AddBackward0>)\n",
      "epoch: 16927 loss is tensor([-0.7249], grad_fn=<AddBackward0>)\n",
      "epoch: 16928 loss is tensor([-0.6849], grad_fn=<AddBackward0>)\n",
      "epoch: 16929 loss is tensor([-0.7091], grad_fn=<AddBackward0>)\n",
      "epoch: 16930 loss is tensor([-0.6687], grad_fn=<AddBackward0>)\n",
      "epoch: 16931 loss is tensor([-0.6899], grad_fn=<AddBackward0>)\n",
      "epoch: 16932 loss is tensor([-0.7587], grad_fn=<AddBackward0>)\n",
      "epoch: 16933 loss is tensor([-0.6980], grad_fn=<AddBackward0>)\n",
      "epoch: 16934 loss is tensor([-0.7131], grad_fn=<AddBackward0>)\n",
      "epoch: 16935 loss is tensor([-0.6799], grad_fn=<AddBackward0>)\n",
      "epoch: 16936 loss is tensor([-0.6612], grad_fn=<AddBackward0>)\n",
      "epoch: 16937 loss is tensor([-0.6733], grad_fn=<AddBackward0>)\n",
      "epoch: 16938 loss is tensor([-0.6599], grad_fn=<AddBackward0>)\n",
      "epoch: 16939 loss is tensor([-0.7107], grad_fn=<AddBackward0>)\n",
      "epoch: 16940 loss is tensor([-0.6577], grad_fn=<AddBackward0>)\n",
      "epoch: 16941 loss is tensor([-0.6709], grad_fn=<AddBackward0>)\n",
      "epoch: 16942 loss is tensor([-0.6518], grad_fn=<AddBackward0>)\n",
      "epoch: 16943 loss is tensor([-0.6552], grad_fn=<AddBackward0>)\n",
      "epoch: 16944 loss is tensor([-0.6380], grad_fn=<AddBackward0>)\n",
      "epoch: 16945 loss is tensor([-0.6563], grad_fn=<AddBackward0>)\n",
      "epoch: 16946 loss is tensor([-0.6647], grad_fn=<AddBackward0>)\n",
      "epoch: 16947 loss is tensor([-0.6471], grad_fn=<AddBackward0>)\n",
      "epoch: 16948 loss is tensor([-0.6541], grad_fn=<AddBackward0>)\n",
      "epoch: 16949 loss is tensor([-0.7125], grad_fn=<AddBackward0>)\n",
      "epoch: 16950 loss is tensor([-0.6274], grad_fn=<AddBackward0>)\n",
      "epoch: 16951 loss is tensor([-0.6607], grad_fn=<AddBackward0>)\n",
      "epoch: 16952 loss is tensor([-0.7091], grad_fn=<AddBackward0>)\n",
      "epoch: 16953 loss is tensor([-0.6969], grad_fn=<AddBackward0>)\n",
      "epoch: 16954 loss is tensor([-0.7339], grad_fn=<AddBackward0>)\n",
      "epoch: 16955 loss is tensor([-0.6624], grad_fn=<AddBackward0>)\n",
      "epoch: 16956 loss is tensor([-0.6818], grad_fn=<AddBackward0>)\n",
      "epoch: 16957 loss is tensor([-0.6489], grad_fn=<AddBackward0>)\n",
      "epoch: 16958 loss is tensor([-0.6886], grad_fn=<AddBackward0>)\n",
      "epoch: 16959 loss is tensor([-0.7507], grad_fn=<AddBackward0>)\n",
      "epoch: 16960 loss is tensor([-0.6514], grad_fn=<AddBackward0>)\n",
      "epoch: 16961 loss is tensor([-0.6444], grad_fn=<AddBackward0>)\n",
      "epoch: 16962 loss is tensor([-0.6493], grad_fn=<AddBackward0>)\n",
      "epoch: 16963 loss is tensor([-0.6563], grad_fn=<AddBackward0>)\n",
      "epoch: 16964 loss is tensor([-0.5853], grad_fn=<AddBackward0>)\n",
      "epoch: 16965 loss is tensor([-0.6504], grad_fn=<AddBackward0>)\n",
      "epoch: 16966 loss is tensor([-0.5983], grad_fn=<AddBackward0>)\n",
      "epoch: 16967 loss is tensor([-0.6726], grad_fn=<AddBackward0>)\n",
      "epoch: 16968 loss is tensor([-0.6839], grad_fn=<AddBackward0>)\n",
      "epoch: 16969 loss is tensor([-0.6327], grad_fn=<AddBackward0>)\n",
      "epoch: 16970 loss is tensor([-0.6802], grad_fn=<AddBackward0>)\n",
      "epoch: 16971 loss is tensor([-0.6505], grad_fn=<AddBackward0>)\n",
      "epoch: 16972 loss is tensor([-0.6837], grad_fn=<AddBackward0>)\n",
      "epoch: 16973 loss is tensor([-0.6789], grad_fn=<AddBackward0>)\n",
      "epoch: 16974 loss is tensor([-0.6730], grad_fn=<AddBackward0>)\n",
      "epoch: 16975 loss is tensor([-0.6683], grad_fn=<AddBackward0>)\n",
      "epoch: 16976 loss is tensor([-0.6506], grad_fn=<AddBackward0>)\n",
      "epoch: 16977 loss is tensor([-0.6791], grad_fn=<AddBackward0>)\n",
      "epoch: 16978 loss is tensor([-0.6852], grad_fn=<AddBackward0>)\n",
      "epoch: 16979 loss is tensor([-0.6297], grad_fn=<AddBackward0>)\n",
      "epoch: 16980 loss is tensor([-0.7194], grad_fn=<AddBackward0>)\n",
      "epoch: 16981 loss is tensor([-0.6351], grad_fn=<AddBackward0>)\n",
      "epoch: 16982 loss is tensor([-0.6188], grad_fn=<AddBackward0>)\n",
      "epoch: 16983 loss is tensor([-0.6163], grad_fn=<AddBackward0>)\n",
      "epoch: 16984 loss is tensor([-0.6454], grad_fn=<AddBackward0>)\n",
      "epoch: 16985 loss is tensor([-0.6267], grad_fn=<AddBackward0>)\n",
      "epoch: 16986 loss is tensor([-0.6297], grad_fn=<AddBackward0>)\n",
      "epoch: 16987 loss is tensor([-0.6907], grad_fn=<AddBackward0>)\n",
      "epoch: 16988 loss is tensor([-0.6497], grad_fn=<AddBackward0>)\n",
      "epoch: 16989 loss is tensor([-0.5966], grad_fn=<AddBackward0>)\n",
      "epoch: 16990 loss is tensor([-0.6720], grad_fn=<AddBackward0>)\n",
      "epoch: 16991 loss is tensor([-0.6577], grad_fn=<AddBackward0>)\n",
      "epoch: 16992 loss is tensor([-0.6435], grad_fn=<AddBackward0>)\n",
      "epoch: 16993 loss is tensor([-0.6463], grad_fn=<AddBackward0>)\n",
      "epoch: 16994 loss is tensor([-0.6940], grad_fn=<AddBackward0>)\n",
      "epoch: 16995 loss is tensor([-0.6229], grad_fn=<AddBackward0>)\n",
      "epoch: 16996 loss is tensor([-0.6374], grad_fn=<AddBackward0>)\n",
      "epoch: 16997 loss is tensor([-0.6652], grad_fn=<AddBackward0>)\n",
      "epoch: 16998 loss is tensor([-0.6089], grad_fn=<AddBackward0>)\n",
      "epoch: 16999 loss is tensor([-0.6205], grad_fn=<AddBackward0>)\n",
      "epoch: 17000 loss is tensor([-0.6169], grad_fn=<AddBackward0>)\n",
      "21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17001 loss is tensor([-0.6200], grad_fn=<AddBackward0>)\n",
      "epoch: 17002 loss is tensor([-0.6744], grad_fn=<AddBackward0>)\n",
      "epoch: 17003 loss is tensor([-0.6470], grad_fn=<AddBackward0>)\n",
      "epoch: 17004 loss is tensor([-0.7015], grad_fn=<AddBackward0>)\n",
      "epoch: 17005 loss is tensor([-0.6851], grad_fn=<AddBackward0>)\n",
      "epoch: 17006 loss is tensor([-0.6080], grad_fn=<AddBackward0>)\n",
      "epoch: 17007 loss is tensor([-0.6484], grad_fn=<AddBackward0>)\n",
      "epoch: 17008 loss is tensor([-0.6177], grad_fn=<AddBackward0>)\n",
      "epoch: 17009 loss is tensor([-0.6326], grad_fn=<AddBackward0>)\n",
      "epoch: 17010 loss is tensor([-0.6595], grad_fn=<AddBackward0>)\n",
      "epoch: 17011 loss is tensor([-0.6536], grad_fn=<AddBackward0>)\n",
      "epoch: 17012 loss is tensor([-0.6679], grad_fn=<AddBackward0>)\n",
      "epoch: 17013 loss is tensor([-0.6264], grad_fn=<AddBackward0>)\n",
      "epoch: 17014 loss is tensor([-0.7052], grad_fn=<AddBackward0>)\n",
      "epoch: 17015 loss is tensor([-0.6818], grad_fn=<AddBackward0>)\n",
      "epoch: 17016 loss is tensor([-0.6135], grad_fn=<AddBackward0>)\n",
      "epoch: 17017 loss is tensor([-0.6708], grad_fn=<AddBackward0>)\n",
      "epoch: 17018 loss is tensor([-0.6744], grad_fn=<AddBackward0>)\n",
      "epoch: 17019 loss is tensor([-0.7138], grad_fn=<AddBackward0>)\n",
      "epoch: 17020 loss is tensor([-0.6593], grad_fn=<AddBackward0>)\n",
      "epoch: 17021 loss is tensor([-0.6253], grad_fn=<AddBackward0>)\n",
      "epoch: 17022 loss is tensor([-0.6901], grad_fn=<AddBackward0>)\n",
      "epoch: 17023 loss is tensor([-0.6504], grad_fn=<AddBackward0>)\n",
      "epoch: 17024 loss is tensor([-0.6695], grad_fn=<AddBackward0>)\n",
      "epoch: 17025 loss is tensor([-0.6758], grad_fn=<AddBackward0>)\n",
      "epoch: 17026 loss is tensor([-0.6901], grad_fn=<AddBackward0>)\n",
      "epoch: 17027 loss is tensor([-0.6297], grad_fn=<AddBackward0>)\n",
      "epoch: 17028 loss is tensor([-0.6344], grad_fn=<AddBackward0>)\n",
      "epoch: 17029 loss is tensor([-0.6561], grad_fn=<AddBackward0>)\n",
      "epoch: 17030 loss is tensor([-0.6452], grad_fn=<AddBackward0>)\n",
      "epoch: 17031 loss is tensor([-0.6455], grad_fn=<AddBackward0>)\n",
      "epoch: 17032 loss is tensor([-0.6655], grad_fn=<AddBackward0>)\n",
      "epoch: 17033 loss is tensor([-0.6679], grad_fn=<AddBackward0>)\n",
      "epoch: 17034 loss is tensor([-0.6886], grad_fn=<AddBackward0>)\n",
      "epoch: 17035 loss is tensor([-0.6992], grad_fn=<AddBackward0>)\n",
      "epoch: 17036 loss is tensor([-0.6776], grad_fn=<AddBackward0>)\n",
      "epoch: 17037 loss is tensor([-0.6820], grad_fn=<AddBackward0>)\n",
      "epoch: 17038 loss is tensor([-0.6673], grad_fn=<AddBackward0>)\n",
      "epoch: 17039 loss is tensor([-0.7419], grad_fn=<AddBackward0>)\n",
      "epoch: 17040 loss is tensor([-0.6293], grad_fn=<AddBackward0>)\n",
      "epoch: 17041 loss is tensor([-0.6083], grad_fn=<AddBackward0>)\n",
      "epoch: 17042 loss is tensor([-0.6705], grad_fn=<AddBackward0>)\n",
      "epoch: 17043 loss is tensor([-0.6791], grad_fn=<AddBackward0>)\n",
      "epoch: 17044 loss is tensor([-0.6085], grad_fn=<AddBackward0>)\n",
      "epoch: 17045 loss is tensor([-0.7046], grad_fn=<AddBackward0>)\n",
      "epoch: 17046 loss is tensor([-0.6557], grad_fn=<AddBackward0>)\n",
      "epoch: 17047 loss is tensor([-0.6027], grad_fn=<AddBackward0>)\n",
      "epoch: 17048 loss is tensor([-0.6482], grad_fn=<AddBackward0>)\n",
      "epoch: 17049 loss is tensor([-0.6717], grad_fn=<AddBackward0>)\n",
      "epoch: 17050 loss is tensor([-0.7232], grad_fn=<AddBackward0>)\n",
      "epoch: 17051 loss is tensor([-0.7035], grad_fn=<AddBackward0>)\n",
      "epoch: 17052 loss is tensor([-0.6721], grad_fn=<AddBackward0>)\n",
      "epoch: 17053 loss is tensor([-0.7173], grad_fn=<AddBackward0>)\n",
      "epoch: 17054 loss is tensor([-0.6724], grad_fn=<AddBackward0>)\n",
      "epoch: 17055 loss is tensor([-0.6848], grad_fn=<AddBackward0>)\n",
      "epoch: 17056 loss is tensor([-0.7204], grad_fn=<AddBackward0>)\n",
      "epoch: 17057 loss is tensor([-0.6796], grad_fn=<AddBackward0>)\n",
      "epoch: 17058 loss is tensor([-0.7240], grad_fn=<AddBackward0>)\n",
      "epoch: 17059 loss is tensor([-0.6763], grad_fn=<AddBackward0>)\n",
      "epoch: 17060 loss is tensor([-0.6846], grad_fn=<AddBackward0>)\n",
      "epoch: 17061 loss is tensor([-0.6399], grad_fn=<AddBackward0>)\n",
      "epoch: 17062 loss is tensor([-0.6471], grad_fn=<AddBackward0>)\n",
      "epoch: 17063 loss is tensor([-0.6395], grad_fn=<AddBackward0>)\n",
      "epoch: 17064 loss is tensor([-0.7358], grad_fn=<AddBackward0>)\n",
      "epoch: 17065 loss is tensor([-0.7330], grad_fn=<AddBackward0>)\n",
      "epoch: 17066 loss is tensor([-0.6350], grad_fn=<AddBackward0>)\n",
      "epoch: 17067 loss is tensor([-0.7188], grad_fn=<AddBackward0>)\n",
      "epoch: 17068 loss is tensor([-0.6872], grad_fn=<AddBackward0>)\n",
      "epoch: 17069 loss is tensor([-0.6719], grad_fn=<AddBackward0>)\n",
      "epoch: 17070 loss is tensor([-0.7543], grad_fn=<AddBackward0>)\n",
      "epoch: 17071 loss is tensor([-0.7003], grad_fn=<AddBackward0>)\n",
      "epoch: 17072 loss is tensor([-0.6347], grad_fn=<AddBackward0>)\n",
      "epoch: 17073 loss is tensor([-0.6871], grad_fn=<AddBackward0>)\n",
      "epoch: 17074 loss is tensor([-0.7247], grad_fn=<AddBackward0>)\n",
      "epoch: 17075 loss is tensor([-0.6639], grad_fn=<AddBackward0>)\n",
      "epoch: 17076 loss is tensor([-0.6634], grad_fn=<AddBackward0>)\n",
      "epoch: 17077 loss is tensor([-0.7106], grad_fn=<AddBackward0>)\n",
      "epoch: 17078 loss is tensor([-0.6606], grad_fn=<AddBackward0>)\n",
      "epoch: 17079 loss is tensor([-0.6540], grad_fn=<AddBackward0>)\n",
      "epoch: 17080 loss is tensor([-0.6585], grad_fn=<AddBackward0>)\n",
      "epoch: 17081 loss is tensor([-0.7034], grad_fn=<AddBackward0>)\n",
      "epoch: 17082 loss is tensor([-0.6872], grad_fn=<AddBackward0>)\n",
      "epoch: 17083 loss is tensor([-0.6721], grad_fn=<AddBackward0>)\n",
      "epoch: 17084 loss is tensor([-0.7216], grad_fn=<AddBackward0>)\n",
      "epoch: 17085 loss is tensor([-0.6684], grad_fn=<AddBackward0>)\n",
      "epoch: 17086 loss is tensor([-0.7107], grad_fn=<AddBackward0>)\n",
      "epoch: 17087 loss is tensor([-0.7181], grad_fn=<AddBackward0>)\n",
      "epoch: 17088 loss is tensor([-0.7130], grad_fn=<AddBackward0>)\n",
      "epoch: 17089 loss is tensor([-0.6362], grad_fn=<AddBackward0>)\n",
      "epoch: 17090 loss is tensor([-0.6834], grad_fn=<AddBackward0>)\n",
      "epoch: 17091 loss is tensor([-0.6609], grad_fn=<AddBackward0>)\n",
      "epoch: 17092 loss is tensor([-0.6596], grad_fn=<AddBackward0>)\n",
      "epoch: 17093 loss is tensor([-0.5940], grad_fn=<AddBackward0>)\n",
      "epoch: 17094 loss is tensor([-0.6586], grad_fn=<AddBackward0>)\n",
      "epoch: 17095 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 17096 loss is tensor([-0.6320], grad_fn=<AddBackward0>)\n",
      "epoch: 17097 loss is tensor([-0.5867], grad_fn=<AddBackward0>)\n",
      "epoch: 17098 loss is tensor([-0.6651], grad_fn=<AddBackward0>)\n",
      "epoch: 17099 loss is tensor([-0.6612], grad_fn=<AddBackward0>)\n",
      "epoch: 17100 loss is tensor([-0.6589], grad_fn=<AddBackward0>)\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17101 loss is tensor([-0.6732], grad_fn=<AddBackward0>)\n",
      "epoch: 17102 loss is tensor([-0.6356], grad_fn=<AddBackward0>)\n",
      "epoch: 17103 loss is tensor([-0.6954], grad_fn=<AddBackward0>)\n",
      "epoch: 17104 loss is tensor([-0.6959], grad_fn=<AddBackward0>)\n",
      "epoch: 17105 loss is tensor([-0.7000], grad_fn=<AddBackward0>)\n",
      "epoch: 17106 loss is tensor([-0.6797], grad_fn=<AddBackward0>)\n",
      "epoch: 17107 loss is tensor([-0.7268], grad_fn=<AddBackward0>)\n",
      "epoch: 17108 loss is tensor([-0.6042], grad_fn=<AddBackward0>)\n",
      "epoch: 17109 loss is tensor([-0.6956], grad_fn=<AddBackward0>)\n",
      "epoch: 17110 loss is tensor([-0.7437], grad_fn=<AddBackward0>)\n",
      "epoch: 17111 loss is tensor([-0.6381], grad_fn=<AddBackward0>)\n",
      "epoch: 17112 loss is tensor([-0.6409], grad_fn=<AddBackward0>)\n",
      "epoch: 17113 loss is tensor([-0.6338], grad_fn=<AddBackward0>)\n",
      "epoch: 17114 loss is tensor([-0.6816], grad_fn=<AddBackward0>)\n",
      "epoch: 17115 loss is tensor([-0.6704], grad_fn=<AddBackward0>)\n",
      "epoch: 17116 loss is tensor([-0.7250], grad_fn=<AddBackward0>)\n",
      "epoch: 17117 loss is tensor([-0.6815], grad_fn=<AddBackward0>)\n",
      "epoch: 17118 loss is tensor([-0.6253], grad_fn=<AddBackward0>)\n",
      "epoch: 17119 loss is tensor([-0.7200], grad_fn=<AddBackward0>)\n",
      "epoch: 17120 loss is tensor([-0.7199], grad_fn=<AddBackward0>)\n",
      "epoch: 17121 loss is tensor([-0.6809], grad_fn=<AddBackward0>)\n",
      "epoch: 17122 loss is tensor([-0.6771], grad_fn=<AddBackward0>)\n",
      "epoch: 17123 loss is tensor([-0.6456], grad_fn=<AddBackward0>)\n",
      "epoch: 17124 loss is tensor([-0.6899], grad_fn=<AddBackward0>)\n",
      "epoch: 17125 loss is tensor([-0.6344], grad_fn=<AddBackward0>)\n",
      "epoch: 17126 loss is tensor([-0.6179], grad_fn=<AddBackward0>)\n",
      "epoch: 17127 loss is tensor([-0.5950], grad_fn=<AddBackward0>)\n",
      "epoch: 17128 loss is tensor([-0.6283], grad_fn=<AddBackward0>)\n",
      "epoch: 17129 loss is tensor([-0.6351], grad_fn=<AddBackward0>)\n",
      "epoch: 17130 loss is tensor([-0.6250], grad_fn=<AddBackward0>)\n",
      "epoch: 17131 loss is tensor([-0.6333], grad_fn=<AddBackward0>)\n",
      "epoch: 17132 loss is tensor([-0.5631], grad_fn=<AddBackward0>)\n",
      "epoch: 17133 loss is tensor([-0.5972], grad_fn=<AddBackward0>)\n",
      "epoch: 17134 loss is tensor([-0.5729], grad_fn=<AddBackward0>)\n",
      "epoch: 17135 loss is tensor([-0.3880], grad_fn=<AddBackward0>)\n",
      "epoch: 17136 loss is tensor([-0.4569], grad_fn=<AddBackward0>)\n",
      "epoch: 17137 loss is tensor([-0.5625], grad_fn=<AddBackward0>)\n",
      "epoch: 17138 loss is tensor([-0.5547], grad_fn=<AddBackward0>)\n",
      "epoch: 17139 loss is tensor([-0.5540], grad_fn=<AddBackward0>)\n",
      "epoch: 17140 loss is tensor([-0.6089], grad_fn=<AddBackward0>)\n",
      "epoch: 17141 loss is tensor([-0.6626], grad_fn=<AddBackward0>)\n",
      "epoch: 17142 loss is tensor([-0.6352], grad_fn=<AddBackward0>)\n",
      "epoch: 17143 loss is tensor([-0.6223], grad_fn=<AddBackward0>)\n",
      "epoch: 17144 loss is tensor([-0.6828], grad_fn=<AddBackward0>)\n",
      "epoch: 17145 loss is tensor([-0.5948], grad_fn=<AddBackward0>)\n",
      "epoch: 17146 loss is tensor([-0.5979], grad_fn=<AddBackward0>)\n",
      "epoch: 17147 loss is tensor([-0.6012], grad_fn=<AddBackward0>)\n",
      "epoch: 17148 loss is tensor([-0.6612], grad_fn=<AddBackward0>)\n",
      "epoch: 17149 loss is tensor([-0.6781], grad_fn=<AddBackward0>)\n",
      "epoch: 17150 loss is tensor([-0.7016], grad_fn=<AddBackward0>)\n",
      "epoch: 17151 loss is tensor([-0.5624], grad_fn=<AddBackward0>)\n",
      "epoch: 17152 loss is tensor([-0.6539], grad_fn=<AddBackward0>)\n",
      "epoch: 17153 loss is tensor([-0.6213], grad_fn=<AddBackward0>)\n",
      "epoch: 17154 loss is tensor([-0.6734], grad_fn=<AddBackward0>)\n",
      "epoch: 17155 loss is tensor([-0.6249], grad_fn=<AddBackward0>)\n",
      "epoch: 17156 loss is tensor([-0.6537], grad_fn=<AddBackward0>)\n",
      "epoch: 17157 loss is tensor([-0.5774], grad_fn=<AddBackward0>)\n",
      "epoch: 17158 loss is tensor([-0.6672], grad_fn=<AddBackward0>)\n",
      "epoch: 17159 loss is tensor([-0.6231], grad_fn=<AddBackward0>)\n",
      "epoch: 17160 loss is tensor([-0.6790], grad_fn=<AddBackward0>)\n",
      "epoch: 17161 loss is tensor([-0.6455], grad_fn=<AddBackward0>)\n",
      "epoch: 17162 loss is tensor([-0.6368], grad_fn=<AddBackward0>)\n",
      "epoch: 17163 loss is tensor([-0.6896], grad_fn=<AddBackward0>)\n",
      "epoch: 17164 loss is tensor([-0.6644], grad_fn=<AddBackward0>)\n",
      "epoch: 17165 loss is tensor([-0.6791], grad_fn=<AddBackward0>)\n",
      "epoch: 17166 loss is tensor([-0.6806], grad_fn=<AddBackward0>)\n",
      "epoch: 17167 loss is tensor([-0.6886], grad_fn=<AddBackward0>)\n",
      "epoch: 17168 loss is tensor([-0.6331], grad_fn=<AddBackward0>)\n",
      "epoch: 17169 loss is tensor([-0.6820], grad_fn=<AddBackward0>)\n",
      "epoch: 17170 loss is tensor([-0.5914], grad_fn=<AddBackward0>)\n",
      "epoch: 17171 loss is tensor([-0.6349], grad_fn=<AddBackward0>)\n",
      "epoch: 17172 loss is tensor([-0.6079], grad_fn=<AddBackward0>)\n",
      "epoch: 17173 loss is tensor([-0.6524], grad_fn=<AddBackward0>)\n",
      "epoch: 17174 loss is tensor([-0.6051], grad_fn=<AddBackward0>)\n",
      "epoch: 17175 loss is tensor([-0.6313], grad_fn=<AddBackward0>)\n",
      "epoch: 17176 loss is tensor([-0.6309], grad_fn=<AddBackward0>)\n",
      "epoch: 17177 loss is tensor([-0.6556], grad_fn=<AddBackward0>)\n",
      "epoch: 17178 loss is tensor([-0.6521], grad_fn=<AddBackward0>)\n",
      "epoch: 17179 loss is tensor([-0.5921], grad_fn=<AddBackward0>)\n",
      "epoch: 17180 loss is tensor([-0.6414], grad_fn=<AddBackward0>)\n",
      "epoch: 17181 loss is tensor([-0.6307], grad_fn=<AddBackward0>)\n",
      "epoch: 17182 loss is tensor([-0.6215], grad_fn=<AddBackward0>)\n",
      "epoch: 17183 loss is tensor([-0.6607], grad_fn=<AddBackward0>)\n",
      "epoch: 17184 loss is tensor([-0.6317], grad_fn=<AddBackward0>)\n",
      "epoch: 17185 loss is tensor([-0.7080], grad_fn=<AddBackward0>)\n",
      "epoch: 17186 loss is tensor([-0.6616], grad_fn=<AddBackward0>)\n",
      "epoch: 17187 loss is tensor([-0.6589], grad_fn=<AddBackward0>)\n",
      "epoch: 17188 loss is tensor([-0.6542], grad_fn=<AddBackward0>)\n",
      "epoch: 17189 loss is tensor([-0.6731], grad_fn=<AddBackward0>)\n",
      "epoch: 17190 loss is tensor([-0.6857], grad_fn=<AddBackward0>)\n",
      "epoch: 17191 loss is tensor([-0.6476], grad_fn=<AddBackward0>)\n",
      "epoch: 17192 loss is tensor([-0.6923], grad_fn=<AddBackward0>)\n",
      "epoch: 17193 loss is tensor([-0.6709], grad_fn=<AddBackward0>)\n",
      "epoch: 17194 loss is tensor([-0.7584], grad_fn=<AddBackward0>)\n",
      "epoch: 17195 loss is tensor([-0.6779], grad_fn=<AddBackward0>)\n",
      "epoch: 17196 loss is tensor([-0.6513], grad_fn=<AddBackward0>)\n",
      "epoch: 17197 loss is tensor([-0.7359], grad_fn=<AddBackward0>)\n",
      "epoch: 17198 loss is tensor([-0.6596], grad_fn=<AddBackward0>)\n",
      "epoch: 17199 loss is tensor([-0.6545], grad_fn=<AddBackward0>)\n",
      "epoch: 17200 loss is tensor([-0.6764], grad_fn=<AddBackward0>)\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17201 loss is tensor([-0.6395], grad_fn=<AddBackward0>)\n",
      "epoch: 17202 loss is tensor([-0.7025], grad_fn=<AddBackward0>)\n",
      "epoch: 17203 loss is tensor([-0.6922], grad_fn=<AddBackward0>)\n",
      "epoch: 17204 loss is tensor([-0.7033], grad_fn=<AddBackward0>)\n",
      "epoch: 17205 loss is tensor([-0.7128], grad_fn=<AddBackward0>)\n",
      "epoch: 17206 loss is tensor([-0.6160], grad_fn=<AddBackward0>)\n",
      "epoch: 17207 loss is tensor([-0.7065], grad_fn=<AddBackward0>)\n",
      "epoch: 17208 loss is tensor([-0.7028], grad_fn=<AddBackward0>)\n",
      "epoch: 17209 loss is tensor([-0.6366], grad_fn=<AddBackward0>)\n",
      "epoch: 17210 loss is tensor([-0.6942], grad_fn=<AddBackward0>)\n",
      "epoch: 17211 loss is tensor([-0.6714], grad_fn=<AddBackward0>)\n",
      "epoch: 17212 loss is tensor([-0.7070], grad_fn=<AddBackward0>)\n",
      "epoch: 17213 loss is tensor([-0.6820], grad_fn=<AddBackward0>)\n",
      "epoch: 17214 loss is tensor([-0.6380], grad_fn=<AddBackward0>)\n",
      "epoch: 17215 loss is tensor([-0.6545], grad_fn=<AddBackward0>)\n",
      "epoch: 17216 loss is tensor([-0.6900], grad_fn=<AddBackward0>)\n",
      "epoch: 17217 loss is tensor([-0.6879], grad_fn=<AddBackward0>)\n",
      "epoch: 17218 loss is tensor([-0.6691], grad_fn=<AddBackward0>)\n",
      "epoch: 17219 loss is tensor([-0.5902], grad_fn=<AddBackward0>)\n",
      "epoch: 17220 loss is tensor([-0.6877], grad_fn=<AddBackward0>)\n",
      "epoch: 17221 loss is tensor([-0.7246], grad_fn=<AddBackward0>)\n",
      "epoch: 17222 loss is tensor([-0.6916], grad_fn=<AddBackward0>)\n",
      "epoch: 17223 loss is tensor([-0.6743], grad_fn=<AddBackward0>)\n",
      "epoch: 17224 loss is tensor([-0.6317], grad_fn=<AddBackward0>)\n",
      "epoch: 17225 loss is tensor([-0.6426], grad_fn=<AddBackward0>)\n",
      "epoch: 17226 loss is tensor([-0.6903], grad_fn=<AddBackward0>)\n",
      "epoch: 17227 loss is tensor([-0.7104], grad_fn=<AddBackward0>)\n",
      "epoch: 17228 loss is tensor([-0.6657], grad_fn=<AddBackward0>)\n",
      "epoch: 17229 loss is tensor([-0.6248], grad_fn=<AddBackward0>)\n",
      "epoch: 17230 loss is tensor([-0.6230], grad_fn=<AddBackward0>)\n",
      "epoch: 17231 loss is tensor([-0.6974], grad_fn=<AddBackward0>)\n",
      "epoch: 17232 loss is tensor([-0.6751], grad_fn=<AddBackward0>)\n",
      "epoch: 17233 loss is tensor([-0.6738], grad_fn=<AddBackward0>)\n",
      "epoch: 17234 loss is tensor([-0.6467], grad_fn=<AddBackward0>)\n",
      "epoch: 17235 loss is tensor([-0.6770], grad_fn=<AddBackward0>)\n",
      "epoch: 17236 loss is tensor([-0.6198], grad_fn=<AddBackward0>)\n",
      "epoch: 17237 loss is tensor([-0.6551], grad_fn=<AddBackward0>)\n",
      "epoch: 17238 loss is tensor([-0.6344], grad_fn=<AddBackward0>)\n",
      "epoch: 17239 loss is tensor([-0.6654], grad_fn=<AddBackward0>)\n",
      "epoch: 17240 loss is tensor([-0.7044], grad_fn=<AddBackward0>)\n",
      "epoch: 17241 loss is tensor([-0.7002], grad_fn=<AddBackward0>)\n",
      "epoch: 17242 loss is tensor([-0.6144], grad_fn=<AddBackward0>)\n",
      "epoch: 17243 loss is tensor([-0.6522], grad_fn=<AddBackward0>)\n",
      "epoch: 17244 loss is tensor([-0.7130], grad_fn=<AddBackward0>)\n",
      "epoch: 17245 loss is tensor([-0.6951], grad_fn=<AddBackward0>)\n",
      "epoch: 17246 loss is tensor([-0.6333], grad_fn=<AddBackward0>)\n",
      "epoch: 17247 loss is tensor([-0.6390], grad_fn=<AddBackward0>)\n",
      "epoch: 17248 loss is tensor([-0.6560], grad_fn=<AddBackward0>)\n",
      "epoch: 17249 loss is tensor([-0.6376], grad_fn=<AddBackward0>)\n",
      "epoch: 17250 loss is tensor([-0.6209], grad_fn=<AddBackward0>)\n",
      "epoch: 17251 loss is tensor([-0.6458], grad_fn=<AddBackward0>)\n",
      "epoch: 17252 loss is tensor([-0.6734], grad_fn=<AddBackward0>)\n",
      "epoch: 17253 loss is tensor([-0.6651], grad_fn=<AddBackward0>)\n",
      "epoch: 17254 loss is tensor([-0.6985], grad_fn=<AddBackward0>)\n",
      "epoch: 17255 loss is tensor([-0.6149], grad_fn=<AddBackward0>)\n",
      "epoch: 17256 loss is tensor([-0.6417], grad_fn=<AddBackward0>)\n",
      "epoch: 17257 loss is tensor([-0.6634], grad_fn=<AddBackward0>)\n",
      "epoch: 17258 loss is tensor([-0.6918], grad_fn=<AddBackward0>)\n",
      "epoch: 17259 loss is tensor([-0.6265], grad_fn=<AddBackward0>)\n",
      "epoch: 17260 loss is tensor([-0.7010], grad_fn=<AddBackward0>)\n",
      "epoch: 17261 loss is tensor([-0.6369], grad_fn=<AddBackward0>)\n",
      "epoch: 17262 loss is tensor([-0.6195], grad_fn=<AddBackward0>)\n",
      "epoch: 17263 loss is tensor([-0.6626], grad_fn=<AddBackward0>)\n",
      "epoch: 17264 loss is tensor([-0.6648], grad_fn=<AddBackward0>)\n",
      "epoch: 17265 loss is tensor([-0.6442], grad_fn=<AddBackward0>)\n",
      "epoch: 17266 loss is tensor([-0.6481], grad_fn=<AddBackward0>)\n",
      "epoch: 17267 loss is tensor([-0.6521], grad_fn=<AddBackward0>)\n",
      "epoch: 17268 loss is tensor([-0.6918], grad_fn=<AddBackward0>)\n",
      "epoch: 17269 loss is tensor([-0.7099], grad_fn=<AddBackward0>)\n",
      "epoch: 17270 loss is tensor([-0.6485], grad_fn=<AddBackward0>)\n",
      "epoch: 17271 loss is tensor([-0.6631], grad_fn=<AddBackward0>)\n",
      "epoch: 17272 loss is tensor([-0.6743], grad_fn=<AddBackward0>)\n",
      "epoch: 17273 loss is tensor([-0.7176], grad_fn=<AddBackward0>)\n",
      "epoch: 17274 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 17275 loss is tensor([-0.7149], grad_fn=<AddBackward0>)\n",
      "epoch: 17276 loss is tensor([-0.7130], grad_fn=<AddBackward0>)\n",
      "epoch: 17277 loss is tensor([-0.7123], grad_fn=<AddBackward0>)\n",
      "epoch: 17278 loss is tensor([-0.6802], grad_fn=<AddBackward0>)\n",
      "epoch: 17279 loss is tensor([-0.6876], grad_fn=<AddBackward0>)\n",
      "epoch: 17280 loss is tensor([-0.7038], grad_fn=<AddBackward0>)\n",
      "epoch: 17281 loss is tensor([-0.6714], grad_fn=<AddBackward0>)\n",
      "epoch: 17282 loss is tensor([-0.6538], grad_fn=<AddBackward0>)\n",
      "epoch: 17283 loss is tensor([-0.7426], grad_fn=<AddBackward0>)\n",
      "epoch: 17284 loss is tensor([-0.6898], grad_fn=<AddBackward0>)\n",
      "epoch: 17285 loss is tensor([-0.6846], grad_fn=<AddBackward0>)\n",
      "epoch: 17286 loss is tensor([-0.7178], grad_fn=<AddBackward0>)\n",
      "epoch: 17287 loss is tensor([-0.7004], grad_fn=<AddBackward0>)\n",
      "epoch: 17288 loss is tensor([-0.6838], grad_fn=<AddBackward0>)\n",
      "epoch: 17289 loss is tensor([-0.7251], grad_fn=<AddBackward0>)\n",
      "epoch: 17290 loss is tensor([-0.6950], grad_fn=<AddBackward0>)\n",
      "epoch: 17291 loss is tensor([-0.6570], grad_fn=<AddBackward0>)\n",
      "epoch: 17292 loss is tensor([-0.7108], grad_fn=<AddBackward0>)\n",
      "epoch: 17293 loss is tensor([-0.7088], grad_fn=<AddBackward0>)\n",
      "epoch: 17294 loss is tensor([-0.7236], grad_fn=<AddBackward0>)\n",
      "epoch: 17295 loss is tensor([-0.7055], grad_fn=<AddBackward0>)\n",
      "epoch: 17296 loss is tensor([-0.7353], grad_fn=<AddBackward0>)\n",
      "epoch: 17297 loss is tensor([-0.6979], grad_fn=<AddBackward0>)\n",
      "epoch: 17298 loss is tensor([-0.6596], grad_fn=<AddBackward0>)\n",
      "epoch: 17299 loss is tensor([-0.7399], grad_fn=<AddBackward0>)\n",
      "epoch: 17300 loss is tensor([-0.7185], grad_fn=<AddBackward0>)\n",
      "22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17301 loss is tensor([-0.6808], grad_fn=<AddBackward0>)\n",
      "epoch: 17302 loss is tensor([-0.7215], grad_fn=<AddBackward0>)\n",
      "epoch: 17303 loss is tensor([-0.6834], grad_fn=<AddBackward0>)\n",
      "epoch: 17304 loss is tensor([-0.6991], grad_fn=<AddBackward0>)\n",
      "epoch: 17305 loss is tensor([-0.6522], grad_fn=<AddBackward0>)\n",
      "epoch: 17306 loss is tensor([-0.6795], grad_fn=<AddBackward0>)\n",
      "epoch: 17307 loss is tensor([-0.7028], grad_fn=<AddBackward0>)\n",
      "epoch: 17308 loss is tensor([-0.7200], grad_fn=<AddBackward0>)\n",
      "epoch: 17309 loss is tensor([-0.6996], grad_fn=<AddBackward0>)\n",
      "epoch: 17310 loss is tensor([-0.6193], grad_fn=<AddBackward0>)\n",
      "epoch: 17311 loss is tensor([-0.6368], grad_fn=<AddBackward0>)\n",
      "epoch: 17312 loss is tensor([-0.7118], grad_fn=<AddBackward0>)\n",
      "epoch: 17313 loss is tensor([-0.7033], grad_fn=<AddBackward0>)\n",
      "epoch: 17314 loss is tensor([-0.6697], grad_fn=<AddBackward0>)\n",
      "epoch: 17315 loss is tensor([-0.6686], grad_fn=<AddBackward0>)\n",
      "epoch: 17316 loss is tensor([-0.6994], grad_fn=<AddBackward0>)\n",
      "epoch: 17317 loss is tensor([-0.6816], grad_fn=<AddBackward0>)\n",
      "epoch: 17318 loss is tensor([-0.6535], grad_fn=<AddBackward0>)\n",
      "epoch: 17319 loss is tensor([-0.5899], grad_fn=<AddBackward0>)\n",
      "epoch: 17320 loss is tensor([-0.6417], grad_fn=<AddBackward0>)\n",
      "epoch: 17321 loss is tensor([-0.6417], grad_fn=<AddBackward0>)\n",
      "epoch: 17322 loss is tensor([-0.6450], grad_fn=<AddBackward0>)\n",
      "epoch: 17323 loss is tensor([-0.6674], grad_fn=<AddBackward0>)\n",
      "epoch: 17324 loss is tensor([-0.6524], grad_fn=<AddBackward0>)\n",
      "epoch: 17325 loss is tensor([-0.6827], grad_fn=<AddBackward0>)\n",
      "epoch: 17326 loss is tensor([-0.7077], grad_fn=<AddBackward0>)\n",
      "epoch: 17327 loss is tensor([-0.6475], grad_fn=<AddBackward0>)\n",
      "epoch: 17328 loss is tensor([-0.6671], grad_fn=<AddBackward0>)\n",
      "epoch: 17329 loss is tensor([-0.6683], grad_fn=<AddBackward0>)\n",
      "epoch: 17330 loss is tensor([-0.7178], grad_fn=<AddBackward0>)\n",
      "epoch: 17331 loss is tensor([-0.6992], grad_fn=<AddBackward0>)\n",
      "epoch: 17332 loss is tensor([-0.6991], grad_fn=<AddBackward0>)\n",
      "epoch: 17333 loss is tensor([-0.6279], grad_fn=<AddBackward0>)\n",
      "epoch: 17334 loss is tensor([-0.6575], grad_fn=<AddBackward0>)\n",
      "epoch: 17335 loss is tensor([-0.7035], grad_fn=<AddBackward0>)\n",
      "epoch: 17336 loss is tensor([-0.6786], grad_fn=<AddBackward0>)\n",
      "epoch: 17337 loss is tensor([-0.6183], grad_fn=<AddBackward0>)\n",
      "epoch: 17338 loss is tensor([-0.6808], grad_fn=<AddBackward0>)\n",
      "epoch: 17339 loss is tensor([-0.7718], grad_fn=<AddBackward0>)\n",
      "epoch: 17340 loss is tensor([-0.6669], grad_fn=<AddBackward0>)\n",
      "epoch: 17341 loss is tensor([-0.6885], grad_fn=<AddBackward0>)\n",
      "epoch: 17342 loss is tensor([-0.7294], grad_fn=<AddBackward0>)\n",
      "epoch: 17343 loss is tensor([-0.7231], grad_fn=<AddBackward0>)\n",
      "epoch: 17344 loss is tensor([-0.6062], grad_fn=<AddBackward0>)\n",
      "epoch: 17345 loss is tensor([-0.6572], grad_fn=<AddBackward0>)\n",
      "epoch: 17346 loss is tensor([-0.6994], grad_fn=<AddBackward0>)\n",
      "epoch: 17347 loss is tensor([-0.7036], grad_fn=<AddBackward0>)\n",
      "epoch: 17348 loss is tensor([-0.6991], grad_fn=<AddBackward0>)\n",
      "epoch: 17349 loss is tensor([-0.6949], grad_fn=<AddBackward0>)\n",
      "epoch: 17350 loss is tensor([-0.6121], grad_fn=<AddBackward0>)\n",
      "epoch: 17351 loss is tensor([-0.7181], grad_fn=<AddBackward0>)\n",
      "epoch: 17352 loss is tensor([-0.6772], grad_fn=<AddBackward0>)\n",
      "epoch: 17353 loss is tensor([-0.6817], grad_fn=<AddBackward0>)\n",
      "epoch: 17354 loss is tensor([-0.6530], grad_fn=<AddBackward0>)\n",
      "epoch: 17355 loss is tensor([-0.6569], grad_fn=<AddBackward0>)\n",
      "epoch: 17356 loss is tensor([-0.6651], grad_fn=<AddBackward0>)\n",
      "epoch: 17357 loss is tensor([-0.6721], grad_fn=<AddBackward0>)\n",
      "epoch: 17358 loss is tensor([-0.6676], grad_fn=<AddBackward0>)\n",
      "epoch: 17359 loss is tensor([-0.6857], grad_fn=<AddBackward0>)\n",
      "epoch: 17360 loss is tensor([-0.6560], grad_fn=<AddBackward0>)\n",
      "epoch: 17361 loss is tensor([-0.6317], grad_fn=<AddBackward0>)\n",
      "epoch: 17362 loss is tensor([-0.6232], grad_fn=<AddBackward0>)\n",
      "epoch: 17363 loss is tensor([-0.6521], grad_fn=<AddBackward0>)\n",
      "epoch: 17364 loss is tensor([-0.6386], grad_fn=<AddBackward0>)\n",
      "epoch: 17365 loss is tensor([-0.6714], grad_fn=<AddBackward0>)\n",
      "epoch: 17366 loss is tensor([-0.6508], grad_fn=<AddBackward0>)\n",
      "epoch: 17367 loss is tensor([-0.6946], grad_fn=<AddBackward0>)\n",
      "epoch: 17368 loss is tensor([-0.6849], grad_fn=<AddBackward0>)\n",
      "epoch: 17369 loss is tensor([-0.6483], grad_fn=<AddBackward0>)\n",
      "epoch: 17370 loss is tensor([-0.7270], grad_fn=<AddBackward0>)\n",
      "epoch: 17371 loss is tensor([-0.6840], grad_fn=<AddBackward0>)\n",
      "epoch: 17372 loss is tensor([-0.6562], grad_fn=<AddBackward0>)\n",
      "epoch: 17373 loss is tensor([-0.6531], grad_fn=<AddBackward0>)\n",
      "epoch: 17374 loss is tensor([-0.6295], grad_fn=<AddBackward0>)\n",
      "epoch: 17375 loss is tensor([-0.5682], grad_fn=<AddBackward0>)\n",
      "epoch: 17376 loss is tensor([-0.5909], grad_fn=<AddBackward0>)\n",
      "epoch: 17377 loss is tensor([-0.5440], grad_fn=<AddBackward0>)\n",
      "epoch: 17378 loss is tensor([-0.5035], grad_fn=<AddBackward0>)\n",
      "epoch: 17379 loss is tensor([-0.5678], grad_fn=<AddBackward0>)\n",
      "epoch: 17380 loss is tensor([-0.6310], grad_fn=<AddBackward0>)\n",
      "epoch: 17381 loss is tensor([-0.6346], grad_fn=<AddBackward0>)\n",
      "epoch: 17382 loss is tensor([-0.5801], grad_fn=<AddBackward0>)\n",
      "epoch: 17383 loss is tensor([-0.5821], grad_fn=<AddBackward0>)\n",
      "epoch: 17384 loss is tensor([-0.6614], grad_fn=<AddBackward0>)\n",
      "epoch: 17385 loss is tensor([-0.6364], grad_fn=<AddBackward0>)\n",
      "epoch: 17386 loss is tensor([-0.6405], grad_fn=<AddBackward0>)\n",
      "epoch: 17387 loss is tensor([-0.6253], grad_fn=<AddBackward0>)\n",
      "epoch: 17388 loss is tensor([-0.6275], grad_fn=<AddBackward0>)\n",
      "epoch: 17389 loss is tensor([-0.6775], grad_fn=<AddBackward0>)\n",
      "epoch: 17390 loss is tensor([-0.6739], grad_fn=<AddBackward0>)\n",
      "epoch: 17391 loss is tensor([-0.6921], grad_fn=<AddBackward0>)\n",
      "epoch: 17392 loss is tensor([-0.6123], grad_fn=<AddBackward0>)\n",
      "epoch: 17393 loss is tensor([-0.6500], grad_fn=<AddBackward0>)\n",
      "epoch: 17394 loss is tensor([-0.6654], grad_fn=<AddBackward0>)\n",
      "epoch: 17395 loss is tensor([-0.6766], grad_fn=<AddBackward0>)\n",
      "epoch: 17396 loss is tensor([-0.6482], grad_fn=<AddBackward0>)\n",
      "epoch: 17397 loss is tensor([-0.6866], grad_fn=<AddBackward0>)\n",
      "epoch: 17398 loss is tensor([-0.5647], grad_fn=<AddBackward0>)\n",
      "epoch: 17399 loss is tensor([-0.6357], grad_fn=<AddBackward0>)\n",
      "epoch: 17400 loss is tensor([-0.6879], grad_fn=<AddBackward0>)\n",
      "46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17401 loss is tensor([-0.6398], grad_fn=<AddBackward0>)\n",
      "epoch: 17402 loss is tensor([-0.6771], grad_fn=<AddBackward0>)\n",
      "epoch: 17403 loss is tensor([-0.6785], grad_fn=<AddBackward0>)\n",
      "epoch: 17404 loss is tensor([-0.6800], grad_fn=<AddBackward0>)\n",
      "epoch: 17405 loss is tensor([-0.6500], grad_fn=<AddBackward0>)\n",
      "epoch: 17406 loss is tensor([-0.6856], grad_fn=<AddBackward0>)\n",
      "epoch: 17407 loss is tensor([-0.6834], grad_fn=<AddBackward0>)\n",
      "epoch: 17408 loss is tensor([-0.6191], grad_fn=<AddBackward0>)\n",
      "epoch: 17409 loss is tensor([-0.6535], grad_fn=<AddBackward0>)\n",
      "epoch: 17410 loss is tensor([-0.7222], grad_fn=<AddBackward0>)\n",
      "epoch: 17411 loss is tensor([-0.6220], grad_fn=<AddBackward0>)\n",
      "epoch: 17412 loss is tensor([-0.6616], grad_fn=<AddBackward0>)\n",
      "epoch: 17413 loss is tensor([-0.7051], grad_fn=<AddBackward0>)\n",
      "epoch: 17414 loss is tensor([-0.6674], grad_fn=<AddBackward0>)\n",
      "epoch: 17415 loss is tensor([-0.6960], grad_fn=<AddBackward0>)\n",
      "epoch: 17416 loss is tensor([-0.6972], grad_fn=<AddBackward0>)\n",
      "epoch: 17417 loss is tensor([-0.6798], grad_fn=<AddBackward0>)\n",
      "epoch: 17418 loss is tensor([-0.6873], grad_fn=<AddBackward0>)\n",
      "epoch: 17419 loss is tensor([-0.6407], grad_fn=<AddBackward0>)\n",
      "epoch: 17420 loss is tensor([-0.6858], grad_fn=<AddBackward0>)\n",
      "epoch: 17421 loss is tensor([-0.7245], grad_fn=<AddBackward0>)\n",
      "epoch: 17422 loss is tensor([-0.6937], grad_fn=<AddBackward0>)\n",
      "epoch: 17423 loss is tensor([-0.6144], grad_fn=<AddBackward0>)\n",
      "epoch: 17424 loss is tensor([-0.6568], grad_fn=<AddBackward0>)\n",
      "epoch: 17425 loss is tensor([-0.6827], grad_fn=<AddBackward0>)\n",
      "epoch: 17426 loss is tensor([-0.6939], grad_fn=<AddBackward0>)\n",
      "epoch: 17427 loss is tensor([-0.6281], grad_fn=<AddBackward0>)\n",
      "epoch: 17428 loss is tensor([-0.7117], grad_fn=<AddBackward0>)\n",
      "epoch: 17429 loss is tensor([-0.6785], grad_fn=<AddBackward0>)\n",
      "epoch: 17430 loss is tensor([-0.6403], grad_fn=<AddBackward0>)\n",
      "epoch: 17431 loss is tensor([-0.6759], grad_fn=<AddBackward0>)\n",
      "epoch: 17432 loss is tensor([-0.6859], grad_fn=<AddBackward0>)\n",
      "epoch: 17433 loss is tensor([-0.6543], grad_fn=<AddBackward0>)\n",
      "epoch: 17434 loss is tensor([-0.6175], grad_fn=<AddBackward0>)\n",
      "epoch: 17435 loss is tensor([-0.6711], grad_fn=<AddBackward0>)\n",
      "epoch: 17436 loss is tensor([-0.6393], grad_fn=<AddBackward0>)\n",
      "epoch: 17437 loss is tensor([-0.6594], grad_fn=<AddBackward0>)\n",
      "epoch: 17438 loss is tensor([-0.6776], grad_fn=<AddBackward0>)\n",
      "epoch: 17439 loss is tensor([-0.6977], grad_fn=<AddBackward0>)\n",
      "epoch: 17440 loss is tensor([-0.6449], grad_fn=<AddBackward0>)\n",
      "epoch: 17441 loss is tensor([-0.6625], grad_fn=<AddBackward0>)\n",
      "epoch: 17442 loss is tensor([-0.6895], grad_fn=<AddBackward0>)\n",
      "epoch: 17443 loss is tensor([-0.6212], grad_fn=<AddBackward0>)\n",
      "epoch: 17444 loss is tensor([-0.6905], grad_fn=<AddBackward0>)\n",
      "epoch: 17445 loss is tensor([-0.6839], grad_fn=<AddBackward0>)\n",
      "epoch: 17446 loss is tensor([-0.6453], grad_fn=<AddBackward0>)\n",
      "epoch: 17447 loss is tensor([-0.6425], grad_fn=<AddBackward0>)\n",
      "epoch: 17448 loss is tensor([-0.6509], grad_fn=<AddBackward0>)\n",
      "epoch: 17449 loss is tensor([-0.6866], grad_fn=<AddBackward0>)\n",
      "epoch: 17450 loss is tensor([-0.6734], grad_fn=<AddBackward0>)\n",
      "epoch: 17451 loss is tensor([-0.6839], grad_fn=<AddBackward0>)\n",
      "epoch: 17452 loss is tensor([-0.6724], grad_fn=<AddBackward0>)\n",
      "epoch: 17453 loss is tensor([-0.6543], grad_fn=<AddBackward0>)\n",
      "epoch: 17454 loss is tensor([-0.6470], grad_fn=<AddBackward0>)\n",
      "epoch: 17455 loss is tensor([-0.6469], grad_fn=<AddBackward0>)\n",
      "epoch: 17456 loss is tensor([-0.6429], grad_fn=<AddBackward0>)\n",
      "epoch: 17457 loss is tensor([-0.6933], grad_fn=<AddBackward0>)\n",
      "epoch: 17458 loss is tensor([-0.6728], grad_fn=<AddBackward0>)\n",
      "epoch: 17459 loss is tensor([-0.6643], grad_fn=<AddBackward0>)\n",
      "epoch: 17460 loss is tensor([-0.7082], grad_fn=<AddBackward0>)\n",
      "epoch: 17461 loss is tensor([-0.6554], grad_fn=<AddBackward0>)\n",
      "epoch: 17462 loss is tensor([-0.6754], grad_fn=<AddBackward0>)\n",
      "epoch: 17463 loss is tensor([-0.6494], grad_fn=<AddBackward0>)\n",
      "epoch: 17464 loss is tensor([-0.6726], grad_fn=<AddBackward0>)\n",
      "epoch: 17465 loss is tensor([-0.6802], grad_fn=<AddBackward0>)\n",
      "epoch: 17466 loss is tensor([-0.6576], grad_fn=<AddBackward0>)\n",
      "epoch: 17467 loss is tensor([-0.6508], grad_fn=<AddBackward0>)\n",
      "epoch: 17468 loss is tensor([-0.5977], grad_fn=<AddBackward0>)\n",
      "epoch: 17469 loss is tensor([-0.6923], grad_fn=<AddBackward0>)\n",
      "epoch: 17470 loss is tensor([-0.6496], grad_fn=<AddBackward0>)\n",
      "epoch: 17471 loss is tensor([-0.6340], grad_fn=<AddBackward0>)\n",
      "epoch: 17472 loss is tensor([-0.6808], grad_fn=<AddBackward0>)\n",
      "epoch: 17473 loss is tensor([-0.6717], grad_fn=<AddBackward0>)\n",
      "epoch: 17474 loss is tensor([-0.6628], grad_fn=<AddBackward0>)\n",
      "epoch: 17475 loss is tensor([-0.6589], grad_fn=<AddBackward0>)\n",
      "epoch: 17476 loss is tensor([-0.6849], grad_fn=<AddBackward0>)\n",
      "epoch: 17477 loss is tensor([-0.6160], grad_fn=<AddBackward0>)\n",
      "epoch: 17478 loss is tensor([-0.6875], grad_fn=<AddBackward0>)\n",
      "epoch: 17479 loss is tensor([-0.6953], grad_fn=<AddBackward0>)\n",
      "epoch: 17480 loss is tensor([-0.6867], grad_fn=<AddBackward0>)\n",
      "epoch: 17481 loss is tensor([-0.6649], grad_fn=<AddBackward0>)\n",
      "epoch: 17482 loss is tensor([-0.7062], grad_fn=<AddBackward0>)\n",
      "epoch: 17483 loss is tensor([-0.6527], grad_fn=<AddBackward0>)\n",
      "epoch: 17484 loss is tensor([-0.7022], grad_fn=<AddBackward0>)\n",
      "epoch: 17485 loss is tensor([-0.7102], grad_fn=<AddBackward0>)\n",
      "epoch: 17486 loss is tensor([-0.6963], grad_fn=<AddBackward0>)\n",
      "epoch: 17487 loss is tensor([-0.6983], grad_fn=<AddBackward0>)\n",
      "epoch: 17488 loss is tensor([-0.6275], grad_fn=<AddBackward0>)\n",
      "epoch: 17489 loss is tensor([-0.7122], grad_fn=<AddBackward0>)\n",
      "epoch: 17490 loss is tensor([-0.7186], grad_fn=<AddBackward0>)\n",
      "epoch: 17491 loss is tensor([-0.6708], grad_fn=<AddBackward0>)\n",
      "epoch: 17492 loss is tensor([-0.7344], grad_fn=<AddBackward0>)\n",
      "epoch: 17493 loss is tensor([-0.7069], grad_fn=<AddBackward0>)\n",
      "epoch: 17494 loss is tensor([-0.7351], grad_fn=<AddBackward0>)\n",
      "epoch: 17495 loss is tensor([-0.7015], grad_fn=<AddBackward0>)\n",
      "epoch: 17496 loss is tensor([-0.6756], grad_fn=<AddBackward0>)\n",
      "epoch: 17497 loss is tensor([-0.6797], grad_fn=<AddBackward0>)\n",
      "epoch: 17498 loss is tensor([-0.7154], grad_fn=<AddBackward0>)\n",
      "epoch: 17499 loss is tensor([-0.7292], grad_fn=<AddBackward0>)\n",
      "epoch: 17500 loss is tensor([-0.7012], grad_fn=<AddBackward0>)\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17501 loss is tensor([-0.6949], grad_fn=<AddBackward0>)\n",
      "epoch: 17502 loss is tensor([-0.6963], grad_fn=<AddBackward0>)\n",
      "epoch: 17503 loss is tensor([-0.7513], grad_fn=<AddBackward0>)\n",
      "epoch: 17504 loss is tensor([-0.7129], grad_fn=<AddBackward0>)\n",
      "epoch: 17505 loss is tensor([-0.7302], grad_fn=<AddBackward0>)\n",
      "epoch: 17506 loss is tensor([-0.7128], grad_fn=<AddBackward0>)\n",
      "epoch: 17507 loss is tensor([-0.7082], grad_fn=<AddBackward0>)\n",
      "epoch: 17508 loss is tensor([-0.6963], grad_fn=<AddBackward0>)\n",
      "epoch: 17509 loss is tensor([-0.6471], grad_fn=<AddBackward0>)\n",
      "epoch: 17510 loss is tensor([-0.6883], grad_fn=<AddBackward0>)\n",
      "epoch: 17511 loss is tensor([-0.6273], grad_fn=<AddBackward0>)\n",
      "epoch: 17512 loss is tensor([-0.7316], grad_fn=<AddBackward0>)\n",
      "epoch: 17513 loss is tensor([-0.7023], grad_fn=<AddBackward0>)\n",
      "epoch: 17514 loss is tensor([-0.6535], grad_fn=<AddBackward0>)\n",
      "epoch: 17515 loss is tensor([-0.6675], grad_fn=<AddBackward0>)\n",
      "epoch: 17516 loss is tensor([-0.6451], grad_fn=<AddBackward0>)\n",
      "epoch: 17517 loss is tensor([-0.6967], grad_fn=<AddBackward0>)\n",
      "epoch: 17518 loss is tensor([-0.6460], grad_fn=<AddBackward0>)\n",
      "epoch: 17519 loss is tensor([-0.6355], grad_fn=<AddBackward0>)\n",
      "epoch: 17520 loss is tensor([-0.7265], grad_fn=<AddBackward0>)\n",
      "epoch: 17521 loss is tensor([-0.6656], grad_fn=<AddBackward0>)\n",
      "epoch: 17522 loss is tensor([-0.6694], grad_fn=<AddBackward0>)\n",
      "epoch: 17523 loss is tensor([-0.7078], grad_fn=<AddBackward0>)\n",
      "epoch: 17524 loss is tensor([-0.5907], grad_fn=<AddBackward0>)\n",
      "epoch: 17525 loss is tensor([-0.6735], grad_fn=<AddBackward0>)\n",
      "epoch: 17526 loss is tensor([-0.6425], grad_fn=<AddBackward0>)\n",
      "epoch: 17527 loss is tensor([-0.6136], grad_fn=<AddBackward0>)\n",
      "epoch: 17528 loss is tensor([-0.6405], grad_fn=<AddBackward0>)\n",
      "epoch: 17529 loss is tensor([-0.6844], grad_fn=<AddBackward0>)\n",
      "epoch: 17530 loss is tensor([-0.6568], grad_fn=<AddBackward0>)\n",
      "epoch: 17531 loss is tensor([-0.7172], grad_fn=<AddBackward0>)\n",
      "epoch: 17532 loss is tensor([-0.7265], grad_fn=<AddBackward0>)\n",
      "epoch: 17533 loss is tensor([-0.6690], grad_fn=<AddBackward0>)\n",
      "epoch: 17534 loss is tensor([-0.6540], grad_fn=<AddBackward0>)\n",
      "epoch: 17535 loss is tensor([-0.6559], grad_fn=<AddBackward0>)\n",
      "epoch: 17536 loss is tensor([-0.7324], grad_fn=<AddBackward0>)\n",
      "epoch: 17537 loss is tensor([-0.6817], grad_fn=<AddBackward0>)\n",
      "epoch: 17538 loss is tensor([-0.7245], grad_fn=<AddBackward0>)\n",
      "epoch: 17539 loss is tensor([-0.6406], grad_fn=<AddBackward0>)\n",
      "epoch: 17540 loss is tensor([-0.6845], grad_fn=<AddBackward0>)\n",
      "epoch: 17541 loss is tensor([-0.7048], grad_fn=<AddBackward0>)\n",
      "epoch: 17542 loss is tensor([-0.6730], grad_fn=<AddBackward0>)\n",
      "epoch: 17543 loss is tensor([-0.6876], grad_fn=<AddBackward0>)\n",
      "epoch: 17544 loss is tensor([-0.6679], grad_fn=<AddBackward0>)\n",
      "epoch: 17545 loss is tensor([-0.6556], grad_fn=<AddBackward0>)\n",
      "epoch: 17546 loss is tensor([-0.7356], grad_fn=<AddBackward0>)\n",
      "epoch: 17547 loss is tensor([-0.6999], grad_fn=<AddBackward0>)\n",
      "epoch: 17548 loss is tensor([-0.7027], grad_fn=<AddBackward0>)\n",
      "epoch: 17549 loss is tensor([-0.6873], grad_fn=<AddBackward0>)\n",
      "epoch: 17550 loss is tensor([-0.6730], grad_fn=<AddBackward0>)\n",
      "epoch: 17551 loss is tensor([-0.6435], grad_fn=<AddBackward0>)\n",
      "epoch: 17552 loss is tensor([-0.6934], grad_fn=<AddBackward0>)\n",
      "epoch: 17553 loss is tensor([-0.7390], grad_fn=<AddBackward0>)\n",
      "epoch: 17554 loss is tensor([-0.6647], grad_fn=<AddBackward0>)\n",
      "epoch: 17555 loss is tensor([-0.6980], grad_fn=<AddBackward0>)\n",
      "epoch: 17556 loss is tensor([-0.7254], grad_fn=<AddBackward0>)\n",
      "epoch: 17557 loss is tensor([-0.7585], grad_fn=<AddBackward0>)\n",
      "epoch: 17558 loss is tensor([-0.7188], grad_fn=<AddBackward0>)\n",
      "epoch: 17559 loss is tensor([-0.6917], grad_fn=<AddBackward0>)\n",
      "epoch: 17560 loss is tensor([-0.6855], grad_fn=<AddBackward0>)\n",
      "epoch: 17561 loss is tensor([-0.6953], grad_fn=<AddBackward0>)\n",
      "epoch: 17562 loss is tensor([-0.6974], grad_fn=<AddBackward0>)\n",
      "epoch: 17563 loss is tensor([-0.6443], grad_fn=<AddBackward0>)\n",
      "epoch: 17564 loss is tensor([-0.7010], grad_fn=<AddBackward0>)\n",
      "epoch: 17565 loss is tensor([-0.6740], grad_fn=<AddBackward0>)\n",
      "epoch: 17566 loss is tensor([-0.6408], grad_fn=<AddBackward0>)\n",
      "epoch: 17567 loss is tensor([-0.7208], grad_fn=<AddBackward0>)\n",
      "epoch: 17568 loss is tensor([-0.6627], grad_fn=<AddBackward0>)\n",
      "epoch: 17569 loss is tensor([-0.7037], grad_fn=<AddBackward0>)\n",
      "epoch: 17570 loss is tensor([-0.6719], grad_fn=<AddBackward0>)\n",
      "epoch: 17571 loss is tensor([-0.7319], grad_fn=<AddBackward0>)\n",
      "epoch: 17572 loss is tensor([-0.6725], grad_fn=<AddBackward0>)\n",
      "epoch: 17573 loss is tensor([-0.6773], grad_fn=<AddBackward0>)\n",
      "epoch: 17574 loss is tensor([-0.7034], grad_fn=<AddBackward0>)\n",
      "epoch: 17575 loss is tensor([-0.6399], grad_fn=<AddBackward0>)\n",
      "epoch: 17576 loss is tensor([-0.6729], grad_fn=<AddBackward0>)\n",
      "epoch: 17577 loss is tensor([-0.6084], grad_fn=<AddBackward0>)\n",
      "epoch: 17578 loss is tensor([-0.6721], grad_fn=<AddBackward0>)\n",
      "epoch: 17579 loss is tensor([-0.6437], grad_fn=<AddBackward0>)\n",
      "epoch: 17580 loss is tensor([-0.7219], grad_fn=<AddBackward0>)\n",
      "epoch: 17581 loss is tensor([-0.7168], grad_fn=<AddBackward0>)\n",
      "epoch: 17582 loss is tensor([-0.6785], grad_fn=<AddBackward0>)\n",
      "epoch: 17583 loss is tensor([-0.7067], grad_fn=<AddBackward0>)\n",
      "epoch: 17584 loss is tensor([-0.7167], grad_fn=<AddBackward0>)\n",
      "epoch: 17585 loss is tensor([-0.6417], grad_fn=<AddBackward0>)\n",
      "epoch: 17586 loss is tensor([-0.6607], grad_fn=<AddBackward0>)\n",
      "epoch: 17587 loss is tensor([-0.7005], grad_fn=<AddBackward0>)\n",
      "epoch: 17588 loss is tensor([-0.6196], grad_fn=<AddBackward0>)\n",
      "epoch: 17589 loss is tensor([-0.6969], grad_fn=<AddBackward0>)\n",
      "epoch: 17590 loss is tensor([-0.6598], grad_fn=<AddBackward0>)\n",
      "epoch: 17591 loss is tensor([-0.6799], grad_fn=<AddBackward0>)\n",
      "epoch: 17592 loss is tensor([-0.6828], grad_fn=<AddBackward0>)\n",
      "epoch: 17593 loss is tensor([-0.6383], grad_fn=<AddBackward0>)\n",
      "epoch: 17594 loss is tensor([-0.6948], grad_fn=<AddBackward0>)\n",
      "epoch: 17595 loss is tensor([-0.7120], grad_fn=<AddBackward0>)\n",
      "epoch: 17596 loss is tensor([-0.6308], grad_fn=<AddBackward0>)\n",
      "epoch: 17597 loss is tensor([-0.6852], grad_fn=<AddBackward0>)\n",
      "epoch: 17598 loss is tensor([-0.6939], grad_fn=<AddBackward0>)\n",
      "epoch: 17599 loss is tensor([-0.6535], grad_fn=<AddBackward0>)\n",
      "epoch: 17600 loss is tensor([-0.6545], grad_fn=<AddBackward0>)\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17601 loss is tensor([-0.6965], grad_fn=<AddBackward0>)\n",
      "epoch: 17602 loss is tensor([-0.6873], grad_fn=<AddBackward0>)\n",
      "epoch: 17603 loss is tensor([-0.6794], grad_fn=<AddBackward0>)\n",
      "epoch: 17604 loss is tensor([-0.7008], grad_fn=<AddBackward0>)\n",
      "epoch: 17605 loss is tensor([-0.6851], grad_fn=<AddBackward0>)\n",
      "epoch: 17606 loss is tensor([-0.6558], grad_fn=<AddBackward0>)\n",
      "epoch: 17607 loss is tensor([-0.6343], grad_fn=<AddBackward0>)\n",
      "epoch: 17608 loss is tensor([-0.6751], grad_fn=<AddBackward0>)\n",
      "epoch: 17609 loss is tensor([-0.7379], grad_fn=<AddBackward0>)\n",
      "epoch: 17610 loss is tensor([-0.6605], grad_fn=<AddBackward0>)\n",
      "epoch: 17611 loss is tensor([-0.6382], grad_fn=<AddBackward0>)\n",
      "epoch: 17612 loss is tensor([-0.7297], grad_fn=<AddBackward0>)\n",
      "epoch: 17613 loss is tensor([-0.6963], grad_fn=<AddBackward0>)\n",
      "epoch: 17614 loss is tensor([-0.6914], grad_fn=<AddBackward0>)\n",
      "epoch: 17615 loss is tensor([-0.6972], grad_fn=<AddBackward0>)\n",
      "epoch: 17616 loss is tensor([-0.7050], grad_fn=<AddBackward0>)\n",
      "epoch: 17617 loss is tensor([-0.7048], grad_fn=<AddBackward0>)\n",
      "epoch: 17618 loss is tensor([-0.6812], grad_fn=<AddBackward0>)\n",
      "epoch: 17619 loss is tensor([-0.6888], grad_fn=<AddBackward0>)\n",
      "epoch: 17620 loss is tensor([-0.7278], grad_fn=<AddBackward0>)\n",
      "epoch: 17621 loss is tensor([-0.6934], grad_fn=<AddBackward0>)\n",
      "epoch: 17622 loss is tensor([-0.6661], grad_fn=<AddBackward0>)\n",
      "epoch: 17623 loss is tensor([-0.6579], grad_fn=<AddBackward0>)\n",
      "epoch: 17624 loss is tensor([-0.7124], grad_fn=<AddBackward0>)\n",
      "epoch: 17625 loss is tensor([-0.6878], grad_fn=<AddBackward0>)\n",
      "epoch: 17626 loss is tensor([-0.7129], grad_fn=<AddBackward0>)\n",
      "epoch: 17627 loss is tensor([-0.7402], grad_fn=<AddBackward0>)\n",
      "epoch: 17628 loss is tensor([-0.6778], grad_fn=<AddBackward0>)\n",
      "epoch: 17629 loss is tensor([-0.6558], grad_fn=<AddBackward0>)\n",
      "epoch: 17630 loss is tensor([-0.7433], grad_fn=<AddBackward0>)\n",
      "epoch: 17631 loss is tensor([-0.6464], grad_fn=<AddBackward0>)\n",
      "epoch: 17632 loss is tensor([-0.6796], grad_fn=<AddBackward0>)\n",
      "epoch: 17633 loss is tensor([-0.6903], grad_fn=<AddBackward0>)\n",
      "epoch: 17634 loss is tensor([-0.6708], grad_fn=<AddBackward0>)\n",
      "epoch: 17635 loss is tensor([-0.6552], grad_fn=<AddBackward0>)\n",
      "epoch: 17636 loss is tensor([-0.7076], grad_fn=<AddBackward0>)\n",
      "epoch: 17637 loss is tensor([-0.6766], grad_fn=<AddBackward0>)\n",
      "epoch: 17638 loss is tensor([-0.6654], grad_fn=<AddBackward0>)\n",
      "epoch: 17639 loss is tensor([-0.6461], grad_fn=<AddBackward0>)\n",
      "epoch: 17640 loss is tensor([-0.6717], grad_fn=<AddBackward0>)\n",
      "epoch: 17641 loss is tensor([-0.7177], grad_fn=<AddBackward0>)\n",
      "epoch: 17642 loss is tensor([-0.7074], grad_fn=<AddBackward0>)\n",
      "epoch: 17643 loss is tensor([-0.7336], grad_fn=<AddBackward0>)\n",
      "epoch: 17644 loss is tensor([-0.6892], grad_fn=<AddBackward0>)\n",
      "epoch: 17645 loss is tensor([-0.7282], grad_fn=<AddBackward0>)\n",
      "epoch: 17646 loss is tensor([-0.6852], grad_fn=<AddBackward0>)\n",
      "epoch: 17647 loss is tensor([-0.7070], grad_fn=<AddBackward0>)\n",
      "epoch: 17648 loss is tensor([-0.7115], grad_fn=<AddBackward0>)\n",
      "epoch: 17649 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 17650 loss is tensor([-0.6870], grad_fn=<AddBackward0>)\n",
      "epoch: 17651 loss is tensor([-0.6790], grad_fn=<AddBackward0>)\n",
      "epoch: 17652 loss is tensor([-0.6786], grad_fn=<AddBackward0>)\n",
      "epoch: 17653 loss is tensor([-0.6332], grad_fn=<AddBackward0>)\n",
      "epoch: 17654 loss is tensor([-0.6782], grad_fn=<AddBackward0>)\n",
      "epoch: 17655 loss is tensor([-0.7030], grad_fn=<AddBackward0>)\n",
      "epoch: 17656 loss is tensor([-0.6865], grad_fn=<AddBackward0>)\n",
      "epoch: 17657 loss is tensor([-0.6657], grad_fn=<AddBackward0>)\n",
      "epoch: 17658 loss is tensor([-0.7115], grad_fn=<AddBackward0>)\n",
      "epoch: 17659 loss is tensor([-0.7307], grad_fn=<AddBackward0>)\n",
      "epoch: 17660 loss is tensor([-0.7188], grad_fn=<AddBackward0>)\n",
      "epoch: 17661 loss is tensor([-0.7176], grad_fn=<AddBackward0>)\n",
      "epoch: 17662 loss is tensor([-0.6763], grad_fn=<AddBackward0>)\n",
      "epoch: 17663 loss is tensor([-0.6984], grad_fn=<AddBackward0>)\n",
      "epoch: 17664 loss is tensor([-0.7361], grad_fn=<AddBackward0>)\n",
      "epoch: 17665 loss is tensor([-0.7017], grad_fn=<AddBackward0>)\n",
      "epoch: 17666 loss is tensor([-0.6857], grad_fn=<AddBackward0>)\n",
      "epoch: 17667 loss is tensor([-0.7458], grad_fn=<AddBackward0>)\n",
      "epoch: 17668 loss is tensor([-0.7124], grad_fn=<AddBackward0>)\n",
      "epoch: 17669 loss is tensor([-0.6759], grad_fn=<AddBackward0>)\n",
      "epoch: 17670 loss is tensor([-0.7484], grad_fn=<AddBackward0>)\n",
      "epoch: 17671 loss is tensor([-0.6707], grad_fn=<AddBackward0>)\n",
      "epoch: 17672 loss is tensor([-0.6349], grad_fn=<AddBackward0>)\n",
      "epoch: 17673 loss is tensor([-0.6897], grad_fn=<AddBackward0>)\n",
      "epoch: 17674 loss is tensor([-0.6588], grad_fn=<AddBackward0>)\n",
      "epoch: 17675 loss is tensor([-0.6406], grad_fn=<AddBackward0>)\n",
      "epoch: 17676 loss is tensor([-0.6184], grad_fn=<AddBackward0>)\n",
      "epoch: 17677 loss is tensor([-0.5753], grad_fn=<AddBackward0>)\n",
      "epoch: 17678 loss is tensor([-0.6679], grad_fn=<AddBackward0>)\n",
      "epoch: 17679 loss is tensor([-0.6987], grad_fn=<AddBackward0>)\n",
      "epoch: 17680 loss is tensor([-0.6163], grad_fn=<AddBackward0>)\n",
      "epoch: 17681 loss is tensor([-0.6246], grad_fn=<AddBackward0>)\n",
      "epoch: 17682 loss is tensor([-0.6260], grad_fn=<AddBackward0>)\n",
      "epoch: 17683 loss is tensor([-0.6684], grad_fn=<AddBackward0>)\n",
      "epoch: 17684 loss is tensor([-0.6539], grad_fn=<AddBackward0>)\n",
      "epoch: 17685 loss is tensor([-0.6750], grad_fn=<AddBackward0>)\n",
      "epoch: 17686 loss is tensor([-0.6803], grad_fn=<AddBackward0>)\n",
      "epoch: 17687 loss is tensor([-0.6978], grad_fn=<AddBackward0>)\n",
      "epoch: 17688 loss is tensor([-0.6723], grad_fn=<AddBackward0>)\n",
      "epoch: 17689 loss is tensor([-0.6976], grad_fn=<AddBackward0>)\n",
      "epoch: 17690 loss is tensor([-0.7229], grad_fn=<AddBackward0>)\n",
      "epoch: 17691 loss is tensor([-0.7261], grad_fn=<AddBackward0>)\n",
      "epoch: 17692 loss is tensor([-0.7139], grad_fn=<AddBackward0>)\n",
      "epoch: 17693 loss is tensor([-0.7461], grad_fn=<AddBackward0>)\n",
      "epoch: 17694 loss is tensor([-0.6600], grad_fn=<AddBackward0>)\n",
      "epoch: 17695 loss is tensor([-0.6384], grad_fn=<AddBackward0>)\n",
      "epoch: 17696 loss is tensor([-0.6583], grad_fn=<AddBackward0>)\n",
      "epoch: 17697 loss is tensor([-0.7151], grad_fn=<AddBackward0>)\n",
      "epoch: 17698 loss is tensor([-0.7139], grad_fn=<AddBackward0>)\n",
      "epoch: 17699 loss is tensor([-0.6951], grad_fn=<AddBackward0>)\n",
      "epoch: 17700 loss is tensor([-0.6954], grad_fn=<AddBackward0>)\n",
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17701 loss is tensor([-0.6838], grad_fn=<AddBackward0>)\n",
      "epoch: 17702 loss is tensor([-0.7016], grad_fn=<AddBackward0>)\n",
      "epoch: 17703 loss is tensor([-0.6603], grad_fn=<AddBackward0>)\n",
      "epoch: 17704 loss is tensor([-0.6427], grad_fn=<AddBackward0>)\n",
      "epoch: 17705 loss is tensor([-0.6587], grad_fn=<AddBackward0>)\n",
      "epoch: 17706 loss is tensor([-0.7045], grad_fn=<AddBackward0>)\n",
      "epoch: 17707 loss is tensor([-0.7317], grad_fn=<AddBackward0>)\n",
      "epoch: 17708 loss is tensor([-0.7051], grad_fn=<AddBackward0>)\n",
      "epoch: 17709 loss is tensor([-0.6736], grad_fn=<AddBackward0>)\n",
      "epoch: 17710 loss is tensor([-0.7454], grad_fn=<AddBackward0>)\n",
      "epoch: 17711 loss is tensor([-0.6997], grad_fn=<AddBackward0>)\n",
      "epoch: 17712 loss is tensor([-0.7019], grad_fn=<AddBackward0>)\n",
      "epoch: 17713 loss is tensor([-0.7236], grad_fn=<AddBackward0>)\n",
      "epoch: 17714 loss is tensor([-0.7217], grad_fn=<AddBackward0>)\n",
      "epoch: 17715 loss is tensor([-0.7516], grad_fn=<AddBackward0>)\n",
      "epoch: 17716 loss is tensor([-0.7339], grad_fn=<AddBackward0>)\n",
      "epoch: 17717 loss is tensor([-0.7167], grad_fn=<AddBackward0>)\n",
      "epoch: 17718 loss is tensor([-0.6923], grad_fn=<AddBackward0>)\n",
      "epoch: 17719 loss is tensor([-0.6977], grad_fn=<AddBackward0>)\n",
      "epoch: 17720 loss is tensor([-0.6761], grad_fn=<AddBackward0>)\n",
      "epoch: 17721 loss is tensor([-0.6619], grad_fn=<AddBackward0>)\n",
      "epoch: 17722 loss is tensor([-0.6325], grad_fn=<AddBackward0>)\n",
      "epoch: 17723 loss is tensor([-0.6521], grad_fn=<AddBackward0>)\n",
      "epoch: 17724 loss is tensor([-0.6525], grad_fn=<AddBackward0>)\n",
      "epoch: 17725 loss is tensor([-0.6947], grad_fn=<AddBackward0>)\n",
      "epoch: 17726 loss is tensor([-0.7521], grad_fn=<AddBackward0>)\n",
      "epoch: 17727 loss is tensor([-0.6851], grad_fn=<AddBackward0>)\n",
      "epoch: 17728 loss is tensor([-0.7136], grad_fn=<AddBackward0>)\n",
      "epoch: 17729 loss is tensor([-0.6584], grad_fn=<AddBackward0>)\n",
      "epoch: 17730 loss is tensor([-0.6790], grad_fn=<AddBackward0>)\n",
      "epoch: 17731 loss is tensor([-0.6456], grad_fn=<AddBackward0>)\n",
      "epoch: 17732 loss is tensor([-0.6578], grad_fn=<AddBackward0>)\n",
      "epoch: 17733 loss is tensor([-0.6714], grad_fn=<AddBackward0>)\n",
      "epoch: 17734 loss is tensor([-0.6753], grad_fn=<AddBackward0>)\n",
      "epoch: 17735 loss is tensor([-0.6441], grad_fn=<AddBackward0>)\n",
      "epoch: 17736 loss is tensor([-0.6474], grad_fn=<AddBackward0>)\n",
      "epoch: 17737 loss is tensor([-0.6013], grad_fn=<AddBackward0>)\n",
      "epoch: 17738 loss is tensor([-0.6511], grad_fn=<AddBackward0>)\n",
      "epoch: 17739 loss is tensor([-0.6707], grad_fn=<AddBackward0>)\n",
      "epoch: 17740 loss is tensor([-0.6667], grad_fn=<AddBackward0>)\n",
      "epoch: 17741 loss is tensor([-0.6529], grad_fn=<AddBackward0>)\n",
      "epoch: 17742 loss is tensor([-0.6785], grad_fn=<AddBackward0>)\n",
      "epoch: 17743 loss is tensor([-0.7392], grad_fn=<AddBackward0>)\n",
      "epoch: 17744 loss is tensor([-0.7174], grad_fn=<AddBackward0>)\n",
      "epoch: 17745 loss is tensor([-0.6629], grad_fn=<AddBackward0>)\n",
      "epoch: 17746 loss is tensor([-0.7033], grad_fn=<AddBackward0>)\n",
      "epoch: 17747 loss is tensor([-0.6962], grad_fn=<AddBackward0>)\n",
      "epoch: 17748 loss is tensor([-0.6702], grad_fn=<AddBackward0>)\n",
      "epoch: 17749 loss is tensor([-0.6674], grad_fn=<AddBackward0>)\n",
      "epoch: 17750 loss is tensor([-0.6432], grad_fn=<AddBackward0>)\n",
      "epoch: 17751 loss is tensor([-0.7193], grad_fn=<AddBackward0>)\n",
      "epoch: 17752 loss is tensor([-0.6638], grad_fn=<AddBackward0>)\n",
      "epoch: 17753 loss is tensor([-0.6819], grad_fn=<AddBackward0>)\n",
      "epoch: 17754 loss is tensor([-0.6207], grad_fn=<AddBackward0>)\n",
      "epoch: 17755 loss is tensor([-0.6753], grad_fn=<AddBackward0>)\n",
      "epoch: 17756 loss is tensor([-0.6722], grad_fn=<AddBackward0>)\n",
      "epoch: 17757 loss is tensor([-0.7114], grad_fn=<AddBackward0>)\n",
      "epoch: 17758 loss is tensor([-0.7146], grad_fn=<AddBackward0>)\n",
      "epoch: 17759 loss is tensor([-0.6429], grad_fn=<AddBackward0>)\n",
      "epoch: 17760 loss is tensor([-0.7032], grad_fn=<AddBackward0>)\n",
      "epoch: 17761 loss is tensor([-0.6961], grad_fn=<AddBackward0>)\n",
      "epoch: 17762 loss is tensor([-0.6449], grad_fn=<AddBackward0>)\n",
      "epoch: 17763 loss is tensor([-0.7217], grad_fn=<AddBackward0>)\n",
      "epoch: 17764 loss is tensor([-0.6765], grad_fn=<AddBackward0>)\n",
      "epoch: 17765 loss is tensor([-0.6882], grad_fn=<AddBackward0>)\n",
      "epoch: 17766 loss is tensor([-0.6333], grad_fn=<AddBackward0>)\n",
      "epoch: 17767 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 17768 loss is tensor([-0.7000], grad_fn=<AddBackward0>)\n",
      "epoch: 17769 loss is tensor([-0.6810], grad_fn=<AddBackward0>)\n",
      "epoch: 17770 loss is tensor([-0.6949], grad_fn=<AddBackward0>)\n",
      "epoch: 17771 loss is tensor([-0.6422], grad_fn=<AddBackward0>)\n",
      "epoch: 17772 loss is tensor([-0.6910], grad_fn=<AddBackward0>)\n",
      "epoch: 17773 loss is tensor([-0.6527], grad_fn=<AddBackward0>)\n",
      "epoch: 17774 loss is tensor([-0.6936], grad_fn=<AddBackward0>)\n",
      "epoch: 17775 loss is tensor([-0.6689], grad_fn=<AddBackward0>)\n",
      "epoch: 17776 loss is tensor([-0.6584], grad_fn=<AddBackward0>)\n",
      "epoch: 17777 loss is tensor([-0.6910], grad_fn=<AddBackward0>)\n",
      "epoch: 17778 loss is tensor([-0.6942], grad_fn=<AddBackward0>)\n",
      "epoch: 17779 loss is tensor([-0.7138], grad_fn=<AddBackward0>)\n",
      "epoch: 17780 loss is tensor([-0.6957], grad_fn=<AddBackward0>)\n",
      "epoch: 17781 loss is tensor([-0.7037], grad_fn=<AddBackward0>)\n",
      "epoch: 17782 loss is tensor([-0.6803], grad_fn=<AddBackward0>)\n",
      "epoch: 17783 loss is tensor([-0.6251], grad_fn=<AddBackward0>)\n",
      "epoch: 17784 loss is tensor([-0.6752], grad_fn=<AddBackward0>)\n",
      "epoch: 17785 loss is tensor([-0.6747], grad_fn=<AddBackward0>)\n",
      "epoch: 17786 loss is tensor([-0.6509], grad_fn=<AddBackward0>)\n",
      "epoch: 17787 loss is tensor([-0.6412], grad_fn=<AddBackward0>)\n",
      "epoch: 17788 loss is tensor([-0.6856], grad_fn=<AddBackward0>)\n",
      "epoch: 17789 loss is tensor([-0.5775], grad_fn=<AddBackward0>)\n",
      "epoch: 17790 loss is tensor([-0.6147], grad_fn=<AddBackward0>)\n",
      "epoch: 17791 loss is tensor([-0.6608], grad_fn=<AddBackward0>)\n",
      "epoch: 17792 loss is tensor([-0.6686], grad_fn=<AddBackward0>)\n",
      "epoch: 17793 loss is tensor([-0.6511], grad_fn=<AddBackward0>)\n",
      "epoch: 17794 loss is tensor([-0.6474], grad_fn=<AddBackward0>)\n",
      "epoch: 17795 loss is tensor([-0.5878], grad_fn=<AddBackward0>)\n",
      "epoch: 17796 loss is tensor([-0.6061], grad_fn=<AddBackward0>)\n",
      "epoch: 17797 loss is tensor([-0.6873], grad_fn=<AddBackward0>)\n",
      "epoch: 17798 loss is tensor([-0.6851], grad_fn=<AddBackward0>)\n",
      "epoch: 17799 loss is tensor([-0.6988], grad_fn=<AddBackward0>)\n",
      "epoch: 17800 loss is tensor([-0.6956], grad_fn=<AddBackward0>)\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17801 loss is tensor([-0.6755], grad_fn=<AddBackward0>)\n",
      "epoch: 17802 loss is tensor([-0.6570], grad_fn=<AddBackward0>)\n",
      "epoch: 17803 loss is tensor([-0.6628], grad_fn=<AddBackward0>)\n",
      "epoch: 17804 loss is tensor([-0.6412], grad_fn=<AddBackward0>)\n",
      "epoch: 17805 loss is tensor([-0.6761], grad_fn=<AddBackward0>)\n",
      "epoch: 17806 loss is tensor([-0.6571], grad_fn=<AddBackward0>)\n",
      "epoch: 17807 loss is tensor([-0.7099], grad_fn=<AddBackward0>)\n",
      "epoch: 17808 loss is tensor([-0.6023], grad_fn=<AddBackward0>)\n",
      "epoch: 17809 loss is tensor([-0.7352], grad_fn=<AddBackward0>)\n",
      "epoch: 17810 loss is tensor([-0.6847], grad_fn=<AddBackward0>)\n",
      "epoch: 17811 loss is tensor([-0.6492], grad_fn=<AddBackward0>)\n",
      "epoch: 17812 loss is tensor([-0.6501], grad_fn=<AddBackward0>)\n",
      "epoch: 17813 loss is tensor([-0.7097], grad_fn=<AddBackward0>)\n",
      "epoch: 17814 loss is tensor([-0.6705], grad_fn=<AddBackward0>)\n",
      "epoch: 17815 loss is tensor([-0.6810], grad_fn=<AddBackward0>)\n",
      "epoch: 17816 loss is tensor([-0.6710], grad_fn=<AddBackward0>)\n",
      "epoch: 17817 loss is tensor([-0.6190], grad_fn=<AddBackward0>)\n",
      "epoch: 17818 loss is tensor([-0.7366], grad_fn=<AddBackward0>)\n",
      "epoch: 17819 loss is tensor([-0.6980], grad_fn=<AddBackward0>)\n",
      "epoch: 17820 loss is tensor([-0.6862], grad_fn=<AddBackward0>)\n",
      "epoch: 17821 loss is tensor([-0.6664], grad_fn=<AddBackward0>)\n",
      "epoch: 17822 loss is tensor([-0.6731], grad_fn=<AddBackward0>)\n",
      "epoch: 17823 loss is tensor([-0.6802], grad_fn=<AddBackward0>)\n",
      "epoch: 17824 loss is tensor([-0.6933], grad_fn=<AddBackward0>)\n",
      "epoch: 17825 loss is tensor([-0.7658], grad_fn=<AddBackward0>)\n",
      "epoch: 17826 loss is tensor([-0.7008], grad_fn=<AddBackward0>)\n",
      "epoch: 17827 loss is tensor([-0.6639], grad_fn=<AddBackward0>)\n",
      "epoch: 17828 loss is tensor([-0.7214], grad_fn=<AddBackward0>)\n",
      "epoch: 17829 loss is tensor([-0.6787], grad_fn=<AddBackward0>)\n",
      "epoch: 17830 loss is tensor([-0.7237], grad_fn=<AddBackward0>)\n",
      "epoch: 17831 loss is tensor([-0.7250], grad_fn=<AddBackward0>)\n",
      "epoch: 17832 loss is tensor([-0.6762], grad_fn=<AddBackward0>)\n",
      "epoch: 17833 loss is tensor([-0.6919], grad_fn=<AddBackward0>)\n",
      "epoch: 17834 loss is tensor([-0.7024], grad_fn=<AddBackward0>)\n",
      "epoch: 17835 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 17836 loss is tensor([-0.7263], grad_fn=<AddBackward0>)\n",
      "epoch: 17837 loss is tensor([-0.7114], grad_fn=<AddBackward0>)\n",
      "epoch: 17838 loss is tensor([-0.7288], grad_fn=<AddBackward0>)\n",
      "epoch: 17839 loss is tensor([-0.7166], grad_fn=<AddBackward0>)\n",
      "epoch: 17840 loss is tensor([-0.6827], grad_fn=<AddBackward0>)\n",
      "epoch: 17841 loss is tensor([-0.7003], grad_fn=<AddBackward0>)\n",
      "epoch: 17842 loss is tensor([-0.6466], grad_fn=<AddBackward0>)\n",
      "epoch: 17843 loss is tensor([-0.6886], grad_fn=<AddBackward0>)\n",
      "epoch: 17844 loss is tensor([-0.6678], grad_fn=<AddBackward0>)\n",
      "epoch: 17845 loss is tensor([-0.6679], grad_fn=<AddBackward0>)\n",
      "epoch: 17846 loss is tensor([-0.6739], grad_fn=<AddBackward0>)\n",
      "epoch: 17847 loss is tensor([-0.6534], grad_fn=<AddBackward0>)\n",
      "epoch: 17848 loss is tensor([-0.6978], grad_fn=<AddBackward0>)\n",
      "epoch: 17849 loss is tensor([-0.6801], grad_fn=<AddBackward0>)\n",
      "epoch: 17850 loss is tensor([-0.6612], grad_fn=<AddBackward0>)\n",
      "epoch: 17851 loss is tensor([-0.7083], grad_fn=<AddBackward0>)\n",
      "epoch: 17852 loss is tensor([-0.7066], grad_fn=<AddBackward0>)\n",
      "epoch: 17853 loss is tensor([-0.6351], grad_fn=<AddBackward0>)\n",
      "epoch: 17854 loss is tensor([-0.6914], grad_fn=<AddBackward0>)\n",
      "epoch: 17855 loss is tensor([-0.6568], grad_fn=<AddBackward0>)\n",
      "epoch: 17856 loss is tensor([-0.6631], grad_fn=<AddBackward0>)\n",
      "epoch: 17857 loss is tensor([-0.6278], grad_fn=<AddBackward0>)\n",
      "epoch: 17858 loss is tensor([-0.7320], grad_fn=<AddBackward0>)\n",
      "epoch: 17859 loss is tensor([-0.6811], grad_fn=<AddBackward0>)\n",
      "epoch: 17860 loss is tensor([-0.6993], grad_fn=<AddBackward0>)\n",
      "epoch: 17861 loss is tensor([-0.6910], grad_fn=<AddBackward0>)\n",
      "epoch: 17862 loss is tensor([-0.6706], grad_fn=<AddBackward0>)\n",
      "epoch: 17863 loss is tensor([-0.6614], grad_fn=<AddBackward0>)\n",
      "epoch: 17864 loss is tensor([-0.6630], grad_fn=<AddBackward0>)\n",
      "epoch: 17865 loss is tensor([-0.6715], grad_fn=<AddBackward0>)\n",
      "epoch: 17866 loss is tensor([-0.6984], grad_fn=<AddBackward0>)\n",
      "epoch: 17867 loss is tensor([-0.6877], grad_fn=<AddBackward0>)\n",
      "epoch: 17868 loss is tensor([-0.6814], grad_fn=<AddBackward0>)\n",
      "epoch: 17869 loss is tensor([-0.6894], grad_fn=<AddBackward0>)\n",
      "epoch: 17870 loss is tensor([-0.6694], grad_fn=<AddBackward0>)\n",
      "epoch: 17871 loss is tensor([-0.6423], grad_fn=<AddBackward0>)\n",
      "epoch: 17872 loss is tensor([-0.6510], grad_fn=<AddBackward0>)\n",
      "epoch: 17873 loss is tensor([-0.7165], grad_fn=<AddBackward0>)\n",
      "epoch: 17874 loss is tensor([-0.6577], grad_fn=<AddBackward0>)\n",
      "epoch: 17875 loss is tensor([-0.6756], grad_fn=<AddBackward0>)\n",
      "epoch: 17876 loss is tensor([-0.7081], grad_fn=<AddBackward0>)\n",
      "epoch: 17877 loss is tensor([-0.7530], grad_fn=<AddBackward0>)\n",
      "epoch: 17878 loss is tensor([-0.7065], grad_fn=<AddBackward0>)\n",
      "epoch: 17879 loss is tensor([-0.7006], grad_fn=<AddBackward0>)\n",
      "epoch: 17880 loss is tensor([-0.7140], grad_fn=<AddBackward0>)\n",
      "epoch: 17881 loss is tensor([-0.6785], grad_fn=<AddBackward0>)\n",
      "epoch: 17882 loss is tensor([-0.7479], grad_fn=<AddBackward0>)\n",
      "epoch: 17883 loss is tensor([-0.7403], grad_fn=<AddBackward0>)\n",
      "epoch: 17884 loss is tensor([-0.6864], grad_fn=<AddBackward0>)\n",
      "epoch: 17885 loss is tensor([-0.6776], grad_fn=<AddBackward0>)\n",
      "epoch: 17886 loss is tensor([-0.6399], grad_fn=<AddBackward0>)\n",
      "epoch: 17887 loss is tensor([-0.7043], grad_fn=<AddBackward0>)\n",
      "epoch: 17888 loss is tensor([-0.7005], grad_fn=<AddBackward0>)\n",
      "epoch: 17889 loss is tensor([-0.6580], grad_fn=<AddBackward0>)\n",
      "epoch: 17890 loss is tensor([-0.5788], grad_fn=<AddBackward0>)\n",
      "epoch: 17891 loss is tensor([-0.6919], grad_fn=<AddBackward0>)\n",
      "epoch: 17892 loss is tensor([-0.6573], grad_fn=<AddBackward0>)\n",
      "epoch: 17893 loss is tensor([-0.6188], grad_fn=<AddBackward0>)\n",
      "epoch: 17894 loss is tensor([-0.6211], grad_fn=<AddBackward0>)\n",
      "epoch: 17895 loss is tensor([-0.6203], grad_fn=<AddBackward0>)\n",
      "epoch: 17896 loss is tensor([-0.6626], grad_fn=<AddBackward0>)\n",
      "epoch: 17897 loss is tensor([-0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 17898 loss is tensor([-0.6158], grad_fn=<AddBackward0>)\n",
      "epoch: 17899 loss is tensor([-0.6364], grad_fn=<AddBackward0>)\n",
      "epoch: 17900 loss is tensor([-0.6823], grad_fn=<AddBackward0>)\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 17901 loss is tensor([-0.6272], grad_fn=<AddBackward0>)\n",
      "epoch: 17902 loss is tensor([-0.6529], grad_fn=<AddBackward0>)\n",
      "epoch: 17903 loss is tensor([-0.6803], grad_fn=<AddBackward0>)\n",
      "epoch: 17904 loss is tensor([-0.6399], grad_fn=<AddBackward0>)\n",
      "epoch: 17905 loss is tensor([-0.6296], grad_fn=<AddBackward0>)\n",
      "epoch: 17906 loss is tensor([-0.6680], grad_fn=<AddBackward0>)\n",
      "epoch: 17907 loss is tensor([-0.6831], grad_fn=<AddBackward0>)\n",
      "epoch: 17908 loss is tensor([-0.7261], grad_fn=<AddBackward0>)\n",
      "epoch: 17909 loss is tensor([-0.7226], grad_fn=<AddBackward0>)\n",
      "epoch: 17910 loss is tensor([-0.6619], grad_fn=<AddBackward0>)\n",
      "epoch: 17911 loss is tensor([-0.6999], grad_fn=<AddBackward0>)\n",
      "epoch: 17912 loss is tensor([-0.6804], grad_fn=<AddBackward0>)\n",
      "epoch: 17913 loss is tensor([-0.7024], grad_fn=<AddBackward0>)\n",
      "epoch: 17914 loss is tensor([-0.7090], grad_fn=<AddBackward0>)\n",
      "epoch: 17915 loss is tensor([-0.7242], grad_fn=<AddBackward0>)\n",
      "epoch: 17916 loss is tensor([-0.7481], grad_fn=<AddBackward0>)\n",
      "epoch: 17917 loss is tensor([-0.7103], grad_fn=<AddBackward0>)\n",
      "epoch: 17918 loss is tensor([-0.7208], grad_fn=<AddBackward0>)\n",
      "epoch: 17919 loss is tensor([-0.6430], grad_fn=<AddBackward0>)\n",
      "epoch: 17920 loss is tensor([-0.6735], grad_fn=<AddBackward0>)\n",
      "epoch: 17921 loss is tensor([-0.6993], grad_fn=<AddBackward0>)\n",
      "epoch: 17922 loss is tensor([-0.7447], grad_fn=<AddBackward0>)\n",
      "epoch: 17923 loss is tensor([-0.6516], grad_fn=<AddBackward0>)\n",
      "epoch: 17924 loss is tensor([-0.6903], grad_fn=<AddBackward0>)\n",
      "epoch: 17925 loss is tensor([-0.7501], grad_fn=<AddBackward0>)\n",
      "epoch: 17926 loss is tensor([-0.6448], grad_fn=<AddBackward0>)\n",
      "epoch: 17927 loss is tensor([-0.6991], grad_fn=<AddBackward0>)\n",
      "epoch: 17928 loss is tensor([-0.7216], grad_fn=<AddBackward0>)\n",
      "epoch: 17929 loss is tensor([-0.6886], grad_fn=<AddBackward0>)\n",
      "epoch: 17930 loss is tensor([-0.7364], grad_fn=<AddBackward0>)\n",
      "epoch: 17931 loss is tensor([-0.7410], grad_fn=<AddBackward0>)\n",
      "epoch: 17932 loss is tensor([-0.6945], grad_fn=<AddBackward0>)\n",
      "epoch: 17933 loss is tensor([-0.7054], grad_fn=<AddBackward0>)\n",
      "epoch: 17934 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 17935 loss is tensor([-0.7162], grad_fn=<AddBackward0>)\n",
      "epoch: 17936 loss is tensor([-0.6950], grad_fn=<AddBackward0>)\n",
      "epoch: 17937 loss is tensor([-0.7052], grad_fn=<AddBackward0>)\n",
      "epoch: 17938 loss is tensor([-0.7109], grad_fn=<AddBackward0>)\n",
      "epoch: 17939 loss is tensor([-0.6885], grad_fn=<AddBackward0>)\n",
      "epoch: 17940 loss is tensor([-0.6688], grad_fn=<AddBackward0>)\n",
      "epoch: 17941 loss is tensor([-0.7252], grad_fn=<AddBackward0>)\n",
      "epoch: 17942 loss is tensor([-0.7564], grad_fn=<AddBackward0>)\n",
      "epoch: 17943 loss is tensor([-0.6503], grad_fn=<AddBackward0>)\n",
      "epoch: 17944 loss is tensor([-0.7128], grad_fn=<AddBackward0>)\n",
      "epoch: 17945 loss is tensor([-0.6712], grad_fn=<AddBackward0>)\n",
      "epoch: 17946 loss is tensor([-0.7484], grad_fn=<AddBackward0>)\n",
      "epoch: 17947 loss is tensor([-0.6519], grad_fn=<AddBackward0>)\n",
      "epoch: 17948 loss is tensor([-0.6770], grad_fn=<AddBackward0>)\n",
      "epoch: 17949 loss is tensor([-0.6114], grad_fn=<AddBackward0>)\n",
      "epoch: 17950 loss is tensor([-0.6356], grad_fn=<AddBackward0>)\n",
      "epoch: 17951 loss is tensor([-0.6554], grad_fn=<AddBackward0>)\n",
      "epoch: 17952 loss is tensor([-0.7062], grad_fn=<AddBackward0>)\n",
      "epoch: 17953 loss is tensor([-0.6910], grad_fn=<AddBackward0>)\n",
      "epoch: 17954 loss is tensor([-0.6558], grad_fn=<AddBackward0>)\n",
      "epoch: 17955 loss is tensor([-0.7079], grad_fn=<AddBackward0>)\n",
      "epoch: 17956 loss is tensor([-0.7241], grad_fn=<AddBackward0>)\n",
      "epoch: 17957 loss is tensor([-0.7327], grad_fn=<AddBackward0>)\n",
      "epoch: 17958 loss is tensor([-0.7084], grad_fn=<AddBackward0>)\n",
      "epoch: 17959 loss is tensor([-0.7131], grad_fn=<AddBackward0>)\n",
      "epoch: 17960 loss is tensor([-0.7556], grad_fn=<AddBackward0>)\n",
      "epoch: 17961 loss is tensor([-0.6977], grad_fn=<AddBackward0>)\n",
      "epoch: 17962 loss is tensor([-0.6919], grad_fn=<AddBackward0>)\n",
      "epoch: 17963 loss is tensor([-0.7403], grad_fn=<AddBackward0>)\n",
      "epoch: 17964 loss is tensor([-0.7303], grad_fn=<AddBackward0>)\n",
      "epoch: 17965 loss is tensor([-0.7267], grad_fn=<AddBackward0>)\n",
      "epoch: 17966 loss is tensor([-0.7303], grad_fn=<AddBackward0>)\n",
      "epoch: 17967 loss is tensor([-0.6868], grad_fn=<AddBackward0>)\n",
      "epoch: 17968 loss is tensor([-0.7243], grad_fn=<AddBackward0>)\n",
      "epoch: 17969 loss is tensor([-0.6905], grad_fn=<AddBackward0>)\n",
      "epoch: 17970 loss is tensor([-0.6800], grad_fn=<AddBackward0>)\n",
      "epoch: 17971 loss is tensor([-0.7438], grad_fn=<AddBackward0>)\n",
      "epoch: 17972 loss is tensor([-0.6857], grad_fn=<AddBackward0>)\n",
      "epoch: 17973 loss is tensor([-0.7318], grad_fn=<AddBackward0>)\n",
      "epoch: 17974 loss is tensor([-0.6690], grad_fn=<AddBackward0>)\n",
      "epoch: 17975 loss is tensor([-0.6785], grad_fn=<AddBackward0>)\n",
      "epoch: 17976 loss is tensor([-0.7123], grad_fn=<AddBackward0>)\n",
      "epoch: 17977 loss is tensor([-0.6612], grad_fn=<AddBackward0>)\n",
      "epoch: 17978 loss is tensor([-0.6404], grad_fn=<AddBackward0>)\n",
      "epoch: 17979 loss is tensor([-0.6581], grad_fn=<AddBackward0>)\n",
      "epoch: 17980 loss is tensor([-0.7186], grad_fn=<AddBackward0>)\n",
      "epoch: 17981 loss is tensor([-0.6789], grad_fn=<AddBackward0>)\n",
      "epoch: 17982 loss is tensor([-0.7146], grad_fn=<AddBackward0>)\n",
      "epoch: 17983 loss is tensor([-0.6846], grad_fn=<AddBackward0>)\n",
      "epoch: 17984 loss is tensor([-0.7233], grad_fn=<AddBackward0>)\n",
      "epoch: 17985 loss is tensor([-0.7324], grad_fn=<AddBackward0>)\n",
      "epoch: 17986 loss is tensor([-0.6823], grad_fn=<AddBackward0>)\n",
      "epoch: 17987 loss is tensor([-0.7136], grad_fn=<AddBackward0>)\n",
      "epoch: 17988 loss is tensor([-0.7150], grad_fn=<AddBackward0>)\n",
      "epoch: 17989 loss is tensor([-0.6657], grad_fn=<AddBackward0>)\n",
      "epoch: 17990 loss is tensor([-0.6481], grad_fn=<AddBackward0>)\n",
      "epoch: 17991 loss is tensor([-0.6943], grad_fn=<AddBackward0>)\n",
      "epoch: 17992 loss is tensor([-0.6865], grad_fn=<AddBackward0>)\n",
      "epoch: 17993 loss is tensor([-0.7156], grad_fn=<AddBackward0>)\n",
      "epoch: 17994 loss is tensor([-0.6908], grad_fn=<AddBackward0>)\n",
      "epoch: 17995 loss is tensor([-0.6711], grad_fn=<AddBackward0>)\n",
      "epoch: 17996 loss is tensor([-0.7145], grad_fn=<AddBackward0>)\n",
      "epoch: 17997 loss is tensor([-0.7255], grad_fn=<AddBackward0>)\n",
      "epoch: 17998 loss is tensor([-0.6901], grad_fn=<AddBackward0>)\n",
      "epoch: 17999 loss is tensor([-0.6674], grad_fn=<AddBackward0>)\n",
      "epoch: 18000 loss is tensor([-0.6752], grad_fn=<AddBackward0>)\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18001 loss is tensor([-0.6771], grad_fn=<AddBackward0>)\n",
      "epoch: 18002 loss is tensor([-0.7042], grad_fn=<AddBackward0>)\n",
      "epoch: 18003 loss is tensor([-0.7247], grad_fn=<AddBackward0>)\n",
      "epoch: 18004 loss is tensor([-0.6871], grad_fn=<AddBackward0>)\n",
      "epoch: 18005 loss is tensor([-0.6496], grad_fn=<AddBackward0>)\n",
      "epoch: 18006 loss is tensor([-0.6423], grad_fn=<AddBackward0>)\n",
      "epoch: 18007 loss is tensor([-0.6839], grad_fn=<AddBackward0>)\n",
      "epoch: 18008 loss is tensor([-0.7198], grad_fn=<AddBackward0>)\n",
      "epoch: 18009 loss is tensor([-0.6211], grad_fn=<AddBackward0>)\n",
      "epoch: 18010 loss is tensor([-0.6735], grad_fn=<AddBackward0>)\n",
      "epoch: 18011 loss is tensor([-0.7277], grad_fn=<AddBackward0>)\n",
      "epoch: 18012 loss is tensor([-0.7350], grad_fn=<AddBackward0>)\n",
      "epoch: 18013 loss is tensor([-0.6944], grad_fn=<AddBackward0>)\n",
      "epoch: 18014 loss is tensor([-0.7112], grad_fn=<AddBackward0>)\n",
      "epoch: 18015 loss is tensor([-0.6483], grad_fn=<AddBackward0>)\n",
      "epoch: 18016 loss is tensor([-0.7169], grad_fn=<AddBackward0>)\n",
      "epoch: 18017 loss is tensor([-0.7483], grad_fn=<AddBackward0>)\n",
      "epoch: 18018 loss is tensor([-0.6949], grad_fn=<AddBackward0>)\n",
      "epoch: 18019 loss is tensor([-0.6666], grad_fn=<AddBackward0>)\n",
      "epoch: 18020 loss is tensor([-0.6904], grad_fn=<AddBackward0>)\n",
      "epoch: 18021 loss is tensor([-0.7185], grad_fn=<AddBackward0>)\n",
      "epoch: 18022 loss is tensor([-0.6502], grad_fn=<AddBackward0>)\n",
      "epoch: 18023 loss is tensor([-0.7282], grad_fn=<AddBackward0>)\n",
      "epoch: 18024 loss is tensor([-0.7508], grad_fn=<AddBackward0>)\n",
      "epoch: 18025 loss is tensor([-0.7372], grad_fn=<AddBackward0>)\n",
      "epoch: 18026 loss is tensor([-0.7699], grad_fn=<AddBackward0>)\n",
      "epoch: 18027 loss is tensor([-0.7165], grad_fn=<AddBackward0>)\n",
      "epoch: 18028 loss is tensor([-0.7269], grad_fn=<AddBackward0>)\n",
      "epoch: 18029 loss is tensor([-0.6944], grad_fn=<AddBackward0>)\n",
      "epoch: 18030 loss is tensor([-0.6972], grad_fn=<AddBackward0>)\n",
      "epoch: 18031 loss is tensor([-0.7143], grad_fn=<AddBackward0>)\n",
      "epoch: 18032 loss is tensor([-0.7121], grad_fn=<AddBackward0>)\n",
      "epoch: 18033 loss is tensor([-0.7257], grad_fn=<AddBackward0>)\n",
      "epoch: 18034 loss is tensor([-0.6061], grad_fn=<AddBackward0>)\n",
      "epoch: 18035 loss is tensor([-0.6819], grad_fn=<AddBackward0>)\n",
      "epoch: 18036 loss is tensor([-0.7457], grad_fn=<AddBackward0>)\n",
      "epoch: 18037 loss is tensor([-0.7164], grad_fn=<AddBackward0>)\n",
      "epoch: 18038 loss is tensor([-0.6837], grad_fn=<AddBackward0>)\n",
      "epoch: 18039 loss is tensor([-0.7289], grad_fn=<AddBackward0>)\n",
      "epoch: 18040 loss is tensor([-0.6823], grad_fn=<AddBackward0>)\n",
      "epoch: 18041 loss is tensor([-0.6448], grad_fn=<AddBackward0>)\n",
      "epoch: 18042 loss is tensor([-0.6517], grad_fn=<AddBackward0>)\n",
      "epoch: 18043 loss is tensor([-0.6766], grad_fn=<AddBackward0>)\n",
      "epoch: 18044 loss is tensor([-0.7198], grad_fn=<AddBackward0>)\n",
      "epoch: 18045 loss is tensor([-0.6875], grad_fn=<AddBackward0>)\n",
      "epoch: 18046 loss is tensor([-0.7612], grad_fn=<AddBackward0>)\n",
      "epoch: 18047 loss is tensor([-0.7037], grad_fn=<AddBackward0>)\n",
      "epoch: 18048 loss is tensor([-0.7083], grad_fn=<AddBackward0>)\n",
      "epoch: 18049 loss is tensor([-0.7098], grad_fn=<AddBackward0>)\n",
      "epoch: 18050 loss is tensor([-0.7395], grad_fn=<AddBackward0>)\n",
      "epoch: 18051 loss is tensor([-0.6885], grad_fn=<AddBackward0>)\n",
      "epoch: 18052 loss is tensor([-0.6688], grad_fn=<AddBackward0>)\n",
      "epoch: 18053 loss is tensor([-0.6717], grad_fn=<AddBackward0>)\n",
      "epoch: 18054 loss is tensor([-0.6823], grad_fn=<AddBackward0>)\n",
      "epoch: 18055 loss is tensor([-0.6680], grad_fn=<AddBackward0>)\n",
      "epoch: 18056 loss is tensor([-0.6903], grad_fn=<AddBackward0>)\n",
      "epoch: 18057 loss is tensor([-0.6957], grad_fn=<AddBackward0>)\n",
      "epoch: 18058 loss is tensor([-0.6595], grad_fn=<AddBackward0>)\n",
      "epoch: 18059 loss is tensor([-0.7359], grad_fn=<AddBackward0>)\n",
      "epoch: 18060 loss is tensor([-0.6793], grad_fn=<AddBackward0>)\n",
      "epoch: 18061 loss is tensor([-0.7482], grad_fn=<AddBackward0>)\n",
      "epoch: 18062 loss is tensor([-0.7413], grad_fn=<AddBackward0>)\n",
      "epoch: 18063 loss is tensor([-0.7787], grad_fn=<AddBackward0>)\n",
      "epoch: 18064 loss is tensor([-0.6856], grad_fn=<AddBackward0>)\n",
      "epoch: 18065 loss is tensor([-0.7024], grad_fn=<AddBackward0>)\n",
      "epoch: 18066 loss is tensor([-0.6850], grad_fn=<AddBackward0>)\n",
      "epoch: 18067 loss is tensor([-0.7407], grad_fn=<AddBackward0>)\n",
      "epoch: 18068 loss is tensor([-0.6808], grad_fn=<AddBackward0>)\n",
      "epoch: 18069 loss is tensor([-0.6622], grad_fn=<AddBackward0>)\n",
      "epoch: 18070 loss is tensor([-0.6967], grad_fn=<AddBackward0>)\n",
      "epoch: 18071 loss is tensor([-0.6926], grad_fn=<AddBackward0>)\n",
      "epoch: 18072 loss is tensor([-0.6892], grad_fn=<AddBackward0>)\n",
      "epoch: 18073 loss is tensor([-0.6976], grad_fn=<AddBackward0>)\n",
      "epoch: 18074 loss is tensor([-0.6748], grad_fn=<AddBackward0>)\n",
      "epoch: 18075 loss is tensor([-0.7020], grad_fn=<AddBackward0>)\n",
      "epoch: 18076 loss is tensor([-0.6829], grad_fn=<AddBackward0>)\n",
      "epoch: 18077 loss is tensor([-0.6734], grad_fn=<AddBackward0>)\n",
      "epoch: 18078 loss is tensor([-0.6715], grad_fn=<AddBackward0>)\n",
      "epoch: 18079 loss is tensor([-0.6124], grad_fn=<AddBackward0>)\n",
      "epoch: 18080 loss is tensor([-0.6802], grad_fn=<AddBackward0>)\n",
      "epoch: 18081 loss is tensor([-0.6885], grad_fn=<AddBackward0>)\n",
      "epoch: 18082 loss is tensor([-0.6522], grad_fn=<AddBackward0>)\n",
      "epoch: 18083 loss is tensor([-0.6559], grad_fn=<AddBackward0>)\n",
      "epoch: 18084 loss is tensor([-0.7104], grad_fn=<AddBackward0>)\n",
      "epoch: 18085 loss is tensor([-0.7379], grad_fn=<AddBackward0>)\n",
      "epoch: 18086 loss is tensor([-0.6392], grad_fn=<AddBackward0>)\n",
      "epoch: 18087 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 18088 loss is tensor([-0.6721], grad_fn=<AddBackward0>)\n",
      "epoch: 18089 loss is tensor([-0.7281], grad_fn=<AddBackward0>)\n",
      "epoch: 18090 loss is tensor([-0.6875], grad_fn=<AddBackward0>)\n",
      "epoch: 18091 loss is tensor([-0.6670], grad_fn=<AddBackward0>)\n",
      "epoch: 18092 loss is tensor([-0.6329], grad_fn=<AddBackward0>)\n",
      "epoch: 18093 loss is tensor([-0.7051], grad_fn=<AddBackward0>)\n",
      "epoch: 18094 loss is tensor([-0.7126], grad_fn=<AddBackward0>)\n",
      "epoch: 18095 loss is tensor([-0.6885], grad_fn=<AddBackward0>)\n",
      "epoch: 18096 loss is tensor([-0.6722], grad_fn=<AddBackward0>)\n",
      "epoch: 18097 loss is tensor([-0.7158], grad_fn=<AddBackward0>)\n",
      "epoch: 18098 loss is tensor([-0.6823], grad_fn=<AddBackward0>)\n",
      "epoch: 18099 loss is tensor([-0.6406], grad_fn=<AddBackward0>)\n",
      "epoch: 18100 loss is tensor([-0.7113], grad_fn=<AddBackward0>)\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18101 loss is tensor([-0.7001], grad_fn=<AddBackward0>)\n",
      "epoch: 18102 loss is tensor([-0.6757], grad_fn=<AddBackward0>)\n",
      "epoch: 18103 loss is tensor([-0.6856], grad_fn=<AddBackward0>)\n",
      "epoch: 18104 loss is tensor([-0.7441], grad_fn=<AddBackward0>)\n",
      "epoch: 18105 loss is tensor([-0.7034], grad_fn=<AddBackward0>)\n",
      "epoch: 18106 loss is tensor([-0.7096], grad_fn=<AddBackward0>)\n",
      "epoch: 18107 loss is tensor([-0.6897], grad_fn=<AddBackward0>)\n",
      "epoch: 18108 loss is tensor([-0.6789], grad_fn=<AddBackward0>)\n",
      "epoch: 18109 loss is tensor([-0.6699], grad_fn=<AddBackward0>)\n",
      "epoch: 18110 loss is tensor([-0.7160], grad_fn=<AddBackward0>)\n",
      "epoch: 18111 loss is tensor([-0.6725], grad_fn=<AddBackward0>)\n",
      "epoch: 18112 loss is tensor([-0.6331], grad_fn=<AddBackward0>)\n",
      "epoch: 18113 loss is tensor([-0.6996], grad_fn=<AddBackward0>)\n",
      "epoch: 18114 loss is tensor([-0.7464], grad_fn=<AddBackward0>)\n",
      "epoch: 18115 loss is tensor([-0.7337], grad_fn=<AddBackward0>)\n",
      "epoch: 18116 loss is tensor([-0.7292], grad_fn=<AddBackward0>)\n",
      "epoch: 18117 loss is tensor([-0.7582], grad_fn=<AddBackward0>)\n",
      "epoch: 18118 loss is tensor([-0.7074], grad_fn=<AddBackward0>)\n",
      "epoch: 18119 loss is tensor([-0.6408], grad_fn=<AddBackward0>)\n",
      "epoch: 18120 loss is tensor([-0.7103], grad_fn=<AddBackward0>)\n",
      "epoch: 18121 loss is tensor([-0.6946], grad_fn=<AddBackward0>)\n",
      "epoch: 18122 loss is tensor([-0.6900], grad_fn=<AddBackward0>)\n",
      "epoch: 18123 loss is tensor([-0.7123], grad_fn=<AddBackward0>)\n",
      "epoch: 18124 loss is tensor([-0.7003], grad_fn=<AddBackward0>)\n",
      "epoch: 18125 loss is tensor([-0.7007], grad_fn=<AddBackward0>)\n",
      "epoch: 18126 loss is tensor([-0.6988], grad_fn=<AddBackward0>)\n",
      "epoch: 18127 loss is tensor([-0.7106], grad_fn=<AddBackward0>)\n",
      "epoch: 18128 loss is tensor([-0.7425], grad_fn=<AddBackward0>)\n",
      "epoch: 18129 loss is tensor([-0.7270], grad_fn=<AddBackward0>)\n",
      "epoch: 18130 loss is tensor([-0.6641], grad_fn=<AddBackward0>)\n",
      "epoch: 18131 loss is tensor([-0.6755], grad_fn=<AddBackward0>)\n",
      "epoch: 18132 loss is tensor([-0.6755], grad_fn=<AddBackward0>)\n",
      "epoch: 18133 loss is tensor([-0.7007], grad_fn=<AddBackward0>)\n",
      "epoch: 18134 loss is tensor([-0.7060], grad_fn=<AddBackward0>)\n",
      "epoch: 18135 loss is tensor([-0.6680], grad_fn=<AddBackward0>)\n",
      "epoch: 18136 loss is tensor([-0.7095], grad_fn=<AddBackward0>)\n",
      "epoch: 18137 loss is tensor([-0.7110], grad_fn=<AddBackward0>)\n",
      "epoch: 18138 loss is tensor([-0.6983], grad_fn=<AddBackward0>)\n",
      "epoch: 18139 loss is tensor([-0.6943], grad_fn=<AddBackward0>)\n",
      "epoch: 18140 loss is tensor([-0.6854], grad_fn=<AddBackward0>)\n",
      "epoch: 18141 loss is tensor([-0.6844], grad_fn=<AddBackward0>)\n",
      "epoch: 18142 loss is tensor([-0.6685], grad_fn=<AddBackward0>)\n",
      "epoch: 18143 loss is tensor([-0.6925], grad_fn=<AddBackward0>)\n",
      "epoch: 18144 loss is tensor([-0.6986], grad_fn=<AddBackward0>)\n",
      "epoch: 18145 loss is tensor([-0.7413], grad_fn=<AddBackward0>)\n",
      "epoch: 18146 loss is tensor([-0.7159], grad_fn=<AddBackward0>)\n",
      "epoch: 18147 loss is tensor([-0.6933], grad_fn=<AddBackward0>)\n",
      "epoch: 18148 loss is tensor([-0.7032], grad_fn=<AddBackward0>)\n",
      "epoch: 18149 loss is tensor([-0.6860], grad_fn=<AddBackward0>)\n",
      "epoch: 18150 loss is tensor([-0.7113], grad_fn=<AddBackward0>)\n",
      "epoch: 18151 loss is tensor([-0.6305], grad_fn=<AddBackward0>)\n",
      "epoch: 18152 loss is tensor([-0.7713], grad_fn=<AddBackward0>)\n",
      "epoch: 18153 loss is tensor([-0.7001], grad_fn=<AddBackward0>)\n",
      "epoch: 18154 loss is tensor([-0.7562], grad_fn=<AddBackward0>)\n",
      "epoch: 18155 loss is tensor([-0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 18156 loss is tensor([-0.6974], grad_fn=<AddBackward0>)\n",
      "epoch: 18157 loss is tensor([-0.7519], grad_fn=<AddBackward0>)\n",
      "epoch: 18158 loss is tensor([-0.6640], grad_fn=<AddBackward0>)\n",
      "epoch: 18159 loss is tensor([-0.6955], grad_fn=<AddBackward0>)\n",
      "epoch: 18160 loss is tensor([-0.7210], grad_fn=<AddBackward0>)\n",
      "epoch: 18161 loss is tensor([-0.7019], grad_fn=<AddBackward0>)\n",
      "epoch: 18162 loss is tensor([-0.7342], grad_fn=<AddBackward0>)\n",
      "epoch: 18163 loss is tensor([-0.6702], grad_fn=<AddBackward0>)\n",
      "epoch: 18164 loss is tensor([-0.7262], grad_fn=<AddBackward0>)\n",
      "epoch: 18165 loss is tensor([-0.7214], grad_fn=<AddBackward0>)\n",
      "epoch: 18166 loss is tensor([-0.6658], grad_fn=<AddBackward0>)\n",
      "epoch: 18167 loss is tensor([-0.6192], grad_fn=<AddBackward0>)\n",
      "epoch: 18168 loss is tensor([-0.6725], grad_fn=<AddBackward0>)\n",
      "epoch: 18169 loss is tensor([-0.6983], grad_fn=<AddBackward0>)\n",
      "epoch: 18170 loss is tensor([-0.7128], grad_fn=<AddBackward0>)\n",
      "epoch: 18171 loss is tensor([-0.7159], grad_fn=<AddBackward0>)\n",
      "epoch: 18172 loss is tensor([-0.7244], grad_fn=<AddBackward0>)\n",
      "epoch: 18173 loss is tensor([-0.7442], grad_fn=<AddBackward0>)\n",
      "epoch: 18174 loss is tensor([-0.7562], grad_fn=<AddBackward0>)\n",
      "epoch: 18175 loss is tensor([-0.6976], grad_fn=<AddBackward0>)\n",
      "epoch: 18176 loss is tensor([-0.6891], grad_fn=<AddBackward0>)\n",
      "epoch: 18177 loss is tensor([-0.7244], grad_fn=<AddBackward0>)\n",
      "epoch: 18178 loss is tensor([-0.7819], grad_fn=<AddBackward0>)\n",
      "epoch: 18179 loss is tensor([-0.6765], grad_fn=<AddBackward0>)\n",
      "epoch: 18180 loss is tensor([-0.7161], grad_fn=<AddBackward0>)\n",
      "epoch: 18181 loss is tensor([-0.7631], grad_fn=<AddBackward0>)\n",
      "epoch: 18182 loss is tensor([-0.7117], grad_fn=<AddBackward0>)\n",
      "epoch: 18183 loss is tensor([-0.7156], grad_fn=<AddBackward0>)\n",
      "epoch: 18184 loss is tensor([-0.6946], grad_fn=<AddBackward0>)\n",
      "epoch: 18185 loss is tensor([-0.7016], grad_fn=<AddBackward0>)\n",
      "epoch: 18186 loss is tensor([-0.6079], grad_fn=<AddBackward0>)\n",
      "epoch: 18187 loss is tensor([-0.7301], grad_fn=<AddBackward0>)\n",
      "epoch: 18188 loss is tensor([-0.7113], grad_fn=<AddBackward0>)\n",
      "epoch: 18189 loss is tensor([-0.7172], grad_fn=<AddBackward0>)\n",
      "epoch: 18190 loss is tensor([-0.6891], grad_fn=<AddBackward0>)\n",
      "epoch: 18191 loss is tensor([-0.6825], grad_fn=<AddBackward0>)\n",
      "epoch: 18192 loss is tensor([-0.7079], grad_fn=<AddBackward0>)\n",
      "epoch: 18193 loss is tensor([-0.6655], grad_fn=<AddBackward0>)\n",
      "epoch: 18194 loss is tensor([-0.7390], grad_fn=<AddBackward0>)\n",
      "epoch: 18195 loss is tensor([-0.6974], grad_fn=<AddBackward0>)\n",
      "epoch: 18196 loss is tensor([-0.6399], grad_fn=<AddBackward0>)\n",
      "epoch: 18197 loss is tensor([-0.6776], grad_fn=<AddBackward0>)\n",
      "epoch: 18198 loss is tensor([-0.7053], grad_fn=<AddBackward0>)\n",
      "epoch: 18199 loss is tensor([-0.6792], grad_fn=<AddBackward0>)\n",
      "epoch: 18200 loss is tensor([-0.7008], grad_fn=<AddBackward0>)\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18201 loss is tensor([-0.7131], grad_fn=<AddBackward0>)\n",
      "epoch: 18202 loss is tensor([-0.7149], grad_fn=<AddBackward0>)\n",
      "epoch: 18203 loss is tensor([-0.7070], grad_fn=<AddBackward0>)\n",
      "epoch: 18204 loss is tensor([-0.6742], grad_fn=<AddBackward0>)\n",
      "epoch: 18205 loss is tensor([-0.6925], grad_fn=<AddBackward0>)\n",
      "epoch: 18206 loss is tensor([-0.7007], grad_fn=<AddBackward0>)\n",
      "epoch: 18207 loss is tensor([-0.6848], grad_fn=<AddBackward0>)\n",
      "epoch: 18208 loss is tensor([-0.6678], grad_fn=<AddBackward0>)\n",
      "epoch: 18209 loss is tensor([-0.6453], grad_fn=<AddBackward0>)\n",
      "epoch: 18210 loss is tensor([-0.6573], grad_fn=<AddBackward0>)\n",
      "epoch: 18211 loss is tensor([-0.7730], grad_fn=<AddBackward0>)\n",
      "epoch: 18212 loss is tensor([-0.6713], grad_fn=<AddBackward0>)\n",
      "epoch: 18213 loss is tensor([-0.6753], grad_fn=<AddBackward0>)\n",
      "epoch: 18214 loss is tensor([-0.6821], grad_fn=<AddBackward0>)\n",
      "epoch: 18215 loss is tensor([-0.6933], grad_fn=<AddBackward0>)\n",
      "epoch: 18216 loss is tensor([-0.6829], grad_fn=<AddBackward0>)\n",
      "epoch: 18217 loss is tensor([-0.7136], grad_fn=<AddBackward0>)\n",
      "epoch: 18218 loss is tensor([-0.6899], grad_fn=<AddBackward0>)\n",
      "epoch: 18219 loss is tensor([-0.7084], grad_fn=<AddBackward0>)\n",
      "epoch: 18220 loss is tensor([-0.7409], grad_fn=<AddBackward0>)\n",
      "epoch: 18221 loss is tensor([-0.7215], grad_fn=<AddBackward0>)\n",
      "epoch: 18222 loss is tensor([-0.7068], grad_fn=<AddBackward0>)\n",
      "epoch: 18223 loss is tensor([-0.6688], grad_fn=<AddBackward0>)\n",
      "epoch: 18224 loss is tensor([-0.6952], grad_fn=<AddBackward0>)\n",
      "epoch: 18225 loss is tensor([-0.6688], grad_fn=<AddBackward0>)\n",
      "epoch: 18226 loss is tensor([-0.7153], grad_fn=<AddBackward0>)\n",
      "epoch: 18227 loss is tensor([-0.7215], grad_fn=<AddBackward0>)\n",
      "epoch: 18228 loss is tensor([-0.7416], grad_fn=<AddBackward0>)\n",
      "epoch: 18229 loss is tensor([-0.6916], grad_fn=<AddBackward0>)\n",
      "epoch: 18230 loss is tensor([-0.6847], grad_fn=<AddBackward0>)\n",
      "epoch: 18231 loss is tensor([-0.7112], grad_fn=<AddBackward0>)\n",
      "epoch: 18232 loss is tensor([-0.7303], grad_fn=<AddBackward0>)\n",
      "epoch: 18233 loss is tensor([-0.7402], grad_fn=<AddBackward0>)\n",
      "epoch: 18234 loss is tensor([-0.6942], grad_fn=<AddBackward0>)\n",
      "epoch: 18235 loss is tensor([-0.7375], grad_fn=<AddBackward0>)\n",
      "epoch: 18236 loss is tensor([-0.6371], grad_fn=<AddBackward0>)\n",
      "epoch: 18237 loss is tensor([-0.6825], grad_fn=<AddBackward0>)\n",
      "epoch: 18238 loss is tensor([-0.7412], grad_fn=<AddBackward0>)\n",
      "epoch: 18239 loss is tensor([-0.6712], grad_fn=<AddBackward0>)\n",
      "epoch: 18240 loss is tensor([-0.6665], grad_fn=<AddBackward0>)\n",
      "epoch: 18241 loss is tensor([-0.7103], grad_fn=<AddBackward0>)\n",
      "epoch: 18242 loss is tensor([-0.6473], grad_fn=<AddBackward0>)\n",
      "epoch: 18243 loss is tensor([-0.6510], grad_fn=<AddBackward0>)\n",
      "epoch: 18244 loss is tensor([-0.6800], grad_fn=<AddBackward0>)\n",
      "epoch: 18245 loss is tensor([-0.6278], grad_fn=<AddBackward0>)\n",
      "epoch: 18246 loss is tensor([-0.6656], grad_fn=<AddBackward0>)\n",
      "epoch: 18247 loss is tensor([-0.7002], grad_fn=<AddBackward0>)\n",
      "epoch: 18248 loss is tensor([-0.7310], grad_fn=<AddBackward0>)\n",
      "epoch: 18249 loss is tensor([-0.7023], grad_fn=<AddBackward0>)\n",
      "epoch: 18250 loss is tensor([-0.6832], grad_fn=<AddBackward0>)\n",
      "epoch: 18251 loss is tensor([-0.6709], grad_fn=<AddBackward0>)\n",
      "epoch: 18252 loss is tensor([-0.6812], grad_fn=<AddBackward0>)\n",
      "epoch: 18253 loss is tensor([-0.7279], grad_fn=<AddBackward0>)\n",
      "epoch: 18254 loss is tensor([-0.7340], grad_fn=<AddBackward0>)\n",
      "epoch: 18255 loss is tensor([-0.7049], grad_fn=<AddBackward0>)\n",
      "epoch: 18256 loss is tensor([-0.7346], grad_fn=<AddBackward0>)\n",
      "epoch: 18257 loss is tensor([-0.7411], grad_fn=<AddBackward0>)\n",
      "epoch: 18258 loss is tensor([-0.6435], grad_fn=<AddBackward0>)\n",
      "epoch: 18259 loss is tensor([-0.7425], grad_fn=<AddBackward0>)\n",
      "epoch: 18260 loss is tensor([-0.7077], grad_fn=<AddBackward0>)\n",
      "epoch: 18261 loss is tensor([-0.7096], grad_fn=<AddBackward0>)\n",
      "epoch: 18262 loss is tensor([-0.7543], grad_fn=<AddBackward0>)\n",
      "epoch: 18263 loss is tensor([-0.7434], grad_fn=<AddBackward0>)\n",
      "epoch: 18264 loss is tensor([-0.7202], grad_fn=<AddBackward0>)\n",
      "epoch: 18265 loss is tensor([-0.6752], grad_fn=<AddBackward0>)\n",
      "epoch: 18266 loss is tensor([-0.7134], grad_fn=<AddBackward0>)\n",
      "epoch: 18267 loss is tensor([-0.7078], grad_fn=<AddBackward0>)\n",
      "epoch: 18268 loss is tensor([-0.7166], grad_fn=<AddBackward0>)\n",
      "epoch: 18269 loss is tensor([-0.7118], grad_fn=<AddBackward0>)\n",
      "epoch: 18270 loss is tensor([-0.6682], grad_fn=<AddBackward0>)\n",
      "epoch: 18271 loss is tensor([-0.7090], grad_fn=<AddBackward0>)\n",
      "epoch: 18272 loss is tensor([-0.7253], grad_fn=<AddBackward0>)\n",
      "epoch: 18273 loss is tensor([-0.7184], grad_fn=<AddBackward0>)\n",
      "epoch: 18274 loss is tensor([-0.6948], grad_fn=<AddBackward0>)\n",
      "epoch: 18275 loss is tensor([-0.6596], grad_fn=<AddBackward0>)\n",
      "epoch: 18276 loss is tensor([-0.6907], grad_fn=<AddBackward0>)\n",
      "epoch: 18277 loss is tensor([-0.7095], grad_fn=<AddBackward0>)\n",
      "epoch: 18278 loss is tensor([-0.6671], grad_fn=<AddBackward0>)\n",
      "epoch: 18279 loss is tensor([-0.6548], grad_fn=<AddBackward0>)\n",
      "epoch: 18280 loss is tensor([-0.7256], grad_fn=<AddBackward0>)\n",
      "epoch: 18281 loss is tensor([-0.7043], grad_fn=<AddBackward0>)\n",
      "epoch: 18282 loss is tensor([-0.7441], grad_fn=<AddBackward0>)\n",
      "epoch: 18283 loss is tensor([-0.6920], grad_fn=<AddBackward0>)\n",
      "epoch: 18284 loss is tensor([-0.7049], grad_fn=<AddBackward0>)\n",
      "epoch: 18285 loss is tensor([-0.6878], grad_fn=<AddBackward0>)\n",
      "epoch: 18286 loss is tensor([-0.7754], grad_fn=<AddBackward0>)\n",
      "epoch: 18287 loss is tensor([-0.6786], grad_fn=<AddBackward0>)\n",
      "epoch: 18288 loss is tensor([-0.7039], grad_fn=<AddBackward0>)\n",
      "epoch: 18289 loss is tensor([-0.6989], grad_fn=<AddBackward0>)\n",
      "epoch: 18290 loss is tensor([-0.6845], grad_fn=<AddBackward0>)\n",
      "epoch: 18291 loss is tensor([-0.6594], grad_fn=<AddBackward0>)\n",
      "epoch: 18292 loss is tensor([-0.7113], grad_fn=<AddBackward0>)\n",
      "epoch: 18293 loss is tensor([-0.6350], grad_fn=<AddBackward0>)\n",
      "epoch: 18294 loss is tensor([-0.7086], grad_fn=<AddBackward0>)\n",
      "epoch: 18295 loss is tensor([-0.6934], grad_fn=<AddBackward0>)\n",
      "epoch: 18296 loss is tensor([-0.5951], grad_fn=<AddBackward0>)\n",
      "epoch: 18297 loss is tensor([-0.6245], grad_fn=<AddBackward0>)\n",
      "epoch: 18298 loss is tensor([-0.6711], grad_fn=<AddBackward0>)\n",
      "epoch: 18299 loss is tensor([-0.6623], grad_fn=<AddBackward0>)\n",
      "epoch: 18300 loss is tensor([-0.6431], grad_fn=<AddBackward0>)\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18301 loss is tensor([-0.6417], grad_fn=<AddBackward0>)\n",
      "epoch: 18302 loss is tensor([-0.6336], grad_fn=<AddBackward0>)\n",
      "epoch: 18303 loss is tensor([-0.6339], grad_fn=<AddBackward0>)\n",
      "epoch: 18304 loss is tensor([-0.6419], grad_fn=<AddBackward0>)\n",
      "epoch: 18305 loss is tensor([-0.6754], grad_fn=<AddBackward0>)\n",
      "epoch: 18306 loss is tensor([-0.6532], grad_fn=<AddBackward0>)\n",
      "epoch: 18307 loss is tensor([-0.6857], grad_fn=<AddBackward0>)\n",
      "epoch: 18308 loss is tensor([-0.6699], grad_fn=<AddBackward0>)\n",
      "epoch: 18309 loss is tensor([-0.6676], grad_fn=<AddBackward0>)\n",
      "epoch: 18310 loss is tensor([-0.6689], grad_fn=<AddBackward0>)\n",
      "epoch: 18311 loss is tensor([-0.6409], grad_fn=<AddBackward0>)\n",
      "epoch: 18312 loss is tensor([-0.6243], grad_fn=<AddBackward0>)\n",
      "epoch: 18313 loss is tensor([-0.6815], grad_fn=<AddBackward0>)\n",
      "epoch: 18314 loss is tensor([-0.6878], grad_fn=<AddBackward0>)\n",
      "epoch: 18315 loss is tensor([-0.7193], grad_fn=<AddBackward0>)\n",
      "epoch: 18316 loss is tensor([-0.6358], grad_fn=<AddBackward0>)\n",
      "epoch: 18317 loss is tensor([-0.6463], grad_fn=<AddBackward0>)\n",
      "epoch: 18318 loss is tensor([-0.6935], grad_fn=<AddBackward0>)\n",
      "epoch: 18319 loss is tensor([-0.6768], grad_fn=<AddBackward0>)\n",
      "epoch: 18320 loss is tensor([-0.6167], grad_fn=<AddBackward0>)\n",
      "epoch: 18321 loss is tensor([-0.7044], grad_fn=<AddBackward0>)\n",
      "epoch: 18322 loss is tensor([-0.6091], grad_fn=<AddBackward0>)\n",
      "epoch: 18323 loss is tensor([-0.7015], grad_fn=<AddBackward0>)\n",
      "epoch: 18324 loss is tensor([-0.6684], grad_fn=<AddBackward0>)\n",
      "epoch: 18325 loss is tensor([-0.6902], grad_fn=<AddBackward0>)\n",
      "epoch: 18326 loss is tensor([-0.6342], grad_fn=<AddBackward0>)\n",
      "epoch: 18327 loss is tensor([-0.6470], grad_fn=<AddBackward0>)\n",
      "epoch: 18328 loss is tensor([-0.7202], grad_fn=<AddBackward0>)\n",
      "epoch: 18329 loss is tensor([-0.6354], grad_fn=<AddBackward0>)\n",
      "epoch: 18330 loss is tensor([-0.6603], grad_fn=<AddBackward0>)\n",
      "epoch: 18331 loss is tensor([-0.6553], grad_fn=<AddBackward0>)\n",
      "epoch: 18332 loss is tensor([-0.7086], grad_fn=<AddBackward0>)\n",
      "epoch: 18333 loss is tensor([-0.7398], grad_fn=<AddBackward0>)\n",
      "epoch: 18334 loss is tensor([-0.7482], grad_fn=<AddBackward0>)\n",
      "epoch: 18335 loss is tensor([-0.6982], grad_fn=<AddBackward0>)\n",
      "epoch: 18336 loss is tensor([-0.7354], grad_fn=<AddBackward0>)\n",
      "epoch: 18337 loss is tensor([-0.6902], grad_fn=<AddBackward0>)\n",
      "epoch: 18338 loss is tensor([-0.7028], grad_fn=<AddBackward0>)\n",
      "epoch: 18339 loss is tensor([-0.7281], grad_fn=<AddBackward0>)\n",
      "epoch: 18340 loss is tensor([-0.6739], grad_fn=<AddBackward0>)\n",
      "epoch: 18341 loss is tensor([-0.6300], grad_fn=<AddBackward0>)\n",
      "epoch: 18342 loss is tensor([-0.5708], grad_fn=<AddBackward0>)\n",
      "epoch: 18343 loss is tensor([-0.7148], grad_fn=<AddBackward0>)\n",
      "epoch: 18344 loss is tensor([-0.6107], grad_fn=<AddBackward0>)\n",
      "epoch: 18345 loss is tensor([-0.6075], grad_fn=<AddBackward0>)\n",
      "epoch: 18346 loss is tensor([-0.6247], grad_fn=<AddBackward0>)\n",
      "epoch: 18347 loss is tensor([-0.6728], grad_fn=<AddBackward0>)\n",
      "epoch: 18348 loss is tensor([-0.6237], grad_fn=<AddBackward0>)\n",
      "epoch: 18349 loss is tensor([-0.6384], grad_fn=<AddBackward0>)\n",
      "epoch: 18350 loss is tensor([-0.6865], grad_fn=<AddBackward0>)\n",
      "epoch: 18351 loss is tensor([-0.6939], grad_fn=<AddBackward0>)\n",
      "epoch: 18352 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 18353 loss is tensor([-0.7286], grad_fn=<AddBackward0>)\n",
      "epoch: 18354 loss is tensor([-0.7087], grad_fn=<AddBackward0>)\n",
      "epoch: 18355 loss is tensor([-0.6442], grad_fn=<AddBackward0>)\n",
      "epoch: 18356 loss is tensor([-0.6688], grad_fn=<AddBackward0>)\n",
      "epoch: 18357 loss is tensor([-0.5576], grad_fn=<AddBackward0>)\n",
      "epoch: 18358 loss is tensor([-0.6438], grad_fn=<AddBackward0>)\n",
      "epoch: 18359 loss is tensor([-0.6486], grad_fn=<AddBackward0>)\n",
      "epoch: 18360 loss is tensor([-0.5992], grad_fn=<AddBackward0>)\n",
      "epoch: 18361 loss is tensor([-0.6123], grad_fn=<AddBackward0>)\n",
      "epoch: 18362 loss is tensor([-0.6326], grad_fn=<AddBackward0>)\n",
      "epoch: 18363 loss is tensor([-0.6857], grad_fn=<AddBackward0>)\n",
      "epoch: 18364 loss is tensor([-0.6361], grad_fn=<AddBackward0>)\n",
      "epoch: 18365 loss is tensor([-0.6722], grad_fn=<AddBackward0>)\n",
      "epoch: 18366 loss is tensor([-0.7262], grad_fn=<AddBackward0>)\n",
      "epoch: 18367 loss is tensor([-0.6791], grad_fn=<AddBackward0>)\n",
      "epoch: 18368 loss is tensor([-0.6684], grad_fn=<AddBackward0>)\n",
      "epoch: 18369 loss is tensor([-0.7063], grad_fn=<AddBackward0>)\n",
      "epoch: 18370 loss is tensor([-0.6689], grad_fn=<AddBackward0>)\n",
      "epoch: 18371 loss is tensor([-0.6838], grad_fn=<AddBackward0>)\n",
      "epoch: 18372 loss is tensor([-0.6409], grad_fn=<AddBackward0>)\n",
      "epoch: 18373 loss is tensor([-0.6754], grad_fn=<AddBackward0>)\n",
      "epoch: 18374 loss is tensor([-0.7001], grad_fn=<AddBackward0>)\n",
      "epoch: 18375 loss is tensor([-0.7622], grad_fn=<AddBackward0>)\n",
      "epoch: 18376 loss is tensor([-0.7005], grad_fn=<AddBackward0>)\n",
      "epoch: 18377 loss is tensor([-0.6647], grad_fn=<AddBackward0>)\n",
      "epoch: 18378 loss is tensor([-0.6965], grad_fn=<AddBackward0>)\n",
      "epoch: 18379 loss is tensor([-0.7044], grad_fn=<AddBackward0>)\n",
      "epoch: 18380 loss is tensor([-0.6858], grad_fn=<AddBackward0>)\n",
      "epoch: 18381 loss is tensor([-0.6698], grad_fn=<AddBackward0>)\n",
      "epoch: 18382 loss is tensor([-0.7054], grad_fn=<AddBackward0>)\n",
      "epoch: 18383 loss is tensor([-0.7350], grad_fn=<AddBackward0>)\n",
      "epoch: 18384 loss is tensor([-0.7329], grad_fn=<AddBackward0>)\n",
      "epoch: 18385 loss is tensor([-0.6510], grad_fn=<AddBackward0>)\n",
      "epoch: 18386 loss is tensor([-0.6559], grad_fn=<AddBackward0>)\n",
      "epoch: 18387 loss is tensor([-0.7317], grad_fn=<AddBackward0>)\n",
      "epoch: 18388 loss is tensor([-0.6882], grad_fn=<AddBackward0>)\n",
      "epoch: 18389 loss is tensor([-0.6945], grad_fn=<AddBackward0>)\n",
      "epoch: 18390 loss is tensor([-0.6950], grad_fn=<AddBackward0>)\n",
      "epoch: 18391 loss is tensor([-0.7120], grad_fn=<AddBackward0>)\n",
      "epoch: 18392 loss is tensor([-0.6838], grad_fn=<AddBackward0>)\n",
      "epoch: 18393 loss is tensor([-0.6880], grad_fn=<AddBackward0>)\n",
      "epoch: 18394 loss is tensor([-0.7178], grad_fn=<AddBackward0>)\n",
      "epoch: 18395 loss is tensor([-0.7151], grad_fn=<AddBackward0>)\n",
      "epoch: 18396 loss is tensor([-0.7389], grad_fn=<AddBackward0>)\n",
      "epoch: 18397 loss is tensor([-0.6632], grad_fn=<AddBackward0>)\n",
      "epoch: 18398 loss is tensor([-0.6902], grad_fn=<AddBackward0>)\n",
      "epoch: 18399 loss is tensor([-0.6713], grad_fn=<AddBackward0>)\n",
      "epoch: 18400 loss is tensor([-0.6840], grad_fn=<AddBackward0>)\n",
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18401 loss is tensor([-0.7298], grad_fn=<AddBackward0>)\n",
      "epoch: 18402 loss is tensor([-0.7479], grad_fn=<AddBackward0>)\n",
      "epoch: 18403 loss is tensor([-0.7155], grad_fn=<AddBackward0>)\n",
      "epoch: 18404 loss is tensor([-0.7348], grad_fn=<AddBackward0>)\n",
      "epoch: 18405 loss is tensor([-0.7270], grad_fn=<AddBackward0>)\n",
      "epoch: 18406 loss is tensor([-0.7147], grad_fn=<AddBackward0>)\n",
      "epoch: 18407 loss is tensor([-0.7152], grad_fn=<AddBackward0>)\n",
      "epoch: 18408 loss is tensor([-0.7000], grad_fn=<AddBackward0>)\n",
      "epoch: 18409 loss is tensor([-0.6686], grad_fn=<AddBackward0>)\n",
      "epoch: 18410 loss is tensor([-0.6816], grad_fn=<AddBackward0>)\n",
      "epoch: 18411 loss is tensor([-0.6923], grad_fn=<AddBackward0>)\n",
      "epoch: 18412 loss is tensor([-0.6699], grad_fn=<AddBackward0>)\n",
      "epoch: 18413 loss is tensor([-0.7258], grad_fn=<AddBackward0>)\n",
      "epoch: 18414 loss is tensor([-0.6974], grad_fn=<AddBackward0>)\n",
      "epoch: 18415 loss is tensor([-0.6740], grad_fn=<AddBackward0>)\n",
      "epoch: 18416 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 18417 loss is tensor([-0.6837], grad_fn=<AddBackward0>)\n",
      "epoch: 18418 loss is tensor([-0.7120], grad_fn=<AddBackward0>)\n",
      "epoch: 18419 loss is tensor([-0.6910], grad_fn=<AddBackward0>)\n",
      "epoch: 18420 loss is tensor([-0.7102], grad_fn=<AddBackward0>)\n",
      "epoch: 18421 loss is tensor([-0.7241], grad_fn=<AddBackward0>)\n",
      "epoch: 18422 loss is tensor([-0.7381], grad_fn=<AddBackward0>)\n",
      "epoch: 18423 loss is tensor([-0.7324], grad_fn=<AddBackward0>)\n",
      "epoch: 18424 loss is tensor([-0.6449], grad_fn=<AddBackward0>)\n",
      "epoch: 18425 loss is tensor([-0.7293], grad_fn=<AddBackward0>)\n",
      "epoch: 18426 loss is tensor([-0.6757], grad_fn=<AddBackward0>)\n",
      "epoch: 18427 loss is tensor([-0.7075], grad_fn=<AddBackward0>)\n",
      "epoch: 18428 loss is tensor([-0.6564], grad_fn=<AddBackward0>)\n",
      "epoch: 18429 loss is tensor([-0.6714], grad_fn=<AddBackward0>)\n",
      "epoch: 18430 loss is tensor([-0.7089], grad_fn=<AddBackward0>)\n",
      "epoch: 18431 loss is tensor([-0.7000], grad_fn=<AddBackward0>)\n",
      "epoch: 18432 loss is tensor([-0.7053], grad_fn=<AddBackward0>)\n",
      "epoch: 18433 loss is tensor([-0.7292], grad_fn=<AddBackward0>)\n",
      "epoch: 18434 loss is tensor([-0.6225], grad_fn=<AddBackward0>)\n",
      "epoch: 18435 loss is tensor([-0.7659], grad_fn=<AddBackward0>)\n",
      "epoch: 18436 loss is tensor([-0.7373], grad_fn=<AddBackward0>)\n",
      "epoch: 18437 loss is tensor([-0.6945], grad_fn=<AddBackward0>)\n",
      "epoch: 18438 loss is tensor([-0.7004], grad_fn=<AddBackward0>)\n",
      "epoch: 18439 loss is tensor([-0.7586], grad_fn=<AddBackward0>)\n",
      "epoch: 18440 loss is tensor([-0.6971], grad_fn=<AddBackward0>)\n",
      "epoch: 18441 loss is tensor([-0.6962], grad_fn=<AddBackward0>)\n",
      "epoch: 18442 loss is tensor([-0.7072], grad_fn=<AddBackward0>)\n",
      "epoch: 18443 loss is tensor([-0.7147], grad_fn=<AddBackward0>)\n",
      "epoch: 18444 loss is tensor([-0.6922], grad_fn=<AddBackward0>)\n",
      "epoch: 18445 loss is tensor([-0.7069], grad_fn=<AddBackward0>)\n",
      "epoch: 18446 loss is tensor([-0.7081], grad_fn=<AddBackward0>)\n",
      "epoch: 18447 loss is tensor([-0.6872], grad_fn=<AddBackward0>)\n",
      "epoch: 18448 loss is tensor([-0.7491], grad_fn=<AddBackward0>)\n",
      "epoch: 18449 loss is tensor([-0.6575], grad_fn=<AddBackward0>)\n",
      "epoch: 18450 loss is tensor([-0.7080], grad_fn=<AddBackward0>)\n",
      "epoch: 18451 loss is tensor([-0.6676], grad_fn=<AddBackward0>)\n",
      "epoch: 18452 loss is tensor([-0.7148], grad_fn=<AddBackward0>)\n",
      "epoch: 18453 loss is tensor([-0.7443], grad_fn=<AddBackward0>)\n",
      "epoch: 18454 loss is tensor([-0.7400], grad_fn=<AddBackward0>)\n",
      "epoch: 18455 loss is tensor([-0.7094], grad_fn=<AddBackward0>)\n",
      "epoch: 18456 loss is tensor([-0.6948], grad_fn=<AddBackward0>)\n",
      "epoch: 18457 loss is tensor([-0.7020], grad_fn=<AddBackward0>)\n",
      "epoch: 18458 loss is tensor([-0.7280], grad_fn=<AddBackward0>)\n",
      "epoch: 18459 loss is tensor([-0.6822], grad_fn=<AddBackward0>)\n",
      "epoch: 18460 loss is tensor([-0.7002], grad_fn=<AddBackward0>)\n",
      "epoch: 18461 loss is tensor([-0.6908], grad_fn=<AddBackward0>)\n",
      "epoch: 18462 loss is tensor([-0.6976], grad_fn=<AddBackward0>)\n",
      "epoch: 18463 loss is tensor([-0.7645], grad_fn=<AddBackward0>)\n",
      "epoch: 18464 loss is tensor([-0.7675], grad_fn=<AddBackward0>)\n",
      "epoch: 18465 loss is tensor([-0.7213], grad_fn=<AddBackward0>)\n",
      "epoch: 18466 loss is tensor([-0.7229], grad_fn=<AddBackward0>)\n",
      "epoch: 18467 loss is tensor([-0.7116], grad_fn=<AddBackward0>)\n",
      "epoch: 18468 loss is tensor([-0.7269], grad_fn=<AddBackward0>)\n",
      "epoch: 18469 loss is tensor([-0.7033], grad_fn=<AddBackward0>)\n",
      "epoch: 18470 loss is tensor([-0.6979], grad_fn=<AddBackward0>)\n",
      "epoch: 18471 loss is tensor([-0.6536], grad_fn=<AddBackward0>)\n",
      "epoch: 18472 loss is tensor([-0.6909], grad_fn=<AddBackward0>)\n",
      "epoch: 18473 loss is tensor([-0.6991], grad_fn=<AddBackward0>)\n",
      "epoch: 18474 loss is tensor([-0.7244], grad_fn=<AddBackward0>)\n",
      "epoch: 18475 loss is tensor([-0.6953], grad_fn=<AddBackward0>)\n",
      "epoch: 18476 loss is tensor([-0.6922], grad_fn=<AddBackward0>)\n",
      "epoch: 18477 loss is tensor([-0.7329], grad_fn=<AddBackward0>)\n",
      "epoch: 18478 loss is tensor([-0.6996], grad_fn=<AddBackward0>)\n",
      "epoch: 18479 loss is tensor([-0.6963], grad_fn=<AddBackward0>)\n",
      "epoch: 18480 loss is tensor([-0.7540], grad_fn=<AddBackward0>)\n",
      "epoch: 18481 loss is tensor([-0.7534], grad_fn=<AddBackward0>)\n",
      "epoch: 18482 loss is tensor([-0.7205], grad_fn=<AddBackward0>)\n",
      "epoch: 18483 loss is tensor([-0.6789], grad_fn=<AddBackward0>)\n",
      "epoch: 18484 loss is tensor([-0.7291], grad_fn=<AddBackward0>)\n",
      "epoch: 18485 loss is tensor([-0.7144], grad_fn=<AddBackward0>)\n",
      "epoch: 18486 loss is tensor([-0.7031], grad_fn=<AddBackward0>)\n",
      "epoch: 18487 loss is tensor([-0.7200], grad_fn=<AddBackward0>)\n",
      "epoch: 18488 loss is tensor([-0.6533], grad_fn=<AddBackward0>)\n",
      "epoch: 18489 loss is tensor([-0.7358], grad_fn=<AddBackward0>)\n",
      "epoch: 18490 loss is tensor([-0.6829], grad_fn=<AddBackward0>)\n",
      "epoch: 18491 loss is tensor([-0.7444], grad_fn=<AddBackward0>)\n",
      "epoch: 18492 loss is tensor([-0.7043], grad_fn=<AddBackward0>)\n",
      "epoch: 18493 loss is tensor([-0.6554], grad_fn=<AddBackward0>)\n",
      "epoch: 18494 loss is tensor([-0.7003], grad_fn=<AddBackward0>)\n",
      "epoch: 18495 loss is tensor([-0.6283], grad_fn=<AddBackward0>)\n",
      "epoch: 18496 loss is tensor([-0.7219], grad_fn=<AddBackward0>)\n",
      "epoch: 18497 loss is tensor([-0.6884], grad_fn=<AddBackward0>)\n",
      "epoch: 18498 loss is tensor([-0.6837], grad_fn=<AddBackward0>)\n",
      "epoch: 18499 loss is tensor([-0.6904], grad_fn=<AddBackward0>)\n",
      "epoch: 18500 loss is tensor([-0.6587], grad_fn=<AddBackward0>)\n",
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18501 loss is tensor([-0.6863], grad_fn=<AddBackward0>)\n",
      "epoch: 18502 loss is tensor([-0.7101], grad_fn=<AddBackward0>)\n",
      "epoch: 18503 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 18504 loss is tensor([-0.7488], grad_fn=<AddBackward0>)\n",
      "epoch: 18505 loss is tensor([-0.6726], grad_fn=<AddBackward0>)\n",
      "epoch: 18506 loss is tensor([-0.6913], grad_fn=<AddBackward0>)\n",
      "epoch: 18507 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 18508 loss is tensor([-0.6968], grad_fn=<AddBackward0>)\n",
      "epoch: 18509 loss is tensor([-0.7211], grad_fn=<AddBackward0>)\n",
      "epoch: 18510 loss is tensor([-0.6973], grad_fn=<AddBackward0>)\n",
      "epoch: 18511 loss is tensor([-0.6839], grad_fn=<AddBackward0>)\n",
      "epoch: 18512 loss is tensor([-0.7451], grad_fn=<AddBackward0>)\n",
      "epoch: 18513 loss is tensor([-0.7148], grad_fn=<AddBackward0>)\n",
      "epoch: 18514 loss is tensor([-0.7471], grad_fn=<AddBackward0>)\n",
      "epoch: 18515 loss is tensor([-0.6648], grad_fn=<AddBackward0>)\n",
      "epoch: 18516 loss is tensor([-0.7387], grad_fn=<AddBackward0>)\n",
      "epoch: 18517 loss is tensor([-0.7854], grad_fn=<AddBackward0>)\n",
      "epoch: 18518 loss is tensor([-0.7105], grad_fn=<AddBackward0>)\n",
      "epoch: 18519 loss is tensor([-0.7027], grad_fn=<AddBackward0>)\n",
      "epoch: 18520 loss is tensor([-0.7617], grad_fn=<AddBackward0>)\n",
      "epoch: 18521 loss is tensor([-0.7670], grad_fn=<AddBackward0>)\n",
      "epoch: 18522 loss is tensor([-0.7528], grad_fn=<AddBackward0>)\n",
      "epoch: 18523 loss is tensor([-0.7256], grad_fn=<AddBackward0>)\n",
      "epoch: 18524 loss is tensor([-0.7709], grad_fn=<AddBackward0>)\n",
      "epoch: 18525 loss is tensor([-0.7253], grad_fn=<AddBackward0>)\n",
      "epoch: 18526 loss is tensor([-0.7336], grad_fn=<AddBackward0>)\n",
      "epoch: 18527 loss is tensor([-0.7755], grad_fn=<AddBackward0>)\n",
      "epoch: 18528 loss is tensor([-0.7950], grad_fn=<AddBackward0>)\n",
      "epoch: 18529 loss is tensor([-0.7950], grad_fn=<AddBackward0>)\n",
      "epoch: 18530 loss is tensor([-0.7523], grad_fn=<AddBackward0>)\n",
      "epoch: 18531 loss is tensor([-0.7450], grad_fn=<AddBackward0>)\n",
      "epoch: 18532 loss is tensor([-0.6860], grad_fn=<AddBackward0>)\n",
      "epoch: 18533 loss is tensor([-0.7097], grad_fn=<AddBackward0>)\n",
      "epoch: 18534 loss is tensor([-0.7321], grad_fn=<AddBackward0>)\n",
      "epoch: 18535 loss is tensor([-0.6753], grad_fn=<AddBackward0>)\n",
      "epoch: 18536 loss is tensor([-0.6918], grad_fn=<AddBackward0>)\n",
      "epoch: 18537 loss is tensor([-0.7414], grad_fn=<AddBackward0>)\n",
      "epoch: 18538 loss is tensor([-0.6844], grad_fn=<AddBackward0>)\n",
      "epoch: 18539 loss is tensor([-0.6670], grad_fn=<AddBackward0>)\n",
      "epoch: 18540 loss is tensor([-0.7701], grad_fn=<AddBackward0>)\n",
      "epoch: 18541 loss is tensor([-0.7426], grad_fn=<AddBackward0>)\n",
      "epoch: 18542 loss is tensor([-0.7386], grad_fn=<AddBackward0>)\n",
      "epoch: 18543 loss is tensor([-0.7356], grad_fn=<AddBackward0>)\n",
      "epoch: 18544 loss is tensor([-0.7496], grad_fn=<AddBackward0>)\n",
      "epoch: 18545 loss is tensor([-0.7195], grad_fn=<AddBackward0>)\n",
      "epoch: 18546 loss is tensor([-0.7157], grad_fn=<AddBackward0>)\n",
      "epoch: 18547 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 18548 loss is tensor([-0.7315], grad_fn=<AddBackward0>)\n",
      "epoch: 18549 loss is tensor([-0.6586], grad_fn=<AddBackward0>)\n",
      "epoch: 18550 loss is tensor([-0.7081], grad_fn=<AddBackward0>)\n",
      "epoch: 18551 loss is tensor([-0.6744], grad_fn=<AddBackward0>)\n",
      "epoch: 18552 loss is tensor([-0.6764], grad_fn=<AddBackward0>)\n",
      "epoch: 18553 loss is tensor([-0.7072], grad_fn=<AddBackward0>)\n",
      "epoch: 18554 loss is tensor([-0.6601], grad_fn=<AddBackward0>)\n",
      "epoch: 18555 loss is tensor([-0.6643], grad_fn=<AddBackward0>)\n",
      "epoch: 18556 loss is tensor([-0.7318], grad_fn=<AddBackward0>)\n",
      "epoch: 18557 loss is tensor([-0.7020], grad_fn=<AddBackward0>)\n",
      "epoch: 18558 loss is tensor([-0.6783], grad_fn=<AddBackward0>)\n",
      "epoch: 18559 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 18560 loss is tensor([-0.6809], grad_fn=<AddBackward0>)\n",
      "epoch: 18561 loss is tensor([-0.7244], grad_fn=<AddBackward0>)\n",
      "epoch: 18562 loss is tensor([-0.6938], grad_fn=<AddBackward0>)\n",
      "epoch: 18563 loss is tensor([-0.7082], grad_fn=<AddBackward0>)\n",
      "epoch: 18564 loss is tensor([-0.7276], grad_fn=<AddBackward0>)\n",
      "epoch: 18565 loss is tensor([-0.7017], grad_fn=<AddBackward0>)\n",
      "epoch: 18566 loss is tensor([-0.7188], grad_fn=<AddBackward0>)\n",
      "epoch: 18567 loss is tensor([-0.7280], grad_fn=<AddBackward0>)\n",
      "epoch: 18568 loss is tensor([-0.7141], grad_fn=<AddBackward0>)\n",
      "epoch: 18569 loss is tensor([-0.7175], grad_fn=<AddBackward0>)\n",
      "epoch: 18570 loss is tensor([-0.6927], grad_fn=<AddBackward0>)\n",
      "epoch: 18571 loss is tensor([-0.7436], grad_fn=<AddBackward0>)\n",
      "epoch: 18572 loss is tensor([-0.6773], grad_fn=<AddBackward0>)\n",
      "epoch: 18573 loss is tensor([-0.6974], grad_fn=<AddBackward0>)\n",
      "epoch: 18574 loss is tensor([-0.6925], grad_fn=<AddBackward0>)\n",
      "epoch: 18575 loss is tensor([-0.6729], grad_fn=<AddBackward0>)\n",
      "epoch: 18576 loss is tensor([-0.7416], grad_fn=<AddBackward0>)\n",
      "epoch: 18577 loss is tensor([-0.6698], grad_fn=<AddBackward0>)\n",
      "epoch: 18578 loss is tensor([-0.7070], grad_fn=<AddBackward0>)\n",
      "epoch: 18579 loss is tensor([-0.6808], grad_fn=<AddBackward0>)\n",
      "epoch: 18580 loss is tensor([-0.7239], grad_fn=<AddBackward0>)\n",
      "epoch: 18581 loss is tensor([-0.7331], grad_fn=<AddBackward0>)\n",
      "epoch: 18582 loss is tensor([-0.7117], grad_fn=<AddBackward0>)\n",
      "epoch: 18583 loss is tensor([-0.6934], grad_fn=<AddBackward0>)\n",
      "epoch: 18584 loss is tensor([-0.7185], grad_fn=<AddBackward0>)\n",
      "epoch: 18585 loss is tensor([-0.7139], grad_fn=<AddBackward0>)\n",
      "epoch: 18586 loss is tensor([-0.7178], grad_fn=<AddBackward0>)\n",
      "epoch: 18587 loss is tensor([-0.7009], grad_fn=<AddBackward0>)\n",
      "epoch: 18588 loss is tensor([-0.7096], grad_fn=<AddBackward0>)\n",
      "epoch: 18589 loss is tensor([-0.7529], grad_fn=<AddBackward0>)\n",
      "epoch: 18590 loss is tensor([-0.7394], grad_fn=<AddBackward0>)\n",
      "epoch: 18591 loss is tensor([-0.7304], grad_fn=<AddBackward0>)\n",
      "epoch: 18592 loss is tensor([-0.6938], grad_fn=<AddBackward0>)\n",
      "epoch: 18593 loss is tensor([-0.6928], grad_fn=<AddBackward0>)\n",
      "epoch: 18594 loss is tensor([-0.6803], grad_fn=<AddBackward0>)\n",
      "epoch: 18595 loss is tensor([-0.7394], grad_fn=<AddBackward0>)\n",
      "epoch: 18596 loss is tensor([-0.7092], grad_fn=<AddBackward0>)\n",
      "epoch: 18597 loss is tensor([-0.7262], grad_fn=<AddBackward0>)\n",
      "epoch: 18598 loss is tensor([-0.7336], grad_fn=<AddBackward0>)\n",
      "epoch: 18599 loss is tensor([-0.7320], grad_fn=<AddBackward0>)\n",
      "epoch: 18600 loss is tensor([-0.7040], grad_fn=<AddBackward0>)\n",
      "36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18601 loss is tensor([-0.7643], grad_fn=<AddBackward0>)\n",
      "epoch: 18602 loss is tensor([-0.7440], grad_fn=<AddBackward0>)\n",
      "epoch: 18603 loss is tensor([-0.7341], grad_fn=<AddBackward0>)\n",
      "epoch: 18604 loss is tensor([-0.6885], grad_fn=<AddBackward0>)\n",
      "epoch: 18605 loss is tensor([-0.7337], grad_fn=<AddBackward0>)\n",
      "epoch: 18606 loss is tensor([-0.7309], grad_fn=<AddBackward0>)\n",
      "epoch: 18607 loss is tensor([-0.6545], grad_fn=<AddBackward0>)\n",
      "epoch: 18608 loss is tensor([-0.6680], grad_fn=<AddBackward0>)\n",
      "epoch: 18609 loss is tensor([-0.6838], grad_fn=<AddBackward0>)\n",
      "epoch: 18610 loss is tensor([-0.6623], grad_fn=<AddBackward0>)\n",
      "epoch: 18611 loss is tensor([-0.6578], grad_fn=<AddBackward0>)\n",
      "epoch: 18612 loss is tensor([-0.7327], grad_fn=<AddBackward0>)\n",
      "epoch: 18613 loss is tensor([-0.6863], grad_fn=<AddBackward0>)\n",
      "epoch: 18614 loss is tensor([-0.6318], grad_fn=<AddBackward0>)\n",
      "epoch: 18615 loss is tensor([-0.7099], grad_fn=<AddBackward0>)\n",
      "epoch: 18616 loss is tensor([-0.6601], grad_fn=<AddBackward0>)\n",
      "epoch: 18617 loss is tensor([-0.6653], grad_fn=<AddBackward0>)\n",
      "epoch: 18618 loss is tensor([-0.6611], grad_fn=<AddBackward0>)\n",
      "epoch: 18619 loss is tensor([-0.6656], grad_fn=<AddBackward0>)\n",
      "epoch: 18620 loss is tensor([-0.7442], grad_fn=<AddBackward0>)\n",
      "epoch: 18621 loss is tensor([-0.6817], grad_fn=<AddBackward0>)\n",
      "epoch: 18622 loss is tensor([-0.6981], grad_fn=<AddBackward0>)\n",
      "epoch: 18623 loss is tensor([-0.6406], grad_fn=<AddBackward0>)\n",
      "epoch: 18624 loss is tensor([-0.7116], grad_fn=<AddBackward0>)\n",
      "epoch: 18625 loss is tensor([-0.6584], grad_fn=<AddBackward0>)\n",
      "epoch: 18626 loss is tensor([-0.5962], grad_fn=<AddBackward0>)\n",
      "epoch: 18627 loss is tensor([-0.6221], grad_fn=<AddBackward0>)\n",
      "epoch: 18628 loss is tensor([-0.6755], grad_fn=<AddBackward0>)\n",
      "epoch: 18629 loss is tensor([-0.6630], grad_fn=<AddBackward0>)\n",
      "epoch: 18630 loss is tensor([-0.6684], grad_fn=<AddBackward0>)\n",
      "epoch: 18631 loss is tensor([-0.6754], grad_fn=<AddBackward0>)\n",
      "epoch: 18632 loss is tensor([-0.7052], grad_fn=<AddBackward0>)\n",
      "epoch: 18633 loss is tensor([-0.7196], grad_fn=<AddBackward0>)\n",
      "epoch: 18634 loss is tensor([-0.7449], grad_fn=<AddBackward0>)\n",
      "epoch: 18635 loss is tensor([-0.7192], grad_fn=<AddBackward0>)\n",
      "epoch: 18636 loss is tensor([-0.7456], grad_fn=<AddBackward0>)\n",
      "epoch: 18637 loss is tensor([-0.7097], grad_fn=<AddBackward0>)\n",
      "epoch: 18638 loss is tensor([-0.7575], grad_fn=<AddBackward0>)\n",
      "epoch: 18639 loss is tensor([-0.6914], grad_fn=<AddBackward0>)\n",
      "epoch: 18640 loss is tensor([-0.7039], grad_fn=<AddBackward0>)\n",
      "epoch: 18641 loss is tensor([-0.7275], grad_fn=<AddBackward0>)\n",
      "epoch: 18642 loss is tensor([-0.7190], grad_fn=<AddBackward0>)\n",
      "epoch: 18643 loss is tensor([-0.6751], grad_fn=<AddBackward0>)\n",
      "epoch: 18644 loss is tensor([-0.6935], grad_fn=<AddBackward0>)\n",
      "epoch: 18645 loss is tensor([-0.7659], grad_fn=<AddBackward0>)\n",
      "epoch: 18646 loss is tensor([-0.7677], grad_fn=<AddBackward0>)\n",
      "epoch: 18647 loss is tensor([-0.7051], grad_fn=<AddBackward0>)\n",
      "epoch: 18648 loss is tensor([-0.7103], grad_fn=<AddBackward0>)\n",
      "epoch: 18649 loss is tensor([-0.7008], grad_fn=<AddBackward0>)\n",
      "epoch: 18650 loss is tensor([-0.7146], grad_fn=<AddBackward0>)\n",
      "epoch: 18651 loss is tensor([-0.6568], grad_fn=<AddBackward0>)\n",
      "epoch: 18652 loss is tensor([-0.6863], grad_fn=<AddBackward0>)\n",
      "epoch: 18653 loss is tensor([-0.7260], grad_fn=<AddBackward0>)\n",
      "epoch: 18654 loss is tensor([-0.7661], grad_fn=<AddBackward0>)\n",
      "epoch: 18655 loss is tensor([-0.7417], grad_fn=<AddBackward0>)\n",
      "epoch: 18656 loss is tensor([-0.6944], grad_fn=<AddBackward0>)\n",
      "epoch: 18657 loss is tensor([-0.7347], grad_fn=<AddBackward0>)\n",
      "epoch: 18658 loss is tensor([-0.7455], grad_fn=<AddBackward0>)\n",
      "epoch: 18659 loss is tensor([-0.7599], grad_fn=<AddBackward0>)\n",
      "epoch: 18660 loss is tensor([-0.7114], grad_fn=<AddBackward0>)\n",
      "epoch: 18661 loss is tensor([-0.7368], grad_fn=<AddBackward0>)\n",
      "epoch: 18662 loss is tensor([-0.6992], grad_fn=<AddBackward0>)\n",
      "epoch: 18663 loss is tensor([-0.7883], grad_fn=<AddBackward0>)\n",
      "epoch: 18664 loss is tensor([-0.7372], grad_fn=<AddBackward0>)\n",
      "epoch: 18665 loss is tensor([-0.7151], grad_fn=<AddBackward0>)\n",
      "epoch: 18666 loss is tensor([-0.7598], grad_fn=<AddBackward0>)\n",
      "epoch: 18667 loss is tensor([-0.7199], grad_fn=<AddBackward0>)\n",
      "epoch: 18668 loss is tensor([-0.7082], grad_fn=<AddBackward0>)\n",
      "epoch: 18669 loss is tensor([-0.7114], grad_fn=<AddBackward0>)\n",
      "epoch: 18670 loss is tensor([-0.6834], grad_fn=<AddBackward0>)\n",
      "epoch: 18671 loss is tensor([-0.7255], grad_fn=<AddBackward0>)\n",
      "epoch: 18672 loss is tensor([-0.7028], grad_fn=<AddBackward0>)\n",
      "epoch: 18673 loss is tensor([-0.6822], grad_fn=<AddBackward0>)\n",
      "epoch: 18674 loss is tensor([-0.7453], grad_fn=<AddBackward0>)\n",
      "epoch: 18675 loss is tensor([-0.7203], grad_fn=<AddBackward0>)\n",
      "epoch: 18676 loss is tensor([-0.7607], grad_fn=<AddBackward0>)\n",
      "epoch: 18677 loss is tensor([-0.7611], grad_fn=<AddBackward0>)\n",
      "epoch: 18678 loss is tensor([-0.7516], grad_fn=<AddBackward0>)\n",
      "epoch: 18679 loss is tensor([-0.7369], grad_fn=<AddBackward0>)\n",
      "epoch: 18680 loss is tensor([-0.7572], grad_fn=<AddBackward0>)\n",
      "epoch: 18681 loss is tensor([-0.7324], grad_fn=<AddBackward0>)\n",
      "epoch: 18682 loss is tensor([-0.7393], grad_fn=<AddBackward0>)\n",
      "epoch: 18683 loss is tensor([-0.7153], grad_fn=<AddBackward0>)\n",
      "epoch: 18684 loss is tensor([-0.6591], grad_fn=<AddBackward0>)\n",
      "epoch: 18685 loss is tensor([-0.7329], grad_fn=<AddBackward0>)\n",
      "epoch: 18686 loss is tensor([-0.7164], grad_fn=<AddBackward0>)\n",
      "epoch: 18687 loss is tensor([-0.7087], grad_fn=<AddBackward0>)\n",
      "epoch: 18688 loss is tensor([-0.6653], grad_fn=<AddBackward0>)\n",
      "epoch: 18689 loss is tensor([-0.6952], grad_fn=<AddBackward0>)\n",
      "epoch: 18690 loss is tensor([-0.6961], grad_fn=<AddBackward0>)\n",
      "epoch: 18691 loss is tensor([-0.6899], grad_fn=<AddBackward0>)\n",
      "epoch: 18692 loss is tensor([-0.7213], grad_fn=<AddBackward0>)\n",
      "epoch: 18693 loss is tensor([-0.7344], grad_fn=<AddBackward0>)\n",
      "epoch: 18694 loss is tensor([-0.6870], grad_fn=<AddBackward0>)\n",
      "epoch: 18695 loss is tensor([-0.7260], grad_fn=<AddBackward0>)\n",
      "epoch: 18696 loss is tensor([-0.7331], grad_fn=<AddBackward0>)\n",
      "epoch: 18697 loss is tensor([-0.7388], grad_fn=<AddBackward0>)\n",
      "epoch: 18698 loss is tensor([-0.7354], grad_fn=<AddBackward0>)\n",
      "epoch: 18699 loss is tensor([-0.7044], grad_fn=<AddBackward0>)\n",
      "epoch: 18700 loss is tensor([-0.7545], grad_fn=<AddBackward0>)\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18701 loss is tensor([-0.6960], grad_fn=<AddBackward0>)\n",
      "epoch: 18702 loss is tensor([-0.7070], grad_fn=<AddBackward0>)\n",
      "epoch: 18703 loss is tensor([-0.7384], grad_fn=<AddBackward0>)\n",
      "epoch: 18704 loss is tensor([-0.6984], grad_fn=<AddBackward0>)\n",
      "epoch: 18705 loss is tensor([-0.7500], grad_fn=<AddBackward0>)\n",
      "epoch: 18706 loss is tensor([-0.7453], grad_fn=<AddBackward0>)\n",
      "epoch: 18707 loss is tensor([-0.7249], grad_fn=<AddBackward0>)\n",
      "epoch: 18708 loss is tensor([-0.7301], grad_fn=<AddBackward0>)\n",
      "epoch: 18709 loss is tensor([-0.7269], grad_fn=<AddBackward0>)\n",
      "epoch: 18710 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 18711 loss is tensor([-0.7330], grad_fn=<AddBackward0>)\n",
      "epoch: 18712 loss is tensor([-0.7218], grad_fn=<AddBackward0>)\n",
      "epoch: 18713 loss is tensor([-0.6979], grad_fn=<AddBackward0>)\n",
      "epoch: 18714 loss is tensor([-0.7793], grad_fn=<AddBackward0>)\n",
      "epoch: 18715 loss is tensor([-0.7483], grad_fn=<AddBackward0>)\n",
      "epoch: 18716 loss is tensor([-0.7541], grad_fn=<AddBackward0>)\n",
      "epoch: 18717 loss is tensor([-0.7021], grad_fn=<AddBackward0>)\n",
      "epoch: 18718 loss is tensor([-0.6820], grad_fn=<AddBackward0>)\n",
      "epoch: 18719 loss is tensor([-0.6934], grad_fn=<AddBackward0>)\n",
      "epoch: 18720 loss is tensor([-0.6694], grad_fn=<AddBackward0>)\n",
      "epoch: 18721 loss is tensor([-0.7310], grad_fn=<AddBackward0>)\n",
      "epoch: 18722 loss is tensor([-0.7548], grad_fn=<AddBackward0>)\n",
      "epoch: 18723 loss is tensor([-0.7201], grad_fn=<AddBackward0>)\n",
      "epoch: 18724 loss is tensor([-0.6672], grad_fn=<AddBackward0>)\n",
      "epoch: 18725 loss is tensor([-0.6748], grad_fn=<AddBackward0>)\n",
      "epoch: 18726 loss is tensor([-0.7401], grad_fn=<AddBackward0>)\n",
      "epoch: 18727 loss is tensor([-0.6844], grad_fn=<AddBackward0>)\n",
      "epoch: 18728 loss is tensor([-0.7092], grad_fn=<AddBackward0>)\n",
      "epoch: 18729 loss is tensor([-0.7402], grad_fn=<AddBackward0>)\n",
      "epoch: 18730 loss is tensor([-0.6907], grad_fn=<AddBackward0>)\n",
      "epoch: 18731 loss is tensor([-0.6797], grad_fn=<AddBackward0>)\n",
      "epoch: 18732 loss is tensor([-0.6642], grad_fn=<AddBackward0>)\n",
      "epoch: 18733 loss is tensor([-0.7220], grad_fn=<AddBackward0>)\n",
      "epoch: 18734 loss is tensor([-0.6335], grad_fn=<AddBackward0>)\n",
      "epoch: 18735 loss is tensor([-0.7280], grad_fn=<AddBackward0>)\n",
      "epoch: 18736 loss is tensor([-0.7198], grad_fn=<AddBackward0>)\n",
      "epoch: 18737 loss is tensor([-0.7008], grad_fn=<AddBackward0>)\n",
      "epoch: 18738 loss is tensor([-0.6608], grad_fn=<AddBackward0>)\n",
      "epoch: 18739 loss is tensor([-0.6786], grad_fn=<AddBackward0>)\n",
      "epoch: 18740 loss is tensor([-0.7194], grad_fn=<AddBackward0>)\n",
      "epoch: 18741 loss is tensor([-0.7054], grad_fn=<AddBackward0>)\n",
      "epoch: 18742 loss is tensor([-0.6794], grad_fn=<AddBackward0>)\n",
      "epoch: 18743 loss is tensor([-0.6996], grad_fn=<AddBackward0>)\n",
      "epoch: 18744 loss is tensor([-0.7309], grad_fn=<AddBackward0>)\n",
      "epoch: 18745 loss is tensor([-0.7269], grad_fn=<AddBackward0>)\n",
      "epoch: 18746 loss is tensor([-0.7098], grad_fn=<AddBackward0>)\n",
      "epoch: 18747 loss is tensor([-0.6777], grad_fn=<AddBackward0>)\n",
      "epoch: 18748 loss is tensor([-0.6937], grad_fn=<AddBackward0>)\n",
      "epoch: 18749 loss is tensor([-0.7719], grad_fn=<AddBackward0>)\n",
      "epoch: 18750 loss is tensor([-0.7057], grad_fn=<AddBackward0>)\n",
      "epoch: 18751 loss is tensor([-0.7181], grad_fn=<AddBackward0>)\n",
      "epoch: 18752 loss is tensor([-0.7117], grad_fn=<AddBackward0>)\n",
      "epoch: 18753 loss is tensor([-0.6600], grad_fn=<AddBackward0>)\n",
      "epoch: 18754 loss is tensor([-0.7372], grad_fn=<AddBackward0>)\n",
      "epoch: 18755 loss is tensor([-0.6889], grad_fn=<AddBackward0>)\n",
      "epoch: 18756 loss is tensor([-0.7082], grad_fn=<AddBackward0>)\n",
      "epoch: 18757 loss is tensor([-0.7246], grad_fn=<AddBackward0>)\n",
      "epoch: 18758 loss is tensor([-0.7569], grad_fn=<AddBackward0>)\n",
      "epoch: 18759 loss is tensor([-0.6811], grad_fn=<AddBackward0>)\n",
      "epoch: 18760 loss is tensor([-0.7067], grad_fn=<AddBackward0>)\n",
      "epoch: 18761 loss is tensor([-0.6916], grad_fn=<AddBackward0>)\n",
      "epoch: 18762 loss is tensor([-0.7211], grad_fn=<AddBackward0>)\n",
      "epoch: 18763 loss is tensor([-0.6673], grad_fn=<AddBackward0>)\n",
      "epoch: 18764 loss is tensor([-0.6853], grad_fn=<AddBackward0>)\n",
      "epoch: 18765 loss is tensor([-0.6759], grad_fn=<AddBackward0>)\n",
      "epoch: 18766 loss is tensor([-0.7377], grad_fn=<AddBackward0>)\n",
      "epoch: 18767 loss is tensor([-0.7506], grad_fn=<AddBackward0>)\n",
      "epoch: 18768 loss is tensor([-0.6698], grad_fn=<AddBackward0>)\n",
      "epoch: 18769 loss is tensor([-0.7519], grad_fn=<AddBackward0>)\n",
      "epoch: 18770 loss is tensor([-0.6905], grad_fn=<AddBackward0>)\n",
      "epoch: 18771 loss is tensor([-0.7167], grad_fn=<AddBackward0>)\n",
      "epoch: 18772 loss is tensor([-0.6926], grad_fn=<AddBackward0>)\n",
      "epoch: 18773 loss is tensor([-0.6237], grad_fn=<AddBackward0>)\n",
      "epoch: 18774 loss is tensor([-0.6729], grad_fn=<AddBackward0>)\n",
      "epoch: 18775 loss is tensor([-0.6479], grad_fn=<AddBackward0>)\n",
      "epoch: 18776 loss is tensor([-0.6227], grad_fn=<AddBackward0>)\n",
      "epoch: 18777 loss is tensor([-0.5889], grad_fn=<AddBackward0>)\n",
      "epoch: 18778 loss is tensor([-0.5600], grad_fn=<AddBackward0>)\n",
      "epoch: 18779 loss is tensor([-0.6683], grad_fn=<AddBackward0>)\n",
      "epoch: 18780 loss is tensor([-0.6508], grad_fn=<AddBackward0>)\n",
      "epoch: 18781 loss is tensor([-0.6628], grad_fn=<AddBackward0>)\n",
      "epoch: 18782 loss is tensor([-0.6075], grad_fn=<AddBackward0>)\n",
      "epoch: 18783 loss is tensor([-0.6709], grad_fn=<AddBackward0>)\n",
      "epoch: 18784 loss is tensor([-0.6629], grad_fn=<AddBackward0>)\n",
      "epoch: 18785 loss is tensor([-0.6750], grad_fn=<AddBackward0>)\n",
      "epoch: 18786 loss is tensor([-0.7025], grad_fn=<AddBackward0>)\n",
      "epoch: 18787 loss is tensor([-0.6846], grad_fn=<AddBackward0>)\n",
      "epoch: 18788 loss is tensor([-0.6630], grad_fn=<AddBackward0>)\n",
      "epoch: 18789 loss is tensor([-0.6984], grad_fn=<AddBackward0>)\n",
      "epoch: 18790 loss is tensor([-0.7029], grad_fn=<AddBackward0>)\n",
      "epoch: 18791 loss is tensor([-0.7052], grad_fn=<AddBackward0>)\n",
      "epoch: 18792 loss is tensor([-0.7345], grad_fn=<AddBackward0>)\n",
      "epoch: 18793 loss is tensor([-0.6704], grad_fn=<AddBackward0>)\n",
      "epoch: 18794 loss is tensor([-0.6917], grad_fn=<AddBackward0>)\n",
      "epoch: 18795 loss is tensor([-0.7021], grad_fn=<AddBackward0>)\n",
      "epoch: 18796 loss is tensor([-0.7055], grad_fn=<AddBackward0>)\n",
      "epoch: 18797 loss is tensor([-0.6570], grad_fn=<AddBackward0>)\n",
      "epoch: 18798 loss is tensor([-0.7202], grad_fn=<AddBackward0>)\n",
      "epoch: 18799 loss is tensor([-0.7683], grad_fn=<AddBackward0>)\n",
      "epoch: 18800 loss is tensor([-0.7249], grad_fn=<AddBackward0>)\n",
      "38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18801 loss is tensor([-0.7567], grad_fn=<AddBackward0>)\n",
      "epoch: 18802 loss is tensor([-0.7118], grad_fn=<AddBackward0>)\n",
      "epoch: 18803 loss is tensor([-0.7306], grad_fn=<AddBackward0>)\n",
      "epoch: 18804 loss is tensor([-0.6952], grad_fn=<AddBackward0>)\n",
      "epoch: 18805 loss is tensor([-0.7635], grad_fn=<AddBackward0>)\n",
      "epoch: 18806 loss is tensor([-0.7107], grad_fn=<AddBackward0>)\n",
      "epoch: 18807 loss is tensor([-0.6930], grad_fn=<AddBackward0>)\n",
      "epoch: 18808 loss is tensor([-0.7555], grad_fn=<AddBackward0>)\n",
      "epoch: 18809 loss is tensor([-0.6941], grad_fn=<AddBackward0>)\n",
      "epoch: 18810 loss is tensor([-0.7294], grad_fn=<AddBackward0>)\n",
      "epoch: 18811 loss is tensor([-0.7637], grad_fn=<AddBackward0>)\n",
      "epoch: 18812 loss is tensor([-0.7621], grad_fn=<AddBackward0>)\n",
      "epoch: 18813 loss is tensor([-0.7507], grad_fn=<AddBackward0>)\n",
      "epoch: 18814 loss is tensor([-0.7203], grad_fn=<AddBackward0>)\n",
      "epoch: 18815 loss is tensor([-0.6973], grad_fn=<AddBackward0>)\n",
      "epoch: 18816 loss is tensor([-0.7575], grad_fn=<AddBackward0>)\n",
      "epoch: 18817 loss is tensor([-0.7442], grad_fn=<AddBackward0>)\n",
      "epoch: 18818 loss is tensor([-0.6713], grad_fn=<AddBackward0>)\n",
      "epoch: 18819 loss is tensor([-0.7203], grad_fn=<AddBackward0>)\n",
      "epoch: 18820 loss is tensor([-0.6870], grad_fn=<AddBackward0>)\n",
      "epoch: 18821 loss is tensor([-0.7053], grad_fn=<AddBackward0>)\n",
      "epoch: 18822 loss is tensor([-0.6623], grad_fn=<AddBackward0>)\n",
      "epoch: 18823 loss is tensor([-0.6653], grad_fn=<AddBackward0>)\n",
      "epoch: 18824 loss is tensor([-0.6759], grad_fn=<AddBackward0>)\n",
      "epoch: 18825 loss is tensor([-0.6075], grad_fn=<AddBackward0>)\n",
      "epoch: 18826 loss is tensor([-0.7067], grad_fn=<AddBackward0>)\n",
      "epoch: 18827 loss is tensor([-0.6576], grad_fn=<AddBackward0>)\n",
      "epoch: 18828 loss is tensor([-0.6695], grad_fn=<AddBackward0>)\n",
      "epoch: 18829 loss is tensor([-0.6699], grad_fn=<AddBackward0>)\n",
      "epoch: 18830 loss is tensor([-0.6620], grad_fn=<AddBackward0>)\n",
      "epoch: 18831 loss is tensor([-0.6425], grad_fn=<AddBackward0>)\n",
      "epoch: 18832 loss is tensor([-0.7103], grad_fn=<AddBackward0>)\n",
      "epoch: 18833 loss is tensor([-0.7314], grad_fn=<AddBackward0>)\n",
      "epoch: 18834 loss is tensor([-0.6162], grad_fn=<AddBackward0>)\n",
      "epoch: 18835 loss is tensor([-0.7123], grad_fn=<AddBackward0>)\n",
      "epoch: 18836 loss is tensor([-0.6928], grad_fn=<AddBackward0>)\n",
      "epoch: 18837 loss is tensor([-0.7086], grad_fn=<AddBackward0>)\n",
      "epoch: 18838 loss is tensor([-0.7041], grad_fn=<AddBackward0>)\n",
      "epoch: 18839 loss is tensor([-0.6802], grad_fn=<AddBackward0>)\n",
      "epoch: 18840 loss is tensor([-0.7457], grad_fn=<AddBackward0>)\n",
      "epoch: 18841 loss is tensor([-0.6539], grad_fn=<AddBackward0>)\n",
      "epoch: 18842 loss is tensor([-0.6747], grad_fn=<AddBackward0>)\n",
      "epoch: 18843 loss is tensor([-0.7009], grad_fn=<AddBackward0>)\n",
      "epoch: 18844 loss is tensor([-0.6896], grad_fn=<AddBackward0>)\n",
      "epoch: 18845 loss is tensor([-0.6976], grad_fn=<AddBackward0>)\n",
      "epoch: 18846 loss is tensor([-0.7305], grad_fn=<AddBackward0>)\n",
      "epoch: 18847 loss is tensor([-0.7045], grad_fn=<AddBackward0>)\n",
      "epoch: 18848 loss is tensor([-0.7016], grad_fn=<AddBackward0>)\n",
      "epoch: 18849 loss is tensor([-0.6991], grad_fn=<AddBackward0>)\n",
      "epoch: 18850 loss is tensor([-0.6698], grad_fn=<AddBackward0>)\n",
      "epoch: 18851 loss is tensor([-0.6756], grad_fn=<AddBackward0>)\n",
      "epoch: 18852 loss is tensor([-0.6919], grad_fn=<AddBackward0>)\n",
      "epoch: 18853 loss is tensor([-0.7138], grad_fn=<AddBackward0>)\n",
      "epoch: 18854 loss is tensor([-0.6738], grad_fn=<AddBackward0>)\n",
      "epoch: 18855 loss is tensor([-0.7458], grad_fn=<AddBackward0>)\n",
      "epoch: 18856 loss is tensor([-0.6912], grad_fn=<AddBackward0>)\n",
      "epoch: 18857 loss is tensor([-0.7154], grad_fn=<AddBackward0>)\n",
      "epoch: 18858 loss is tensor([-0.6738], grad_fn=<AddBackward0>)\n",
      "epoch: 18859 loss is tensor([-0.7378], grad_fn=<AddBackward0>)\n",
      "epoch: 18860 loss is tensor([-0.6826], grad_fn=<AddBackward0>)\n",
      "epoch: 18861 loss is tensor([-0.6666], grad_fn=<AddBackward0>)\n",
      "epoch: 18862 loss is tensor([-0.7295], grad_fn=<AddBackward0>)\n",
      "epoch: 18863 loss is tensor([-0.7198], grad_fn=<AddBackward0>)\n",
      "epoch: 18864 loss is tensor([-0.6639], grad_fn=<AddBackward0>)\n",
      "epoch: 18865 loss is tensor([-0.6701], grad_fn=<AddBackward0>)\n",
      "epoch: 18866 loss is tensor([-0.7424], grad_fn=<AddBackward0>)\n",
      "epoch: 18867 loss is tensor([-0.7173], grad_fn=<AddBackward0>)\n",
      "epoch: 18868 loss is tensor([-0.6818], grad_fn=<AddBackward0>)\n",
      "epoch: 18869 loss is tensor([-0.7110], grad_fn=<AddBackward0>)\n",
      "epoch: 18870 loss is tensor([-0.7163], grad_fn=<AddBackward0>)\n",
      "epoch: 18871 loss is tensor([-0.6601], grad_fn=<AddBackward0>)\n",
      "epoch: 18872 loss is tensor([-0.6892], grad_fn=<AddBackward0>)\n",
      "epoch: 18873 loss is tensor([-0.7214], grad_fn=<AddBackward0>)\n",
      "epoch: 18874 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 18875 loss is tensor([-0.7616], grad_fn=<AddBackward0>)\n",
      "epoch: 18876 loss is tensor([-0.7402], grad_fn=<AddBackward0>)\n",
      "epoch: 18877 loss is tensor([-0.7137], grad_fn=<AddBackward0>)\n",
      "epoch: 18878 loss is tensor([-0.7099], grad_fn=<AddBackward0>)\n",
      "epoch: 18879 loss is tensor([-0.7406], grad_fn=<AddBackward0>)\n",
      "epoch: 18880 loss is tensor([-0.7017], grad_fn=<AddBackward0>)\n",
      "epoch: 18881 loss is tensor([-0.7335], grad_fn=<AddBackward0>)\n",
      "epoch: 18882 loss is tensor([-0.7564], grad_fn=<AddBackward0>)\n",
      "epoch: 18883 loss is tensor([-0.7179], grad_fn=<AddBackward0>)\n",
      "epoch: 18884 loss is tensor([-0.7457], grad_fn=<AddBackward0>)\n",
      "epoch: 18885 loss is tensor([-0.7524], grad_fn=<AddBackward0>)\n",
      "epoch: 18886 loss is tensor([-0.7193], grad_fn=<AddBackward0>)\n",
      "epoch: 18887 loss is tensor([-0.7132], grad_fn=<AddBackward0>)\n",
      "epoch: 18888 loss is tensor([-0.7149], grad_fn=<AddBackward0>)\n",
      "epoch: 18889 loss is tensor([-0.7052], grad_fn=<AddBackward0>)\n",
      "epoch: 18890 loss is tensor([-0.7289], grad_fn=<AddBackward0>)\n",
      "epoch: 18891 loss is tensor([-0.7641], grad_fn=<AddBackward0>)\n",
      "epoch: 18892 loss is tensor([-0.7605], grad_fn=<AddBackward0>)\n",
      "epoch: 18893 loss is tensor([-0.7273], grad_fn=<AddBackward0>)\n",
      "epoch: 18894 loss is tensor([-0.7204], grad_fn=<AddBackward0>)\n",
      "epoch: 18895 loss is tensor([-0.7427], grad_fn=<AddBackward0>)\n",
      "epoch: 18896 loss is tensor([-0.7422], grad_fn=<AddBackward0>)\n",
      "epoch: 18897 loss is tensor([-0.7637], grad_fn=<AddBackward0>)\n",
      "epoch: 18898 loss is tensor([-0.7491], grad_fn=<AddBackward0>)\n",
      "epoch: 18899 loss is tensor([-0.7422], grad_fn=<AddBackward0>)\n",
      "epoch: 18900 loss is tensor([-0.7591], grad_fn=<AddBackward0>)\n",
      "49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 18901 loss is tensor([-0.7240], grad_fn=<AddBackward0>)\n",
      "epoch: 18902 loss is tensor([-0.6416], grad_fn=<AddBackward0>)\n",
      "epoch: 18903 loss is tensor([-0.7516], grad_fn=<AddBackward0>)\n",
      "epoch: 18904 loss is tensor([-0.7334], grad_fn=<AddBackward0>)\n",
      "epoch: 18905 loss is tensor([-0.7329], grad_fn=<AddBackward0>)\n",
      "epoch: 18906 loss is tensor([-0.7380], grad_fn=<AddBackward0>)\n",
      "epoch: 18907 loss is tensor([-0.7257], grad_fn=<AddBackward0>)\n",
      "epoch: 18908 loss is tensor([-0.6682], grad_fn=<AddBackward0>)\n",
      "epoch: 18909 loss is tensor([-0.7111], grad_fn=<AddBackward0>)\n",
      "epoch: 18910 loss is tensor([-0.7418], grad_fn=<AddBackward0>)\n",
      "epoch: 18911 loss is tensor([-0.7599], grad_fn=<AddBackward0>)\n",
      "epoch: 18912 loss is tensor([-0.7346], grad_fn=<AddBackward0>)\n",
      "epoch: 18913 loss is tensor([-0.6996], grad_fn=<AddBackward0>)\n",
      "epoch: 18914 loss is tensor([-0.7302], grad_fn=<AddBackward0>)\n",
      "epoch: 18915 loss is tensor([-0.7454], grad_fn=<AddBackward0>)\n",
      "epoch: 18916 loss is tensor([-0.6841], grad_fn=<AddBackward0>)\n",
      "epoch: 18917 loss is tensor([-0.6955], grad_fn=<AddBackward0>)\n",
      "epoch: 18918 loss is tensor([-0.7327], grad_fn=<AddBackward0>)\n",
      "epoch: 18919 loss is tensor([-0.6793], grad_fn=<AddBackward0>)\n",
      "epoch: 18920 loss is tensor([-0.7056], grad_fn=<AddBackward0>)\n",
      "epoch: 18921 loss is tensor([-0.7337], grad_fn=<AddBackward0>)\n",
      "epoch: 18922 loss is tensor([-0.7874], grad_fn=<AddBackward0>)\n",
      "epoch: 18923 loss is tensor([-0.7304], grad_fn=<AddBackward0>)\n",
      "epoch: 18924 loss is tensor([-0.7398], grad_fn=<AddBackward0>)\n",
      "epoch: 18925 loss is tensor([-0.7843], grad_fn=<AddBackward0>)\n",
      "epoch: 18926 loss is tensor([-0.7414], grad_fn=<AddBackward0>)\n",
      "epoch: 18927 loss is tensor([-0.7304], grad_fn=<AddBackward0>)\n",
      "epoch: 18928 loss is tensor([-0.7211], grad_fn=<AddBackward0>)\n",
      "epoch: 18929 loss is tensor([-0.7038], grad_fn=<AddBackward0>)\n",
      "epoch: 18930 loss is tensor([-0.6855], grad_fn=<AddBackward0>)\n",
      "epoch: 18931 loss is tensor([-0.6246], grad_fn=<AddBackward0>)\n",
      "epoch: 18932 loss is tensor([-0.7004], grad_fn=<AddBackward0>)\n",
      "epoch: 18933 loss is tensor([-0.6680], grad_fn=<AddBackward0>)\n",
      "epoch: 18934 loss is tensor([-0.6728], grad_fn=<AddBackward0>)\n",
      "epoch: 18935 loss is tensor([-0.7513], grad_fn=<AddBackward0>)\n",
      "epoch: 18936 loss is tensor([-0.7712], grad_fn=<AddBackward0>)\n",
      "epoch: 18937 loss is tensor([-0.6866], grad_fn=<AddBackward0>)\n",
      "epoch: 18938 loss is tensor([-0.7554], grad_fn=<AddBackward0>)\n",
      "epoch: 18939 loss is tensor([-0.7212], grad_fn=<AddBackward0>)\n",
      "epoch: 18940 loss is tensor([-0.6883], grad_fn=<AddBackward0>)\n",
      "epoch: 18941 loss is tensor([-0.7076], grad_fn=<AddBackward0>)\n",
      "epoch: 18942 loss is tensor([-0.7074], grad_fn=<AddBackward0>)\n",
      "epoch: 18943 loss is tensor([-0.7189], grad_fn=<AddBackward0>)\n",
      "epoch: 18944 loss is tensor([-0.7138], grad_fn=<AddBackward0>)\n",
      "epoch: 18945 loss is tensor([-0.7585], grad_fn=<AddBackward0>)\n",
      "epoch: 18946 loss is tensor([-0.7217], grad_fn=<AddBackward0>)\n",
      "epoch: 18947 loss is tensor([-0.7101], grad_fn=<AddBackward0>)\n",
      "epoch: 18948 loss is tensor([-0.7816], grad_fn=<AddBackward0>)\n",
      "epoch: 18949 loss is tensor([-0.7185], grad_fn=<AddBackward0>)\n",
      "epoch: 18950 loss is tensor([-0.7437], grad_fn=<AddBackward0>)\n",
      "epoch: 18951 loss is tensor([-0.7605], grad_fn=<AddBackward0>)\n",
      "epoch: 18952 loss is tensor([-0.7049], grad_fn=<AddBackward0>)\n",
      "epoch: 18953 loss is tensor([-0.6715], grad_fn=<AddBackward0>)\n",
      "epoch: 18954 loss is tensor([-0.7111], grad_fn=<AddBackward0>)\n",
      "epoch: 18955 loss is tensor([-0.7204], grad_fn=<AddBackward0>)\n",
      "epoch: 18956 loss is tensor([-0.7536], grad_fn=<AddBackward0>)\n",
      "epoch: 18957 loss is tensor([-0.7364], grad_fn=<AddBackward0>)\n",
      "epoch: 18958 loss is tensor([-0.6925], grad_fn=<AddBackward0>)\n",
      "epoch: 18959 loss is tensor([-0.7174], grad_fn=<AddBackward0>)\n",
      "epoch: 18960 loss is tensor([-0.6631], grad_fn=<AddBackward0>)\n",
      "epoch: 18961 loss is tensor([-0.7018], grad_fn=<AddBackward0>)\n",
      "epoch: 18962 loss is tensor([-0.7192], grad_fn=<AddBackward0>)\n",
      "epoch: 18963 loss is tensor([-0.7504], grad_fn=<AddBackward0>)\n",
      "epoch: 18964 loss is tensor([-0.6924], grad_fn=<AddBackward0>)\n",
      "epoch: 18965 loss is tensor([-0.6732], grad_fn=<AddBackward0>)\n",
      "epoch: 18966 loss is tensor([-0.7286], grad_fn=<AddBackward0>)\n",
      "epoch: 18967 loss is tensor([-0.7724], grad_fn=<AddBackward0>)\n",
      "epoch: 18968 loss is tensor([-0.7068], grad_fn=<AddBackward0>)\n",
      "epoch: 18969 loss is tensor([-0.7400], grad_fn=<AddBackward0>)\n",
      "epoch: 18970 loss is tensor([-0.6846], grad_fn=<AddBackward0>)\n",
      "epoch: 18971 loss is tensor([-0.7199], grad_fn=<AddBackward0>)\n",
      "epoch: 18972 loss is tensor([-0.7103], grad_fn=<AddBackward0>)\n",
      "epoch: 18973 loss is tensor([-0.8006], grad_fn=<AddBackward0>)\n",
      "epoch: 18974 loss is tensor([-0.7110], grad_fn=<AddBackward0>)\n",
      "epoch: 18975 loss is tensor([-0.6712], grad_fn=<AddBackward0>)\n",
      "epoch: 18976 loss is tensor([-0.7265], grad_fn=<AddBackward0>)\n",
      "epoch: 18977 loss is tensor([-0.7265], grad_fn=<AddBackward0>)\n",
      "epoch: 18978 loss is tensor([-0.7332], grad_fn=<AddBackward0>)\n",
      "epoch: 18979 loss is tensor([-0.7307], grad_fn=<AddBackward0>)\n",
      "epoch: 18980 loss is tensor([-0.7759], grad_fn=<AddBackward0>)\n",
      "epoch: 18981 loss is tensor([-0.7361], grad_fn=<AddBackward0>)\n",
      "epoch: 18982 loss is tensor([-0.6638], grad_fn=<AddBackward0>)\n",
      "epoch: 18983 loss is tensor([-0.6926], grad_fn=<AddBackward0>)\n",
      "epoch: 18984 loss is tensor([-0.7089], grad_fn=<AddBackward0>)\n",
      "epoch: 18985 loss is tensor([-0.6409], grad_fn=<AddBackward0>)\n",
      "epoch: 18986 loss is tensor([-0.7939], grad_fn=<AddBackward0>)\n",
      "epoch: 18987 loss is tensor([-0.6483], grad_fn=<AddBackward0>)\n",
      "epoch: 18988 loss is tensor([-0.7100], grad_fn=<AddBackward0>)\n",
      "epoch: 18989 loss is tensor([-0.7181], grad_fn=<AddBackward0>)\n",
      "epoch: 18990 loss is tensor([-0.7180], grad_fn=<AddBackward0>)\n",
      "epoch: 18991 loss is tensor([-0.7508], grad_fn=<AddBackward0>)\n",
      "epoch: 18992 loss is tensor([-0.7482], grad_fn=<AddBackward0>)\n",
      "epoch: 18993 loss is tensor([-0.7144], grad_fn=<AddBackward0>)\n",
      "epoch: 18994 loss is tensor([-0.7598], grad_fn=<AddBackward0>)\n",
      "epoch: 18995 loss is tensor([-0.7437], grad_fn=<AddBackward0>)\n",
      "epoch: 18996 loss is tensor([-0.6910], grad_fn=<AddBackward0>)\n",
      "epoch: 18997 loss is tensor([-0.7238], grad_fn=<AddBackward0>)\n",
      "epoch: 18998 loss is tensor([-0.6990], grad_fn=<AddBackward0>)\n",
      "epoch: 18999 loss is tensor([-0.7388], grad_fn=<AddBackward0>)\n",
      "epoch: 19000 loss is tensor([-0.7157], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19001 loss is tensor([-0.7271], grad_fn=<AddBackward0>)\n",
      "epoch: 19002 loss is tensor([-0.7444], grad_fn=<AddBackward0>)\n",
      "epoch: 19003 loss is tensor([-0.6727], grad_fn=<AddBackward0>)\n",
      "epoch: 19004 loss is tensor([-0.8224], grad_fn=<AddBackward0>)\n",
      "epoch: 19005 loss is tensor([-0.7669], grad_fn=<AddBackward0>)\n",
      "epoch: 19006 loss is tensor([-0.7069], grad_fn=<AddBackward0>)\n",
      "epoch: 19007 loss is tensor([-0.7563], grad_fn=<AddBackward0>)\n",
      "epoch: 19008 loss is tensor([-0.7186], grad_fn=<AddBackward0>)\n",
      "epoch: 19009 loss is tensor([-0.7460], grad_fn=<AddBackward0>)\n",
      "epoch: 19010 loss is tensor([-0.6977], grad_fn=<AddBackward0>)\n",
      "epoch: 19011 loss is tensor([-0.7314], grad_fn=<AddBackward0>)\n",
      "epoch: 19012 loss is tensor([-0.7318], grad_fn=<AddBackward0>)\n",
      "epoch: 19013 loss is tensor([-0.7386], grad_fn=<AddBackward0>)\n",
      "epoch: 19014 loss is tensor([-0.7289], grad_fn=<AddBackward0>)\n",
      "epoch: 19015 loss is tensor([-0.7256], grad_fn=<AddBackward0>)\n",
      "epoch: 19016 loss is tensor([-0.7526], grad_fn=<AddBackward0>)\n",
      "epoch: 19017 loss is tensor([-0.6746], grad_fn=<AddBackward0>)\n",
      "epoch: 19018 loss is tensor([-0.7289], grad_fn=<AddBackward0>)\n",
      "epoch: 19019 loss is tensor([-0.7578], grad_fn=<AddBackward0>)\n",
      "epoch: 19020 loss is tensor([-0.8174], grad_fn=<AddBackward0>)\n",
      "epoch: 19021 loss is tensor([-0.7480], grad_fn=<AddBackward0>)\n",
      "epoch: 19022 loss is tensor([-0.7222], grad_fn=<AddBackward0>)\n",
      "epoch: 19023 loss is tensor([-0.7657], grad_fn=<AddBackward0>)\n",
      "epoch: 19024 loss is tensor([-0.7475], grad_fn=<AddBackward0>)\n",
      "epoch: 19025 loss is tensor([-0.7635], grad_fn=<AddBackward0>)\n",
      "epoch: 19026 loss is tensor([-0.7783], grad_fn=<AddBackward0>)\n",
      "epoch: 19027 loss is tensor([-0.7373], grad_fn=<AddBackward0>)\n",
      "epoch: 19028 loss is tensor([-0.7196], grad_fn=<AddBackward0>)\n",
      "epoch: 19029 loss is tensor([-0.7120], grad_fn=<AddBackward0>)\n",
      "epoch: 19030 loss is tensor([-0.6814], grad_fn=<AddBackward0>)\n",
      "epoch: 19031 loss is tensor([-0.7267], grad_fn=<AddBackward0>)\n",
      "epoch: 19032 loss is tensor([-0.7382], grad_fn=<AddBackward0>)\n",
      "epoch: 19033 loss is tensor([-0.7623], grad_fn=<AddBackward0>)\n",
      "epoch: 19034 loss is tensor([-0.7413], grad_fn=<AddBackward0>)\n",
      "epoch: 19035 loss is tensor([-0.6905], grad_fn=<AddBackward0>)\n",
      "epoch: 19036 loss is tensor([-0.7081], grad_fn=<AddBackward0>)\n",
      "epoch: 19037 loss is tensor([-0.6979], grad_fn=<AddBackward0>)\n",
      "epoch: 19038 loss is tensor([-0.7141], grad_fn=<AddBackward0>)\n",
      "epoch: 19039 loss is tensor([-0.7002], grad_fn=<AddBackward0>)\n",
      "epoch: 19040 loss is tensor([-0.7251], grad_fn=<AddBackward0>)\n",
      "epoch: 19041 loss is tensor([-0.7470], grad_fn=<AddBackward0>)\n",
      "epoch: 19042 loss is tensor([-0.7530], grad_fn=<AddBackward0>)\n",
      "epoch: 19043 loss is tensor([-0.7378], grad_fn=<AddBackward0>)\n",
      "epoch: 19044 loss is tensor([-0.7434], grad_fn=<AddBackward0>)\n",
      "epoch: 19045 loss is tensor([-0.7278], grad_fn=<AddBackward0>)\n",
      "epoch: 19046 loss is tensor([-0.7180], grad_fn=<AddBackward0>)\n",
      "epoch: 19047 loss is tensor([-0.7775], grad_fn=<AddBackward0>)\n",
      "epoch: 19048 loss is tensor([-0.7366], grad_fn=<AddBackward0>)\n",
      "epoch: 19049 loss is tensor([-0.7160], grad_fn=<AddBackward0>)\n",
      "epoch: 19050 loss is tensor([-0.7384], grad_fn=<AddBackward0>)\n",
      "epoch: 19051 loss is tensor([-0.6953], grad_fn=<AddBackward0>)\n",
      "epoch: 19052 loss is tensor([-0.7646], grad_fn=<AddBackward0>)\n",
      "epoch: 19053 loss is tensor([-0.6984], grad_fn=<AddBackward0>)\n",
      "epoch: 19054 loss is tensor([-0.7092], grad_fn=<AddBackward0>)\n",
      "epoch: 19055 loss is tensor([-0.7319], grad_fn=<AddBackward0>)\n",
      "epoch: 19056 loss is tensor([-0.6742], grad_fn=<AddBackward0>)\n",
      "epoch: 19057 loss is tensor([-0.7257], grad_fn=<AddBackward0>)\n",
      "epoch: 19058 loss is tensor([-0.6601], grad_fn=<AddBackward0>)\n",
      "epoch: 19059 loss is tensor([-0.7384], grad_fn=<AddBackward0>)\n",
      "epoch: 19060 loss is tensor([-0.6854], grad_fn=<AddBackward0>)\n",
      "epoch: 19061 loss is tensor([-0.7219], grad_fn=<AddBackward0>)\n",
      "epoch: 19062 loss is tensor([-0.7307], grad_fn=<AddBackward0>)\n",
      "epoch: 19063 loss is tensor([-0.7194], grad_fn=<AddBackward0>)\n",
      "epoch: 19064 loss is tensor([-0.7189], grad_fn=<AddBackward0>)\n",
      "epoch: 19065 loss is tensor([-0.7535], grad_fn=<AddBackward0>)\n",
      "epoch: 19066 loss is tensor([-0.7462], grad_fn=<AddBackward0>)\n",
      "epoch: 19067 loss is tensor([-0.7333], grad_fn=<AddBackward0>)\n",
      "epoch: 19068 loss is tensor([-0.7345], grad_fn=<AddBackward0>)\n",
      "epoch: 19069 loss is tensor([-0.7644], grad_fn=<AddBackward0>)\n",
      "epoch: 19070 loss is tensor([-0.7341], grad_fn=<AddBackward0>)\n",
      "epoch: 19071 loss is tensor([-0.7361], grad_fn=<AddBackward0>)\n",
      "epoch: 19072 loss is tensor([-0.7400], grad_fn=<AddBackward0>)\n",
      "epoch: 19073 loss is tensor([-0.7460], grad_fn=<AddBackward0>)\n",
      "epoch: 19074 loss is tensor([-0.6872], grad_fn=<AddBackward0>)\n",
      "epoch: 19075 loss is tensor([-0.7432], grad_fn=<AddBackward0>)\n",
      "epoch: 19076 loss is tensor([-0.7343], grad_fn=<AddBackward0>)\n",
      "epoch: 19077 loss is tensor([-0.7400], grad_fn=<AddBackward0>)\n",
      "epoch: 19078 loss is tensor([-0.7259], grad_fn=<AddBackward0>)\n",
      "epoch: 19079 loss is tensor([-0.7212], grad_fn=<AddBackward0>)\n",
      "epoch: 19080 loss is tensor([-0.7818], grad_fn=<AddBackward0>)\n",
      "epoch: 19081 loss is tensor([-0.7423], grad_fn=<AddBackward0>)\n",
      "epoch: 19082 loss is tensor([-0.6996], grad_fn=<AddBackward0>)\n",
      "epoch: 19083 loss is tensor([-0.7308], grad_fn=<AddBackward0>)\n",
      "epoch: 19084 loss is tensor([-0.6906], grad_fn=<AddBackward0>)\n",
      "epoch: 19085 loss is tensor([-0.7027], grad_fn=<AddBackward0>)\n",
      "epoch: 19086 loss is tensor([-0.7112], grad_fn=<AddBackward0>)\n",
      "epoch: 19087 loss is tensor([-0.7291], grad_fn=<AddBackward0>)\n",
      "epoch: 19088 loss is tensor([-0.7103], grad_fn=<AddBackward0>)\n",
      "epoch: 19089 loss is tensor([-0.7246], grad_fn=<AddBackward0>)\n",
      "epoch: 19090 loss is tensor([-0.6779], grad_fn=<AddBackward0>)\n",
      "epoch: 19091 loss is tensor([-0.7132], grad_fn=<AddBackward0>)\n",
      "epoch: 19092 loss is tensor([-0.7222], grad_fn=<AddBackward0>)\n",
      "epoch: 19093 loss is tensor([-0.7566], grad_fn=<AddBackward0>)\n",
      "epoch: 19094 loss is tensor([-0.7178], grad_fn=<AddBackward0>)\n",
      "epoch: 19095 loss is tensor([-0.7305], grad_fn=<AddBackward0>)\n",
      "epoch: 19096 loss is tensor([-0.7741], grad_fn=<AddBackward0>)\n",
      "epoch: 19097 loss is tensor([-0.6564], grad_fn=<AddBackward0>)\n",
      "epoch: 19098 loss is tensor([-0.7138], grad_fn=<AddBackward0>)\n",
      "epoch: 19099 loss is tensor([-0.6900], grad_fn=<AddBackward0>)\n",
      "epoch: 19100 loss is tensor([-0.7083], grad_fn=<AddBackward0>)\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19101 loss is tensor([-0.7429], grad_fn=<AddBackward0>)\n",
      "epoch: 19102 loss is tensor([-0.7289], grad_fn=<AddBackward0>)\n",
      "epoch: 19103 loss is tensor([-0.7166], grad_fn=<AddBackward0>)\n",
      "epoch: 19104 loss is tensor([-0.7390], grad_fn=<AddBackward0>)\n",
      "epoch: 19105 loss is tensor([-0.7136], grad_fn=<AddBackward0>)\n",
      "epoch: 19106 loss is tensor([-0.7727], grad_fn=<AddBackward0>)\n",
      "epoch: 19107 loss is tensor([-0.7026], grad_fn=<AddBackward0>)\n",
      "epoch: 19108 loss is tensor([-0.7151], grad_fn=<AddBackward0>)\n",
      "epoch: 19109 loss is tensor([-0.7324], grad_fn=<AddBackward0>)\n",
      "epoch: 19110 loss is tensor([-0.7017], grad_fn=<AddBackward0>)\n",
      "epoch: 19111 loss is tensor([-0.6364], grad_fn=<AddBackward0>)\n",
      "epoch: 19112 loss is tensor([-0.7744], grad_fn=<AddBackward0>)\n",
      "epoch: 19113 loss is tensor([-0.7439], grad_fn=<AddBackward0>)\n",
      "epoch: 19114 loss is tensor([-0.6955], grad_fn=<AddBackward0>)\n",
      "epoch: 19115 loss is tensor([-0.7663], grad_fn=<AddBackward0>)\n",
      "epoch: 19116 loss is tensor([-0.7480], grad_fn=<AddBackward0>)\n",
      "epoch: 19117 loss is tensor([-0.7346], grad_fn=<AddBackward0>)\n",
      "epoch: 19118 loss is tensor([-0.7025], grad_fn=<AddBackward0>)\n",
      "epoch: 19119 loss is tensor([-0.7161], grad_fn=<AddBackward0>)\n",
      "epoch: 19120 loss is tensor([-0.6904], grad_fn=<AddBackward0>)\n",
      "epoch: 19121 loss is tensor([-0.7348], grad_fn=<AddBackward0>)\n",
      "epoch: 19122 loss is tensor([-0.7111], grad_fn=<AddBackward0>)\n",
      "epoch: 19123 loss is tensor([-0.7022], grad_fn=<AddBackward0>)\n",
      "epoch: 19124 loss is tensor([-0.7013], grad_fn=<AddBackward0>)\n",
      "epoch: 19125 loss is tensor([-0.7221], grad_fn=<AddBackward0>)\n",
      "epoch: 19126 loss is tensor([-0.6830], grad_fn=<AddBackward0>)\n",
      "epoch: 19127 loss is tensor([-0.6897], grad_fn=<AddBackward0>)\n",
      "epoch: 19128 loss is tensor([-0.7434], grad_fn=<AddBackward0>)\n",
      "epoch: 19129 loss is tensor([-0.7075], grad_fn=<AddBackward0>)\n",
      "epoch: 19130 loss is tensor([-0.6821], grad_fn=<AddBackward0>)\n",
      "epoch: 19131 loss is tensor([-0.6846], grad_fn=<AddBackward0>)\n",
      "epoch: 19132 loss is tensor([-0.7124], grad_fn=<AddBackward0>)\n",
      "epoch: 19133 loss is tensor([-0.7681], grad_fn=<AddBackward0>)\n",
      "epoch: 19134 loss is tensor([-0.7616], grad_fn=<AddBackward0>)\n",
      "epoch: 19135 loss is tensor([-0.7199], grad_fn=<AddBackward0>)\n",
      "epoch: 19136 loss is tensor([-0.7801], grad_fn=<AddBackward0>)\n",
      "epoch: 19137 loss is tensor([-0.7368], grad_fn=<AddBackward0>)\n",
      "epoch: 19138 loss is tensor([-0.7253], grad_fn=<AddBackward0>)\n",
      "epoch: 19139 loss is tensor([-0.7006], grad_fn=<AddBackward0>)\n",
      "epoch: 19140 loss is tensor([-0.7042], grad_fn=<AddBackward0>)\n",
      "epoch: 19141 loss is tensor([-0.6780], grad_fn=<AddBackward0>)\n",
      "epoch: 19142 loss is tensor([-0.7585], grad_fn=<AddBackward0>)\n",
      "epoch: 19143 loss is tensor([-0.6344], grad_fn=<AddBackward0>)\n",
      "epoch: 19144 loss is tensor([-0.7305], grad_fn=<AddBackward0>)\n",
      "epoch: 19145 loss is tensor([-0.7068], grad_fn=<AddBackward0>)\n",
      "epoch: 19146 loss is tensor([-0.6802], grad_fn=<AddBackward0>)\n",
      "epoch: 19147 loss is tensor([-0.7029], grad_fn=<AddBackward0>)\n",
      "epoch: 19148 loss is tensor([-0.6513], grad_fn=<AddBackward0>)\n",
      "epoch: 19149 loss is tensor([-0.6710], grad_fn=<AddBackward0>)\n",
      "epoch: 19150 loss is tensor([-0.7269], grad_fn=<AddBackward0>)\n",
      "epoch: 19151 loss is tensor([-0.7512], grad_fn=<AddBackward0>)\n",
      "epoch: 19152 loss is tensor([-0.7153], grad_fn=<AddBackward0>)\n",
      "epoch: 19153 loss is tensor([-0.7422], grad_fn=<AddBackward0>)\n",
      "epoch: 19154 loss is tensor([-0.7456], grad_fn=<AddBackward0>)\n",
      "epoch: 19155 loss is tensor([-0.7196], grad_fn=<AddBackward0>)\n",
      "epoch: 19156 loss is tensor([-0.7477], grad_fn=<AddBackward0>)\n",
      "epoch: 19157 loss is tensor([-0.7066], grad_fn=<AddBackward0>)\n",
      "epoch: 19158 loss is tensor([-0.7500], grad_fn=<AddBackward0>)\n",
      "epoch: 19159 loss is tensor([-0.7257], grad_fn=<AddBackward0>)\n",
      "epoch: 19160 loss is tensor([-0.7465], grad_fn=<AddBackward0>)\n",
      "epoch: 19161 loss is tensor([-0.7582], grad_fn=<AddBackward0>)\n",
      "epoch: 19162 loss is tensor([-0.7412], grad_fn=<AddBackward0>)\n",
      "epoch: 19163 loss is tensor([-0.7386], grad_fn=<AddBackward0>)\n",
      "epoch: 19164 loss is tensor([-0.7501], grad_fn=<AddBackward0>)\n",
      "epoch: 19165 loss is tensor([-0.7188], grad_fn=<AddBackward0>)\n",
      "epoch: 19166 loss is tensor([-0.6545], grad_fn=<AddBackward0>)\n",
      "epoch: 19167 loss is tensor([-0.7340], grad_fn=<AddBackward0>)\n",
      "epoch: 19168 loss is tensor([-0.7476], grad_fn=<AddBackward0>)\n",
      "epoch: 19169 loss is tensor([-0.6852], grad_fn=<AddBackward0>)\n",
      "epoch: 19170 loss is tensor([-0.7638], grad_fn=<AddBackward0>)\n",
      "epoch: 19171 loss is tensor([-0.7436], grad_fn=<AddBackward0>)\n",
      "epoch: 19172 loss is tensor([-0.7375], grad_fn=<AddBackward0>)\n",
      "epoch: 19173 loss is tensor([-0.7591], grad_fn=<AddBackward0>)\n",
      "epoch: 19174 loss is tensor([-0.7744], grad_fn=<AddBackward0>)\n",
      "epoch: 19175 loss is tensor([-0.7287], grad_fn=<AddBackward0>)\n",
      "epoch: 19176 loss is tensor([-0.7762], grad_fn=<AddBackward0>)\n",
      "epoch: 19177 loss is tensor([-0.6970], grad_fn=<AddBackward0>)\n",
      "epoch: 19178 loss is tensor([-0.7353], grad_fn=<AddBackward0>)\n",
      "epoch: 19179 loss is tensor([-0.7569], grad_fn=<AddBackward0>)\n",
      "epoch: 19180 loss is tensor([-0.7667], grad_fn=<AddBackward0>)\n",
      "epoch: 19181 loss is tensor([-0.8008], grad_fn=<AddBackward0>)\n",
      "epoch: 19182 loss is tensor([-0.7511], grad_fn=<AddBackward0>)\n",
      "epoch: 19183 loss is tensor([-0.7225], grad_fn=<AddBackward0>)\n",
      "epoch: 19184 loss is tensor([-0.7341], grad_fn=<AddBackward0>)\n",
      "epoch: 19185 loss is tensor([-0.7215], grad_fn=<AddBackward0>)\n",
      "epoch: 19186 loss is tensor([-0.7273], grad_fn=<AddBackward0>)\n",
      "epoch: 19187 loss is tensor([-0.7843], grad_fn=<AddBackward0>)\n",
      "epoch: 19188 loss is tensor([-0.7915], grad_fn=<AddBackward0>)\n",
      "epoch: 19189 loss is tensor([-0.7476], grad_fn=<AddBackward0>)\n",
      "epoch: 19190 loss is tensor([-0.7456], grad_fn=<AddBackward0>)\n",
      "epoch: 19191 loss is tensor([-0.7443], grad_fn=<AddBackward0>)\n",
      "epoch: 19192 loss is tensor([-0.7650], grad_fn=<AddBackward0>)\n",
      "epoch: 19193 loss is tensor([-0.7414], grad_fn=<AddBackward0>)\n",
      "epoch: 19194 loss is tensor([-0.6825], grad_fn=<AddBackward0>)\n",
      "epoch: 19195 loss is tensor([-0.7304], grad_fn=<AddBackward0>)\n",
      "epoch: 19196 loss is tensor([-0.7343], grad_fn=<AddBackward0>)\n",
      "epoch: 19197 loss is tensor([-0.7189], grad_fn=<AddBackward0>)\n",
      "epoch: 19198 loss is tensor([-0.7496], grad_fn=<AddBackward0>)\n",
      "epoch: 19199 loss is tensor([-0.7490], grad_fn=<AddBackward0>)\n",
      "epoch: 19200 loss is tensor([-0.7046], grad_fn=<AddBackward0>)\n",
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19201 loss is tensor([-0.7242], grad_fn=<AddBackward0>)\n",
      "epoch: 19202 loss is tensor([-0.7136], grad_fn=<AddBackward0>)\n",
      "epoch: 19203 loss is tensor([-0.7421], grad_fn=<AddBackward0>)\n",
      "epoch: 19204 loss is tensor([-0.7779], grad_fn=<AddBackward0>)\n",
      "epoch: 19205 loss is tensor([-0.7094], grad_fn=<AddBackward0>)\n",
      "epoch: 19206 loss is tensor([-0.7049], grad_fn=<AddBackward0>)\n",
      "epoch: 19207 loss is tensor([-0.7604], grad_fn=<AddBackward0>)\n",
      "epoch: 19208 loss is tensor([-0.6947], grad_fn=<AddBackward0>)\n",
      "epoch: 19209 loss is tensor([-0.7350], grad_fn=<AddBackward0>)\n",
      "epoch: 19210 loss is tensor([-0.7434], grad_fn=<AddBackward0>)\n",
      "epoch: 19211 loss is tensor([-0.7417], grad_fn=<AddBackward0>)\n",
      "epoch: 19212 loss is tensor([-0.7353], grad_fn=<AddBackward0>)\n",
      "epoch: 19213 loss is tensor([-0.7292], grad_fn=<AddBackward0>)\n",
      "epoch: 19214 loss is tensor([-0.7319], grad_fn=<AddBackward0>)\n",
      "epoch: 19215 loss is tensor([-0.7426], grad_fn=<AddBackward0>)\n",
      "epoch: 19216 loss is tensor([-0.7425], grad_fn=<AddBackward0>)\n",
      "epoch: 19217 loss is tensor([-0.7212], grad_fn=<AddBackward0>)\n",
      "epoch: 19218 loss is tensor([-0.7738], grad_fn=<AddBackward0>)\n",
      "epoch: 19219 loss is tensor([-0.7588], grad_fn=<AddBackward0>)\n",
      "epoch: 19220 loss is tensor([-0.7764], grad_fn=<AddBackward0>)\n",
      "epoch: 19221 loss is tensor([-0.7032], grad_fn=<AddBackward0>)\n",
      "epoch: 19222 loss is tensor([-0.7200], grad_fn=<AddBackward0>)\n",
      "epoch: 19223 loss is tensor([-0.7958], grad_fn=<AddBackward0>)\n",
      "epoch: 19224 loss is tensor([-0.6939], grad_fn=<AddBackward0>)\n",
      "epoch: 19225 loss is tensor([-0.7623], grad_fn=<AddBackward0>)\n",
      "epoch: 19226 loss is tensor([-0.7291], grad_fn=<AddBackward0>)\n",
      "epoch: 19227 loss is tensor([-0.7134], grad_fn=<AddBackward0>)\n",
      "epoch: 19228 loss is tensor([-0.7345], grad_fn=<AddBackward0>)\n",
      "epoch: 19229 loss is tensor([-0.7129], grad_fn=<AddBackward0>)\n",
      "epoch: 19230 loss is tensor([-0.7402], grad_fn=<AddBackward0>)\n",
      "epoch: 19231 loss is tensor([-0.7298], grad_fn=<AddBackward0>)\n",
      "epoch: 19232 loss is tensor([-0.7470], grad_fn=<AddBackward0>)\n",
      "epoch: 19233 loss is tensor([-0.7415], grad_fn=<AddBackward0>)\n",
      "epoch: 19234 loss is tensor([-0.6860], grad_fn=<AddBackward0>)\n",
      "epoch: 19235 loss is tensor([-0.6651], grad_fn=<AddBackward0>)\n",
      "epoch: 19236 loss is tensor([-0.6872], grad_fn=<AddBackward0>)\n",
      "epoch: 19237 loss is tensor([-0.7101], grad_fn=<AddBackward0>)\n",
      "epoch: 19238 loss is tensor([-0.7380], grad_fn=<AddBackward0>)\n",
      "epoch: 19239 loss is tensor([-0.7470], grad_fn=<AddBackward0>)\n",
      "epoch: 19240 loss is tensor([-0.7436], grad_fn=<AddBackward0>)\n",
      "epoch: 19241 loss is tensor([-0.7271], grad_fn=<AddBackward0>)\n",
      "epoch: 19242 loss is tensor([-0.7172], grad_fn=<AddBackward0>)\n",
      "epoch: 19243 loss is tensor([-0.7513], grad_fn=<AddBackward0>)\n",
      "epoch: 19244 loss is tensor([-0.7069], grad_fn=<AddBackward0>)\n",
      "epoch: 19245 loss is tensor([-0.6735], grad_fn=<AddBackward0>)\n",
      "epoch: 19246 loss is tensor([-0.7276], grad_fn=<AddBackward0>)\n",
      "epoch: 19247 loss is tensor([-0.7616], grad_fn=<AddBackward0>)\n",
      "epoch: 19248 loss is tensor([-0.6933], grad_fn=<AddBackward0>)\n",
      "epoch: 19249 loss is tensor([-0.7212], grad_fn=<AddBackward0>)\n",
      "epoch: 19250 loss is tensor([-0.7497], grad_fn=<AddBackward0>)\n",
      "epoch: 19251 loss is tensor([-0.7234], grad_fn=<AddBackward0>)\n",
      "epoch: 19252 loss is tensor([-0.7514], grad_fn=<AddBackward0>)\n",
      "epoch: 19253 loss is tensor([-0.7016], grad_fn=<AddBackward0>)\n",
      "epoch: 19254 loss is tensor([-0.7656], grad_fn=<AddBackward0>)\n",
      "epoch: 19255 loss is tensor([-0.7466], grad_fn=<AddBackward0>)\n",
      "epoch: 19256 loss is tensor([-0.7186], grad_fn=<AddBackward0>)\n",
      "epoch: 19257 loss is tensor([-0.7786], grad_fn=<AddBackward0>)\n",
      "epoch: 19258 loss is tensor([-0.7654], grad_fn=<AddBackward0>)\n",
      "epoch: 19259 loss is tensor([-0.7760], grad_fn=<AddBackward0>)\n",
      "epoch: 19260 loss is tensor([-0.6495], grad_fn=<AddBackward0>)\n",
      "epoch: 19261 loss is tensor([-0.7477], grad_fn=<AddBackward0>)\n",
      "epoch: 19262 loss is tensor([-0.7064], grad_fn=<AddBackward0>)\n",
      "epoch: 19263 loss is tensor([-0.6933], grad_fn=<AddBackward0>)\n",
      "epoch: 19264 loss is tensor([-0.6857], grad_fn=<AddBackward0>)\n",
      "epoch: 19265 loss is tensor([-0.7242], grad_fn=<AddBackward0>)\n",
      "epoch: 19266 loss is tensor([-0.6810], grad_fn=<AddBackward0>)\n",
      "epoch: 19267 loss is tensor([-0.7540], grad_fn=<AddBackward0>)\n",
      "epoch: 19268 loss is tensor([-0.7552], grad_fn=<AddBackward0>)\n",
      "epoch: 19269 loss is tensor([-0.6937], grad_fn=<AddBackward0>)\n",
      "epoch: 19270 loss is tensor([-0.7184], grad_fn=<AddBackward0>)\n",
      "epoch: 19271 loss is tensor([-0.6783], grad_fn=<AddBackward0>)\n",
      "epoch: 19272 loss is tensor([-0.7645], grad_fn=<AddBackward0>)\n",
      "epoch: 19273 loss is tensor([-0.7262], grad_fn=<AddBackward0>)\n",
      "epoch: 19274 loss is tensor([-0.7321], grad_fn=<AddBackward0>)\n",
      "epoch: 19275 loss is tensor([-0.8102], grad_fn=<AddBackward0>)\n",
      "epoch: 19276 loss is tensor([-0.7294], grad_fn=<AddBackward0>)\n",
      "epoch: 19277 loss is tensor([-0.7753], grad_fn=<AddBackward0>)\n",
      "epoch: 19278 loss is tensor([-0.7004], grad_fn=<AddBackward0>)\n",
      "epoch: 19279 loss is tensor([-0.7059], grad_fn=<AddBackward0>)\n",
      "epoch: 19280 loss is tensor([-0.7103], grad_fn=<AddBackward0>)\n",
      "epoch: 19281 loss is tensor([-0.7344], grad_fn=<AddBackward0>)\n",
      "epoch: 19282 loss is tensor([-0.7492], grad_fn=<AddBackward0>)\n",
      "epoch: 19283 loss is tensor([-0.6813], grad_fn=<AddBackward0>)\n",
      "epoch: 19284 loss is tensor([-0.7151], grad_fn=<AddBackward0>)\n",
      "epoch: 19285 loss is tensor([-0.7578], grad_fn=<AddBackward0>)\n",
      "epoch: 19286 loss is tensor([-0.6935], grad_fn=<AddBackward0>)\n",
      "epoch: 19287 loss is tensor([-0.7058], grad_fn=<AddBackward0>)\n",
      "epoch: 19288 loss is tensor([-0.7068], grad_fn=<AddBackward0>)\n",
      "epoch: 19289 loss is tensor([-0.7733], grad_fn=<AddBackward0>)\n",
      "epoch: 19290 loss is tensor([-0.6987], grad_fn=<AddBackward0>)\n",
      "epoch: 19291 loss is tensor([-0.5740], grad_fn=<AddBackward0>)\n",
      "epoch: 19292 loss is tensor([-0.6988], grad_fn=<AddBackward0>)\n",
      "epoch: 19293 loss is tensor([-0.7063], grad_fn=<AddBackward0>)\n",
      "epoch: 19294 loss is tensor([-0.7767], grad_fn=<AddBackward0>)\n",
      "epoch: 19295 loss is tensor([-0.7233], grad_fn=<AddBackward0>)\n",
      "epoch: 19296 loss is tensor([-0.7309], grad_fn=<AddBackward0>)\n",
      "epoch: 19297 loss is tensor([-0.7572], grad_fn=<AddBackward0>)\n",
      "epoch: 19298 loss is tensor([-0.7183], grad_fn=<AddBackward0>)\n",
      "epoch: 19299 loss is tensor([-0.7149], grad_fn=<AddBackward0>)\n",
      "epoch: 19300 loss is tensor([-0.6879], grad_fn=<AddBackward0>)\n",
      "26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19301 loss is tensor([-0.7450], grad_fn=<AddBackward0>)\n",
      "epoch: 19302 loss is tensor([-0.7203], grad_fn=<AddBackward0>)\n",
      "epoch: 19303 loss is tensor([-0.6380], grad_fn=<AddBackward0>)\n",
      "epoch: 19304 loss is tensor([-0.7084], grad_fn=<AddBackward0>)\n",
      "epoch: 19305 loss is tensor([-0.7466], grad_fn=<AddBackward0>)\n",
      "epoch: 19306 loss is tensor([-0.6868], grad_fn=<AddBackward0>)\n",
      "epoch: 19307 loss is tensor([-0.6868], grad_fn=<AddBackward0>)\n",
      "epoch: 19308 loss is tensor([-0.7461], grad_fn=<AddBackward0>)\n",
      "epoch: 19309 loss is tensor([-0.7191], grad_fn=<AddBackward0>)\n",
      "epoch: 19310 loss is tensor([-0.6911], grad_fn=<AddBackward0>)\n",
      "epoch: 19311 loss is tensor([-0.6627], grad_fn=<AddBackward0>)\n",
      "epoch: 19312 loss is tensor([-0.7027], grad_fn=<AddBackward0>)\n",
      "epoch: 19313 loss is tensor([-0.6977], grad_fn=<AddBackward0>)\n",
      "epoch: 19314 loss is tensor([-0.7423], grad_fn=<AddBackward0>)\n",
      "epoch: 19315 loss is tensor([-0.7454], grad_fn=<AddBackward0>)\n",
      "epoch: 19316 loss is tensor([-0.7214], grad_fn=<AddBackward0>)\n",
      "epoch: 19317 loss is tensor([-0.7525], grad_fn=<AddBackward0>)\n",
      "epoch: 19318 loss is tensor([-0.7422], grad_fn=<AddBackward0>)\n",
      "epoch: 19319 loss is tensor([-0.7714], grad_fn=<AddBackward0>)\n",
      "epoch: 19320 loss is tensor([-0.6466], grad_fn=<AddBackward0>)\n",
      "epoch: 19321 loss is tensor([-0.7277], grad_fn=<AddBackward0>)\n",
      "epoch: 19322 loss is tensor([-0.7491], grad_fn=<AddBackward0>)\n",
      "epoch: 19323 loss is tensor([-0.7901], grad_fn=<AddBackward0>)\n",
      "epoch: 19324 loss is tensor([-0.7626], grad_fn=<AddBackward0>)\n",
      "epoch: 19325 loss is tensor([-0.7845], grad_fn=<AddBackward0>)\n",
      "epoch: 19326 loss is tensor([-0.7147], grad_fn=<AddBackward0>)\n",
      "epoch: 19327 loss is tensor([-0.7281], grad_fn=<AddBackward0>)\n",
      "epoch: 19328 loss is tensor([-0.7386], grad_fn=<AddBackward0>)\n",
      "epoch: 19329 loss is tensor([-0.7327], grad_fn=<AddBackward0>)\n",
      "epoch: 19330 loss is tensor([-0.6683], grad_fn=<AddBackward0>)\n",
      "epoch: 19331 loss is tensor([-0.6866], grad_fn=<AddBackward0>)\n",
      "epoch: 19332 loss is tensor([-0.6994], grad_fn=<AddBackward0>)\n",
      "epoch: 19333 loss is tensor([-0.6837], grad_fn=<AddBackward0>)\n",
      "epoch: 19334 loss is tensor([-0.7382], grad_fn=<AddBackward0>)\n",
      "epoch: 19335 loss is tensor([-0.7050], grad_fn=<AddBackward0>)\n",
      "epoch: 19336 loss is tensor([-0.7175], grad_fn=<AddBackward0>)\n",
      "epoch: 19337 loss is tensor([-0.7390], grad_fn=<AddBackward0>)\n",
      "epoch: 19338 loss is tensor([-0.7496], grad_fn=<AddBackward0>)\n",
      "epoch: 19339 loss is tensor([-0.7097], grad_fn=<AddBackward0>)\n",
      "epoch: 19340 loss is tensor([-0.7168], grad_fn=<AddBackward0>)\n",
      "epoch: 19341 loss is tensor([-0.7663], grad_fn=<AddBackward0>)\n",
      "epoch: 19342 loss is tensor([-0.7327], grad_fn=<AddBackward0>)\n",
      "epoch: 19343 loss is tensor([-0.6617], grad_fn=<AddBackward0>)\n",
      "epoch: 19344 loss is tensor([-0.7182], grad_fn=<AddBackward0>)\n",
      "epoch: 19345 loss is tensor([-0.7438], grad_fn=<AddBackward0>)\n",
      "epoch: 19346 loss is tensor([-0.6891], grad_fn=<AddBackward0>)\n",
      "epoch: 19347 loss is tensor([-0.6978], grad_fn=<AddBackward0>)\n",
      "epoch: 19348 loss is tensor([-0.7419], grad_fn=<AddBackward0>)\n",
      "epoch: 19349 loss is tensor([-0.7426], grad_fn=<AddBackward0>)\n",
      "epoch: 19350 loss is tensor([-0.6939], grad_fn=<AddBackward0>)\n",
      "epoch: 19351 loss is tensor([-0.6713], grad_fn=<AddBackward0>)\n",
      "epoch: 19352 loss is tensor([-0.6900], grad_fn=<AddBackward0>)\n",
      "epoch: 19353 loss is tensor([-0.6779], grad_fn=<AddBackward0>)\n",
      "epoch: 19354 loss is tensor([-0.7373], grad_fn=<AddBackward0>)\n",
      "epoch: 19355 loss is tensor([-0.8192], grad_fn=<AddBackward0>)\n",
      "epoch: 19356 loss is tensor([-0.7309], grad_fn=<AddBackward0>)\n",
      "epoch: 19357 loss is tensor([-0.6783], grad_fn=<AddBackward0>)\n",
      "epoch: 19358 loss is tensor([-0.6932], grad_fn=<AddBackward0>)\n",
      "epoch: 19359 loss is tensor([-0.7079], grad_fn=<AddBackward0>)\n",
      "epoch: 19360 loss is tensor([-0.7317], grad_fn=<AddBackward0>)\n",
      "epoch: 19361 loss is tensor([-0.7304], grad_fn=<AddBackward0>)\n",
      "epoch: 19362 loss is tensor([-0.7404], grad_fn=<AddBackward0>)\n",
      "epoch: 19363 loss is tensor([-0.6660], grad_fn=<AddBackward0>)\n",
      "epoch: 19364 loss is tensor([-0.7522], grad_fn=<AddBackward0>)\n",
      "epoch: 19365 loss is tensor([-0.6973], grad_fn=<AddBackward0>)\n",
      "epoch: 19366 loss is tensor([-0.6766], grad_fn=<AddBackward0>)\n",
      "epoch: 19367 loss is tensor([-0.6745], grad_fn=<AddBackward0>)\n",
      "epoch: 19368 loss is tensor([-0.7129], grad_fn=<AddBackward0>)\n",
      "epoch: 19369 loss is tensor([-0.6819], grad_fn=<AddBackward0>)\n",
      "epoch: 19370 loss is tensor([-0.6905], grad_fn=<AddBackward0>)\n",
      "epoch: 19371 loss is tensor([-0.6886], grad_fn=<AddBackward0>)\n",
      "epoch: 19372 loss is tensor([-0.7308], grad_fn=<AddBackward0>)\n",
      "epoch: 19373 loss is tensor([-0.7495], grad_fn=<AddBackward0>)\n",
      "epoch: 19374 loss is tensor([-0.6841], grad_fn=<AddBackward0>)\n",
      "epoch: 19375 loss is tensor([-0.7263], grad_fn=<AddBackward0>)\n",
      "epoch: 19376 loss is tensor([-0.7138], grad_fn=<AddBackward0>)\n",
      "epoch: 19377 loss is tensor([-0.7314], grad_fn=<AddBackward0>)\n",
      "epoch: 19378 loss is tensor([-0.7608], grad_fn=<AddBackward0>)\n",
      "epoch: 19379 loss is tensor([-0.7161], grad_fn=<AddBackward0>)\n",
      "epoch: 19380 loss is tensor([-0.7170], grad_fn=<AddBackward0>)\n",
      "epoch: 19381 loss is tensor([-0.6835], grad_fn=<AddBackward0>)\n",
      "epoch: 19382 loss is tensor([-0.6678], grad_fn=<AddBackward0>)\n",
      "epoch: 19383 loss is tensor([-0.7581], grad_fn=<AddBackward0>)\n",
      "epoch: 19384 loss is tensor([-0.6652], grad_fn=<AddBackward0>)\n",
      "epoch: 19385 loss is tensor([-0.7326], grad_fn=<AddBackward0>)\n",
      "epoch: 19386 loss is tensor([-0.6503], grad_fn=<AddBackward0>)\n",
      "epoch: 19387 loss is tensor([-0.7610], grad_fn=<AddBackward0>)\n",
      "epoch: 19388 loss is tensor([-0.7152], grad_fn=<AddBackward0>)\n",
      "epoch: 19389 loss is tensor([-0.7341], grad_fn=<AddBackward0>)\n",
      "epoch: 19390 loss is tensor([-0.6882], grad_fn=<AddBackward0>)\n",
      "epoch: 19391 loss is tensor([-0.7322], grad_fn=<AddBackward0>)\n",
      "epoch: 19392 loss is tensor([-0.6684], grad_fn=<AddBackward0>)\n",
      "epoch: 19393 loss is tensor([-0.7791], grad_fn=<AddBackward0>)\n",
      "epoch: 19394 loss is tensor([-0.7268], grad_fn=<AddBackward0>)\n",
      "epoch: 19395 loss is tensor([-0.7126], grad_fn=<AddBackward0>)\n",
      "epoch: 19396 loss is tensor([-0.6934], grad_fn=<AddBackward0>)\n",
      "epoch: 19397 loss is tensor([-0.7292], grad_fn=<AddBackward0>)\n",
      "epoch: 19398 loss is tensor([-0.6653], grad_fn=<AddBackward0>)\n",
      "epoch: 19399 loss is tensor([-0.6401], grad_fn=<AddBackward0>)\n",
      "epoch: 19400 loss is tensor([-0.6655], grad_fn=<AddBackward0>)\n",
      "23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19401 loss is tensor([-0.7115], grad_fn=<AddBackward0>)\n",
      "epoch: 19402 loss is tensor([-0.7316], grad_fn=<AddBackward0>)\n",
      "epoch: 19403 loss is tensor([-0.7240], grad_fn=<AddBackward0>)\n",
      "epoch: 19404 loss is tensor([-0.7450], grad_fn=<AddBackward0>)\n",
      "epoch: 19405 loss is tensor([-0.7205], grad_fn=<AddBackward0>)\n",
      "epoch: 19406 loss is tensor([-0.7621], grad_fn=<AddBackward0>)\n",
      "epoch: 19407 loss is tensor([-0.7338], grad_fn=<AddBackward0>)\n",
      "epoch: 19408 loss is tensor([-0.7280], grad_fn=<AddBackward0>)\n",
      "epoch: 19409 loss is tensor([-0.7196], grad_fn=<AddBackward0>)\n",
      "epoch: 19410 loss is tensor([-0.7431], grad_fn=<AddBackward0>)\n",
      "epoch: 19411 loss is tensor([-0.7196], grad_fn=<AddBackward0>)\n",
      "epoch: 19412 loss is tensor([-0.7231], grad_fn=<AddBackward0>)\n",
      "epoch: 19413 loss is tensor([-0.6751], grad_fn=<AddBackward0>)\n",
      "epoch: 19414 loss is tensor([-0.7421], grad_fn=<AddBackward0>)\n",
      "epoch: 19415 loss is tensor([-0.7436], grad_fn=<AddBackward0>)\n",
      "epoch: 19416 loss is tensor([-0.7120], grad_fn=<AddBackward0>)\n",
      "epoch: 19417 loss is tensor([-0.7494], grad_fn=<AddBackward0>)\n",
      "epoch: 19418 loss is tensor([-0.6909], grad_fn=<AddBackward0>)\n",
      "epoch: 19419 loss is tensor([-0.7165], grad_fn=<AddBackward0>)\n",
      "epoch: 19420 loss is tensor([-0.6800], grad_fn=<AddBackward0>)\n",
      "epoch: 19421 loss is tensor([-0.6635], grad_fn=<AddBackward0>)\n",
      "epoch: 19422 loss is tensor([-0.7068], grad_fn=<AddBackward0>)\n",
      "epoch: 19423 loss is tensor([-0.6951], grad_fn=<AddBackward0>)\n",
      "epoch: 19424 loss is tensor([-0.6867], grad_fn=<AddBackward0>)\n",
      "epoch: 19425 loss is tensor([-0.7222], grad_fn=<AddBackward0>)\n",
      "epoch: 19426 loss is tensor([-0.6878], grad_fn=<AddBackward0>)\n",
      "epoch: 19427 loss is tensor([-0.6754], grad_fn=<AddBackward0>)\n",
      "epoch: 19428 loss is tensor([-0.7721], grad_fn=<AddBackward0>)\n",
      "epoch: 19429 loss is tensor([-0.7161], grad_fn=<AddBackward0>)\n",
      "epoch: 19430 loss is tensor([-0.7086], grad_fn=<AddBackward0>)\n",
      "epoch: 19431 loss is tensor([-0.7057], grad_fn=<AddBackward0>)\n",
      "epoch: 19432 loss is tensor([-0.7209], grad_fn=<AddBackward0>)\n",
      "epoch: 19433 loss is tensor([-0.7018], grad_fn=<AddBackward0>)\n",
      "epoch: 19434 loss is tensor([-0.6847], grad_fn=<AddBackward0>)\n",
      "epoch: 19435 loss is tensor([-0.6799], grad_fn=<AddBackward0>)\n",
      "epoch: 19436 loss is tensor([-0.7313], grad_fn=<AddBackward0>)\n",
      "epoch: 19437 loss is tensor([-0.7540], grad_fn=<AddBackward0>)\n",
      "epoch: 19438 loss is tensor([-0.6806], grad_fn=<AddBackward0>)\n",
      "epoch: 19439 loss is tensor([-0.7127], grad_fn=<AddBackward0>)\n",
      "epoch: 19440 loss is tensor([-0.6935], grad_fn=<AddBackward0>)\n",
      "epoch: 19441 loss is tensor([-0.7115], grad_fn=<AddBackward0>)\n",
      "epoch: 19442 loss is tensor([-0.7023], grad_fn=<AddBackward0>)\n",
      "epoch: 19443 loss is tensor([-0.6659], grad_fn=<AddBackward0>)\n",
      "epoch: 19444 loss is tensor([-0.7372], grad_fn=<AddBackward0>)\n",
      "epoch: 19445 loss is tensor([-0.7208], grad_fn=<AddBackward0>)\n",
      "epoch: 19446 loss is tensor([-0.7397], grad_fn=<AddBackward0>)\n",
      "epoch: 19447 loss is tensor([-0.7195], grad_fn=<AddBackward0>)\n",
      "epoch: 19448 loss is tensor([-0.6878], grad_fn=<AddBackward0>)\n",
      "epoch: 19449 loss is tensor([-0.7686], grad_fn=<AddBackward0>)\n",
      "epoch: 19450 loss is tensor([-0.7420], grad_fn=<AddBackward0>)\n",
      "epoch: 19451 loss is tensor([-0.6683], grad_fn=<AddBackward0>)\n",
      "epoch: 19452 loss is tensor([-0.6820], grad_fn=<AddBackward0>)\n",
      "epoch: 19453 loss is tensor([-0.7074], grad_fn=<AddBackward0>)\n",
      "epoch: 19454 loss is tensor([-0.7151], grad_fn=<AddBackward0>)\n",
      "epoch: 19455 loss is tensor([-0.7028], grad_fn=<AddBackward0>)\n",
      "epoch: 19456 loss is tensor([-0.7129], grad_fn=<AddBackward0>)\n",
      "epoch: 19457 loss is tensor([-0.6982], grad_fn=<AddBackward0>)\n",
      "epoch: 19458 loss is tensor([-0.6725], grad_fn=<AddBackward0>)\n",
      "epoch: 19459 loss is tensor([-0.7086], grad_fn=<AddBackward0>)\n",
      "epoch: 19460 loss is tensor([-0.6880], grad_fn=<AddBackward0>)\n",
      "epoch: 19461 loss is tensor([-0.6583], grad_fn=<AddBackward0>)\n",
      "epoch: 19462 loss is tensor([-0.7434], grad_fn=<AddBackward0>)\n",
      "epoch: 19463 loss is tensor([-0.7276], grad_fn=<AddBackward0>)\n",
      "epoch: 19464 loss is tensor([-0.7449], grad_fn=<AddBackward0>)\n",
      "epoch: 19465 loss is tensor([-0.7099], grad_fn=<AddBackward0>)\n",
      "epoch: 19466 loss is tensor([-0.7064], grad_fn=<AddBackward0>)\n",
      "epoch: 19467 loss is tensor([-0.6771], grad_fn=<AddBackward0>)\n",
      "epoch: 19468 loss is tensor([-0.6991], grad_fn=<AddBackward0>)\n",
      "epoch: 19469 loss is tensor([-0.6299], grad_fn=<AddBackward0>)\n",
      "epoch: 19470 loss is tensor([-0.7336], grad_fn=<AddBackward0>)\n",
      "epoch: 19471 loss is tensor([-0.6904], grad_fn=<AddBackward0>)\n",
      "epoch: 19472 loss is tensor([-0.6607], grad_fn=<AddBackward0>)\n",
      "epoch: 19473 loss is tensor([-0.7192], grad_fn=<AddBackward0>)\n",
      "epoch: 19474 loss is tensor([-0.7157], grad_fn=<AddBackward0>)\n",
      "epoch: 19475 loss is tensor([-0.7889], grad_fn=<AddBackward0>)\n",
      "epoch: 19476 loss is tensor([-0.7412], grad_fn=<AddBackward0>)\n",
      "epoch: 19477 loss is tensor([-0.7269], grad_fn=<AddBackward0>)\n",
      "epoch: 19478 loss is tensor([-0.6849], grad_fn=<AddBackward0>)\n",
      "epoch: 19479 loss is tensor([-0.7728], grad_fn=<AddBackward0>)\n",
      "epoch: 19480 loss is tensor([-0.7294], grad_fn=<AddBackward0>)\n",
      "epoch: 19481 loss is tensor([-0.7242], grad_fn=<AddBackward0>)\n",
      "epoch: 19482 loss is tensor([-0.7293], grad_fn=<AddBackward0>)\n",
      "epoch: 19483 loss is tensor([-0.7239], grad_fn=<AddBackward0>)\n",
      "epoch: 19484 loss is tensor([-0.7085], grad_fn=<AddBackward0>)\n",
      "epoch: 19485 loss is tensor([-0.6939], grad_fn=<AddBackward0>)\n",
      "epoch: 19486 loss is tensor([-0.7510], grad_fn=<AddBackward0>)\n",
      "epoch: 19487 loss is tensor([-0.7559], grad_fn=<AddBackward0>)\n",
      "epoch: 19488 loss is tensor([-0.7759], grad_fn=<AddBackward0>)\n",
      "epoch: 19489 loss is tensor([-0.7294], grad_fn=<AddBackward0>)\n",
      "epoch: 19490 loss is tensor([-0.7101], grad_fn=<AddBackward0>)\n",
      "epoch: 19491 loss is tensor([-0.6937], grad_fn=<AddBackward0>)\n",
      "epoch: 19492 loss is tensor([-0.7041], grad_fn=<AddBackward0>)\n",
      "epoch: 19493 loss is tensor([-0.6903], grad_fn=<AddBackward0>)\n",
      "epoch: 19494 loss is tensor([-0.7563], grad_fn=<AddBackward0>)\n",
      "epoch: 19495 loss is tensor([-0.7332], grad_fn=<AddBackward0>)\n",
      "epoch: 19496 loss is tensor([-0.6946], grad_fn=<AddBackward0>)\n",
      "epoch: 19497 loss is tensor([-0.7164], grad_fn=<AddBackward0>)\n",
      "epoch: 19498 loss is tensor([-0.7167], grad_fn=<AddBackward0>)\n",
      "epoch: 19499 loss is tensor([-0.7261], grad_fn=<AddBackward0>)\n",
      "epoch: 19500 loss is tensor([-0.7030], grad_fn=<AddBackward0>)\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19501 loss is tensor([-0.7450], grad_fn=<AddBackward0>)\n",
      "epoch: 19502 loss is tensor([-0.6984], grad_fn=<AddBackward0>)\n",
      "epoch: 19503 loss is tensor([-0.7166], grad_fn=<AddBackward0>)\n",
      "epoch: 19504 loss is tensor([-0.7319], grad_fn=<AddBackward0>)\n",
      "epoch: 19505 loss is tensor([-0.7903], grad_fn=<AddBackward0>)\n",
      "epoch: 19506 loss is tensor([-0.7363], grad_fn=<AddBackward0>)\n",
      "epoch: 19507 loss is tensor([-0.7940], grad_fn=<AddBackward0>)\n",
      "epoch: 19508 loss is tensor([-0.7407], grad_fn=<AddBackward0>)\n",
      "epoch: 19509 loss is tensor([-0.7193], grad_fn=<AddBackward0>)\n",
      "epoch: 19510 loss is tensor([-0.7841], grad_fn=<AddBackward0>)\n",
      "epoch: 19511 loss is tensor([-0.7505], grad_fn=<AddBackward0>)\n",
      "epoch: 19512 loss is tensor([-0.7160], grad_fn=<AddBackward0>)\n",
      "epoch: 19513 loss is tensor([-0.7341], grad_fn=<AddBackward0>)\n",
      "epoch: 19514 loss is tensor([-0.7292], grad_fn=<AddBackward0>)\n",
      "epoch: 19515 loss is tensor([-0.7260], grad_fn=<AddBackward0>)\n",
      "epoch: 19516 loss is tensor([-0.7325], grad_fn=<AddBackward0>)\n",
      "epoch: 19517 loss is tensor([-0.7222], grad_fn=<AddBackward0>)\n",
      "epoch: 19518 loss is tensor([-0.7542], grad_fn=<AddBackward0>)\n",
      "epoch: 19519 loss is tensor([-0.7910], grad_fn=<AddBackward0>)\n",
      "epoch: 19520 loss is tensor([-0.8073], grad_fn=<AddBackward0>)\n",
      "epoch: 19521 loss is tensor([-0.7537], grad_fn=<AddBackward0>)\n",
      "epoch: 19522 loss is tensor([-0.7473], grad_fn=<AddBackward0>)\n",
      "epoch: 19523 loss is tensor([-0.7048], grad_fn=<AddBackward0>)\n",
      "epoch: 19524 loss is tensor([-0.6869], grad_fn=<AddBackward0>)\n",
      "epoch: 19525 loss is tensor([-0.6895], grad_fn=<AddBackward0>)\n",
      "epoch: 19526 loss is tensor([-0.7580], grad_fn=<AddBackward0>)\n",
      "epoch: 19527 loss is tensor([-0.7512], grad_fn=<AddBackward0>)\n",
      "epoch: 19528 loss is tensor([-0.7275], grad_fn=<AddBackward0>)\n",
      "epoch: 19529 loss is tensor([-0.7538], grad_fn=<AddBackward0>)\n",
      "epoch: 19530 loss is tensor([-0.7548], grad_fn=<AddBackward0>)\n",
      "epoch: 19531 loss is tensor([-0.7561], grad_fn=<AddBackward0>)\n",
      "epoch: 19532 loss is tensor([-0.7487], grad_fn=<AddBackward0>)\n",
      "epoch: 19533 loss is tensor([-0.7690], grad_fn=<AddBackward0>)\n",
      "epoch: 19534 loss is tensor([-0.7576], grad_fn=<AddBackward0>)\n",
      "epoch: 19535 loss is tensor([-0.7733], grad_fn=<AddBackward0>)\n",
      "epoch: 19536 loss is tensor([-0.7387], grad_fn=<AddBackward0>)\n",
      "epoch: 19537 loss is tensor([-0.7424], grad_fn=<AddBackward0>)\n",
      "epoch: 19538 loss is tensor([-0.7468], grad_fn=<AddBackward0>)\n",
      "epoch: 19539 loss is tensor([-0.7668], grad_fn=<AddBackward0>)\n",
      "epoch: 19540 loss is tensor([-0.7041], grad_fn=<AddBackward0>)\n",
      "epoch: 19541 loss is tensor([-0.6670], grad_fn=<AddBackward0>)\n",
      "epoch: 19542 loss is tensor([-0.7449], grad_fn=<AddBackward0>)\n",
      "epoch: 19543 loss is tensor([-0.8141], grad_fn=<AddBackward0>)\n",
      "epoch: 19544 loss is tensor([-0.7373], grad_fn=<AddBackward0>)\n",
      "epoch: 19545 loss is tensor([-0.7717], grad_fn=<AddBackward0>)\n",
      "epoch: 19546 loss is tensor([-0.7215], grad_fn=<AddBackward0>)\n",
      "epoch: 19547 loss is tensor([-0.6966], grad_fn=<AddBackward0>)\n",
      "epoch: 19548 loss is tensor([-0.7586], grad_fn=<AddBackward0>)\n",
      "epoch: 19549 loss is tensor([-0.7784], grad_fn=<AddBackward0>)\n",
      "epoch: 19550 loss is tensor([-0.6923], grad_fn=<AddBackward0>)\n",
      "epoch: 19551 loss is tensor([-0.7355], grad_fn=<AddBackward0>)\n",
      "epoch: 19552 loss is tensor([-0.7014], grad_fn=<AddBackward0>)\n",
      "epoch: 19553 loss is tensor([-0.7759], grad_fn=<AddBackward0>)\n",
      "epoch: 19554 loss is tensor([-0.7679], grad_fn=<AddBackward0>)\n",
      "epoch: 19555 loss is tensor([-0.7027], grad_fn=<AddBackward0>)\n",
      "epoch: 19556 loss is tensor([-0.7069], grad_fn=<AddBackward0>)\n",
      "epoch: 19557 loss is tensor([-0.7250], grad_fn=<AddBackward0>)\n",
      "epoch: 19558 loss is tensor([-0.7242], grad_fn=<AddBackward0>)\n",
      "epoch: 19559 loss is tensor([-0.7294], grad_fn=<AddBackward0>)\n",
      "epoch: 19560 loss is tensor([-0.7205], grad_fn=<AddBackward0>)\n",
      "epoch: 19561 loss is tensor([-0.7723], grad_fn=<AddBackward0>)\n",
      "epoch: 19562 loss is tensor([-0.7862], grad_fn=<AddBackward0>)\n",
      "epoch: 19563 loss is tensor([-0.7574], grad_fn=<AddBackward0>)\n",
      "epoch: 19564 loss is tensor([-0.7177], grad_fn=<AddBackward0>)\n",
      "epoch: 19565 loss is tensor([-0.7411], grad_fn=<AddBackward0>)\n",
      "epoch: 19566 loss is tensor([-0.7330], grad_fn=<AddBackward0>)\n",
      "epoch: 19567 loss is tensor([-0.7624], grad_fn=<AddBackward0>)\n",
      "epoch: 19568 loss is tensor([-0.7739], grad_fn=<AddBackward0>)\n",
      "epoch: 19569 loss is tensor([-0.7639], grad_fn=<AddBackward0>)\n",
      "epoch: 19570 loss is tensor([-0.7712], grad_fn=<AddBackward0>)\n",
      "epoch: 19571 loss is tensor([-0.7204], grad_fn=<AddBackward0>)\n",
      "epoch: 19572 loss is tensor([-0.7157], grad_fn=<AddBackward0>)\n",
      "epoch: 19573 loss is tensor([-0.7512], grad_fn=<AddBackward0>)\n",
      "epoch: 19574 loss is tensor([-0.7373], grad_fn=<AddBackward0>)\n",
      "epoch: 19575 loss is tensor([-0.7190], grad_fn=<AddBackward0>)\n",
      "epoch: 19576 loss is tensor([-0.7027], grad_fn=<AddBackward0>)\n",
      "epoch: 19577 loss is tensor([-0.7316], grad_fn=<AddBackward0>)\n",
      "epoch: 19578 loss is tensor([-0.7583], grad_fn=<AddBackward0>)\n",
      "epoch: 19579 loss is tensor([-0.7642], grad_fn=<AddBackward0>)\n",
      "epoch: 19580 loss is tensor([-0.7381], grad_fn=<AddBackward0>)\n",
      "epoch: 19581 loss is tensor([-0.7241], grad_fn=<AddBackward0>)\n",
      "epoch: 19582 loss is tensor([-0.7420], grad_fn=<AddBackward0>)\n",
      "epoch: 19583 loss is tensor([-0.7024], grad_fn=<AddBackward0>)\n",
      "epoch: 19584 loss is tensor([-0.7386], grad_fn=<AddBackward0>)\n",
      "epoch: 19585 loss is tensor([-0.7228], grad_fn=<AddBackward0>)\n",
      "epoch: 19586 loss is tensor([-0.7928], grad_fn=<AddBackward0>)\n",
      "epoch: 19587 loss is tensor([-0.7281], grad_fn=<AddBackward0>)\n",
      "epoch: 19588 loss is tensor([-0.7084], grad_fn=<AddBackward0>)\n",
      "epoch: 19589 loss is tensor([-0.7332], grad_fn=<AddBackward0>)\n",
      "epoch: 19590 loss is tensor([-0.7732], grad_fn=<AddBackward0>)\n",
      "epoch: 19591 loss is tensor([-0.7073], grad_fn=<AddBackward0>)\n",
      "epoch: 19592 loss is tensor([-0.7050], grad_fn=<AddBackward0>)\n",
      "epoch: 19593 loss is tensor([-0.7139], grad_fn=<AddBackward0>)\n",
      "epoch: 19594 loss is tensor([-0.7375], grad_fn=<AddBackward0>)\n",
      "epoch: 19595 loss is tensor([-0.7098], grad_fn=<AddBackward0>)\n",
      "epoch: 19596 loss is tensor([-0.7707], grad_fn=<AddBackward0>)\n",
      "epoch: 19597 loss is tensor([-0.6980], grad_fn=<AddBackward0>)\n",
      "epoch: 19598 loss is tensor([-0.7754], grad_fn=<AddBackward0>)\n",
      "epoch: 19599 loss is tensor([-0.6874], grad_fn=<AddBackward0>)\n",
      "epoch: 19600 loss is tensor([-0.7613], grad_fn=<AddBackward0>)\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19601 loss is tensor([-0.7688], grad_fn=<AddBackward0>)\n",
      "epoch: 19602 loss is tensor([-0.7128], grad_fn=<AddBackward0>)\n",
      "epoch: 19603 loss is tensor([-0.7107], grad_fn=<AddBackward0>)\n",
      "epoch: 19604 loss is tensor([-0.7305], grad_fn=<AddBackward0>)\n",
      "epoch: 19605 loss is tensor([-0.7466], grad_fn=<AddBackward0>)\n",
      "epoch: 19606 loss is tensor([-0.6950], grad_fn=<AddBackward0>)\n",
      "epoch: 19607 loss is tensor([-0.7291], grad_fn=<AddBackward0>)\n",
      "epoch: 19608 loss is tensor([-0.7302], grad_fn=<AddBackward0>)\n",
      "epoch: 19609 loss is tensor([-0.6820], grad_fn=<AddBackward0>)\n",
      "epoch: 19610 loss is tensor([-0.7645], grad_fn=<AddBackward0>)\n",
      "epoch: 19611 loss is tensor([-0.7293], grad_fn=<AddBackward0>)\n",
      "epoch: 19612 loss is tensor([-0.7712], grad_fn=<AddBackward0>)\n",
      "epoch: 19613 loss is tensor([-0.7587], grad_fn=<AddBackward0>)\n",
      "epoch: 19614 loss is tensor([-0.7150], grad_fn=<AddBackward0>)\n",
      "epoch: 19615 loss is tensor([-0.7870], grad_fn=<AddBackward0>)\n",
      "epoch: 19616 loss is tensor([-0.7410], grad_fn=<AddBackward0>)\n",
      "epoch: 19617 loss is tensor([-0.7209], grad_fn=<AddBackward0>)\n",
      "epoch: 19618 loss is tensor([-0.7491], grad_fn=<AddBackward0>)\n",
      "epoch: 19619 loss is tensor([-0.6907], grad_fn=<AddBackward0>)\n",
      "epoch: 19620 loss is tensor([-0.7581], grad_fn=<AddBackward0>)\n",
      "epoch: 19621 loss is tensor([-0.7263], grad_fn=<AddBackward0>)\n",
      "epoch: 19622 loss is tensor([-0.7473], grad_fn=<AddBackward0>)\n",
      "epoch: 19623 loss is tensor([-0.7939], grad_fn=<AddBackward0>)\n",
      "epoch: 19624 loss is tensor([-0.7860], grad_fn=<AddBackward0>)\n",
      "epoch: 19625 loss is tensor([-0.7505], grad_fn=<AddBackward0>)\n",
      "epoch: 19626 loss is tensor([-0.7126], grad_fn=<AddBackward0>)\n",
      "epoch: 19627 loss is tensor([-0.7704], grad_fn=<AddBackward0>)\n",
      "epoch: 19628 loss is tensor([-0.8018], grad_fn=<AddBackward0>)\n",
      "epoch: 19629 loss is tensor([-0.7095], grad_fn=<AddBackward0>)\n",
      "epoch: 19630 loss is tensor([-0.7402], grad_fn=<AddBackward0>)\n",
      "epoch: 19631 loss is tensor([-0.7647], grad_fn=<AddBackward0>)\n",
      "epoch: 19632 loss is tensor([-0.7140], grad_fn=<AddBackward0>)\n",
      "epoch: 19633 loss is tensor([-0.7517], grad_fn=<AddBackward0>)\n",
      "epoch: 19634 loss is tensor([-0.8019], grad_fn=<AddBackward0>)\n",
      "epoch: 19635 loss is tensor([-0.7809], grad_fn=<AddBackward0>)\n",
      "epoch: 19636 loss is tensor([-0.7500], grad_fn=<AddBackward0>)\n",
      "epoch: 19637 loss is tensor([-0.7167], grad_fn=<AddBackward0>)\n",
      "epoch: 19638 loss is tensor([-0.7632], grad_fn=<AddBackward0>)\n",
      "epoch: 19639 loss is tensor([-0.7113], grad_fn=<AddBackward0>)\n",
      "epoch: 19640 loss is tensor([-0.7369], grad_fn=<AddBackward0>)\n",
      "epoch: 19641 loss is tensor([-0.7699], grad_fn=<AddBackward0>)\n",
      "epoch: 19642 loss is tensor([-0.7872], grad_fn=<AddBackward0>)\n",
      "epoch: 19643 loss is tensor([-0.7466], grad_fn=<AddBackward0>)\n",
      "epoch: 19644 loss is tensor([-0.7449], grad_fn=<AddBackward0>)\n",
      "epoch: 19645 loss is tensor([-0.7687], grad_fn=<AddBackward0>)\n",
      "epoch: 19646 loss is tensor([-0.7789], grad_fn=<AddBackward0>)\n",
      "epoch: 19647 loss is tensor([-0.7455], grad_fn=<AddBackward0>)\n",
      "epoch: 19648 loss is tensor([-0.7050], grad_fn=<AddBackward0>)\n",
      "epoch: 19649 loss is tensor([-0.7059], grad_fn=<AddBackward0>)\n",
      "epoch: 19650 loss is tensor([-0.7484], grad_fn=<AddBackward0>)\n",
      "epoch: 19651 loss is tensor([-0.7342], grad_fn=<AddBackward0>)\n",
      "epoch: 19652 loss is tensor([-0.7956], grad_fn=<AddBackward0>)\n",
      "epoch: 19653 loss is tensor([-0.7303], grad_fn=<AddBackward0>)\n",
      "epoch: 19654 loss is tensor([-0.7415], grad_fn=<AddBackward0>)\n",
      "epoch: 19655 loss is tensor([-0.7458], grad_fn=<AddBackward0>)\n",
      "epoch: 19656 loss is tensor([-0.7121], grad_fn=<AddBackward0>)\n",
      "epoch: 19657 loss is tensor([-0.7183], grad_fn=<AddBackward0>)\n",
      "epoch: 19658 loss is tensor([-0.7474], grad_fn=<AddBackward0>)\n",
      "epoch: 19659 loss is tensor([-0.7226], grad_fn=<AddBackward0>)\n",
      "epoch: 19660 loss is tensor([-0.6938], grad_fn=<AddBackward0>)\n",
      "epoch: 19661 loss is tensor([-0.7606], grad_fn=<AddBackward0>)\n",
      "epoch: 19662 loss is tensor([-0.7231], grad_fn=<AddBackward0>)\n",
      "epoch: 19663 loss is tensor([-0.6849], grad_fn=<AddBackward0>)\n",
      "epoch: 19664 loss is tensor([-0.6676], grad_fn=<AddBackward0>)\n",
      "epoch: 19665 loss is tensor([-0.7116], grad_fn=<AddBackward0>)\n",
      "epoch: 19666 loss is tensor([-0.7639], grad_fn=<AddBackward0>)\n",
      "epoch: 19667 loss is tensor([-0.7258], grad_fn=<AddBackward0>)\n",
      "epoch: 19668 loss is tensor([-0.7490], grad_fn=<AddBackward0>)\n",
      "epoch: 19669 loss is tensor([-0.7787], grad_fn=<AddBackward0>)\n",
      "epoch: 19670 loss is tensor([-0.7568], grad_fn=<AddBackward0>)\n",
      "epoch: 19671 loss is tensor([-0.6797], grad_fn=<AddBackward0>)\n",
      "epoch: 19672 loss is tensor([-0.7459], grad_fn=<AddBackward0>)\n",
      "epoch: 19673 loss is tensor([-0.7681], grad_fn=<AddBackward0>)\n",
      "epoch: 19674 loss is tensor([-0.7162], grad_fn=<AddBackward0>)\n",
      "epoch: 19675 loss is tensor([-0.7364], grad_fn=<AddBackward0>)\n",
      "epoch: 19676 loss is tensor([-0.7691], grad_fn=<AddBackward0>)\n",
      "epoch: 19677 loss is tensor([-0.7649], grad_fn=<AddBackward0>)\n",
      "epoch: 19678 loss is tensor([-0.7672], grad_fn=<AddBackward0>)\n",
      "epoch: 19679 loss is tensor([-0.7377], grad_fn=<AddBackward0>)\n",
      "epoch: 19680 loss is tensor([-0.7908], grad_fn=<AddBackward0>)\n",
      "epoch: 19681 loss is tensor([-0.7303], grad_fn=<AddBackward0>)\n",
      "epoch: 19682 loss is tensor([-0.6787], grad_fn=<AddBackward0>)\n",
      "epoch: 19683 loss is tensor([-0.7550], grad_fn=<AddBackward0>)\n",
      "epoch: 19684 loss is tensor([-0.7325], grad_fn=<AddBackward0>)\n",
      "epoch: 19685 loss is tensor([-0.7586], grad_fn=<AddBackward0>)\n",
      "epoch: 19686 loss is tensor([-0.6879], grad_fn=<AddBackward0>)\n",
      "epoch: 19687 loss is tensor([-0.7470], grad_fn=<AddBackward0>)\n",
      "epoch: 19688 loss is tensor([-0.7271], grad_fn=<AddBackward0>)\n",
      "epoch: 19689 loss is tensor([-0.7484], grad_fn=<AddBackward0>)\n",
      "epoch: 19690 loss is tensor([-0.7682], grad_fn=<AddBackward0>)\n",
      "epoch: 19691 loss is tensor([-0.7144], grad_fn=<AddBackward0>)\n",
      "epoch: 19692 loss is tensor([-0.7239], grad_fn=<AddBackward0>)\n",
      "epoch: 19693 loss is tensor([-0.7558], grad_fn=<AddBackward0>)\n",
      "epoch: 19694 loss is tensor([-0.7218], grad_fn=<AddBackward0>)\n",
      "epoch: 19695 loss is tensor([-0.7786], grad_fn=<AddBackward0>)\n",
      "epoch: 19696 loss is tensor([-0.7433], grad_fn=<AddBackward0>)\n",
      "epoch: 19697 loss is tensor([-0.7234], grad_fn=<AddBackward0>)\n",
      "epoch: 19698 loss is tensor([-0.7176], grad_fn=<AddBackward0>)\n",
      "epoch: 19699 loss is tensor([-0.7093], grad_fn=<AddBackward0>)\n",
      "epoch: 19700 loss is tensor([-0.7445], grad_fn=<AddBackward0>)\n",
      "29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19701 loss is tensor([-0.7959], grad_fn=<AddBackward0>)\n",
      "epoch: 19702 loss is tensor([-0.7677], grad_fn=<AddBackward0>)\n",
      "epoch: 19703 loss is tensor([-0.7025], grad_fn=<AddBackward0>)\n",
      "epoch: 19704 loss is tensor([-0.7404], grad_fn=<AddBackward0>)\n",
      "epoch: 19705 loss is tensor([-0.7676], grad_fn=<AddBackward0>)\n",
      "epoch: 19706 loss is tensor([-0.7210], grad_fn=<AddBackward0>)\n",
      "epoch: 19707 loss is tensor([-0.8118], grad_fn=<AddBackward0>)\n",
      "epoch: 19708 loss is tensor([-0.7825], grad_fn=<AddBackward0>)\n",
      "epoch: 19709 loss is tensor([-0.7173], grad_fn=<AddBackward0>)\n",
      "epoch: 19710 loss is tensor([-0.7923], grad_fn=<AddBackward0>)\n",
      "epoch: 19711 loss is tensor([-0.7456], grad_fn=<AddBackward0>)\n",
      "epoch: 19712 loss is tensor([-0.7688], grad_fn=<AddBackward0>)\n",
      "epoch: 19713 loss is tensor([-0.7649], grad_fn=<AddBackward0>)\n",
      "epoch: 19714 loss is tensor([-0.8269], grad_fn=<AddBackward0>)\n",
      "epoch: 19715 loss is tensor([-0.7508], grad_fn=<AddBackward0>)\n",
      "epoch: 19716 loss is tensor([-0.7940], grad_fn=<AddBackward0>)\n",
      "epoch: 19717 loss is tensor([-0.7469], grad_fn=<AddBackward0>)\n",
      "epoch: 19718 loss is tensor([-0.7664], grad_fn=<AddBackward0>)\n",
      "epoch: 19719 loss is tensor([-0.7703], grad_fn=<AddBackward0>)\n",
      "epoch: 19720 loss is tensor([-0.8148], grad_fn=<AddBackward0>)\n",
      "epoch: 19721 loss is tensor([-0.7072], grad_fn=<AddBackward0>)\n",
      "epoch: 19722 loss is tensor([-0.7717], grad_fn=<AddBackward0>)\n",
      "epoch: 19723 loss is tensor([-0.7781], grad_fn=<AddBackward0>)\n",
      "epoch: 19724 loss is tensor([-0.7516], grad_fn=<AddBackward0>)\n",
      "epoch: 19725 loss is tensor([-0.7195], grad_fn=<AddBackward0>)\n",
      "epoch: 19726 loss is tensor([-0.7589], grad_fn=<AddBackward0>)\n",
      "epoch: 19727 loss is tensor([-0.7483], grad_fn=<AddBackward0>)\n",
      "epoch: 19728 loss is tensor([-0.7234], grad_fn=<AddBackward0>)\n",
      "epoch: 19729 loss is tensor([-0.7758], grad_fn=<AddBackward0>)\n",
      "epoch: 19730 loss is tensor([-0.7612], grad_fn=<AddBackward0>)\n",
      "epoch: 19731 loss is tensor([-0.6960], grad_fn=<AddBackward0>)\n",
      "epoch: 19732 loss is tensor([-0.7613], grad_fn=<AddBackward0>)\n",
      "epoch: 19733 loss is tensor([-0.6993], grad_fn=<AddBackward0>)\n",
      "epoch: 19734 loss is tensor([-0.7167], grad_fn=<AddBackward0>)\n",
      "epoch: 19735 loss is tensor([-0.7831], grad_fn=<AddBackward0>)\n",
      "epoch: 19736 loss is tensor([-0.7143], grad_fn=<AddBackward0>)\n",
      "epoch: 19737 loss is tensor([-0.7501], grad_fn=<AddBackward0>)\n",
      "epoch: 19738 loss is tensor([-0.6679], grad_fn=<AddBackward0>)\n",
      "epoch: 19739 loss is tensor([-0.7580], grad_fn=<AddBackward0>)\n",
      "epoch: 19740 loss is tensor([-0.7361], grad_fn=<AddBackward0>)\n",
      "epoch: 19741 loss is tensor([-0.7935], grad_fn=<AddBackward0>)\n",
      "epoch: 19742 loss is tensor([-0.7304], grad_fn=<AddBackward0>)\n",
      "epoch: 19743 loss is tensor([-0.7393], grad_fn=<AddBackward0>)\n",
      "epoch: 19744 loss is tensor([-0.7474], grad_fn=<AddBackward0>)\n",
      "epoch: 19745 loss is tensor([-0.7514], grad_fn=<AddBackward0>)\n",
      "epoch: 19746 loss is tensor([-0.7227], grad_fn=<AddBackward0>)\n",
      "epoch: 19747 loss is tensor([-0.7085], grad_fn=<AddBackward0>)\n",
      "epoch: 19748 loss is tensor([-0.7555], grad_fn=<AddBackward0>)\n",
      "epoch: 19749 loss is tensor([-0.7034], grad_fn=<AddBackward0>)\n",
      "epoch: 19750 loss is tensor([-0.7218], grad_fn=<AddBackward0>)\n",
      "epoch: 19751 loss is tensor([-0.7151], grad_fn=<AddBackward0>)\n",
      "epoch: 19752 loss is tensor([-0.7625], grad_fn=<AddBackward0>)\n",
      "epoch: 19753 loss is tensor([-0.7123], grad_fn=<AddBackward0>)\n",
      "epoch: 19754 loss is tensor([-0.7158], grad_fn=<AddBackward0>)\n",
      "epoch: 19755 loss is tensor([-0.7160], grad_fn=<AddBackward0>)\n",
      "epoch: 19756 loss is tensor([-0.6685], grad_fn=<AddBackward0>)\n",
      "epoch: 19757 loss is tensor([-0.7915], grad_fn=<AddBackward0>)\n",
      "epoch: 19758 loss is tensor([-0.7253], grad_fn=<AddBackward0>)\n",
      "epoch: 19759 loss is tensor([-0.7530], grad_fn=<AddBackward0>)\n",
      "epoch: 19760 loss is tensor([-0.7581], grad_fn=<AddBackward0>)\n",
      "epoch: 19761 loss is tensor([-0.6776], grad_fn=<AddBackward0>)\n",
      "epoch: 19762 loss is tensor([-0.7138], grad_fn=<AddBackward0>)\n",
      "epoch: 19763 loss is tensor([-0.7102], grad_fn=<AddBackward0>)\n",
      "epoch: 19764 loss is tensor([-0.6723], grad_fn=<AddBackward0>)\n",
      "epoch: 19765 loss is tensor([-0.7520], grad_fn=<AddBackward0>)\n",
      "epoch: 19766 loss is tensor([-0.7563], grad_fn=<AddBackward0>)\n",
      "epoch: 19767 loss is tensor([-0.7246], grad_fn=<AddBackward0>)\n",
      "epoch: 19768 loss is tensor([-0.7191], grad_fn=<AddBackward0>)\n",
      "epoch: 19769 loss is tensor([-0.7323], grad_fn=<AddBackward0>)\n",
      "epoch: 19770 loss is tensor([-0.6815], grad_fn=<AddBackward0>)\n",
      "epoch: 19771 loss is tensor([-0.6925], grad_fn=<AddBackward0>)\n",
      "epoch: 19772 loss is tensor([-0.6611], grad_fn=<AddBackward0>)\n",
      "epoch: 19773 loss is tensor([-0.6952], grad_fn=<AddBackward0>)\n",
      "epoch: 19774 loss is tensor([-0.6788], grad_fn=<AddBackward0>)\n",
      "epoch: 19775 loss is tensor([-0.7221], grad_fn=<AddBackward0>)\n",
      "epoch: 19776 loss is tensor([-0.7108], grad_fn=<AddBackward0>)\n",
      "epoch: 19777 loss is tensor([-0.7087], grad_fn=<AddBackward0>)\n",
      "epoch: 19778 loss is tensor([-0.7331], grad_fn=<AddBackward0>)\n",
      "epoch: 19779 loss is tensor([-0.7566], grad_fn=<AddBackward0>)\n",
      "epoch: 19780 loss is tensor([-0.6749], grad_fn=<AddBackward0>)\n",
      "epoch: 19781 loss is tensor([-0.7025], grad_fn=<AddBackward0>)\n",
      "epoch: 19782 loss is tensor([-0.7111], grad_fn=<AddBackward0>)\n",
      "epoch: 19783 loss is tensor([-0.7439], grad_fn=<AddBackward0>)\n",
      "epoch: 19784 loss is tensor([-0.6774], grad_fn=<AddBackward0>)\n",
      "epoch: 19785 loss is tensor([-0.7562], grad_fn=<AddBackward0>)\n",
      "epoch: 19786 loss is tensor([-0.7595], grad_fn=<AddBackward0>)\n",
      "epoch: 19787 loss is tensor([-0.7201], grad_fn=<AddBackward0>)\n",
      "epoch: 19788 loss is tensor([-0.7250], grad_fn=<AddBackward0>)\n",
      "epoch: 19789 loss is tensor([-0.7120], grad_fn=<AddBackward0>)\n",
      "epoch: 19790 loss is tensor([-0.7615], grad_fn=<AddBackward0>)\n",
      "epoch: 19791 loss is tensor([-0.7444], grad_fn=<AddBackward0>)\n",
      "epoch: 19792 loss is tensor([-0.7261], grad_fn=<AddBackward0>)\n",
      "epoch: 19793 loss is tensor([-0.6977], grad_fn=<AddBackward0>)\n",
      "epoch: 19794 loss is tensor([-0.6788], grad_fn=<AddBackward0>)\n",
      "epoch: 19795 loss is tensor([-0.6710], grad_fn=<AddBackward0>)\n",
      "epoch: 19796 loss is tensor([-0.7382], grad_fn=<AddBackward0>)\n",
      "epoch: 19797 loss is tensor([-0.6770], grad_fn=<AddBackward0>)\n",
      "epoch: 19798 loss is tensor([-0.7280], grad_fn=<AddBackward0>)\n",
      "epoch: 19799 loss is tensor([-0.7027], grad_fn=<AddBackward0>)\n",
      "epoch: 19800 loss is tensor([-0.7090], grad_fn=<AddBackward0>)\n",
      "31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19801 loss is tensor([-0.6163], grad_fn=<AddBackward0>)\n",
      "epoch: 19802 loss is tensor([-0.6843], grad_fn=<AddBackward0>)\n",
      "epoch: 19803 loss is tensor([-0.7289], grad_fn=<AddBackward0>)\n",
      "epoch: 19804 loss is tensor([-0.7164], grad_fn=<AddBackward0>)\n",
      "epoch: 19805 loss is tensor([-0.7464], grad_fn=<AddBackward0>)\n",
      "epoch: 19806 loss is tensor([-0.7491], grad_fn=<AddBackward0>)\n",
      "epoch: 19807 loss is tensor([-0.6880], grad_fn=<AddBackward0>)\n",
      "epoch: 19808 loss is tensor([-0.7178], grad_fn=<AddBackward0>)\n",
      "epoch: 19809 loss is tensor([-0.7048], grad_fn=<AddBackward0>)\n",
      "epoch: 19810 loss is tensor([-0.7076], grad_fn=<AddBackward0>)\n",
      "epoch: 19811 loss is tensor([-0.6945], grad_fn=<AddBackward0>)\n",
      "epoch: 19812 loss is tensor([-0.7606], grad_fn=<AddBackward0>)\n",
      "epoch: 19813 loss is tensor([-0.7090], grad_fn=<AddBackward0>)\n",
      "epoch: 19814 loss is tensor([-0.7131], grad_fn=<AddBackward0>)\n",
      "epoch: 19815 loss is tensor([-0.7672], grad_fn=<AddBackward0>)\n",
      "epoch: 19816 loss is tensor([-0.7270], grad_fn=<AddBackward0>)\n",
      "epoch: 19817 loss is tensor([-0.7208], grad_fn=<AddBackward0>)\n",
      "epoch: 19818 loss is tensor([-0.7545], grad_fn=<AddBackward0>)\n",
      "epoch: 19819 loss is tensor([-0.7458], grad_fn=<AddBackward0>)\n",
      "epoch: 19820 loss is tensor([-0.7314], grad_fn=<AddBackward0>)\n",
      "epoch: 19821 loss is tensor([-0.6841], grad_fn=<AddBackward0>)\n",
      "epoch: 19822 loss is tensor([-0.6917], grad_fn=<AddBackward0>)\n",
      "epoch: 19823 loss is tensor([-0.7394], grad_fn=<AddBackward0>)\n",
      "epoch: 19824 loss is tensor([-0.7219], grad_fn=<AddBackward0>)\n",
      "epoch: 19825 loss is tensor([-0.7251], grad_fn=<AddBackward0>)\n",
      "epoch: 19826 loss is tensor([-0.6985], grad_fn=<AddBackward0>)\n",
      "epoch: 19827 loss is tensor([-0.7098], grad_fn=<AddBackward0>)\n",
      "epoch: 19828 loss is tensor([-0.7004], grad_fn=<AddBackward0>)\n",
      "epoch: 19829 loss is tensor([-0.7347], grad_fn=<AddBackward0>)\n",
      "epoch: 19830 loss is tensor([-0.7685], grad_fn=<AddBackward0>)\n",
      "epoch: 19831 loss is tensor([-0.7213], grad_fn=<AddBackward0>)\n",
      "epoch: 19832 loss is tensor([-0.7977], grad_fn=<AddBackward0>)\n",
      "epoch: 19833 loss is tensor([-0.7830], grad_fn=<AddBackward0>)\n",
      "epoch: 19834 loss is tensor([-0.7475], grad_fn=<AddBackward0>)\n",
      "epoch: 19835 loss is tensor([-0.7195], grad_fn=<AddBackward0>)\n",
      "epoch: 19836 loss is tensor([-0.7212], grad_fn=<AddBackward0>)\n",
      "epoch: 19837 loss is tensor([-0.7595], grad_fn=<AddBackward0>)\n",
      "epoch: 19838 loss is tensor([-0.7756], grad_fn=<AddBackward0>)\n",
      "epoch: 19839 loss is tensor([-0.7442], grad_fn=<AddBackward0>)\n",
      "epoch: 19840 loss is tensor([-0.6597], grad_fn=<AddBackward0>)\n",
      "epoch: 19841 loss is tensor([-0.7596], grad_fn=<AddBackward0>)\n",
      "epoch: 19842 loss is tensor([-0.7198], grad_fn=<AddBackward0>)\n",
      "epoch: 19843 loss is tensor([-0.7142], grad_fn=<AddBackward0>)\n",
      "epoch: 19844 loss is tensor([-0.7613], grad_fn=<AddBackward0>)\n",
      "epoch: 19845 loss is tensor([-0.7416], grad_fn=<AddBackward0>)\n",
      "epoch: 19846 loss is tensor([-0.7869], grad_fn=<AddBackward0>)\n",
      "epoch: 19847 loss is tensor([-0.7140], grad_fn=<AddBackward0>)\n",
      "epoch: 19848 loss is tensor([-0.7533], grad_fn=<AddBackward0>)\n",
      "epoch: 19849 loss is tensor([-0.7418], grad_fn=<AddBackward0>)\n",
      "epoch: 19850 loss is tensor([-0.7500], grad_fn=<AddBackward0>)\n",
      "epoch: 19851 loss is tensor([-0.6547], grad_fn=<AddBackward0>)\n",
      "epoch: 19852 loss is tensor([-0.7002], grad_fn=<AddBackward0>)\n",
      "epoch: 19853 loss is tensor([-0.7684], grad_fn=<AddBackward0>)\n",
      "epoch: 19854 loss is tensor([-0.7990], grad_fn=<AddBackward0>)\n",
      "epoch: 19855 loss is tensor([-0.7404], grad_fn=<AddBackward0>)\n",
      "epoch: 19856 loss is tensor([-0.7936], grad_fn=<AddBackward0>)\n",
      "epoch: 19857 loss is tensor([-0.7523], grad_fn=<AddBackward0>)\n",
      "epoch: 19858 loss is tensor([-0.7118], grad_fn=<AddBackward0>)\n",
      "epoch: 19859 loss is tensor([-0.7676], grad_fn=<AddBackward0>)\n",
      "epoch: 19860 loss is tensor([-0.7531], grad_fn=<AddBackward0>)\n",
      "epoch: 19861 loss is tensor([-0.7483], grad_fn=<AddBackward0>)\n",
      "epoch: 19862 loss is tensor([-0.7430], grad_fn=<AddBackward0>)\n",
      "epoch: 19863 loss is tensor([-0.7270], grad_fn=<AddBackward0>)\n",
      "epoch: 19864 loss is tensor([-0.7171], grad_fn=<AddBackward0>)\n",
      "epoch: 19865 loss is tensor([-0.7601], grad_fn=<AddBackward0>)\n",
      "epoch: 19866 loss is tensor([-0.7476], grad_fn=<AddBackward0>)\n",
      "epoch: 19867 loss is tensor([-0.6897], grad_fn=<AddBackward0>)\n",
      "epoch: 19868 loss is tensor([-0.6913], grad_fn=<AddBackward0>)\n",
      "epoch: 19869 loss is tensor([-0.7256], grad_fn=<AddBackward0>)\n",
      "epoch: 19870 loss is tensor([-0.7282], grad_fn=<AddBackward0>)\n",
      "epoch: 19871 loss is tensor([-0.7142], grad_fn=<AddBackward0>)\n",
      "epoch: 19872 loss is tensor([-0.7265], grad_fn=<AddBackward0>)\n",
      "epoch: 19873 loss is tensor([-0.7398], grad_fn=<AddBackward0>)\n",
      "epoch: 19874 loss is tensor([-0.6852], grad_fn=<AddBackward0>)\n",
      "epoch: 19875 loss is tensor([-0.7462], grad_fn=<AddBackward0>)\n",
      "epoch: 19876 loss is tensor([-0.6625], grad_fn=<AddBackward0>)\n",
      "epoch: 19877 loss is tensor([-0.7795], grad_fn=<AddBackward0>)\n",
      "epoch: 19878 loss is tensor([-0.7430], grad_fn=<AddBackward0>)\n",
      "epoch: 19879 loss is tensor([-0.7453], grad_fn=<AddBackward0>)\n",
      "epoch: 19880 loss is tensor([-0.7193], grad_fn=<AddBackward0>)\n",
      "epoch: 19881 loss is tensor([-0.7025], grad_fn=<AddBackward0>)\n",
      "epoch: 19882 loss is tensor([-0.6997], grad_fn=<AddBackward0>)\n",
      "epoch: 19883 loss is tensor([-0.7187], grad_fn=<AddBackward0>)\n",
      "epoch: 19884 loss is tensor([-0.7016], grad_fn=<AddBackward0>)\n",
      "epoch: 19885 loss is tensor([-0.7270], grad_fn=<AddBackward0>)\n",
      "epoch: 19886 loss is tensor([-0.7025], grad_fn=<AddBackward0>)\n",
      "epoch: 19887 loss is tensor([-0.7667], grad_fn=<AddBackward0>)\n",
      "epoch: 19888 loss is tensor([-0.7065], grad_fn=<AddBackward0>)\n",
      "epoch: 19889 loss is tensor([-0.7419], grad_fn=<AddBackward0>)\n",
      "epoch: 19890 loss is tensor([-0.7259], grad_fn=<AddBackward0>)\n",
      "epoch: 19891 loss is tensor([-0.7343], grad_fn=<AddBackward0>)\n",
      "epoch: 19892 loss is tensor([-0.7338], grad_fn=<AddBackward0>)\n",
      "epoch: 19893 loss is tensor([-0.7120], grad_fn=<AddBackward0>)\n",
      "epoch: 19894 loss is tensor([-0.7726], grad_fn=<AddBackward0>)\n",
      "epoch: 19895 loss is tensor([-0.7242], grad_fn=<AddBackward0>)\n",
      "epoch: 19896 loss is tensor([-0.7233], grad_fn=<AddBackward0>)\n",
      "epoch: 19897 loss is tensor([-0.7141], grad_fn=<AddBackward0>)\n",
      "epoch: 19898 loss is tensor([-0.7737], grad_fn=<AddBackward0>)\n",
      "epoch: 19899 loss is tensor([-0.7319], grad_fn=<AddBackward0>)\n",
      "epoch: 19900 loss is tensor([-0.6721], grad_fn=<AddBackward0>)\n",
      "42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n",
      "<ipython-input-8-a46aa3dbf1ee>:67: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.encoder.parameters(), hp.grad_clip)\n",
      "<ipython-input-8-a46aa3dbf1ee>:68: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(self.decoder.parameters(), hp.grad_clip)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 19901 loss is tensor([-0.7558], grad_fn=<AddBackward0>)\n",
      "epoch: 19902 loss is tensor([-0.7449], grad_fn=<AddBackward0>)\n",
      "epoch: 19903 loss is tensor([-0.7184], grad_fn=<AddBackward0>)\n",
      "epoch: 19904 loss is tensor([-0.7188], grad_fn=<AddBackward0>)\n",
      "epoch: 19905 loss is tensor([-0.7316], grad_fn=<AddBackward0>)\n",
      "epoch: 19906 loss is tensor([-0.7389], grad_fn=<AddBackward0>)\n",
      "epoch: 19907 loss is tensor([-0.7280], grad_fn=<AddBackward0>)\n",
      "epoch: 19908 loss is tensor([-0.6752], grad_fn=<AddBackward0>)\n",
      "epoch: 19909 loss is tensor([-0.7093], grad_fn=<AddBackward0>)\n",
      "epoch: 19910 loss is tensor([-0.7759], grad_fn=<AddBackward0>)\n",
      "epoch: 19911 loss is tensor([-0.7980], grad_fn=<AddBackward0>)\n",
      "epoch: 19912 loss is tensor([-0.7534], grad_fn=<AddBackward0>)\n",
      "epoch: 19913 loss is tensor([-0.7619], grad_fn=<AddBackward0>)\n",
      "epoch: 19914 loss is tensor([-0.7472], grad_fn=<AddBackward0>)\n",
      "epoch: 19915 loss is tensor([-0.7409], grad_fn=<AddBackward0>)\n",
      "epoch: 19916 loss is tensor([-0.7433], grad_fn=<AddBackward0>)\n",
      "epoch: 19917 loss is tensor([-0.7210], grad_fn=<AddBackward0>)\n",
      "epoch: 19918 loss is tensor([-0.7372], grad_fn=<AddBackward0>)\n",
      "epoch: 19919 loss is tensor([-0.6481], grad_fn=<AddBackward0>)\n",
      "epoch: 19920 loss is tensor([-0.7290], grad_fn=<AddBackward0>)\n",
      "epoch: 19921 loss is tensor([-0.7136], grad_fn=<AddBackward0>)\n",
      "epoch: 19922 loss is tensor([-0.6880], grad_fn=<AddBackward0>)\n",
      "epoch: 19923 loss is tensor([-0.7013], grad_fn=<AddBackward0>)\n",
      "epoch: 19924 loss is tensor([-0.6844], grad_fn=<AddBackward0>)\n",
      "epoch: 19925 loss is tensor([-0.7657], grad_fn=<AddBackward0>)\n",
      "epoch: 19926 loss is tensor([-0.6585], grad_fn=<AddBackward0>)\n",
      "epoch: 19927 loss is tensor([-0.7326], grad_fn=<AddBackward0>)\n",
      "epoch: 19928 loss is tensor([-0.7929], grad_fn=<AddBackward0>)\n",
      "epoch: 19929 loss is tensor([-0.7173], grad_fn=<AddBackward0>)\n",
      "epoch: 19930 loss is tensor([-0.7209], grad_fn=<AddBackward0>)\n",
      "epoch: 19931 loss is tensor([-0.7235], grad_fn=<AddBackward0>)\n",
      "epoch: 19932 loss is tensor([-0.7077], grad_fn=<AddBackward0>)\n",
      "epoch: 19933 loss is tensor([-0.6466], grad_fn=<AddBackward0>)\n",
      "epoch: 19934 loss is tensor([-0.6896], grad_fn=<AddBackward0>)\n",
      "epoch: 19935 loss is tensor([-0.7027], grad_fn=<AddBackward0>)\n",
      "epoch: 19936 loss is tensor([-0.7268], grad_fn=<AddBackward0>)\n",
      "epoch: 19937 loss is tensor([-0.6686], grad_fn=<AddBackward0>)\n",
      "epoch: 19938 loss is tensor([-0.7423], grad_fn=<AddBackward0>)\n",
      "epoch: 19939 loss is tensor([-0.7640], grad_fn=<AddBackward0>)\n",
      "epoch: 19940 loss is tensor([-0.7107], grad_fn=<AddBackward0>)\n",
      "epoch: 19941 loss is tensor([-0.7226], grad_fn=<AddBackward0>)\n",
      "epoch: 19942 loss is tensor([-0.7265], grad_fn=<AddBackward0>)\n",
      "epoch: 19943 loss is tensor([-0.7647], grad_fn=<AddBackward0>)\n",
      "epoch: 19944 loss is tensor([-0.7548], grad_fn=<AddBackward0>)\n",
      "epoch: 19945 loss is tensor([-0.8045], grad_fn=<AddBackward0>)\n",
      "epoch: 19946 loss is tensor([-0.7731], grad_fn=<AddBackward0>)\n",
      "epoch: 19947 loss is tensor([-0.7369], grad_fn=<AddBackward0>)\n",
      "epoch: 19948 loss is tensor([-0.7616], grad_fn=<AddBackward0>)\n",
      "epoch: 19949 loss is tensor([-0.7709], grad_fn=<AddBackward0>)\n",
      "epoch: 19950 loss is tensor([-0.7434], grad_fn=<AddBackward0>)\n",
      "epoch: 19951 loss is tensor([-0.7500], grad_fn=<AddBackward0>)\n",
      "epoch: 19952 loss is tensor([-0.7331], grad_fn=<AddBackward0>)\n",
      "epoch: 19953 loss is tensor([-0.7456], grad_fn=<AddBackward0>)\n",
      "epoch: 19954 loss is tensor([-0.6779], grad_fn=<AddBackward0>)\n",
      "epoch: 19955 loss is tensor([-0.7177], grad_fn=<AddBackward0>)\n",
      "epoch: 19956 loss is tensor([-0.7361], grad_fn=<AddBackward0>)\n",
      "epoch: 19957 loss is tensor([-0.6890], grad_fn=<AddBackward0>)\n",
      "epoch: 19958 loss is tensor([-0.7730], grad_fn=<AddBackward0>)\n",
      "epoch: 19959 loss is tensor([-0.7504], grad_fn=<AddBackward0>)\n",
      "epoch: 19960 loss is tensor([-0.7026], grad_fn=<AddBackward0>)\n",
      "epoch: 19961 loss is tensor([-0.7535], grad_fn=<AddBackward0>)\n",
      "epoch: 19962 loss is tensor([-0.6829], grad_fn=<AddBackward0>)\n",
      "epoch: 19963 loss is tensor([-0.7545], grad_fn=<AddBackward0>)\n",
      "epoch: 19964 loss is tensor([-0.7195], grad_fn=<AddBackward0>)\n",
      "epoch: 19965 loss is tensor([-0.7278], grad_fn=<AddBackward0>)\n",
      "epoch: 19966 loss is tensor([-0.7120], grad_fn=<AddBackward0>)\n",
      "epoch: 19967 loss is tensor([-0.7150], grad_fn=<AddBackward0>)\n",
      "epoch: 19968 loss is tensor([-0.7338], grad_fn=<AddBackward0>)\n",
      "epoch: 19969 loss is tensor([-0.7482], grad_fn=<AddBackward0>)\n",
      "epoch: 19970 loss is tensor([-0.6561], grad_fn=<AddBackward0>)\n",
      "epoch: 19971 loss is tensor([-0.7366], grad_fn=<AddBackward0>)\n",
      "epoch: 19972 loss is tensor([-0.7026], grad_fn=<AddBackward0>)\n",
      "epoch: 19973 loss is tensor([-0.7936], grad_fn=<AddBackward0>)\n",
      "epoch: 19974 loss is tensor([-0.7600], grad_fn=<AddBackward0>)\n",
      "epoch: 19975 loss is tensor([-0.7067], grad_fn=<AddBackward0>)\n",
      "epoch: 19976 loss is tensor([-0.6809], grad_fn=<AddBackward0>)\n",
      "epoch: 19977 loss is tensor([-0.7414], grad_fn=<AddBackward0>)\n",
      "epoch: 19978 loss is tensor([-0.7298], grad_fn=<AddBackward0>)\n",
      "epoch: 19979 loss is tensor([-0.8013], grad_fn=<AddBackward0>)\n",
      "epoch: 19980 loss is tensor([-0.7148], grad_fn=<AddBackward0>)\n",
      "epoch: 19981 loss is tensor([-0.7335], grad_fn=<AddBackward0>)\n",
      "epoch: 19982 loss is tensor([-0.6763], grad_fn=<AddBackward0>)\n",
      "epoch: 19983 loss is tensor([-0.6545], grad_fn=<AddBackward0>)\n",
      "epoch: 19984 loss is tensor([-0.7943], grad_fn=<AddBackward0>)\n",
      "epoch: 19985 loss is tensor([-0.7709], grad_fn=<AddBackward0>)\n",
      "epoch: 19986 loss is tensor([-0.7204], grad_fn=<AddBackward0>)\n",
      "epoch: 19987 loss is tensor([-0.7206], grad_fn=<AddBackward0>)\n",
      "epoch: 19988 loss is tensor([-0.7152], grad_fn=<AddBackward0>)\n",
      "epoch: 19989 loss is tensor([-0.7387], grad_fn=<AddBackward0>)\n",
      "epoch: 19990 loss is tensor([-0.6915], grad_fn=<AddBackward0>)\n",
      "epoch: 19991 loss is tensor([-0.8011], grad_fn=<AddBackward0>)\n",
      "epoch: 19992 loss is tensor([-0.7524], grad_fn=<AddBackward0>)\n",
      "epoch: 19993 loss is tensor([-0.6435], grad_fn=<AddBackward0>)\n",
      "epoch: 19994 loss is tensor([-0.6918], grad_fn=<AddBackward0>)\n",
      "epoch: 19995 loss is tensor([-0.7522], grad_fn=<AddBackward0>)\n",
      "epoch: 19996 loss is tensor([-0.7762], grad_fn=<AddBackward0>)\n",
      "epoch: 19997 loss is tensor([-0.7389], grad_fn=<AddBackward0>)\n",
      "epoch: 19998 loss is tensor([-0.7385], grad_fn=<AddBackward0>)\n",
      "epoch: 19999 loss is tensor([-0.7099], grad_fn=<AddBackward0>)\n",
      "epoch: 20000 loss is tensor([-0.8025], grad_fn=<AddBackward0>)\n",
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Acer\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "<ipython-input-7-a948bd699ee2>:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  pi = F.softmax(pi.transpose(0,1).squeeze()).view(len_out,-1,hp.M)\n",
      "<ipython-input-7-a948bd699ee2>:42: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  q = F.softmax(params_pen).view(len_out,-1,3)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-c2496095fb13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m50001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-a46aa3dbf1ee>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLR\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mLKL\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;31m# gradient step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;31m# gradient cliping\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad_clip\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "for epoch in range(50001):\n",
    "    model.train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss = RL + KL + (EL * 2)')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
=======
      "The number of epochs is: 5201\n",
      "The number of epochs is: 5202\n",
      "The number of epochs is: 5203\n",
      "The number of epochs is: 5204\n",
      "The number of epochs is: 5205\n",
      "The number of epochs is: 5206\n",
      "The number of epochs is: 5207\n",
      "The number of epochs is: 5208\n",
      "The number of epochs is: 5209\n",
      "The number of epochs is: 5210\n",
      "The number of epochs is: 5211\n",
      "The number of epochs is: 5212\n",
      "The number of epochs is: 5213\n",
      "The number of epochs is: 5214\n",
      "The number of epochs is: 5215\n",
      "The number of epochs is: 5216\n",
      "The number of epochs is: 5217\n",
      "The number of epochs is: 5218\n",
      "The number of epochs is: 5219\n",
      "The number of epochs is: 5220\n",
      "The number of epochs is: 5221\n",
      "The number of epochs is: 5222\n",
      "The number of epochs is: 5223\n",
      "The number of epochs is: 5224\n",
      "The number of epochs is: 5225\n",
      "The number of epochs is: 5226\n",
      "The number of epochs is: 5227\n",
      "The number of epochs is: 5228\n",
      "The number of epochs is: 5229\n",
      "The number of epochs is: 5230\n",
      "The number of epochs is: 5231\n",
      "The number of epochs is: 5232\n",
      "The number of epochs is: 5233\n",
      "The number of epochs is: 5234\n",
      "The number of epochs is: 5235\n",
      "The number of epochs is: 5236\n",
      "The number of epochs is: 5237\n",
      "The number of epochs is: 5238\n",
      "The number of epochs is: 5239\n",
      "The number of epochs is: 5240\n",
      "The number of epochs is: 5241\n",
      "The number of epochs is: 5242\n",
      "The number of epochs is: 5243\n",
      "The number of epochs is: 5244\n",
      "The number of epochs is: 5245\n",
      "The number of epochs is: 5246\n",
      "The number of epochs is: 5247\n",
      "The number of epochs is: 5248\n",
      "The number of epochs is: 5249\n",
      "The number of epochs is: 5250\n",
      "The number of epochs is: 5251\n",
      "The number of epochs is: 5252\n",
      "The number of epochs is: 5253\n",
      "The number of epochs is: 5254\n",
      "The number of epochs is: 5255\n",
      "The number of epochs is: 5256\n",
      "The number of epochs is: 5257\n",
      "The number of epochs is: 5258\n",
      "The number of epochs is: 5259\n",
      "The number of epochs is: 5260\n",
      "The number of epochs is: 5261\n",
      "The number of epochs is: 5262\n",
      "The number of epochs is: 5263\n",
      "The number of epochs is: 5264\n",
      "The number of epochs is: 5265\n",
      "The number of epochs is: 5266\n",
      "The number of epochs is: 5267\n",
      "The number of epochs is: 5268\n",
      "The number of epochs is: 5269\n",
      "The number of epochs is: 5270\n",
      "The number of epochs is: 5271\n",
      "The number of epochs is: 5272\n",
      "The number of epochs is: 5273\n",
      "The number of epochs is: 5274\n",
      "The number of epochs is: 5275\n",
      "The number of epochs is: 5276\n",
      "The number of epochs is: 5277\n",
      "The number of epochs is: 5278\n",
      "The number of epochs is: 5279\n",
      "The number of epochs is: 5280\n",
      "The number of epochs is: 5281\n",
      "The number of epochs is: 5282\n",
      "The number of epochs is: 5283\n",
      "The number of epochs is: 5284\n",
      "The number of epochs is: 5285\n",
      "The number of epochs is: 5286\n",
      "The number of epochs is: 5287\n",
      "The number of epochs is: 5288\n",
      "The number of epochs is: 5289\n",
      "The number of epochs is: 5290\n",
      "The number of epochs is: 5291\n",
      "The number of epochs is: 5292\n",
      "The number of epochs is: 5293\n",
      "The number of epochs is: 5294\n",
      "The number of epochs is: 5295\n",
      "The number of epochs is: 5296\n",
      "The number of epochs is: 5297\n",
      "The number of epochs is: 5298\n",
      "The number of epochs is: 5299\n",
      "The number of epochs is: 5300\n",
      "The number of epochs is: 5301\n",
      "The number of epochs is: 5302\n",
      "The number of epochs is: 5303\n",
      "The number of epochs is: 5304\n",
      "The number of epochs is: 5305\n",
      "The number of epochs is: 5306\n",
      "The number of epochs is: 5307\n",
      "The number of epochs is: 5308\n",
      "The number of epochs is: 5309\n",
      "The number of epochs is: 5310\n",
      "The number of epochs is: 5311\n",
      "The number of epochs is: 5312\n",
      "The number of epochs is: 5313\n",
      "The number of epochs is: 5314\n",
      "The number of epochs is: 5315\n",
      "The number of epochs is: 5316\n",
      "The number of epochs is: 5317\n",
      "The number of epochs is: 5318\n",
      "The number of epochs is: 5319\n",
      "The number of epochs is: 5320\n",
      "The number of epochs is: 5321\n",
      "The number of epochs is: 5322\n",
      "The number of epochs is: 5323\n",
      "The number of epochs is: 5324\n",
      "The number of epochs is: 5325\n",
      "The number of epochs is: 5326\n",
      "The number of epochs is: 5327\n",
      "The number of epochs is: 5328\n",
      "The number of epochs is: 5329\n",
      "The number of epochs is: 5330\n",
      "The number of epochs is: 5331\n",
      "The number of epochs is: 5332\n",
      "The number of epochs is: 5333\n",
      "The number of epochs is: 5334\n",
      "The number of epochs is: 5335\n",
      "The number of epochs is: 5336\n",
      "The number of epochs is: 5337\n",
      "The number of epochs is: 5338\n",
      "The number of epochs is: 5339\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c2496095fb13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-076318d80f09>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# encode:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;31m# create start of sequence:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f02dc4bb6272>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, batch_size, hidden_cell)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_hidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mhidden_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;31m# hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mhidden_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_backward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/env_pytorch/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 582\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
>>>>>>> master
     "output_type": "display_data"
    }
   ],
   "source": [
<<<<<<< HEAD
    "import matplotlib.pyplot as plt\n",
    "data = [2.2534, 1.0040, 0.7070, 0.6366, 0.5908, 0.4630, 0.4564, 0.4125, 0.4295, 0.2923, 0.3610, 0.2441, 0.2588, 0.1516, 0.2477, 0.1903, 0.1031, 0.1108, 0.1995, 0.1262, 0.0893, 0.1005, 0.0676, 0.0346, 0.0413, 0.0377, 0.0849, 0.0423, -0.0251, -0.0289, -0.0326] \n",
    "plt.plot(data)\n",
    "plt.title('Loss of Sketch-to-Sketch Chair model')\n",
    "plt.xlabel('Epochs * 100')\n",
    "plt.ylabel('Loss = RL + KL + (EL * 2)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
    "model = Model()\n",
    "for epoch in range(50001):\n",
    "    model.train(epoch)"
   ]
>>>>>>> master
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
