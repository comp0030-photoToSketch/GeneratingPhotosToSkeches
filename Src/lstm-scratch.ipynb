{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e28b4f9399e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install torch'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# check the system we're on, gpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "np.load.__defaults__=(None, True, True, 'ASCII')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParameters():\n",
    "    def __init__(self):\n",
    "        self.data = '../Datasets/sketchrnn_chair.npz'\n",
    "        self.encoder_hidden_size = 64\n",
    "        self.decoder_hidden_size = 128\n",
    "        # latent vector\n",
    "        self.Nz = 32\n",
    "        \n",
    "        #return\n",
    "        self.M = 20\n",
    "        self.dropout = 0.9\n",
    "        self.batch_size = 100\n",
    "        self.eta_min = 0.01\n",
    "        self.R = 0.99995\n",
    "        self.KL_min = 0.2\n",
    "        self.wKL = 0.5\n",
    "        self.lr = 0.001\n",
    "        self.lr_decay = 0.9999\n",
    "        self.min_lr = 0.00001\n",
    "        self.grad_clip = 1.\n",
    "        self.temperature = 0.4\n",
    "        self.max_seq_length = 200\n",
    "\n",
    "hp = HyperParameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, file):\n",
    "        self.file = file\n",
    "        \n",
    "    def load_npz(self):\n",
    "        data = np.load(file=self.file, encoding='latin1', allow_pickle=True)\n",
    "        return data\n",
    "\n",
    "dataLoader = DataLoader(hp.data)\n",
    "data = dataLoader.load_npz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataHandler:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def padding(self, strokedata, max_len):\n",
    "        s = torch.from_numpy(strokedata)\n",
    "        s_len = s.shape[0]\n",
    "        result = torch.zeros((max_len, 5))\n",
    "        result[0:s_len, 0:2] = s[:, 0:2]\n",
    "        result[0:s_len, 3] = s[:, 2]\n",
    "        result[0:s_len, 2] = 1 - result[0:s_len, 3]    # 1 to 0, 0 to 1\n",
    "        if s_len < max_len:\n",
    "            result[(s_len - 1):, 4] = 1\n",
    "        return result\n",
    "        \n",
    "    def handle(self):\n",
    "        max_len = 0    # max length of a sketch - max number of stroke vectors in a sketch\n",
    "        for datatype in self.data:\n",
    "            for strokedata in self.data[datatype]:\n",
    "                if strokedata.shape[0] > max_len:\n",
    "                    max_len = strokedata.shape[0]\n",
    "        # lengths of data - the number of sketchs in data\n",
    "        train_len = self.data['train'].shape[0]\n",
    "        test_len = self.data['test'].shape[0]\n",
    "        valid_len = self.data['valid'].shape[0]\n",
    "        \n",
    "        data_train = torch.zeros((train_len, max_len, 5), dtype=float)\n",
    "        data_test = torch.zeros((test_len, max_len, 5), dtype=float)\n",
    "        data_valid = torch.zeros((valid_len, max_len, 5), dtype=float)\n",
    "        \n",
    "        i = j = k = 0\n",
    "        for datatype in self.data:\n",
    "            for strokedata in self.data[datatype]:\n",
    "                if datatype == 'train':\n",
    "                    data_train[i] = self.padding(strokedata, max_len)\n",
    "                    i += 1\n",
    "                if datatype == 'test':\n",
    "                    data_test[j] = self.padding(strokedata, max_len)\n",
    "                    j += 1\n",
    "                if datatype == 'valid':\n",
    "                    data_valid[k] = self.padding(strokedata, max_len)\n",
    "                    k += 1 \n",
    "\n",
    "        return data_train, data_test, data_valid\n",
    "    \n",
    "        \n",
    "datahandler = DataHandler(data)\n",
    "data_train, data_test, data_valid = datahandler.handle()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # bidirectional lstm:\n",
    "#         self.lstm = nn.LSTM(5, hp.encoder_hidden_size, bidirectional=True)\n",
    "#         # create mu and sigma from lstm's last output:\n",
    "#         self.fc_mu = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "#         self.fc_sigma = nn.Linear(2*hp.encoder_hidden_size, hp.Nz)\n",
    "#         # active dropout:\n",
    "#         # self.train()\n",
    "\n",
    "#     def forward(self, inputs, batch_size, hidden_cell=None):\n",
    "#         if hidden_cell is None:\n",
    "#             # then must init with zeros\n",
    "#             if use_cuda:\n",
    "#                 hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "#                 cell = torch.zeros(2, batch_size, hp.encoder_hidden_size).cuda()\n",
    "#             else:\n",
    "#                 hidden = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "#                 cell = torch.zeros(2, batch_size, hp.encoder_hidden_size)\n",
    "#             hidden_cell = (hidden, cell)\n",
    "#         _, (hidden,cell) = self.lstm(inputs.float(), hidden_cell)\n",
    "#         # hidden is (2, batch_size, hidden_size), we want (batch_size, 2*hidden_size):\n",
    "#         hidden_forward, hidden_backward = torch.split(hidden,1,0)\n",
    "#         hidden_cat = torch.cat([hidden_forward.squeeze(0), hidden_backward.squeeze(0)],1)\n",
    "#         # mu and sigma:\n",
    "#         mu = self.fc_mu(hidden_cat)\n",
    "#         sigma_hat = self.fc_sigma(hidden_cat)\n",
    "#         sigma = torch.exp(sigma_hat/2.)\n",
    "#         # N ~ N(0,1)\n",
    "#         z_size = mu.size()\n",
    "                                   \n",
    "#         if use_cuda:\n",
    "#             N = torch.normal(torch.zeros(z_size),torch.ones(z_size)).cuda()\n",
    "#         else:\n",
    "#             N = torch.normal(torch.zeros(z_size),torch.ones(z_size))\n",
    "#         z = mu + sigma*N\n",
    "#         # mu and sigma_hat are needed for LKL loss\n",
    "#         return z, mu, sigma_hat\n",
    "    \n",
    "# encoder = Encoder()\n",
    "# encoder.train()\n",
    "# z, _, _ = encoder(data_train, hp.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = torch.rand((3,1,5))\n",
    "b = torch.tensor([[1,2], \n",
    "                  [3,4]])\n",
    "c = torch.tensor([[5,6],\n",
    "                  [7,8]])\n",
    "d = torch.stack([b.unsqueeze(0),c.unsqueeze(0)], 0)\n",
    "e = (b.unsqueeze(0).contiguous(),c.unsqueeze(0).contiguous())\n",
    "print(d.is_contiguous())\n",
    "print(d)\n",
    "print(e)\n",
    "# print(a.squeeze(0).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(hp.Nz + 5, hp.decoder_hidden_size)\n",
    "        self.fc_hc = nn.Linear(hp.Nz, 2*hp.decoder_hidden_size)\n",
    "        \n",
    "    def forward(self, inputs, z, hidden_cell=None):\n",
    "        if hidden_cell is None:       \n",
    "            hidden, cell = torch.split(F.tanh(self.fc_hc(z)), hp.decoder_hidden_size, 1)\n",
    "            hidden_cell = (hidden.unsqueeze(0), cell.unsqueeze(0))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This for after training, the paper says we can sample sketches for the model.\n",
    "During the sampling process we generate parameters for both GMM and categorical\n",
    "distributions at each time step, and sample an outcome S'i for that time step.\n",
    "\n",
    "GMM ->\n",
    "Categorical distributions -> calculated using outputs as logit values\n",
    "\n",
    "We continue to sample until p3, so the end of sketch, is 1 or the number of iterations\n",
    "has reached Nmax (length of longest sketch in training dataset). \n",
    "Sampling the output is not deterministic, it's a random sequence,\n",
    "conditioned on the input latent vector z. We can control the level of randomness we'd like\n",
    "our samples to have during the sampling process by using a temperature parameter.\n",
    "\n",
    "- state_q_hat tends to state_q_hat / temperature\n",
    "- prime_k tends to prime_k / temperature\n",
    "- sigma_squared_x tends to sigma_squared_temperature_x\n",
    "- sigma_squared_y tends to sigma_squared_temperature_y\n",
    "\n",
    "- the softmax parameters can be scaled, of the categorical distribution and also sigma\n",
    "parameters of the bivariate normal distribution by a temperature parameter temperature.\n",
    "To control the level of randomness in our samples.\n",
    "- temperature is usually between 0 and 1, so limit T -> 0, model becomes deterministic and\n",
    "samples will consist of the most likely point in the probability density function.\n",
    "\n",
    "- Unconditinal generation uses this temperature methodology to control randomness of the\n",
    "distribution\n",
    "\n",
    "Unconditional generation -> train model to generate sketches unconditionally where train\n",
    "decoder RNN module\n",
    "-> Initial hidden states and cell states of decoder RNN are initialized to zero, inputs xi\n",
    "of decoder RNN at each step is only Si-1 or Si-1', don't need to concatenate latent vectorZ\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is part of the model\n",
    "\n",
    "def tweak_temperature(self, pi_pdf):\n",
    "    # sigma x = exp(sigma_hat), sigma_hat = log(Sigma x)\n",
    "    pi_pdf = np.log(pi_pdf)/hp.temperature\n",
    "    pi_pdf -= pi_pdf.max()\n",
    "    pi_pdf = np.exp(pi_pdf)\n",
    "    pi_pdf /= pi_pdf.sum()\n",
    "    return pi_pdf\n",
    "\n",
    "def sample_sketch_state(self):\n",
    "    # Get GMM index for mixture distribution\n",
    "    pi = self.pi.data[0,0,:].numpy()\n",
    "    pi = tweak_temperature(pi)\n",
    "    pi_idx = np.random.choice(hp.M, p=pi)\n",
    "    \n",
    "    # Get the pen state, or sketch state\n",
    "    pi = self.q.data[0,0,:].numpy()\n",
    "    q = tweak_temperature(q)\n",
    "    q_idx = np.random.choice(3, p=q)\n",
    "    \n",
    "    # Get mixture parameters\n",
    "    mu_x = self.mu_x.data[0,0,pi_idx]\n",
    "    mu_y = self.mu_y.data[0,0,pi_idx]\n",
    "    sigma_x = self.sigma_x.data[0,0,pi_idx]\n",
    "    sigma_y = self.sigma_y.data[0,0,pi_idx]\n",
    "    rho_xy = self.rho_xy.data[0,0,pi_idx]\n",
    "    x,y = sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy,greedy=False)\n",
    "    next_state = torch.zeros(5)\n",
    "    next_state[0] = x\n",
    "    next_state[1] = y\n",
    "    next_state[q_idx+2] = 1\n",
    "    return Variable(next_state).view(1,1,-1),x,y,q_idx==1,q_idx==2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        if use_cuda:\n",
    "            self.encoder = EncoderRNN().cuda()\n",
    "            self.decoder = DecoderRNN().cuda()\n",
    "        else:\n",
    "            self.encoder = EncoderRNN()\n",
    "            self.decoder = DecoderRNN()\n",
    "        #self.encoder_optimizer = optim.Adam(self.encoder.parameters(), hp.lr)\n",
    "        #self.decoder_optimizer = optim.Adam(self.decoder.parameters(), hp.lr)\n",
    "        #self.eta_step = hp.eta_min\n",
    "    \n",
    "# take the parameters to form the normal distribution and sample from it\n",
    "    def sample_bivariate_normal(mu_x,mu_y,sigma_x,sigma_y,rho_xy, greedy=False):\n",
    "        if greedy:\n",
    "            return mu_x, mu_y\n",
    "        mean = [mu_x,mu_y]\n",
    "        # https://mathworld.wolfram.com/BivariateNormalDistribution.html\n",
    "    # to read more about bivariate normal and it's parameters\n",
    "        sigma_x *= np.sqrt(hp.temperature)\n",
    "        sigma_y *= np.sqrt(hp.temperature)\n",
    "        covariance = [[sigma_x * sigma_x, rho_xy * sigma_x * sigma_y],[rho_xy * sigma_x * sigma_y, sigma_y * sigma_y]]\n",
    "        x = np.random.multivariate_normal(mean, covariance, 1)\n",
    "        return x[0][0], x[0][1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will sketch the image out using different colours for the strokes\n",
    "def draw_sketch(sequence, epoch, name='_output_'):\n",
    "    strokes = np.split(sequence, np.where(sequence[:,2]>0)[0]+1)\n",
    "    fig = plt.figure()\n",
    "    axis_1 = fig.add_subplot(1,1,1)\n",
    "    \n",
    "    for stroke in strokes:\n",
    "        plt.plot(stoke[:,0],-stroke[:,1], color=numpy.random.rand(3,))\n",
    "    \n",
    "    canvas = plt.get_current_fig_manager().canvas\n",
    "    canvas.draw()\n",
    "    \n",
    "    pil_sketch = PIL.Image.frombytes('RGB', canvas.get_width_height(), canvas.tostring_rgb())\n",
    "    name = 'sketch_after_'+str(epoch)+'epochs'+name+'.jpg'\n",
    "    pil_image.save(name,\"JPEG\")\n",
    "    plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def reconstruction_loss(self, mask, dx, dy, p, epoch):\n",
    "        # compute reconstruction loss (difference between the origin image and the reconstructed image):\n",
    "        pdf = self.bivariate_normal_pdf(dx, dy)\n",
    "        # loss for x and y\n",
    "        LS = -torch.sum(mask*torch.log(1e-5+torch.sum(self.pi * pdf, 2)))\\\n",
    "            /float(Nmax*hp.batch_size)\n",
    "        # loss for pen states\n",
    "        LP = -torch.sum(p*torch.log(self.q))/float(Nmax*hp.batch_size)\n",
    "        return LS+LP\n",
    "\n",
    "    def kullback_leibler_loss(self):\n",
    "        # compute kullbackleibler loss (difference between the distribution of latenet vector and N(0,1)):\n",
    "        LKL = -0.5*torch.sum(1+self.sigma-self.mu**2-torch.exp(self.sigma))\\\n",
    "            /float(hp.Nz*hp.batch_size)\n",
    "        if use_cuda:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min]).cuda()).detach()\n",
    "        else:\n",
    "            KL_min = Variable(torch.Tensor([hp.KL_min])).detach()\n",
    "        return hp.wKL*self.eta_step * torch.max(LKL,KL_min)\n",
    "    \n",
    "    def train_temp(self, epoch):\n",
    "        # compute losses:\n",
    "        LKL = self.kullback_leibler_loss()\n",
    "        LR = self.reconstruction_loss(mask,dx,dy,p,epoch)\n",
    "        loss = LR + LKL\n",
    "        # gradient step\n",
    "        loss.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
